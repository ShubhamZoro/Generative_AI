{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb801c9-e33a-4a41-bdb8-9cacb382535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a8a43d-530e-4031-b42f-5b6bd09af34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfffcbf-d6e3-4e63-85dc-02fb916cee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sset up enviornment\n",
    "\n",
    "load_dotenv()\n",
    "openai=OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048e5e7c-dd7a-469e-9ed5-0c6f75fb0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d989ab-d1e2-4b93-9893-87c40ccde3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt=\"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a02948-86cb-4adc-9d88-977e7ed99c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6819c2cd-80e8-4cba-8472-b5a5729d2530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Let's break down the provided code snippet step by step:\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "### Explanation:\n",
       "\n",
       "1. **Context of `yield`:**\n",
       "   - The keyword `yield` is used in Python to create a generator. A generator is a special type of iterator that allows you to iterate over a sequence of values without storing them all in memory at once. When the `yield` statement is encountered, the function’s state is saved until the next value is requested.\n",
       "\n",
       "2. **`yield from`:**\n",
       "   - `yield from` is a way to delegate part of a generator's output to another generator. It allows you to yield all values from a sub-generator or iterable. In this case, it is yielding values from a set comprehension.\n",
       "\n",
       "3. **Set Comprehension:**\n",
       "   - The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension. Set comprehensions create a set, which is an unordered collection of unique elements.\n",
       "   - Here, `book.get(\"author\")` is called for each `book` in the iterable `books`. The `get` method is used on dictionaries (which we assume `book` is, based on the context) to retrieve the value associated with the key `\"author\"`. If the key does not exist, `get` returns `None` by default.\n",
       "\n",
       "4. **Iteration and Filtering:**\n",
       "   - The comprehension iterates through each `book` in the `books` list and includes the author's name in the set **only** if that author's name exists (i.e., it’s not `None` or an empty value). This is achieved by the conditional clause `if book.get(\"author\")`.\n",
       "   - This effectively filters out any books that do not have an associated author.\n",
       "\n",
       "5. **Resulting Set:**\n",
       "   - As a result, the set comprehension produces a set of unique author names from the books, excluding any entries where the author information is missing.\n",
       "\n",
       "### Practical Example:\n",
       "\n",
       "Suppose you have the following list of book dictionaries:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book A\", \"author\": \"Author 1\"},\n",
       "    {\"title\": \"Book B\", \"author\": \"Author 2\"},\n",
       "    {\"title\": \"Book C\"},  # No author\n",
       "    {\"title\": \"Book D\", \"author\": \"Author 1\"}  # Duplicate author\n",
       "]\n",
       "\n",
       "\n",
       "When executing the code snippet, the following happens:\n",
       "- The generator expression will only add `\"Author 1\"` and `\"Author 2\"` to the set because `\"Book C\"` does not have an author.\n",
       "- The resulting set will be `{\"Author 1\", \"Author 2\"}` (order may vary, as sets are unordered).\n",
       "- When `yield from` is executed, each author in the set will be yielded one by one.\n",
       "\n",
       "### Use Case:\n",
       "This kind of construct is useful when you want to lazily produce a list of unique authors from a potentially large dataset without consuming memory for the entire list of authors at once. It effectively provides a way to iterate over each unique author as needed.\n",
       "\n",
       "### Conclusion:\n",
       "So, the line of code you presented efficiently extracts unique authors from a collection of books, filtering out any that do not have author information, and yields each author one at a time. This is useful in contexts such as data processing or building output streams in applications where memory conservation is important."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream=openai.chat.completions.create(model=MODEL_GPT, messages=messages,stream=True)\n",
    "\n",
    "response=\"\"\n",
    "display_handle=display(Markdown(\"\"),display_id=True)\n",
    "for chunk in stream:\n",
    "    response +=chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(response),display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c15975-ba7d-4964-b94a-5ce105ccc9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "\n",
       "The provided code snippet is written in Python 3.5+ and utilizes the `yield` keyword, which allows us to implement generators.\n",
       "\n",
       "Here's a breakdown of what this specific line does:\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "This line can be read as: \"yield all authors found in `books`\".\n",
       "\n",
       "Let's break it down further:\n",
       "\n",
       "- `{... for ...}` is a dictionary comprehension (also known as a dictionary generator). It creates an iterable sequence of key-value pairs.\n",
       "\n",
       "- `book.get(\"author\")` retrieves the value associated with the key `\"author\"` from each `book` dictionary. If `\"author\"` is not present in the dictionary, it returns `None`.\n",
       "\n",
       "- `for book in books if book.get(\"author\")` filters the dictionaries to only include those that have an `\"author\"` key.\n",
       "\n",
       "- `yield from {...}` takes the iterable sequence created by the dictionary comprehension and yields each of its values individually.\n",
       "\n",
       "**Why**\n",
       "\n",
       "This code snippet appears to be used in a context where we need to extract authors from a list of book dictionaries. The resulting iterator would yield one author at a time, which can be useful for various purposes such as:\n",
       "\n",
       "- Processing books one-by-one while keeping track of the current author.\n",
       "- Creating an iterator that can be paused and resumed later (more on this in the next section).\n",
       "\n",
       "Here's some example code to illustrate how you might use this:\n",
       "\n",
       "```python\n",
       "def extract_authors(books):\n",
       "    return yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "# Usage:\n",
       "for author in extract_authors([\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"\", },\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author C\"}\n",
       "]):\n",
       "    print(f\"Processing book by {author}...\")\n",
       "```\n",
       "\n",
       "**Generator Functions**\n",
       "\n",
       "To fully understand the `yield from` syntax, you should know that generator functions are a type of function in Python that use the `yield` keyword to produce a series of values over time.\n",
       "\n",
       "Here's a simple example of how you can create and use a generator function:\n",
       "\n",
       "```python\n",
       "def infinite_sequence():\n",
       "    n = 0\n",
       "    while True:\n",
       "        yield n\n",
       "        n += 1\n",
       "\n",
       "# Usage:\n",
       "seq_gen = infinite_sequence()\n",
       "for _ in range(5):\n",
       "    print(next(seq_gen))\n",
       "```\n",
       "\n",
       "In this example, `infinite_sequence` is a generator function that yields the numbers from 0 to infinity. We use the `next()` function to retrieve each value produced by the generator.\n",
       "\n",
       "Keep in mind that generators only store the state of the current iteration and do not consume memory like lists or other data structures would. This makes them particularly useful for dealing with large datasets that don't fit into memory at once."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "reply = response['message']['content']\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0a013-c1f2-4f01-8b10-9f68325356e9",
   "metadata": {},
   "source": [
    "# Modify\n",
    "Update such that the question is taken as input and sent to the model for response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f01b258-a293-4afc-a99c-d3cfb624b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_responses(question):\n",
    "    \"\"\"\n",
    "    Takes a question as input, queries GPT-4o-mini and Llama 3.2 models, \n",
    "    and displays their responses.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to be processed by the models.\n",
    "    \"\"\"\n",
    "    # system_prompt is already declared above lets generate a new user prompt so that the input question can be sent\n",
    "    user_input_prompt = f\"Please give a detailed explanation to the following question: {question}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input_prompt}\n",
    "    ]\n",
    "     # GPT-4o-mini Response with Streaming\n",
    "    print(\"Fetching response from GPT-4o-mini...\")\n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "    response_gpt = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response_gpt += chunk.choices[0].delta.content or ''\n",
    "        response_gpt = response_gpt.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response_gpt), display_id=display_handle.display_id)\n",
    "\n",
    "    # Llama 3.2 Response\n",
    "    print(\"Fetching response from Llama 3.2...\")\n",
    "    response_llama = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "    reply_llama = response_llama['message']['content']\n",
    "    display(Markdown(reply_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35ac5e-a934-4c20-9be9-657afef66c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question:  What is Langgraph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching response from GPT-4o-mini...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Langgraph is a conceptual framework and set of tools designed to enhance the development and deployment of applications that utilize large language models (LLMs). While specific implementations or libraries named \"Langgraph\" may vary or evolve, the general idea revolves around leveraging the capabilities of LLMs in a structured way that facilitates the development of complex applications.\n",
       "\n",
       "### Key Features and Components of Langgraph\n",
       "\n",
       "1. **Graph-Based Approach**:\n",
       "   - **Graph Structures**: Langgraph utilizes graph structures to represent relationships between various language components, making it easier to understand, visualize, and manipulate the connections between different elements within a natural language processing (NLP) application.\n",
       "   - **Nodes and Edges**: In a graph, nodes represent entities such as tasks, prompts, or data points, while edges represent the relationships or interactions between these entities. This structure can be beneficial for organizing and managing data and processes in LLM applications.\n",
       "\n",
       "2. **Integration with Language Models**:\n",
       "   - Langgraph is designed to interface with existing LLMs from providers like OpenAI, Google, or Hugging Face. It provides a streamlined way to utilize these models for various tasks, including text generation, summarization, sentiment analysis, and more.\n",
       "   - Users can connect nodes in the graph to invoke different model capabilities, allowing for modular and reusable design patterns.\n",
       "\n",
       "3. **Enhanced Workflows**:\n",
       "   - By structuring workflows as graphs, developers can create complex applications that involve multiple steps or processes, each represented as a node. This modular approach allows for easy debugging, maintenance, and iteration.\n",
       "   - For instance, a user can define a workflow that first processes text input to extract entities and then feeds these into another node that generates a summary based on the extracted information.\n",
       "\n",
       "4. **Visualization and Understanding**:\n",
       "   - Langgraph might include tools for visualizing the graph structure, making it easier for developers and stakeholders to understand how data flows through the application and how different components interact with each other.\n",
       "   - Such visualization aids in explaining complex interactions and relationships within an application that uses LLMs.\n",
       "\n",
       "5. **Support for Customization**:\n",
       "   - Developers can extend the framework to cater to their specific needs by adding custom nodes that represent unique processing tasks or by integrating additional data sources and model types.\n",
       "\n",
       "### Use Cases\n",
       "\n",
       "Langgraph can be particularly useful in several applications:\n",
       "- **Chatbots and Virtual Assistants**: Creating conversational agents that need to manage context, maintain state, and respond to user queries in a structured way.\n",
       "- **Data Enrichment**: Automating workflows that involve data extraction, transformation, and loading (ETL) processes using natural language interfaces.\n",
       "- **Content Creation**: Managing content production workflows where different models or prompts interact to generate articles, blog posts, or other content types.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In summary, Langgraph is a powerful concept aimed at leveraging large language models through a graph-based approach. It promotes structured application development and allows for better organization, visualization, and manipulation of tasks related to natural language processing. By integrating various language models and representing tasks as interconnected nodes, Langgraph helps streamline the creation of sophisticated NLP applications, making it a valuable tool for developers in the AI landscape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching response from Llama 3.2...\n"
     ]
    }
   ],
   "source": [
    " # Prompt user for their question\n",
    "my_question = input(\"Please enter your question: \")\n",
    "# Fetch and display responses from models\n",
    "get_model_responses(my_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acf2af-635f-4216-9f5a-7c08d8313a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
