{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOvAqD+k9v8mSzXU7JDktW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26a16e422dec4982b9bcf58f406513c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d043084d4e6467f9ce9e68fad8c021b",
              "IPY_MODEL_7bf0d41521e7430b8e4f10e2320a889a",
              "IPY_MODEL_dc61c4609c3f485498f6315a93be3af0"
            ],
            "layout": "IPY_MODEL_4e2428f5a7c14a9da0607722b8686cf4"
          }
        },
        "6d043084d4e6467f9ce9e68fad8c021b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b22b289875ea476ca6f476634aca3ffd",
            "placeholder": "​",
            "style": "IPY_MODEL_11ee5cbb225749a6bf07b50e5b225d07",
            "value": "Downloading builder script: 100%"
          }
        },
        "7bf0d41521e7430b8e4f10e2320a889a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed53c739cb4848228e0db3077011d266",
            "max": 5758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d91b04d6dc374d96ba03bc61d45642e5",
            "value": 5758
          }
        },
        "dc61c4609c3f485498f6315a93be3af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_993eded4e4f64c24974bfec41fb80e51",
            "placeholder": "​",
            "style": "IPY_MODEL_d7317e57290e40ba99880e49ac9db994",
            "value": " 5.76k/5.76k [00:00&lt;00:00, 388kB/s]"
          }
        },
        "4e2428f5a7c14a9da0607722b8686cf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b22b289875ea476ca6f476634aca3ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11ee5cbb225749a6bf07b50e5b225d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed53c739cb4848228e0db3077011d266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d91b04d6dc374d96ba03bc61d45642e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "993eded4e4f64c24974bfec41fb80e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7317e57290e40ba99880e49ac9db994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f40cdd1139c42d59c2a5d9fe480a0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61899a0b422c4e4c9a2c36078ce94352",
              "IPY_MODEL_285fc013f5914dd9afbe7d538adae3d1",
              "IPY_MODEL_75ca228865114e09a11c666c0bc1eee5"
            ],
            "layout": "IPY_MODEL_0c7bbe60d43543deb745fe46398d3600"
          }
        },
        "61899a0b422c4e4c9a2c36078ce94352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f04bde7b83b4a9ca422706b8d3bcfc4",
            "placeholder": "​",
            "style": "IPY_MODEL_b447514faf04489d941559b4caa2b947",
            "value": "Downloading readme: 100%"
          }
        },
        "285fc013f5914dd9afbe7d538adae3d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b39a9455c60b44a1a9484e003ffdb988",
            "max": 6243,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1afce534c7244d86bb892df268eca246",
            "value": 6243
          }
        },
        "75ca228865114e09a11c666c0bc1eee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bbdf2d90ab44c5ebdd0670545d27285",
            "placeholder": "​",
            "style": "IPY_MODEL_5c78e562491041de8551938a3ac0d586",
            "value": " 6.24k/6.24k [00:00&lt;00:00, 363kB/s]"
          }
        },
        "0c7bbe60d43543deb745fe46398d3600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f04bde7b83b4a9ca422706b8d3bcfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b447514faf04489d941559b4caa2b947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b39a9455c60b44a1a9484e003ffdb988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1afce534c7244d86bb892df268eca246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bbdf2d90ab44c5ebdd0670545d27285": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c78e562491041de8551938a3ac0d586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f302ede8341d4d93a3e97e3b6e485379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877132b2d90b429b9657e9b81f58a73d",
              "IPY_MODEL_978f2fcea2194099936436a8a26c80c5",
              "IPY_MODEL_a0e08601cc6547c08ac105ed51977439"
            ],
            "layout": "IPY_MODEL_823e3a8de3594d808b310b01441be8f5"
          }
        },
        "877132b2d90b429b9657e9b81f58a73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0ec6dfb67c4896921a200440d3bb44",
            "placeholder": "​",
            "style": "IPY_MODEL_a94aa7ee4f584abb9d9a33492bc54db1",
            "value": "Downloading data: 100%"
          }
        },
        "978f2fcea2194099936436a8a26c80c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad67cd9243a45a79904582ead9bb199",
            "max": 254582292,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba6be8e9de474f68bd059ce9847713e7",
            "value": 254582292
          }
        },
        "a0e08601cc6547c08ac105ed51977439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945a12e8a15f4293a861a2d739bd6073",
            "placeholder": "​",
            "style": "IPY_MODEL_fd1ef1d7b06e4cea9b3b64ef4e93de23",
            "value": " 255M/255M [00:03&lt;00:00, 68.0MB/s]"
          }
        },
        "823e3a8de3594d808b310b01441be8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0ec6dfb67c4896921a200440d3bb44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a94aa7ee4f584abb9d9a33492bc54db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bad67cd9243a45a79904582ead9bb199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba6be8e9de474f68bd059ce9847713e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "945a12e8a15f4293a861a2d739bd6073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1ef1d7b06e4cea9b3b64ef4e93de23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8985cd8e0344880814e09bd5fb844fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0be3626a8eb34f1a93f42b5606ee3a68",
              "IPY_MODEL_a81cc138b0684373902f460385387893",
              "IPY_MODEL_c3c42d3690804452a807938ad1e58d06"
            ],
            "layout": "IPY_MODEL_dfe4cad2ff6243c193239ef899641e47"
          }
        },
        "0be3626a8eb34f1a93f42b5606ee3a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aea49f0767f468f9ebc07e0c796e3f2",
            "placeholder": "​",
            "style": "IPY_MODEL_5f1f22cbc19c468d969aee476bcec9e4",
            "value": "Downloading data: "
          }
        },
        "a81cc138b0684373902f460385387893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e020c3f06a049b7bec2195ccd98ab11",
            "max": 1001503,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ea45a555baa4483ac588f556b431420",
            "value": 1001503
          }
        },
        "c3c42d3690804452a807938ad1e58d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9a0034d4c7b487d94102446da15a151",
            "placeholder": "​",
            "style": "IPY_MODEL_8dee4a7ceca34ba9a49a5e95b76c6203",
            "value": " 2.72M/? [00:00&lt;00:00, 34.1MB/s]"
          }
        },
        "dfe4cad2ff6243c193239ef899641e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aea49f0767f468f9ebc07e0c796e3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1f22cbc19c468d969aee476bcec9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e020c3f06a049b7bec2195ccd98ab11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea45a555baa4483ac588f556b431420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9a0034d4c7b487d94102446da15a151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dee4a7ceca34ba9a49a5e95b76c6203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d15a77cf87f4f718cd2933e35efb381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffaf1872eb5f470eb3b10faafb76cb22",
              "IPY_MODEL_22e819f6e769493c9b6a7b89d47485eb",
              "IPY_MODEL_6c8a0eec7e5a4c0796ebbac1da0230bc"
            ],
            "layout": "IPY_MODEL_75ab6b8b97d34caba7b14ededa1efd96"
          }
        },
        "ffaf1872eb5f470eb3b10faafb76cb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf77e8cec12e40e98744cb853e4d1db9",
            "placeholder": "​",
            "style": "IPY_MODEL_9c805c5501b74d4ca7d3f77081f674c8",
            "value": "Generating train split: 100%"
          }
        },
        "22e819f6e769493c9b6a7b89d47485eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48afd460ea234336be5023dda9041824",
            "max": 204045,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a45f2dd63d4c4323abe5b02d66e32598",
            "value": 204045
          }
        },
        "6c8a0eec7e5a4c0796ebbac1da0230bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff431580a60e46d8af625b657be83ffc",
            "placeholder": "​",
            "style": "IPY_MODEL_ad1293f6c6ee4afe8edc187341ef94b7",
            "value": " 204045/204045 [00:48&lt;00:00, 5151.00 examples/s]"
          }
        },
        "75ab6b8b97d34caba7b14ededa1efd96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf77e8cec12e40e98744cb853e4d1db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c805c5501b74d4ca7d3f77081f674c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48afd460ea234336be5023dda9041824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a45f2dd63d4c4323abe5b02d66e32598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff431580a60e46d8af625b657be83ffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad1293f6c6ee4afe8edc187341ef94b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b321a9ab1d044748099bbc7269178ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47a1a561af654b26b07fb62f7ce4b738",
              "IPY_MODEL_a603e059f3714fdb974ddfd4351d2a7c",
              "IPY_MODEL_d2ba4fb05f2d4c9498cde143a200d142"
            ],
            "layout": "IPY_MODEL_b8eaf9919e5e4013a9aee02462820c63"
          }
        },
        "47a1a561af654b26b07fb62f7ce4b738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_015da7e042ca4091a9721b2c44e21081",
            "placeholder": "​",
            "style": "IPY_MODEL_a9eb2d5e1bc6458e95181b476328bbc2",
            "value": "Generating validation split: 100%"
          }
        },
        "a603e059f3714fdb974ddfd4351d2a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b6f2dfc47e4b91b8c1d5adb92ef245",
            "max": 11332,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8502be2d71594a52989ba737a5c691e0",
            "value": 11332
          }
        },
        "d2ba4fb05f2d4c9498cde143a200d142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80676dd703446dda0da04473a65657c",
            "placeholder": "​",
            "style": "IPY_MODEL_a48ccca1911941569a6f6830286c82e0",
            "value": " 11332/11332 [00:23&lt;00:00, 526.82 examples/s]"
          }
        },
        "b8eaf9919e5e4013a9aee02462820c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "015da7e042ca4091a9721b2c44e21081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9eb2d5e1bc6458e95181b476328bbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74b6f2dfc47e4b91b8c1d5adb92ef245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8502be2d71594a52989ba737a5c691e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e80676dd703446dda0da04473a65657c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a48ccca1911941569a6f6830286c82e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e98c75a9e4c4b23883acc52e014c475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1da41d364f24a6b9bd45273a3c63a70",
              "IPY_MODEL_946637feb88a4eb68a8c5f7ecb5c4943",
              "IPY_MODEL_cef636aeaa2a451b86ca4ba279ddc94f"
            ],
            "layout": "IPY_MODEL_d161ea2b3e2a406c947c8502c0ed6e16"
          }
        },
        "a1da41d364f24a6b9bd45273a3c63a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad294cc43b3842f599c62a7219ae60ee",
            "placeholder": "​",
            "style": "IPY_MODEL_1c4140fe09774ab882a22408b76d5120",
            "value": "Generating test split: 100%"
          }
        },
        "946637feb88a4eb68a8c5f7ecb5c4943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be982cac24c4778adfac39f1fdec705",
            "max": 11334,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5c51f763f9f45009233e015045941a8",
            "value": 11334
          }
        },
        "cef636aeaa2a451b86ca4ba279ddc94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e019f1a59f684316acef6cf4fbabfa38",
            "placeholder": "​",
            "style": "IPY_MODEL_c096d0ab541a4fed940c7e12e85e41f7",
            "value": " 11334/11334 [00:23&lt;00:00, 551.80 examples/s]"
          }
        },
        "d161ea2b3e2a406c947c8502c0ed6e16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad294cc43b3842f599c62a7219ae60ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4140fe09774ab882a22408b76d5120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6be982cac24c4778adfac39f1fdec705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5c51f763f9f45009233e015045941a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e019f1a59f684316acef6cf4fbabfa38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c096d0ab541a4fed940c7e12e85e41f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamZoro/Generative_AI/blob/main/summarize/PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ],
      "metadata": {
        "id": "IaEyvmJ_0KxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bPXHtPvR0K3R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"xsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573,
          "referenced_widgets": [
            "26a16e422dec4982b9bcf58f406513c4",
            "6d043084d4e6467f9ce9e68fad8c021b",
            "7bf0d41521e7430b8e4f10e2320a889a",
            "dc61c4609c3f485498f6315a93be3af0",
            "4e2428f5a7c14a9da0607722b8686cf4",
            "b22b289875ea476ca6f476634aca3ffd",
            "11ee5cbb225749a6bf07b50e5b225d07",
            "ed53c739cb4848228e0db3077011d266",
            "d91b04d6dc374d96ba03bc61d45642e5",
            "993eded4e4f64c24974bfec41fb80e51",
            "d7317e57290e40ba99880e49ac9db994",
            "3f40cdd1139c42d59c2a5d9fe480a0ea",
            "61899a0b422c4e4c9a2c36078ce94352",
            "285fc013f5914dd9afbe7d538adae3d1",
            "75ca228865114e09a11c666c0bc1eee5",
            "0c7bbe60d43543deb745fe46398d3600",
            "5f04bde7b83b4a9ca422706b8d3bcfc4",
            "b447514faf04489d941559b4caa2b947",
            "b39a9455c60b44a1a9484e003ffdb988",
            "1afce534c7244d86bb892df268eca246",
            "2bbdf2d90ab44c5ebdd0670545d27285",
            "5c78e562491041de8551938a3ac0d586",
            "f302ede8341d4d93a3e97e3b6e485379",
            "877132b2d90b429b9657e9b81f58a73d",
            "978f2fcea2194099936436a8a26c80c5",
            "a0e08601cc6547c08ac105ed51977439",
            "823e3a8de3594d808b310b01441be8f5",
            "5b0ec6dfb67c4896921a200440d3bb44",
            "a94aa7ee4f584abb9d9a33492bc54db1",
            "bad67cd9243a45a79904582ead9bb199",
            "ba6be8e9de474f68bd059ce9847713e7",
            "945a12e8a15f4293a861a2d739bd6073",
            "fd1ef1d7b06e4cea9b3b64ef4e93de23",
            "b8985cd8e0344880814e09bd5fb844fb",
            "0be3626a8eb34f1a93f42b5606ee3a68",
            "a81cc138b0684373902f460385387893",
            "c3c42d3690804452a807938ad1e58d06",
            "dfe4cad2ff6243c193239ef899641e47",
            "2aea49f0767f468f9ebc07e0c796e3f2",
            "5f1f22cbc19c468d969aee476bcec9e4",
            "9e020c3f06a049b7bec2195ccd98ab11",
            "2ea45a555baa4483ac588f556b431420",
            "b9a0034d4c7b487d94102446da15a151",
            "8dee4a7ceca34ba9a49a5e95b76c6203",
            "1d15a77cf87f4f718cd2933e35efb381",
            "ffaf1872eb5f470eb3b10faafb76cb22",
            "22e819f6e769493c9b6a7b89d47485eb",
            "6c8a0eec7e5a4c0796ebbac1da0230bc",
            "75ab6b8b97d34caba7b14ededa1efd96",
            "cf77e8cec12e40e98744cb853e4d1db9",
            "9c805c5501b74d4ca7d3f77081f674c8",
            "48afd460ea234336be5023dda9041824",
            "a45f2dd63d4c4323abe5b02d66e32598",
            "ff431580a60e46d8af625b657be83ffc",
            "ad1293f6c6ee4afe8edc187341ef94b7",
            "8b321a9ab1d044748099bbc7269178ff",
            "47a1a561af654b26b07fb62f7ce4b738",
            "a603e059f3714fdb974ddfd4351d2a7c",
            "d2ba4fb05f2d4c9498cde143a200d142",
            "b8eaf9919e5e4013a9aee02462820c63",
            "015da7e042ca4091a9721b2c44e21081",
            "a9eb2d5e1bc6458e95181b476328bbc2",
            "74b6f2dfc47e4b91b8c1d5adb92ef245",
            "8502be2d71594a52989ba737a5c691e0",
            "e80676dd703446dda0da04473a65657c",
            "a48ccca1911941569a6f6830286c82e0",
            "5e98c75a9e4c4b23883acc52e014c475",
            "a1da41d364f24a6b9bd45273a3c63a70",
            "946637feb88a4eb68a8c5f7ecb5c4943",
            "cef636aeaa2a451b86ca4ba279ddc94f",
            "d161ea2b3e2a406c947c8502c0ed6e16",
            "ad294cc43b3842f599c62a7219ae60ee",
            "1c4140fe09774ab882a22408b76d5120",
            "6be982cac24c4778adfac39f1fdec705",
            "c5c51f763f9f45009233e015045941a8",
            "e019f1a59f684316acef6cf4fbabfa38",
            "c096d0ab541a4fed940c7e12e85e41f7"
          ]
        },
        "id": "wfcv1tES0CSd",
        "outputId": "80add1f3-b274-4e97-f044-2e99f6965d16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1454: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.76k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26a16e422dec4982b9bcf58f406513c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/6.24k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f40cdd1139c42d59c2a5d9fe480a0ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/255M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f302ede8341d4d93a3e97e3b6e485379"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8985cd8e0344880814e09bd5fb844fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d15a77cf87f4f718cd2933e35efb381"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b321a9ab1d044748099bbc7269178ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e98c75a9e4c4b23883acc52e014c475"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['document', 'summary', 'id'],\n",
              "        num_rows: 204045\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['document', 'summary', 'id'],\n",
              "        num_rows: 11332\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['document', 'summary', 'id'],\n",
              "        num_rows: 11334\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2TeC0zTx0CdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ],
      "metadata": {
        "id": "Df_8f2lNz59I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "\n",
        "dialogue = dataset['test']['document'][index]\n",
        "summary = dataset['test']['summary'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "PZr_KbX8z6Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"document\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'document', 'summary',])\n",
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "pBTM0sK7z6RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "_lCwJMc8zb1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'dialogue-summary-training'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "P6MnXE_zzb6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "tAOM_a9-zb-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test']['document'][index]\n",
        "human_baseline_summary = dataset['test']['summary'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(next(original_model.parameters()).device)\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n"
      ],
      "metadata": {
        "id": "Yy42-CjwzcCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ],
      "metadata": {
        "id": "OHmyLaWbzcGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model,\n",
        "                            lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "LBtQINL01DbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'Shubham_peft-dialogue-summary-training'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        "    # push_to_hub_model_id=output_dir,\n",
        "    # push_to_hub_organization=\"huggingface\",\n",
        "    # push_to_hub_token=\"\",\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")"
      ],
      "metadata": {
        "id": "9P1X1ykfzcJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "thuP7tutzcOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "CAIgRSDkzac6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.push_to_hub(\"ShubhamZoro/FLan-T5-Summarize\",\n",
        "                  use_auth_token=True,\n",
        "                  commit_message=\"basic training\",\n",
        "                  )"
      ],
      "metadata": {
        "id": "r3luYPbSzRKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIMRhuSvyvDR",
        "outputId": "483acbda-c68c-41cf-a893-b798fb3dce0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.17.0 in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.17.0) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.17.0) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch"
      ],
      "metadata": {
        "id": "inmXDY-Yyxal"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"ShubhamZoro/FLan-T5-Summarize\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model1 = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, return_dict=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
      ],
      "metadata": {
        "id": "1sfz5Q8HAyU6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 100\n",
        "dialogue = dataset['test']['document'][index]\n",
        "baseline_human_summary = dataset['test']['summary'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt,padding='max_length', return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(next(model1.parameters()).device)\n",
        "\n",
        "peft_model_outputs = model1.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
        "\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: {peft_model_text_output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5VW_vt-pyeT_",
        "outputId": "4931e38f-b53f-4bf1-b4e6-aa87a1197bce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "\n  #ifdef __HIPCC__\n  #define ERROR_UNSUPPORTED_CAST ;\n  // corresponds to aten/src/ATen/native/cuda/thread_constants.h\n  #define CUDA_OR_ROCM_NUM_THREADS 256\n  // corresponds to aten/src/ATen/cuda/detail/OffsetCalculator.cuh\n  #define MAX_DIMS 16\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  #else\n  //TODO use _assert_fail, because assert is disabled in non-debug builds\n  #define ERROR_UNSUPPORTED_CAST assert(false);\n  #define CUDA_OR_ROCM_NUM_THREADS 128\n  #define MAX_DIMS 25\n  #endif\n  #define POS_INFINITY __int_as_float(0x7f800000)\n  #define INFINITY POS_INFINITY\n  #define NEG_INFINITY __int_as_float(0xff800000)\n  #define NAN __int_as_float(0x7fffffff)\n\n  typedef long long int int64_t;\n  typedef unsigned int uint32_t;\n  typedef signed char int8_t;\n  typedef unsigned char uint8_t;  // NOTE: this MUST be \"unsigned char\"! \"char\" is equivalent to \"signed char\"\n  typedef short int16_t;\n  static_assert(sizeof(int64_t) == 8, \"expected size does not match\");\n  static_assert(sizeof(uint32_t) == 4, \"expected size does not match\");\n  static_assert(sizeof(int8_t) == 1, \"expected size does not match\");\n  constexpr int num_threads = CUDA_OR_ROCM_NUM_THREADS;\n  constexpr int thread_work_size = 4; // TODO: make template substitution once we decide where those vars live\n  constexpr int block_work_size = thread_work_size * num_threads;\n\n  \n  \n  \n  namespace std {\n  \n  using ::signbit;\n  using ::isfinite;\n  using ::isinf;\n  using ::isnan;\n  \n  using ::abs;\n  \n  using ::acos;\n  using ::acosf;\n  using ::asin;\n  using ::asinf;\n  using ::atan;\n  using ::atanf;\n  using ::atan2;\n  using ::atan2f;\n  using ::ceil;\n  using ::ceilf;\n  using ::cos;\n  using ::cosf;\n  using ::cosh;\n  using ::coshf;\n  \n  using ::exp;\n  using ::expf;\n  \n  using ::fabs;\n  using ::fabsf;\n  using ::floor;\n  using ::floorf;\n  \n  using ::fmod;\n  using ::fmodf;\n  \n  using ::frexp;\n  using ::frexpf;\n  using ::ldexp;\n  using ::ldexpf;\n  \n  using ::log;\n  using ::logf;\n  \n  using ::log10;\n  using ::log10f;\n  using ::modf;\n  using ::modff;\n  \n  using ::pow;\n  using ::powf;\n  \n  using ::sin;\n  using ::sinf;\n  using ::sinh;\n  using ::sinhf;\n  \n  using ::sqrt;\n  using ::sqrtf;\n  using ::tan;\n  using ::tanf;\n  \n  using ::tanh;\n  using ::tanhf;\n  \n  using ::acosh;\n  using ::acoshf;\n  using ::asinh;\n  using ::asinhf;\n  using ::atanh;\n  using ::atanhf;\n  using ::cbrt;\n  using ::cbrtf;\n  \n  using ::copysign;\n  using ::copysignf;\n  \n  using ::erf;\n  using ::erff;\n  using ::erfc;\n  using ::erfcf;\n  using ::exp2;\n  using ::exp2f;\n  using ::expm1;\n  using ::expm1f;\n  using ::fdim;\n  using ::fdimf;\n  using ::fmaf;\n  using ::fma;\n  using ::fmax;\n  using ::fmaxf;\n  using ::fmin;\n  using ::fminf;\n  using ::hypot;\n  using ::hypotf;\n  using ::ilogb;\n  using ::ilogbf;\n  using ::lgamma;\n  using ::lgammaf;\n  using ::llrint;\n  using ::llrintf;\n  using ::llround;\n  using ::llroundf;\n  using ::log1p;\n  using ::log1pf;\n  using ::log2;\n  using ::log2f;\n  using ::logb;\n  using ::logbf;\n  using ::lrint;\n  using ::lrintf;\n  using ::lround;\n  using ::lroundf;\n  \n  using ::nan;\n  using ::nanf;\n  \n  using ::nearbyint;\n  using ::nearbyintf;\n  using ::nextafter;\n  using ::nextafterf;\n  using ::remainder;\n  using ::remainderf;\n  using ::remquo;\n  using ::remquof;\n  using ::rint;\n  using ::rintf;\n  using ::round;\n  using ::roundf;\n  using ::scalbln;\n  using ::scalblnf;\n  using ::scalbn;\n  using ::scalbnf;\n  using ::tgamma;\n  using ::tgammaf;\n  using ::trunc;\n  using ::truncf;\n  \n  } // namespace std\n  \n  \n\n  // NB: Order matters for this macro; it is relied upon in\n  // _promoteTypesLookup and the serialization format.\n  // Note, some types have ctype as void because we don't support them in codegen\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_) \\\n  _(uint8_t, Byte) /* 0 */                               \\\n  _(int8_t, Char) /* 1 */                                \\\n  _(int16_t, Short) /* 2 */                              \\\n  _(int, Int) /* 3 */                                    \\\n  _(int64_t, Long) /* 4 */                               \\\n  _(at::Half, Half) /* 5 */                                  \\\n  _(float, Float) /* 6 */                                \\\n  _(double, Double) /* 7 */                              \\\n  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \\\n  _(std::complex<float>, ComplexFloat) /* 9 */                          \\\n  _(std::complex<double>, ComplexDouble) /* 10 */                         \\\n  _(bool, Bool) /* 11 */                                 \\\n  _(void, QInt8) /* 12 */                          \\\n  _(void, QUInt8) /* 13 */                        \\\n  _(void, QInt32) /* 14 */                        \\\n  _(at::BFloat16, BFloat16) /* 15 */                             \\\n\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_QINT(_)       \\\n  _(uint8_t, Byte)                                                 \\\n  _(int8_t, Char)                                                  \\\n  _(int16_t, Short)                                                \\\n  _(int, Int)                                                      \\\n  _(int64_t, Long)                                                 \\\n  _(at::Half, Half)                                                \\\n  _(float, Float)                                                  \\\n  _(double, Double)                                                \\\n  _(std::complex<at::Half>, ComplexHalf)                           \\\n  _(std::complex<float>, ComplexFloat)                             \\\n  _(std::complex<double>, ComplexDouble)                           \\\n  _(bool, Bool)                                                    \\\n  _(at::BFloat16, BFloat16)\n\n\n  enum class ScalarType : int8_t {\n  #define DEFINE_ENUM(_1, n) n,\n  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ENUM)\n  #undef DEFINE_ENUM\n      Undefined,\n  NumOptions\n  };\n\n  template <typename T, int size>\n  struct Array {\n  T data[size];\n\n  __device__ T operator[](int i) const {\n      return data[i];\n  }\n  __device__ T& operator[](int i) {\n      return data[i];\n  }\n  Array() = default;\n  Array(const Array&) = default;\n  Array& operator=(const Array&) = default;\n  __device__ Array(T x) {\n    for (int i = 0; i < size; i++) {\n      data[i] = x;\n    }\n  }\n  };\n\n  \n  \n  \n  \n  \n\n\n\n  template <typename T>\n  struct DivMod {\n  T div;\n  T mod;\n\n  __device__ DivMod(T _div, T _mod) {\n      div = _div;\n      mod = _mod;\n  }\n  };\n\n  //<unsigned int>\n  struct IntDivider {\n  IntDivider() = default;\n\n  __device__ inline unsigned int div(unsigned int n) const {\n  unsigned int t = __umulhi(n, m1);\n  return (t + n) >> shift;\n  }\n\n  __device__ inline unsigned int mod(unsigned int n) const {\n  return n - div(n) * divisor;\n  }\n\n  __device__ inline DivMod<unsigned int> divmod(unsigned int n) const {\n  unsigned int q = div(n);\n  return DivMod<unsigned int>(q, n - q * divisor);\n  }\n\n  unsigned int divisor;  // d above.\n  unsigned int m1;  // Magic number: m' above.\n  unsigned int shift;  // Shift amounts.\n  };\n\n  template <int NARGS>\n  struct TrivialOffsetCalculator {\n    // The offset for each argument. Wrapper around fixed-size array.\n    // The offsets are in # of elements, not in bytes.\n    Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; arg++) {\n        offsets[arg] = linear_idx;\n      }\n      return offsets;\n    }\n  };\n\n  template<int NARGS>\n  struct OffsetCalculator {\n  OffsetCalculator() = default;\n  __device__ __forceinline__ Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n      offsets[arg] = 0;\n      }\n\n      #pragma unroll\n      for (int dim = 0; dim < MAX_DIMS; ++dim) {\n      if (dim == dims) {\n          break;\n      }\n\n      auto divmod = sizes_[dim].divmod(linear_idx);\n      linear_idx = divmod.div;\n\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n          offsets[arg] += divmod.mod * strides_[dim][arg];\n      }\n      //printf(\"offset calc thread dim size stride offset %d %d %d %d %d %d %d %d\\n\",\n      //threadIdx.x, dim, sizes_[dim].divisor, strides_[dim][0], offsets[0], linear_idx, divmod.div, divmod.mod);\n      }\n      return offsets;\n  }\n\n    int dims;\n    IntDivider sizes_[MAX_DIMS];\n    // NOTE: this approach will not support nInputs == 0\n    unsigned int strides_[MAX_DIMS][NARGS];\n  };\n\n\n\n  #define C10_HOST_DEVICE __host__ __device__\n  #define C10_DEVICE __device__\n  #if defined(__clang__) && defined(__HIP__)\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  // until ROCm support for kernel asserts is restored\n  #define assert(expr) (static_cast<void>(0))\n  #endif\n\n  template <typename T>\n  __device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n  #if defined(__clang__) && defined(__HIP__)\n    return __shfl_down(value, delta, width);\n  #else\n    return __shfl_down_sync(mask, value, delta, width);\n  #endif\n  }\n\n\n  #if 0\n  template <typename T>\n  __device__ __forceinline__ std::complex<T> WARP_SHFL_DOWN(std::complex<T> value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n    return std::complex<T>(\n  #if defined(__clang__) && defined(__HIP__)\n        __shfl_down(value.real(), delta, width),\n        __shfl_down(value.imag(), delta, width));\n  #else\n        __shfl_down_sync(mask, value.real(), delta, width),\n        __shfl_down_sync(mask, value.imag(), delta, width));\n  #endif\n  }\n  #endif\n\n  // aligned vector generates vectorized load/store on CUDA\n  template<typename scalar_t, int vec_size>\n  struct alignas(sizeof(scalar_t) * vec_size) aligned_vector {\n    scalar_t val[vec_size];\n  };\n\n\n  C10_HOST_DEVICE static void reduce_fraction(size_t &numerator, size_t &denominator) {\n    // get GCD of num and denom using Euclid's algorithm.\n    // Can replace this with std::gcd if we ever support c++17.\n    size_t a = denominator;\n    size_t b = numerator;\n    while (b != 0) {\n        a %= b;\n        // swap(a,b)\n        size_t tmp = a;\n        a = b;\n        b = tmp;\n    }\n\n    // a is now the GCD\n    numerator /= a;\n    denominator /= a;\n  }\n\n\n\n\n  struct ReduceConfig {\n  //has to match host-side ReduceConfig in the eager code\n  static constexpr int BLOCK_X = 0;\n  static constexpr int BLOCK_Y = 1;\n  static constexpr int CTA = 2;\n\n  static constexpr int input_vec_size = 4;\n  int element_size_bytes;\n  int num_inputs;\n  int num_outputs;\n  int step_input = 1;\n  int step_output = 1;\n  int ctas_per_output = 1;\n  int input_mult[3] = {0, 0, 0};\n  int output_mult[2] = {0, 0};\n\n  int block_width;\n  int block_height;\n  int num_threads;\n\n  bool vectorize_input = false;\n  int output_vec_size = 1;\n\n  C10_HOST_DEVICE bool should_block_x_reduce() const {\n    return input_mult[BLOCK_X] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_block_y_reduce() const {\n    return input_mult[BLOCK_Y] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_global_reduce() const {\n    return input_mult[CTA] != 0;\n  }\n\n  C10_DEVICE bool should_store(int output_idx) const {\n    return output_idx < num_outputs &&\n      (!should_block_x_reduce() || threadIdx.x == 0) &&\n      (!should_block_y_reduce() || threadIdx.y == 0);\n  }\n\n  C10_DEVICE bool should_reduce_tail() const {\n    return (!should_block_y_reduce() || threadIdx.y == 0) &&\n      (!should_global_reduce() || blockIdx.y == 0);\n  }\n\n  C10_HOST_DEVICE int input_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta2 = blockIdx.y;\n    return (lane * input_mult[BLOCK_X] +\n            warp * input_mult[BLOCK_Y] +\n            cta2 * input_mult[CTA]);\n  }\n\n  template <int output_vec_size>\n  C10_HOST_DEVICE int output_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta1 = blockIdx.x;\n    return (lane * output_mult[BLOCK_X] +\n            warp * output_mult[BLOCK_Y] +\n            cta1 * step_output) * output_vec_size;\n  }\n\n  C10_DEVICE int shared_memory_offset(int offset) const {\n    return threadIdx.x + (threadIdx.y + offset) * blockDim.x;\n  }\n\n  C10_DEVICE int staging_memory_offset(int cta2) const {\n    int offset = cta2 + blockIdx.x * gridDim.y;\n    if (!should_block_x_reduce()) {\n      offset = threadIdx.x + offset * blockDim.x;\n    }\n    return offset;\n  }\n\n\n  };\n\n\n//TODO this will need to be different for more generic reduction functions\nnamespace reducer {\n\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n\n  inline __device__ arg_t combine(arg_t a, arg_t b) { return a * b; }\n\n  inline __device__ out_scalar_t project(arg_t arg) {\n    return (out_scalar_t) arg;\n  }\n\n  inline __device__ arg_t warp_shfl_down(arg_t arg, int offset) {\n    return WARP_SHFL_DOWN(arg, offset);\n  }\n\n  inline __device__ arg_t translate_idx(arg_t acc, int64_t /*idx*/) {\n    return acc;\n  }\n\n  // wrap a normal reduction that ignores the index\n  inline __device__ arg_t reduce(arg_t acc, arg_t val, int64_t idx) {\n     return combine(acc, val);\n  }\n}\n\n\nstruct ReduceJitOp {\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n  using InputCalculator = OffsetCalculator<1>;\n  using OutputCalculator = OffsetCalculator<2>;\n\n//   static constexpr bool can_accumulate_in_output =\n//     std::is_convertible<arg_t, out_scalar_t>::value\n//     && std::is_convertible<out_scalar_t, arg_t>::value;\n\n  static constexpr int input_vec_size = ReduceConfig::input_vec_size;\n\n  arg_t ident;\n  ReduceConfig config;\n  InputCalculator input_calc;\n  OutputCalculator output_calc;\n  const void* src;\n  const char* dst[2]; //it accepts at most two destinations\n  // acc_buf used for accumulation among sub Tensor Iterator when accumulation on\n  // output is not permissible\n  void* acc_buf;\n  // cta_buf used for accumulation between blocks during global reduction\n  void* cta_buf;\n  int* semaphores;\n  int64_t base_idx;\n  bool accumulate;\n  bool final_output;\n  int noutputs;\n\n\n  C10_DEVICE void run() const {\n    extern __shared__ char shared_memory[];\n    uint32_t output_idx = config.output_idx<1>();\n    uint32_t input_idx = config.input_idx();\n    auto base_offsets1 = output_calc.get(output_idx)[1];\n\n    using arg_vec_t = Array<arg_t, 1>;\n    arg_vec_t value;\n\n    if (output_idx < config.num_outputs && input_idx < config.num_inputs) {\n      const scalar_t* input_slice = (const scalar_t*)((const char*)src + base_offsets1);\n\n      value = thread_reduce<1>(input_slice);\n    }\n\n    if (config.should_block_y_reduce()) {\n      value = block_y_reduce<1>(value, shared_memory);\n    }\n    if (config.should_block_x_reduce()) {\n      value = block_x_reduce<1>(value, shared_memory);\n    }\n\n    using out_ptr_vec_t = Array<out_scalar_t*, 1>;\n    using offset_vec_t = Array<uint32_t, 1>;\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < 1; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    arg_vec_t* acc = nullptr;\n    if (acc_buf != nullptr) {\n      size_t numerator = sizeof(arg_t);\n      size_t denominator = sizeof(out_scalar_t);\n      reduce_fraction(numerator, denominator);\n      acc = (arg_vec_t*)((char*)acc_buf + (base_offsets[0] * numerator / denominator));\n    }\n\n    if (config.should_global_reduce()) {\n      value = global_reduce<1>(value, acc, shared_memory);\n    } else if (config.should_store(output_idx)) {\n      if (accumulate) {\n        #pragma unroll\n        for (int i = 0; i < 1; i++) {\n          value[i] = reducer::translate_idx(value[i], base_idx);\n        }\n      }\n\n      if (acc == nullptr) {\n        if (accumulate) {\n          value = accumulate_in_output<1>(out, value);\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            *(out[i]) = get_accumulated_output(out[i], value[i]);\n          }\n        }\n      } else {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            value[i] = reducer::combine((*acc)[i], value[i]);\n          }\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          *acc = value;\n        }\n      }\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce(const scalar_t* data) const {\n    if (config.vectorize_input) {\n      assert(output_vec_size == 1);\n      // reduce at the header of input_slice where memory is not aligned,\n      // so that thread_reduce will have an aligned memory to work on.\n      return {input_vectorized_thread_reduce_impl(data)};\n    } else {\n      uint32_t element_stride = input_calc.strides_[0][0] / sizeof(scalar_t);\n      bool is_contiguous = (input_calc.dims == 1 && element_stride == 1);\n      if (is_contiguous) {\n        return thread_reduce_impl<output_vec_size>(data, [](uint32_t idx) { return idx; });\n      } else if (input_calc.dims == 1) {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return idx * element_stride; });\n      } else {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return input_calc.get(idx)[0] / sizeof(scalar_t); });\n      }\n    }\n  }\n\n  C10_DEVICE arg_t input_vectorized_thread_reduce_impl(const scalar_t* data) const {\n    uint32_t end = config.num_inputs;\n\n    // Handle the head of input slice where data is not aligned\n    arg_t value = ident;\n    constexpr int align_bytes = alignof(aligned_vector<scalar_t, input_vec_size>);\n    constexpr int align_elements = align_bytes / sizeof(scalar_t);\n    int shift = ((int64_t)data) % align_bytes / sizeof(scalar_t);\n    if (shift > 0) {\n      data -= shift;\n      end += shift;\n      if(threadIdx.x >= shift && threadIdx.x < align_elements && config.should_reduce_tail()){\n        value = reducer::reduce(value, data[threadIdx.x], threadIdx.x - shift);\n      }\n      end -= align_elements;\n      data += align_elements;\n      shift = align_elements - shift;\n    }\n\n    // Do the vectorized reduction\n    using load_t = aligned_vector<scalar_t, input_vec_size>;\n\n    uint32_t idx = config.input_idx();\n    const uint32_t stride = config.step_input;\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_t value_list[input_vec_size];\n    value_list[0] = value;\n\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[i] = ident;\n    }\n\n    scalar_t values[input_vec_size];\n\n    load_t *values_vector = reinterpret_cast<load_t*>(&values[0]);\n\n    while (idx * input_vec_size + input_vec_size - 1 < end) {\n      *values_vector = reinterpret_cast<const load_t*>(data)[idx];\n      #pragma unroll\n      for (uint32_t i = 0; i < input_vec_size; i++) {\n        value_list[i] = reducer::reduce(value_list[i], values[i], shift + idx * input_vec_size + i);\n      }\n      idx += stride;\n    }\n\n    // tail\n    uint32_t tail_start = end - end % input_vec_size;\n    if (config.should_reduce_tail()) {\n      int idx = tail_start + threadIdx.x;\n      if (idx < end) {\n        value_list[0] = reducer::reduce(value_list[0], data[idx], idx + shift);\n      }\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[0] = reducer::combine(value_list[0], value_list[i]);\n    }\n    return value_list[0];\n  }\n\n  template <int output_vec_size, typename offset_calc_t>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce_impl(const scalar_t* data_, offset_calc_t calc) const {\n    uint32_t idx = config.input_idx();\n    const uint32_t end = config.num_inputs;\n    const uint32_t stride = config.step_input;\n    const int vt0=4;\n\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using load_t = aligned_vector<scalar_t, output_vec_size>;\n    const load_t* data = reinterpret_cast<const load_t*>(data_);\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_vec_t value_list[vt0];\n\n    #pragma unroll\n    for (int i = 0; i < vt0; i++) {\n      #pragma unroll\n      for (int j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = ident;\n      }\n    }\n\n    load_t values[vt0];\n\n    while (idx + (vt0 - 1) * stride < end) {\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        values[i] = data[calc(idx + i * stride) / output_vec_size];\n      }\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        #pragma unroll\n        for (uint32_t j = 0; j < output_vec_size; j++) {\n          value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx + i * stride);\n        }\n      }\n      idx += stride * vt0;\n    }\n\n    // tail\n    int idx_ = idx;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      values[i] = data[calc(idx) / output_vec_size];\n      idx += stride;\n    }\n    idx = idx_;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx);\n      }\n      idx += stride;\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < vt0; i++) {\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[0][j] = reducer::combine(value_list[0][j], value_list[i][j]);\n      }\n    }\n    return value_list[0];\n  }\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_x_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    int dim_x = blockDim.x;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    if (dim_x > warpSize) {\n      int address_base = threadIdx.x + threadIdx.y*blockDim.x;\n      shared[address_base] = value;\n      for (int offset = dim_x/2; offset >= warpSize; offset >>= 1) {\n        __syncthreads();\n        if (threadIdx.x < offset && threadIdx.x + offset < blockDim.x) {\n          args_vec_t other = shared[address_base + offset];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], other[i]);\n          }\n          shared[address_base] = value;\n        }\n      }\n      dim_x = warpSize;\n    }\n\n    __syncthreads();\n\n    for (int offset = 1; offset < dim_x; offset <<= 1) {\n      #pragma unroll\n      for (int i = 0; i < output_vec_size; i++) {\n        arg_t other = reducer::warp_shfl_down(value[i], offset);\n        value[i] = reducer::combine(value[i], other);\n      }\n    }\n    return value;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_y_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    shared[config.shared_memory_offset(0)] = value;\n    for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) {\n      __syncthreads();\n      if (threadIdx.y < offset && threadIdx.y + offset < blockDim.y) {\n        args_vec_t other = shared[config.shared_memory_offset(offset)];\n        #pragma unroll\n        for (int i = 0; i < output_vec_size; i++) {\n          value[i] = reducer::combine(value[i], other[i]);\n        }\n        shared[config.shared_memory_offset(0)] = value;\n      }\n    }\n    return value;\n  }\n  \n\n  C10_DEVICE bool mark_block_finished() const {\n    __shared__ bool is_last_block_done_shared;\n\n    __syncthreads();\n    if (threadIdx.x == 0 && threadIdx.y == 0) {\n      int prev_blocks_finished = atomicAdd(&semaphores[blockIdx.x], 1);\n      is_last_block_done_shared = (prev_blocks_finished == gridDim.y - 1);\n    }\n\n    __syncthreads();\n\n    return is_last_block_done_shared;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> accumulate_in_output(\n    Array<out_scalar_t*, output_vec_size> out,\n    Array<arg_t, output_vec_size> value\n  ) const {\n    Array<arg_t, output_vec_size> ret;\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      ret[i] = reducer::combine(*(out[i]), value[i]);\n    }\n    return ret;\n  }\n\n\n  C10_DEVICE out_scalar_t get_accumulated_output(\n    out_scalar_t* out, arg_t value\n  ) const {\n    assert(!final_output);\n    return (out_scalar_t)value;\n  }\n\n  template<class T>\n  C10_DEVICE void set_results(const T x, const uint32_t base_offset) const {\n    assert(noutputs == 1);\n    auto res = (out_scalar_t*)((char*)dst[0] + base_offset);\n    *res = x;\n  }\n\n//TODO - multi-output reduction - we won't be able to use thrust::pair\n//just explicitly specify typed output reads/writes\n//Currently implemented for max of two outputs\n//   template<class T1, class T2>\n//   C10_DEVICE void set_results(const thrust::pair<T1, T2> x, const index_t base_offset) const {\n//     if (noutputs >= 1) {\n//       auto res0 = (T1*)((char*)dst[0] + base_offset);\n//       *res0 = x.first;\n//     }\n//     if (noutputs >= 2) {\n//       // base offset is computed assuming element size being sizeof(T1), so we need to make a\n//       // correction to obtain the correct base offset\n//       auto res1 = (T2*) ((char *) dst[1] + base_offset / sizeof(T1) * sizeof(T2));\n//       *res1 = x.second;\n//     }\n//   }\n\n  template <int output_vec_size>\n  C10_DEVICE void set_results_to_output(Array<arg_t, output_vec_size> value, Array<uint32_t, output_vec_size> base_offset) const {\n    assert(final_output);\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      set_results(reducer::project(value[i]), base_offset[i]);\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> global_reduce(Array<arg_t, output_vec_size> value, Array<arg_t, output_vec_size> *acc, char* shared_memory) const {\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using out_ptr_vec_t = Array<out_scalar_t*, output_vec_size>;\n    using offset_vec_t = Array<uint32_t, output_vec_size>;\n\n    arg_vec_t* reduce_buffer = (arg_vec_t*)cta_buf;\n    uint32_t output_idx = config.output_idx<output_vec_size>();\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    bool should_store = config.should_store(output_idx);\n    if (should_store) {\n      uint32_t offset = config.staging_memory_offset(blockIdx.y);\n      reduce_buffer[offset] = value;\n    }\n\n    __threadfence(); // make sure writes are globally visible\n    __syncthreads(); // if multiple warps in this block wrote to staging, make sure they're all done\n    bool is_last_block_done = mark_block_finished();\n\n    if (is_last_block_done) {\n      value = ident;\n      if (config.should_block_x_reduce()) {\n        uint32_t input_offset = threadIdx.x + threadIdx.y * blockDim.x;\n        uint32_t step = blockDim.x * blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      } else {\n        uint32_t input_offset = threadIdx.y;\n        uint32_t step = blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      }\n      value = block_y_reduce(value, shared_memory);\n      if (config.should_block_x_reduce()) {\n        value = block_x_reduce<output_vec_size>(value, shared_memory);\n      }\n      if (should_store) {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::translate_idx(value[i], base_idx);\n          }\n        }\n\n        if (acc == nullptr) {\n          if (accumulate) {\n            value = accumulate_in_output<output_vec_size>(out, value);\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              *(out[i]) = get_accumulated_output(out[i], value[i]);\n            }\n          }\n        } else {\n          if (accumulate) {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              value[i] = reducer::combine((*acc)[i], value[i]);\n            }\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            *acc = value;\n          }\n        }\n      }\n    }\n\n    return value;\n  }\n};\n\nextern \"C\"\n__launch_bounds__(512, 4)\n__global__ void reduction_prod_kernel(ReduceJitOp r){\n  r.run();\n}\nnvrtc: error: failed to open libnvrtc-builtins.so.12.1.\n  Make sure that libnvrtc-builtins.so.12.1 is installed correctly.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b768cc97b2fd>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpeft_model_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mpeft_model_text_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meos_token_id_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 unfinished_sequences = unfinished_sequences.mul(\n\u001b[0;32m-> 2252\u001b[0;31m                     \u001b[0mnext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_token_id_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_token_id_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m                 )\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n  #ifdef __HIPCC__\n  #define ERROR_UNSUPPORTED_CAST ;\n  // corresponds to aten/src/ATen/native/cuda/thread_constants.h\n  #define CUDA_OR_ROCM_NUM_THREADS 256\n  // corresponds to aten/src/ATen/cuda/detail/OffsetCalculator.cuh\n  #define MAX_DIMS 16\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  #else\n  //TODO use _assert_fail, because assert is disabled in non-debug builds\n  #define ERROR_UNSUPPORTED_CAST assert(false);\n  #define CUDA_OR_ROCM_NUM_THREADS 128\n  #define MAX_DIMS 25\n  #endif\n  #define POS_INFINITY __int_as_float(0x7f800000)\n  #define INFINITY POS_INFINITY\n  #define NEG_INFINITY __int_as_float(0xff800000)\n  #define NAN __int_as_float(0x7fffffff)\n\n  typedef long long int int64_t;\n  typedef unsigned int uint32_t;\n  typedef signed char int8_t;\n  typedef unsigned char uint8_t;  // NOTE: this MUST be \"unsigned char\"! \"char\" is equivalent to \"signed char\"\n  typedef short int16_t;\n  static_assert(sizeof(int64_t) == 8, \"expected size does not match\");\n  static_assert(sizeof(uint32_t) == 4, \"expected size does not match\");\n  static_assert(sizeof(int8_t) == 1, \"expected size does not match\");\n  constexpr int num_threads = CUDA_OR_ROCM_NUM_THREADS;\n  constexpr int thread_work_size = 4; // TODO: make template substitution once we decide where those vars live\n  constexpr int block_work_size = thread_work_size * num_threads;\n\n  \n  \n  \n  namespace std {\n  \n  using ::signbit;\n  using ::isfinite;\n  using ::isinf;\n  using ::isnan;\n  \n  using ::abs;\n  \n  using ::acos;\n  using ::acosf;\n  using ::asin;\n  using ::asinf;\n  using ::atan;\n  using ::atanf;\n  using ::atan2;\n  using ::atan2f;\n  using ::ceil;\n  using ::ceilf;\n  using ::cos;\n  using ::cosf;\n  using ::cosh;\n  using ::coshf;\n  \n  using ::exp;\n  using ::expf;\n  \n  using ::fabs;\n  using ::fabsf;\n  using ::floor;\n  using ::floorf;\n  \n  using ::fmod;\n  using ::fmodf;\n  \n  using ::frexp;\n  using ::frexpf;\n  using ::ldexp;\n  using ::ldexpf;\n  \n  using ::log;\n  using ::logf;\n  \n  using ::log10;\n  using ::log10f;\n  using ::modf;\n  using ::modff;\n  \n  using ::pow;\n  using ::powf;\n  \n  using ::sin;\n  using ::sinf;\n  using ::sinh;\n  using ::sinhf;\n  \n  using ::sqrt;\n  using ::sqrtf;\n  using ::tan;\n  using ::tanf;\n  \n  using ::tanh;\n  using ::tanhf;\n  \n  using ::acosh;\n  using ::acoshf;\n  using ::asinh;\n  using ::asinhf;\n  using ::atanh;\n  using ::atanhf;\n  using ::cbrt;\n  using ::cbrtf;\n  \n  using ::copysign;\n  using ::copysignf;\n  \n  using ::erf;\n  using ::erff;\n  using ::erfc;\n  using ::erfcf;\n  using ::exp2;\n  using ::exp2f;\n  using ::expm1;\n  using ::expm1f;\n  using ::fdim;\n  using ::fdimf;\n  using ::fmaf;\n  using ::fma;\n  using ::fmax;\n  using ::fmaxf;\n  using ::fmin;\n  using ::fminf;\n  using ::hypot;\n  using ::hypotf;\n  using ::ilogb;\n  using ::ilogbf;\n  using ::lgamma;\n  using ::lgammaf;\n  using ::llrint;\n  using ::llrintf;\n  using ::llround;\n  using ::llroundf;\n  using ::log1p;\n  using ::log1pf;\n  using ::log2;\n  using ::log2f;\n  using ::logb;\n  using ::logbf;\n  using ::lrint;\n  using ::lrintf;\n  using ::lround;\n  using ::lroundf;\n  \n  using ::nan;\n  using ::nanf;\n  \n  using ::nearbyint;\n  using ::nearbyintf;\n  using ::nextafter;\n  using ::nextafterf;\n  using ::remainder;\n  using ::remainderf;\n  using ::remquo;\n  using ::remquof;\n  using ::rint;\n  using ::rintf;\n  using ::round;\n  using ::roundf;\n  using ::scalbln;\n  using ::scalblnf;\n  using ::scalbn;\n  using ::scalbnf;\n  using ::tgamma;\n  using ::tgammaf;\n  using ::trunc;\n  using ::truncf;\n  \n  } // namespace std\n  \n  \n\n  // NB: Order matters for this macro; it is relied upon in\n  // _promoteTypesLookup and the serialization format.\n  // Note, some types have ctype as void because we don't support them in codegen\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_) \\\n  _(uint8_t, Byte) /* 0 */                               \\\n  _(int8_t, Char) /* 1 */                                \\\n  _(int16_t, Short) /* 2 */                              \\\n  _(int, Int) /* 3 */                                    \\\n  _(int64_t, Long) /* 4 */                               \\\n  _(at::Half, Half) /* 5 */                                  \\\n  _(float, Float) /* 6 */                                \\\n  _(double, Double) /* 7 */                              \\\n  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \\\n  _(std::complex<float>, ComplexFloat) /* 9 */                          \\\n  _(std::complex<double>, ComplexDouble) /* 10 */                         \\\n  _(bool, Bool) /* 11 */                                 \\\n  _(void, QInt8) /* 12 */                          \\\n  _(void, QUInt8) /* 13 */                        \\\n  _(void, QInt32) /* 14 */                        \\\n  _(at::BFloat16, BFloat16) /* 15 */                             \\\n\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_QINT(_)       \\\n  _(uint8_t, Byte)                                                 \\\n  _(int8_t, Char)                                                  \\\n  _(int16_t, Short)                                                \\\n  _(int, Int)                                                      \\\n  _(int64_t, Long)                                                 \\\n  _(at::Half, Half)                                                \\\n  _(float, Float)                                                  \\\n  _(double, Double)                                                \\\n  _(std::complex<at::Half>, ComplexHalf)                           \\\n  _(std::complex<float>, ComplexFloat)                             \\\n  _(std::complex<double>, ComplexDouble)                           \\\n  _(bool, Bool)                                                    \\\n  _(at::BFloat16, BFloat16)\n\n\n  enum class ScalarType : int8_t {\n  #define DEFINE_ENUM(_1, n) n,\n  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ENUM)\n  #undef DEFINE_ENUM\n      Undefined,\n  NumOptions\n  };\n\n  template <typename T, int size>\n  struct Array {\n  T data[size];\n\n  __device__ T operator[](int i) const {\n      return data[i];\n  }\n  __device__ T& operator[](int i) {\n      return data[i];\n  }\n  Array() = default;\n  Array(const Array&) = default;\n  Array& operator=(const Array&) = default;\n  __device__ Array(T x) {\n    for (int i = 0; i < size; i++) {\n      data[i] = x;\n    }\n  }\n  };\n\n  \n  \n  \n  \n  \n\n\n\n  template <typename T>\n  struct DivMod {\n  T div;\n  T mod;\n\n  __device__ DivMod(T _div, T _mod) {\n      div = _div;\n      mod = _mod;\n  }\n  };\n\n  //<unsigned int>\n  struct IntDivider {\n  IntDivider() = default;\n\n  __device__ inline unsigned int div(unsigned int n) const {\n  unsigned int t = __umulhi(n, m1);\n  return (t + n) >> shift;\n  }\n\n  __device__ inline unsigned int mod(unsigned int n) const {\n  return n - div(n) * divisor;\n  }\n\n  __device__ inline DivMod<unsigned int> divmod(unsigned int n) const {\n  unsigned int q = div(n);\n  return DivMod<unsigned int>(q, n - q * divisor);\n  }\n\n  unsigned int divisor;  // d above.\n  unsigned int m1;  // Magic number: m' above.\n  unsigned int shift;  // Shift amounts.\n  };\n\n  template <int NARGS>\n  struct TrivialOffsetCalculator {\n    // The offset for each argument. Wrapper around fixed-size array.\n    // The offsets are in # of elements, not in bytes.\n    Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; arg++) {\n        offsets[arg] = linear_idx;\n      }\n      return offsets;\n    }\n  };\n\n  template<int NARGS>\n  struct OffsetCalculator {\n  OffsetCalculator() = default;\n  __device__ __forceinline__ Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n      offsets[arg] = 0;\n      }\n\n      #pragma unroll\n      for (int dim = 0; dim < MAX_DIMS; ++dim) {\n      if (dim == dims) {\n          break;\n      }\n\n      auto divmod = sizes_[dim].divmod(linear_idx);\n      linear_idx = divmod.div;\n\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n          offsets[arg] += divmod.mod * strides_[dim][arg];\n      }\n      //printf(\"offset calc thread dim size stride offset %d %d %d %d %d %d %d %d\\n\",\n      //threadIdx.x, dim, sizes_[dim].divisor, strides_[dim][0], offsets[0], linear_idx, divmod.div, divmod.mod);\n      }\n      return offsets;\n  }\n\n    int dims;\n    IntDivider sizes_[MAX_DIMS];\n    // NOTE: this approach will not support nInputs == 0\n    unsigned int strides_[MAX_DIMS][NARGS];\n  };\n\n\n\n  #define C10_HOST_DEVICE __host__ __device__\n  #define C10_DEVICE __device__\n  #if defined(__clang__) && defined(__HIP__)\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  // until ROCm support for kernel asserts is restored\n  #define assert(expr) (static_cast<void>(0))\n  #endif\n\n  template <typename T>\n  __device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n  #if defined(__clang__) && defined(__HIP__)\n    return __shfl_down(value, delta, width);\n  #else\n    return __shfl_down_sync(mask, value, delta, width);\n  #endif\n  }\n\n\n  #if 0\n  template <typename T>\n  __device__ __forceinline__ std::complex<T> WARP_SHFL_DOWN(std::complex<T> value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n    return std::complex<T>(\n  #if defined(__clang__) && defined(__HIP__)\n        __shfl_down(value.real(), delta, width),\n        __shfl_down(value.imag(), delta, width));\n  #else\n        __shfl_down_sync(mask, value.real(), delta, width),\n        __shfl_down_sync(mask, value.imag(), delta, width));\n  #endif\n  }\n  #endif\n\n  // aligned vector generates vectorized load/store on CUDA\n  template<typename scalar_t, int vec_size>\n  struct alignas(sizeof(scalar_t) * vec_size) aligned_vector {\n    scalar_t val[vec_size];\n  };\n\n\n  C10_HOST_DEVICE static void reduce_fraction(size_t &numerator, size_t &denominator) {\n    // get GCD of num and denom using Euclid's algorithm.\n    // Can replace this with std::gcd if we ever support c++17.\n    size_t a = denominator;\n    size_t b = numerator;\n    while (b != 0) {\n        a %= b;\n        // swap(a,b)\n        size_t tmp = a;\n        a = b;\n        b = tmp;\n    }\n\n    // a is now the GCD\n    numerator /= a;\n    denominator /= a;\n  }\n\n\n\n\n  struct ReduceConfig {\n  //has to match host-side ReduceConfig in the eager code\n  static constexpr int BLOCK_X = 0;\n  static constexpr int BLOCK_Y = 1;\n  static constexpr int CTA = 2;\n\n  static constexpr int input_vec_size = 4;\n  int element_size_bytes;\n  int num_inputs;\n  int num_outputs;\n  int step_input = 1;\n  int step_output = 1;\n  int ctas_per_output = 1;\n  int input_mult[3] = {0, 0, 0};\n  int output_mult[2] = {0, 0};\n\n  int block_width;\n  int block_height;\n  int num_threads;\n\n  bool vectorize_input = false;\n  int output_vec_size = 1;\n\n  C10_HOST_DEVICE bool should_block_x_reduce() const {\n    return input_mult[BLOCK_X] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_block_y_reduce() const {\n    return input_mult[BLOCK_Y] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_global_reduce() const {\n    return input_mult[CTA] != 0;\n  }\n\n  C10_DEVICE bool should_store(int output_idx) const {\n    return output_idx < num_outputs &&\n      (!should_block_x_reduce() || threadIdx.x == 0) &&\n      (!should_block_y_reduce() || threadIdx.y == 0);\n  }\n\n  C10_DEVICE bool should_reduce_tail() const {\n    return (!should_block_y_reduce() || threadIdx.y == 0) &&\n      (!should_global_reduce() || blockIdx.y == 0);\n  }\n\n  C10_HOST_DEVICE int input_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta2 = blockIdx.y;\n    return (lane * input_mult[BLOCK_X] +\n            warp * input_mult[BLOCK_Y] +\n            cta2 * input_mult[CTA]);\n  }\n\n  template <int output_vec_size>\n  C10_HOST_DEVICE int output_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta1 = blockIdx.x;\n    return (lane * output_mult[BLOCK_X] +\n            warp * output_mult[BLOCK_Y] +\n            cta1 * step_output) * output_vec_size;\n  }\n\n  C10_DEVICE int shared_memory_offset(int offset) const {\n    return threadIdx.x + (threadIdx.y + offset) * blockDim.x;\n  }\n\n  C10_DEVICE int staging_memory_offset(int cta2) const {\n    int offset = cta2 + blockIdx.x * gridDim.y;\n    if (!should_block_x_reduce()) {\n      offset = threadIdx.x + offset * blockDim.x;\n    }\n    return offset;\n  }\n\n\n  };\n\n\n//TODO this will need to be different for more generic reduction functions\nnamespace reducer {\n\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n\n  inline __device__ arg_t combine(arg_t a, arg_t b) { return a * b; }\n\n  inline __device__ out_scalar_t project(arg_t arg) {\n    return (out_scalar_t) arg;\n  }\n\n  inline __device__ arg_t warp_shfl_down(arg_t arg, int offset) {\n    return WARP_SHFL_DOWN(arg, offset);\n  }\n\n  inline __device__ arg_t translate_idx(arg_t acc, int64_t /*idx*/) {\n    return acc;\n  }\n\n  // wrap a normal reduction that ignores the index\n  inline __device__ arg_t reduce(arg_t acc, arg_t val, int64_t idx) {\n     return combine(acc, val);\n  }\n}\n\n\nstruct ReduceJitOp {\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n  using InputCalculator = OffsetCalculator<1>;\n  using OutputCalculator = OffsetCalculator<2>;\n\n//   static constexpr bool can_accumulate_in_output =\n//     std::is_convertible<arg_t, out_scalar_t>::value\n//     && std::is_convertible<out_scalar_t, arg_t>::value;\n\n  static constexpr int input_vec_size = ReduceConfig::input_vec_size;\n\n  arg_t ident;\n  ReduceConfig config;\n  InputCalculator input_calc;\n  OutputCalculator output_calc;\n  const void* src;\n  const char* dst[2]; //it accepts at most two destinations\n  // acc_buf used for accumulation among sub Tensor Iterator when accumulation on\n  // output is not permissible\n  void* acc_buf;\n  // cta_buf used for accumulation between blocks during global reduction\n  void* cta_buf;\n  int* semaphores;\n  int64_t base_idx;\n  bool accumulate;\n  bool final_output;\n  int noutputs;\n\n\n  C10_DEVICE void run() const {\n    extern __shared__ char shared_memory[];\n    uint32_t output_idx = config.output_idx<1>();\n    uint32_t input_idx = config.input_idx();\n    auto base_offsets1 = output_calc.get(output_idx)[1];\n\n    using arg_vec_t = Array<arg_t, 1>;\n    arg_vec_t value;\n\n    if (output_idx < config.num_outputs && input_idx < config.num_inputs) {\n      const scalar_t* input_slice = (const scalar_t*)((const char*)src + base_offsets1);\n\n      value = thread_reduce<1>(input_slice);\n    }\n\n    if (config.should_block_y_reduce()) {\n      value = block_y_reduce<1>(value, shared_memory);\n    }\n    if (config.should_block_x_reduce()) {\n      value = block_x_reduce<1>(value, shared_memory);\n    }\n\n    using out_ptr_vec_t = Array<out_scalar_t*, 1>;\n    using offset_vec_t = Array<uint32_t, 1>;\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < 1; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    arg_vec_t* acc = nullptr;\n    if (acc_buf != nullptr) {\n      size_t numerator = sizeof(arg_t);\n      size_t denominator = sizeof(out_scalar_t);\n      reduce_fraction(numerator, denominator);\n      acc = (arg_vec_t*)((char*)acc_buf + (base_offsets[0] * numerator / denominator));\n    }\n\n    if (config.should_global_reduce()) {\n      value = global_reduce<1>(value, acc, shared_memory);\n    } else if (config.should_store(output_idx)) {\n      if (accumulate) {\n        #pragma unroll\n        for (int i = 0; i < 1; i++) {\n          value[i] = reducer::translate_idx(value[i], base_idx);\n        }\n      }\n\n      if (acc == nullptr) {\n        if (accumulate) {\n          value = accumulate_in_output<1>(out, value);\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            *(out[i]) = get_accumulated_output(out[i], value[i]);\n          }\n        }\n      } else {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            value[i] = reducer::combine((*acc)[i], value[i]);\n          }\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          *acc = value;\n        }\n      }\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce(const scalar_t* data) const {\n    if (config.vectorize_input) {\n      assert(output_vec_size == 1);\n      // reduce at the header of input_slice where memory is not aligned,\n      // so that thread_reduce will have an aligned memory to work on.\n      return {input_vectorized_thread_reduce_impl(data)};\n    } else {\n      uint32_t element_stride = input_calc.strides_[0][0] / sizeof(scalar_t);\n      bool is_contiguous = (input_calc.dims == 1 && element_stride == 1);\n      if (is_contiguous) {\n        return thread_reduce_impl<output_vec_size>(data, [](uint32_t idx) { return idx; });\n      } else if (input_calc.dims == 1) {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return idx * element_stride; });\n      } else {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return input_calc.get(idx)[0] / sizeof(scalar_t); });\n      }\n    }\n  }\n\n  C10_DEVICE arg_t input_vectorized_thread_reduce_impl(const scalar_t* data) const {\n    uint32_t end = config.num_inputs;\n\n    // Handle the head of input slice where data is not aligned\n    arg_t value = ident;\n    constexpr int align_bytes = alignof(aligned_vector<scalar_t, input_vec_size>);\n    constexpr int align_elements = align_bytes / sizeof(scalar_t);\n    int shift = ((int64_t)data) % align_bytes / sizeof(scalar_t);\n    if (shift > 0) {\n      data -= shift;\n      end += shift;\n      if(threadIdx.x >= shift && threadIdx.x < align_elements && config.should_reduce_tail()){\n        value = reducer::reduce(value, data[threadIdx.x], threadIdx.x - shift);\n      }\n      end -= align_elements;\n      data += align_elements;\n      shift = align_elements - shift;\n    }\n\n    // Do the vectorized reduction\n    using load_t = aligned_vector<scalar_t, input_vec_size>;\n\n    uint32_t idx = config.input_idx();\n    const uint32_t stride = config.step_input;\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_t value_list[input_vec_size];\n    value_list[0] = value;\n\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[i] = ident;\n    }\n\n    scalar_t values[input_vec_size];\n\n    load_t *values_vector = reinterpret_cast<load_t*>(&values[0]);\n\n    while (idx * input_vec_size + input_vec_size - 1 < end) {\n      *values_vector = reinterpret_cast<const load_t*>(data)[idx];\n      #pragma unroll\n      for (uint32_t i = 0; i < input_vec_size; i++) {\n        value_list[i] = reducer::reduce(value_list[i], values[i], shift + idx * input_vec_size + i);\n      }\n      idx += stride;\n    }\n\n    // tail\n    uint32_t tail_start = end - end % input_vec_size;\n    if (config.should_reduce_tail()) {\n      int idx = tail_start + threadIdx.x;\n      if (idx < end) {\n        value_list[0] = reducer::reduce(value_list[0], data[idx], idx + shift);\n      }\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[0] = reducer::combine(value_list[0], value_list[i]);\n    }\n    return value_list[0];\n  }\n\n  template <int output_vec_size, typename offset_calc_t>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce_impl(const scalar_t* data_, offset_calc_t calc) const {\n    uint32_t idx = config.input_idx();\n    const uint32_t end = config.num_inputs;\n    const uint32_t stride = config.step_input;\n    const int vt0=4;\n\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using load_t = aligned_vector<scalar_t, output_vec_size>;\n    const load_t* data = reinterpret_cast<const load_t*>(data_);\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_vec_t value_list[vt0];\n\n    #pragma unroll\n    for (int i = 0; i < vt0; i++) {\n      #pragma unroll\n      for (int j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = ident;\n      }\n    }\n\n    load_t values[vt0];\n\n    while (idx + (vt0 - 1) * stride < end) {\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        values[i] = data[calc(idx + i * stride) / output_vec_size];\n      }\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        #pragma unroll\n        for (uint32_t j = 0; j < output_vec_size; j++) {\n          value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx + i * stride);\n        }\n      }\n      idx += stride * vt0;\n    }\n\n    // tail\n    int idx_ = idx;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      values[i] = data[calc(idx) / output_vec_size];\n      idx += stride;\n    }\n    idx = idx_;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx);\n      }\n      idx += stride;\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < vt0; i++) {\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[0][j] = reducer::combine(value_list[0][j], value_list[i][j]);\n      }\n    }\n    return value_list[0];\n  }\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_x_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    int dim_x = blockDim.x;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    if (dim_x > warpSize) {\n      int address_base = threadIdx.x + threadIdx.y*blockDim.x;\n      shared[address_base] = value;\n      for (int offset = dim_x/2; offset >= warpSize; offset >>= 1) {\n        __syncthreads();\n        if (threadIdx.x < offset && threadIdx.x + offset < blockDim.x) {\n          args_vec_t other = shared[address_base + offset];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], other[i]);\n          }\n          shared[address_base] = value;\n        }\n      }\n      dim_x = warpSize;\n    }\n\n    __syncthreads();\n\n    for (int offset = 1; offset < dim_x; offset <<= 1) {\n      #pragma unroll\n      for (int i = 0; i < output_vec_size; i++) {\n        arg_t other = reducer::warp_shfl_down(value[i], offset);\n        value[i] = reducer::combine(value[i], other);\n      }\n    }\n    return value;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_y_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    shared[config.shared_memory_offset(0)] = value;\n    for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) {\n      __syncthreads();\n      if (threadIdx.y < offset && threadIdx.y + offset < blockDim.y) {\n        args_vec_t other = shared[config.shared_memory_offset(offset)];\n        #pragma unroll\n        for (int i = 0; i < output_vec_size; i++) {\n          value[i] = reducer::combine(value[i], other[i]);\n        }\n        shared[config.shared_memory_offset(0)] = value;\n      }\n    }\n    return value;\n  }\n  \n\n  C10_DEVICE bool mark_block_finished() const {\n    __shared__ bool is_last_block_done_shared;\n\n    __syncthreads();\n    if (threadIdx.x == 0 && threadIdx.y == 0) {\n      int prev_blocks_finished = atomicAdd(&semaphores[blockIdx.x], 1);\n      is_last_block_done_shared = (prev_blocks_finished == gridDim.y - 1);\n    }\n\n    __syncthreads();\n\n    return is_last_block_done_shared;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> accumulate_in_output(\n    Array<out_scalar_t*, output_vec_size> out,\n    Array<arg_t, output_vec_size> value\n  ) const {\n    Array<arg_t, output_vec_size> ret;\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      ret[i] = reducer::combine(*(out[i]), value[i]);\n    }\n    return ret;\n  }\n\n\n  C10_DEVICE out_scalar_t get_accumulated_output(\n    out_scalar_t* out, arg_t value\n  ) const {\n    assert(!final_output);\n    return (out_scalar_t)value;\n  }\n\n  template<class T>\n  C10_DEVICE void set_results(const T x, const uint32_t base_offset) const {\n    assert(noutputs == 1);\n    auto res = (out_scalar_t*)((char*)dst[0] + base_offset);\n    *res = x;\n  }\n\n//TODO - multi-output reduction - we won't be able to use thrust::pair\n//just explicitly specify typed output reads/writes\n//Currently implemented for max of two outputs\n//   template<class T1, class T2>\n//   C10_DEVICE void set_results(const thrust::pair<T1, T2> x, const index_t base_offset) const {\n//     if (noutputs >= 1) {\n//       auto res0 = (T1*)((char*)dst[0] + base_offset);\n//       *res0 = x.first;\n//     }\n//     if (noutputs >= 2) {\n//       // base offset is computed assuming element size being sizeof(T1), so we need to make a\n//       // correction to obtain the correct base offset\n//       auto res1 = (T2*) ((char *) dst[1] + base_offset / sizeof(T1) * sizeof(T2));\n//       *res1 = x.second;\n//     }\n//   }\n\n  template <int output_vec_size>\n  C10_DEVICE void set_results_to_output(Array<arg_t, output_vec_size> value, Array<uint32_t, output_vec_size> base_offset) const {\n    assert(final_output);\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      set_results(reducer::project(value[i]), base_offset[i]);\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> global_reduce(Array<arg_t, output_vec_size> value, Array<arg_t, output_vec_size> *acc, char* shared_memory) const {\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using out_ptr_vec_t = Array<out_scalar_t*, output_vec_size>;\n    using offset_vec_t = Array<uint32_t, output_vec_size>;\n\n    arg_vec_t* reduce_buffer = (arg_vec_t*)cta_buf;\n    uint32_t output_idx = config.output_idx<output_vec_size>();\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    bool should_store = config.should_store(output_idx);\n    if (should_store) {\n      uint32_t offset = config.staging_memory_offset(blockIdx.y);\n      reduce_buffer[offset] = value;\n    }\n\n    __threadfence(); // make sure writes are globally visible\n    __syncthreads(); // if multiple warps in this block wrote to staging, make sure they're all done\n    bool is_last_block_done = mark_block_finished();\n\n    if (is_last_block_done) {\n      value = ident;\n      if (config.should_block_x_reduce()) {\n        uint32_t input_offset = threadIdx.x + threadIdx.y * blockDim.x;\n        uint32_t step = blockDim.x * blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      } else {\n        uint32_t input_offset = threadIdx.y;\n        uint32_t step = blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      }\n      value = block_y_reduce(value, shared_memory);\n      if (config.should_block_x_reduce()) {\n        value = block_x_reduce<output_vec_size>(value, shared_memory);\n      }\n      if (should_store) {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::translate_idx(value[i], base_idx);\n          }\n        }\n\n        if (acc == nullptr) {\n          if (accumulate) {\n            value = accumulate_in_output<output_vec_size>(out, value);\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              *(out[i]) = get_accumulated_output(out[i], value[i]);\n            }\n          }\n        } else {\n          if (accumulate) {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              value[i] = reducer::combine((*acc)[i], value[i]);\n            }\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            *acc = value;\n          }\n        }\n      }\n    }\n\n    return value;\n  }\n};\n\nextern \"C\"\n__launch_bounds__(512, 4)\n__global__ void reduction_prod_kernel(ReduceJitOp r){\n  r.run();\n}\nnvrtc: error: failed to open libnvrtc-builtins.so.12.1.\n  Make sure that libnvrtc-builtins.so.12.1 is installed correctly."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 512  # Adjust according to the maximum sequence length supported by your model\n",
        "\n",
        "# Truncate or split the input sequence\n",
        "dialogue_tokens = tokenizer(dialogue, padding='max_length', max_length=max_sequence_length, return_tensors=\"pt\")\n",
        "prompt_tokens = tokenizer(prompt, padding='max_length', max_length=max_sequence_length, return_tensors=\"pt\")\n",
        "\n",
        "# Concatenate the truncated/split input sequences\n",
        "input_ids = torch.cat([dialogue_tokens.input_ids, prompt_tokens.input_ids], dim=1)\n",
        "attention_mask = torch.cat([dialogue_tokens.attention_mask, prompt_tokens.attention_mask], dim=1)\n",
        "\n",
        "# Ensure the input tensors are on the same device as the model's parameters\n",
        "input_ids = input_ids.to(next(model1.parameters()).device)\n",
        "attention_mask = attention_mask.to(next(model1.parameters()).device)\n",
        "\n",
        "# Generate the summary\n",
        "peft_model_outputs = model1.generate(input_ids=input_ids, attention_mask=attention_mask, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the results\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
        "\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: {peft_model_text_output}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mzCUTE6uyhx9",
        "outputId": "fb664b15-0f5e-4d66-bc0f-7d79bddc03ac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "\n  #ifdef __HIPCC__\n  #define ERROR_UNSUPPORTED_CAST ;\n  // corresponds to aten/src/ATen/native/cuda/thread_constants.h\n  #define CUDA_OR_ROCM_NUM_THREADS 256\n  // corresponds to aten/src/ATen/cuda/detail/OffsetCalculator.cuh\n  #define MAX_DIMS 16\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  #else\n  //TODO use _assert_fail, because assert is disabled in non-debug builds\n  #define ERROR_UNSUPPORTED_CAST assert(false);\n  #define CUDA_OR_ROCM_NUM_THREADS 128\n  #define MAX_DIMS 25\n  #endif\n  #define POS_INFINITY __int_as_float(0x7f800000)\n  #define INFINITY POS_INFINITY\n  #define NEG_INFINITY __int_as_float(0xff800000)\n  #define NAN __int_as_float(0x7fffffff)\n\n  typedef long long int int64_t;\n  typedef unsigned int uint32_t;\n  typedef signed char int8_t;\n  typedef unsigned char uint8_t;  // NOTE: this MUST be \"unsigned char\"! \"char\" is equivalent to \"signed char\"\n  typedef short int16_t;\n  static_assert(sizeof(int64_t) == 8, \"expected size does not match\");\n  static_assert(sizeof(uint32_t) == 4, \"expected size does not match\");\n  static_assert(sizeof(int8_t) == 1, \"expected size does not match\");\n  constexpr int num_threads = CUDA_OR_ROCM_NUM_THREADS;\n  constexpr int thread_work_size = 4; // TODO: make template substitution once we decide where those vars live\n  constexpr int block_work_size = thread_work_size * num_threads;\n\n  \n  \n  \n  namespace std {\n  \n  using ::signbit;\n  using ::isfinite;\n  using ::isinf;\n  using ::isnan;\n  \n  using ::abs;\n  \n  using ::acos;\n  using ::acosf;\n  using ::asin;\n  using ::asinf;\n  using ::atan;\n  using ::atanf;\n  using ::atan2;\n  using ::atan2f;\n  using ::ceil;\n  using ::ceilf;\n  using ::cos;\n  using ::cosf;\n  using ::cosh;\n  using ::coshf;\n  \n  using ::exp;\n  using ::expf;\n  \n  using ::fabs;\n  using ::fabsf;\n  using ::floor;\n  using ::floorf;\n  \n  using ::fmod;\n  using ::fmodf;\n  \n  using ::frexp;\n  using ::frexpf;\n  using ::ldexp;\n  using ::ldexpf;\n  \n  using ::log;\n  using ::logf;\n  \n  using ::log10;\n  using ::log10f;\n  using ::modf;\n  using ::modff;\n  \n  using ::pow;\n  using ::powf;\n  \n  using ::sin;\n  using ::sinf;\n  using ::sinh;\n  using ::sinhf;\n  \n  using ::sqrt;\n  using ::sqrtf;\n  using ::tan;\n  using ::tanf;\n  \n  using ::tanh;\n  using ::tanhf;\n  \n  using ::acosh;\n  using ::acoshf;\n  using ::asinh;\n  using ::asinhf;\n  using ::atanh;\n  using ::atanhf;\n  using ::cbrt;\n  using ::cbrtf;\n  \n  using ::copysign;\n  using ::copysignf;\n  \n  using ::erf;\n  using ::erff;\n  using ::erfc;\n  using ::erfcf;\n  using ::exp2;\n  using ::exp2f;\n  using ::expm1;\n  using ::expm1f;\n  using ::fdim;\n  using ::fdimf;\n  using ::fmaf;\n  using ::fma;\n  using ::fmax;\n  using ::fmaxf;\n  using ::fmin;\n  using ::fminf;\n  using ::hypot;\n  using ::hypotf;\n  using ::ilogb;\n  using ::ilogbf;\n  using ::lgamma;\n  using ::lgammaf;\n  using ::llrint;\n  using ::llrintf;\n  using ::llround;\n  using ::llroundf;\n  using ::log1p;\n  using ::log1pf;\n  using ::log2;\n  using ::log2f;\n  using ::logb;\n  using ::logbf;\n  using ::lrint;\n  using ::lrintf;\n  using ::lround;\n  using ::lroundf;\n  \n  using ::nan;\n  using ::nanf;\n  \n  using ::nearbyint;\n  using ::nearbyintf;\n  using ::nextafter;\n  using ::nextafterf;\n  using ::remainder;\n  using ::remainderf;\n  using ::remquo;\n  using ::remquof;\n  using ::rint;\n  using ::rintf;\n  using ::round;\n  using ::roundf;\n  using ::scalbln;\n  using ::scalblnf;\n  using ::scalbn;\n  using ::scalbnf;\n  using ::tgamma;\n  using ::tgammaf;\n  using ::trunc;\n  using ::truncf;\n  \n  } // namespace std\n  \n  \n\n  // NB: Order matters for this macro; it is relied upon in\n  // _promoteTypesLookup and the serialization format.\n  // Note, some types have ctype as void because we don't support them in codegen\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_) \\\n  _(uint8_t, Byte) /* 0 */                               \\\n  _(int8_t, Char) /* 1 */                                \\\n  _(int16_t, Short) /* 2 */                              \\\n  _(int, Int) /* 3 */                                    \\\n  _(int64_t, Long) /* 4 */                               \\\n  _(at::Half, Half) /* 5 */                                  \\\n  _(float, Float) /* 6 */                                \\\n  _(double, Double) /* 7 */                              \\\n  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \\\n  _(std::complex<float>, ComplexFloat) /* 9 */                          \\\n  _(std::complex<double>, ComplexDouble) /* 10 */                         \\\n  _(bool, Bool) /* 11 */                                 \\\n  _(void, QInt8) /* 12 */                          \\\n  _(void, QUInt8) /* 13 */                        \\\n  _(void, QInt32) /* 14 */                        \\\n  _(at::BFloat16, BFloat16) /* 15 */                             \\\n\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_QINT(_)       \\\n  _(uint8_t, Byte)                                                 \\\n  _(int8_t, Char)                                                  \\\n  _(int16_t, Short)                                                \\\n  _(int, Int)                                                      \\\n  _(int64_t, Long)                                                 \\\n  _(at::Half, Half)                                                \\\n  _(float, Float)                                                  \\\n  _(double, Double)                                                \\\n  _(std::complex<at::Half>, ComplexHalf)                           \\\n  _(std::complex<float>, ComplexFloat)                             \\\n  _(std::complex<double>, ComplexDouble)                           \\\n  _(bool, Bool)                                                    \\\n  _(at::BFloat16, BFloat16)\n\n\n  enum class ScalarType : int8_t {\n  #define DEFINE_ENUM(_1, n) n,\n  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ENUM)\n  #undef DEFINE_ENUM\n      Undefined,\n  NumOptions\n  };\n\n  template <typename T, int size>\n  struct Array {\n  T data[size];\n\n  __device__ T operator[](int i) const {\n      return data[i];\n  }\n  __device__ T& operator[](int i) {\n      return data[i];\n  }\n  Array() = default;\n  Array(const Array&) = default;\n  Array& operator=(const Array&) = default;\n  __device__ Array(T x) {\n    for (int i = 0; i < size; i++) {\n      data[i] = x;\n    }\n  }\n  };\n\n  \n  \n  \n  \n  \n\n\n\n  template <typename T>\n  struct DivMod {\n  T div;\n  T mod;\n\n  __device__ DivMod(T _div, T _mod) {\n      div = _div;\n      mod = _mod;\n  }\n  };\n\n  //<unsigned int>\n  struct IntDivider {\n  IntDivider() = default;\n\n  __device__ inline unsigned int div(unsigned int n) const {\n  unsigned int t = __umulhi(n, m1);\n  return (t + n) >> shift;\n  }\n\n  __device__ inline unsigned int mod(unsigned int n) const {\n  return n - div(n) * divisor;\n  }\n\n  __device__ inline DivMod<unsigned int> divmod(unsigned int n) const {\n  unsigned int q = div(n);\n  return DivMod<unsigned int>(q, n - q * divisor);\n  }\n\n  unsigned int divisor;  // d above.\n  unsigned int m1;  // Magic number: m' above.\n  unsigned int shift;  // Shift amounts.\n  };\n\n  template <int NARGS>\n  struct TrivialOffsetCalculator {\n    // The offset for each argument. Wrapper around fixed-size array.\n    // The offsets are in # of elements, not in bytes.\n    Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; arg++) {\n        offsets[arg] = linear_idx;\n      }\n      return offsets;\n    }\n  };\n\n  template<int NARGS>\n  struct OffsetCalculator {\n  OffsetCalculator() = default;\n  __device__ __forceinline__ Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n      offsets[arg] = 0;\n      }\n\n      #pragma unroll\n      for (int dim = 0; dim < MAX_DIMS; ++dim) {\n      if (dim == dims) {\n          break;\n      }\n\n      auto divmod = sizes_[dim].divmod(linear_idx);\n      linear_idx = divmod.div;\n\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n          offsets[arg] += divmod.mod * strides_[dim][arg];\n      }\n      //printf(\"offset calc thread dim size stride offset %d %d %d %d %d %d %d %d\\n\",\n      //threadIdx.x, dim, sizes_[dim].divisor, strides_[dim][0], offsets[0], linear_idx, divmod.div, divmod.mod);\n      }\n      return offsets;\n  }\n\n    int dims;\n    IntDivider sizes_[MAX_DIMS];\n    // NOTE: this approach will not support nInputs == 0\n    unsigned int strides_[MAX_DIMS][NARGS];\n  };\n\n\n\n  #define C10_HOST_DEVICE __host__ __device__\n  #define C10_DEVICE __device__\n  #if defined(__clang__) && defined(__HIP__)\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  // until ROCm support for kernel asserts is restored\n  #define assert(expr) (static_cast<void>(0))\n  #endif\n\n  template <typename T>\n  __device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n  #if defined(__clang__) && defined(__HIP__)\n    return __shfl_down(value, delta, width);\n  #else\n    return __shfl_down_sync(mask, value, delta, width);\n  #endif\n  }\n\n\n  #if 0\n  template <typename T>\n  __device__ __forceinline__ std::complex<T> WARP_SHFL_DOWN(std::complex<T> value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n    return std::complex<T>(\n  #if defined(__clang__) && defined(__HIP__)\n        __shfl_down(value.real(), delta, width),\n        __shfl_down(value.imag(), delta, width));\n  #else\n        __shfl_down_sync(mask, value.real(), delta, width),\n        __shfl_down_sync(mask, value.imag(), delta, width));\n  #endif\n  }\n  #endif\n\n  // aligned vector generates vectorized load/store on CUDA\n  template<typename scalar_t, int vec_size>\n  struct alignas(sizeof(scalar_t) * vec_size) aligned_vector {\n    scalar_t val[vec_size];\n  };\n\n\n  C10_HOST_DEVICE static void reduce_fraction(size_t &numerator, size_t &denominator) {\n    // get GCD of num and denom using Euclid's algorithm.\n    // Can replace this with std::gcd if we ever support c++17.\n    size_t a = denominator;\n    size_t b = numerator;\n    while (b != 0) {\n        a %= b;\n        // swap(a,b)\n        size_t tmp = a;\n        a = b;\n        b = tmp;\n    }\n\n    // a is now the GCD\n    numerator /= a;\n    denominator /= a;\n  }\n\n\n\n\n  struct ReduceConfig {\n  //has to match host-side ReduceConfig in the eager code\n  static constexpr int BLOCK_X = 0;\n  static constexpr int BLOCK_Y = 1;\n  static constexpr int CTA = 2;\n\n  static constexpr int input_vec_size = 4;\n  int element_size_bytes;\n  int num_inputs;\n  int num_outputs;\n  int step_input = 1;\n  int step_output = 1;\n  int ctas_per_output = 1;\n  int input_mult[3] = {0, 0, 0};\n  int output_mult[2] = {0, 0};\n\n  int block_width;\n  int block_height;\n  int num_threads;\n\n  bool vectorize_input = false;\n  int output_vec_size = 1;\n\n  C10_HOST_DEVICE bool should_block_x_reduce() const {\n    return input_mult[BLOCK_X] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_block_y_reduce() const {\n    return input_mult[BLOCK_Y] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_global_reduce() const {\n    return input_mult[CTA] != 0;\n  }\n\n  C10_DEVICE bool should_store(int output_idx) const {\n    return output_idx < num_outputs &&\n      (!should_block_x_reduce() || threadIdx.x == 0) &&\n      (!should_block_y_reduce() || threadIdx.y == 0);\n  }\n\n  C10_DEVICE bool should_reduce_tail() const {\n    return (!should_block_y_reduce() || threadIdx.y == 0) &&\n      (!should_global_reduce() || blockIdx.y == 0);\n  }\n\n  C10_HOST_DEVICE int input_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta2 = blockIdx.y;\n    return (lane * input_mult[BLOCK_X] +\n            warp * input_mult[BLOCK_Y] +\n            cta2 * input_mult[CTA]);\n  }\n\n  template <int output_vec_size>\n  C10_HOST_DEVICE int output_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta1 = blockIdx.x;\n    return (lane * output_mult[BLOCK_X] +\n            warp * output_mult[BLOCK_Y] +\n            cta1 * step_output) * output_vec_size;\n  }\n\n  C10_DEVICE int shared_memory_offset(int offset) const {\n    return threadIdx.x + (threadIdx.y + offset) * blockDim.x;\n  }\n\n  C10_DEVICE int staging_memory_offset(int cta2) const {\n    int offset = cta2 + blockIdx.x * gridDim.y;\n    if (!should_block_x_reduce()) {\n      offset = threadIdx.x + offset * blockDim.x;\n    }\n    return offset;\n  }\n\n\n  };\n\n\n//TODO this will need to be different for more generic reduction functions\nnamespace reducer {\n\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n\n  inline __device__ arg_t combine(arg_t a, arg_t b) { return a * b; }\n\n  inline __device__ out_scalar_t project(arg_t arg) {\n    return (out_scalar_t) arg;\n  }\n\n  inline __device__ arg_t warp_shfl_down(arg_t arg, int offset) {\n    return WARP_SHFL_DOWN(arg, offset);\n  }\n\n  inline __device__ arg_t translate_idx(arg_t acc, int64_t /*idx*/) {\n    return acc;\n  }\n\n  // wrap a normal reduction that ignores the index\n  inline __device__ arg_t reduce(arg_t acc, arg_t val, int64_t idx) {\n     return combine(acc, val);\n  }\n}\n\n\nstruct ReduceJitOp {\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n  using InputCalculator = OffsetCalculator<1>;\n  using OutputCalculator = OffsetCalculator<2>;\n\n//   static constexpr bool can_accumulate_in_output =\n//     std::is_convertible<arg_t, out_scalar_t>::value\n//     && std::is_convertible<out_scalar_t, arg_t>::value;\n\n  static constexpr int input_vec_size = ReduceConfig::input_vec_size;\n\n  arg_t ident;\n  ReduceConfig config;\n  InputCalculator input_calc;\n  OutputCalculator output_calc;\n  const void* src;\n  const char* dst[2]; //it accepts at most two destinations\n  // acc_buf used for accumulation among sub Tensor Iterator when accumulation on\n  // output is not permissible\n  void* acc_buf;\n  // cta_buf used for accumulation between blocks during global reduction\n  void* cta_buf;\n  int* semaphores;\n  int64_t base_idx;\n  bool accumulate;\n  bool final_output;\n  int noutputs;\n\n\n  C10_DEVICE void run() const {\n    extern __shared__ char shared_memory[];\n    uint32_t output_idx = config.output_idx<1>();\n    uint32_t input_idx = config.input_idx();\n    auto base_offsets1 = output_calc.get(output_idx)[1];\n\n    using arg_vec_t = Array<arg_t, 1>;\n    arg_vec_t value;\n\n    if (output_idx < config.num_outputs && input_idx < config.num_inputs) {\n      const scalar_t* input_slice = (const scalar_t*)((const char*)src + base_offsets1);\n\n      value = thread_reduce<1>(input_slice);\n    }\n\n    if (config.should_block_y_reduce()) {\n      value = block_y_reduce<1>(value, shared_memory);\n    }\n    if (config.should_block_x_reduce()) {\n      value = block_x_reduce<1>(value, shared_memory);\n    }\n\n    using out_ptr_vec_t = Array<out_scalar_t*, 1>;\n    using offset_vec_t = Array<uint32_t, 1>;\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < 1; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    arg_vec_t* acc = nullptr;\n    if (acc_buf != nullptr) {\n      size_t numerator = sizeof(arg_t);\n      size_t denominator = sizeof(out_scalar_t);\n      reduce_fraction(numerator, denominator);\n      acc = (arg_vec_t*)((char*)acc_buf + (base_offsets[0] * numerator / denominator));\n    }\n\n    if (config.should_global_reduce()) {\n      value = global_reduce<1>(value, acc, shared_memory);\n    } else if (config.should_store(output_idx)) {\n      if (accumulate) {\n        #pragma unroll\n        for (int i = 0; i < 1; i++) {\n          value[i] = reducer::translate_idx(value[i], base_idx);\n        }\n      }\n\n      if (acc == nullptr) {\n        if (accumulate) {\n          value = accumulate_in_output<1>(out, value);\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            *(out[i]) = get_accumulated_output(out[i], value[i]);\n          }\n        }\n      } else {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            value[i] = reducer::combine((*acc)[i], value[i]);\n          }\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          *acc = value;\n        }\n      }\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce(const scalar_t* data) const {\n    if (config.vectorize_input) {\n      assert(output_vec_size == 1);\n      // reduce at the header of input_slice where memory is not aligned,\n      // so that thread_reduce will have an aligned memory to work on.\n      return {input_vectorized_thread_reduce_impl(data)};\n    } else {\n      uint32_t element_stride = input_calc.strides_[0][0] / sizeof(scalar_t);\n      bool is_contiguous = (input_calc.dims == 1 && element_stride == 1);\n      if (is_contiguous) {\n        return thread_reduce_impl<output_vec_size>(data, [](uint32_t idx) { return idx; });\n      } else if (input_calc.dims == 1) {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return idx * element_stride; });\n      } else {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return input_calc.get(idx)[0] / sizeof(scalar_t); });\n      }\n    }\n  }\n\n  C10_DEVICE arg_t input_vectorized_thread_reduce_impl(const scalar_t* data) const {\n    uint32_t end = config.num_inputs;\n\n    // Handle the head of input slice where data is not aligned\n    arg_t value = ident;\n    constexpr int align_bytes = alignof(aligned_vector<scalar_t, input_vec_size>);\n    constexpr int align_elements = align_bytes / sizeof(scalar_t);\n    int shift = ((int64_t)data) % align_bytes / sizeof(scalar_t);\n    if (shift > 0) {\n      data -= shift;\n      end += shift;\n      if(threadIdx.x >= shift && threadIdx.x < align_elements && config.should_reduce_tail()){\n        value = reducer::reduce(value, data[threadIdx.x], threadIdx.x - shift);\n      }\n      end -= align_elements;\n      data += align_elements;\n      shift = align_elements - shift;\n    }\n\n    // Do the vectorized reduction\n    using load_t = aligned_vector<scalar_t, input_vec_size>;\n\n    uint32_t idx = config.input_idx();\n    const uint32_t stride = config.step_input;\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_t value_list[input_vec_size];\n    value_list[0] = value;\n\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[i] = ident;\n    }\n\n    scalar_t values[input_vec_size];\n\n    load_t *values_vector = reinterpret_cast<load_t*>(&values[0]);\n\n    while (idx * input_vec_size + input_vec_size - 1 < end) {\n      *values_vector = reinterpret_cast<const load_t*>(data)[idx];\n      #pragma unroll\n      for (uint32_t i = 0; i < input_vec_size; i++) {\n        value_list[i] = reducer::reduce(value_list[i], values[i], shift + idx * input_vec_size + i);\n      }\n      idx += stride;\n    }\n\n    // tail\n    uint32_t tail_start = end - end % input_vec_size;\n    if (config.should_reduce_tail()) {\n      int idx = tail_start + threadIdx.x;\n      if (idx < end) {\n        value_list[0] = reducer::reduce(value_list[0], data[idx], idx + shift);\n      }\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[0] = reducer::combine(value_list[0], value_list[i]);\n    }\n    return value_list[0];\n  }\n\n  template <int output_vec_size, typename offset_calc_t>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce_impl(const scalar_t* data_, offset_calc_t calc) const {\n    uint32_t idx = config.input_idx();\n    const uint32_t end = config.num_inputs;\n    const uint32_t stride = config.step_input;\n    const int vt0=4;\n\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using load_t = aligned_vector<scalar_t, output_vec_size>;\n    const load_t* data = reinterpret_cast<const load_t*>(data_);\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_vec_t value_list[vt0];\n\n    #pragma unroll\n    for (int i = 0; i < vt0; i++) {\n      #pragma unroll\n      for (int j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = ident;\n      }\n    }\n\n    load_t values[vt0];\n\n    while (idx + (vt0 - 1) * stride < end) {\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        values[i] = data[calc(idx + i * stride) / output_vec_size];\n      }\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        #pragma unroll\n        for (uint32_t j = 0; j < output_vec_size; j++) {\n          value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx + i * stride);\n        }\n      }\n      idx += stride * vt0;\n    }\n\n    // tail\n    int idx_ = idx;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      values[i] = data[calc(idx) / output_vec_size];\n      idx += stride;\n    }\n    idx = idx_;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx);\n      }\n      idx += stride;\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < vt0; i++) {\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[0][j] = reducer::combine(value_list[0][j], value_list[i][j]);\n      }\n    }\n    return value_list[0];\n  }\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_x_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    int dim_x = blockDim.x;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    if (dim_x > warpSize) {\n      int address_base = threadIdx.x + threadIdx.y*blockDim.x;\n      shared[address_base] = value;\n      for (int offset = dim_x/2; offset >= warpSize; offset >>= 1) {\n        __syncthreads();\n        if (threadIdx.x < offset && threadIdx.x + offset < blockDim.x) {\n          args_vec_t other = shared[address_base + offset];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], other[i]);\n          }\n          shared[address_base] = value;\n        }\n      }\n      dim_x = warpSize;\n    }\n\n    __syncthreads();\n\n    for (int offset = 1; offset < dim_x; offset <<= 1) {\n      #pragma unroll\n      for (int i = 0; i < output_vec_size; i++) {\n        arg_t other = reducer::warp_shfl_down(value[i], offset);\n        value[i] = reducer::combine(value[i], other);\n      }\n    }\n    return value;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_y_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    shared[config.shared_memory_offset(0)] = value;\n    for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) {\n      __syncthreads();\n      if (threadIdx.y < offset && threadIdx.y + offset < blockDim.y) {\n        args_vec_t other = shared[config.shared_memory_offset(offset)];\n        #pragma unroll\n        for (int i = 0; i < output_vec_size; i++) {\n          value[i] = reducer::combine(value[i], other[i]);\n        }\n        shared[config.shared_memory_offset(0)] = value;\n      }\n    }\n    return value;\n  }\n  \n\n  C10_DEVICE bool mark_block_finished() const {\n    __shared__ bool is_last_block_done_shared;\n\n    __syncthreads();\n    if (threadIdx.x == 0 && threadIdx.y == 0) {\n      int prev_blocks_finished = atomicAdd(&semaphores[blockIdx.x], 1);\n      is_last_block_done_shared = (prev_blocks_finished == gridDim.y - 1);\n    }\n\n    __syncthreads();\n\n    return is_last_block_done_shared;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> accumulate_in_output(\n    Array<out_scalar_t*, output_vec_size> out,\n    Array<arg_t, output_vec_size> value\n  ) const {\n    Array<arg_t, output_vec_size> ret;\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      ret[i] = reducer::combine(*(out[i]), value[i]);\n    }\n    return ret;\n  }\n\n\n  C10_DEVICE out_scalar_t get_accumulated_output(\n    out_scalar_t* out, arg_t value\n  ) const {\n    assert(!final_output);\n    return (out_scalar_t)value;\n  }\n\n  template<class T>\n  C10_DEVICE void set_results(const T x, const uint32_t base_offset) const {\n    assert(noutputs == 1);\n    auto res = (out_scalar_t*)((char*)dst[0] + base_offset);\n    *res = x;\n  }\n\n//TODO - multi-output reduction - we won't be able to use thrust::pair\n//just explicitly specify typed output reads/writes\n//Currently implemented for max of two outputs\n//   template<class T1, class T2>\n//   C10_DEVICE void set_results(const thrust::pair<T1, T2> x, const index_t base_offset) const {\n//     if (noutputs >= 1) {\n//       auto res0 = (T1*)((char*)dst[0] + base_offset);\n//       *res0 = x.first;\n//     }\n//     if (noutputs >= 2) {\n//       // base offset is computed assuming element size being sizeof(T1), so we need to make a\n//       // correction to obtain the correct base offset\n//       auto res1 = (T2*) ((char *) dst[1] + base_offset / sizeof(T1) * sizeof(T2));\n//       *res1 = x.second;\n//     }\n//   }\n\n  template <int output_vec_size>\n  C10_DEVICE void set_results_to_output(Array<arg_t, output_vec_size> value, Array<uint32_t, output_vec_size> base_offset) const {\n    assert(final_output);\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      set_results(reducer::project(value[i]), base_offset[i]);\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> global_reduce(Array<arg_t, output_vec_size> value, Array<arg_t, output_vec_size> *acc, char* shared_memory) const {\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using out_ptr_vec_t = Array<out_scalar_t*, output_vec_size>;\n    using offset_vec_t = Array<uint32_t, output_vec_size>;\n\n    arg_vec_t* reduce_buffer = (arg_vec_t*)cta_buf;\n    uint32_t output_idx = config.output_idx<output_vec_size>();\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    bool should_store = config.should_store(output_idx);\n    if (should_store) {\n      uint32_t offset = config.staging_memory_offset(blockIdx.y);\n      reduce_buffer[offset] = value;\n    }\n\n    __threadfence(); // make sure writes are globally visible\n    __syncthreads(); // if multiple warps in this block wrote to staging, make sure they're all done\n    bool is_last_block_done = mark_block_finished();\n\n    if (is_last_block_done) {\n      value = ident;\n      if (config.should_block_x_reduce()) {\n        uint32_t input_offset = threadIdx.x + threadIdx.y * blockDim.x;\n        uint32_t step = blockDim.x * blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      } else {\n        uint32_t input_offset = threadIdx.y;\n        uint32_t step = blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      }\n      value = block_y_reduce(value, shared_memory);\n      if (config.should_block_x_reduce()) {\n        value = block_x_reduce<output_vec_size>(value, shared_memory);\n      }\n      if (should_store) {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::translate_idx(value[i], base_idx);\n          }\n        }\n\n        if (acc == nullptr) {\n          if (accumulate) {\n            value = accumulate_in_output<output_vec_size>(out, value);\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              *(out[i]) = get_accumulated_output(out[i], value[i]);\n            }\n          }\n        } else {\n          if (accumulate) {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              value[i] = reducer::combine((*acc)[i], value[i]);\n            }\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            *acc = value;\n          }\n        }\n      }\n    }\n\n    return value;\n  }\n};\n\nextern \"C\"\n__launch_bounds__(512, 4)\n__global__ void reduction_prod_kernel(ReduceJitOp r){\n  r.run();\n}\nnvrtc: error: failed to open libnvrtc-builtins.so.12.1.\n  Make sure that libnvrtc-builtins.so.12.1 is installed correctly.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9128ecd07b11>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Generate the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpeft_model_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mpeft_model_text_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meos_token_id_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 unfinished_sequences = unfinished_sequences.mul(\n\u001b[0;32m-> 2252\u001b[0;31m                     \u001b[0mnext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_token_id_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_token_id_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m                 )\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n  #ifdef __HIPCC__\n  #define ERROR_UNSUPPORTED_CAST ;\n  // corresponds to aten/src/ATen/native/cuda/thread_constants.h\n  #define CUDA_OR_ROCM_NUM_THREADS 256\n  // corresponds to aten/src/ATen/cuda/detail/OffsetCalculator.cuh\n  #define MAX_DIMS 16\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  #else\n  //TODO use _assert_fail, because assert is disabled in non-debug builds\n  #define ERROR_UNSUPPORTED_CAST assert(false);\n  #define CUDA_OR_ROCM_NUM_THREADS 128\n  #define MAX_DIMS 25\n  #endif\n  #define POS_INFINITY __int_as_float(0x7f800000)\n  #define INFINITY POS_INFINITY\n  #define NEG_INFINITY __int_as_float(0xff800000)\n  #define NAN __int_as_float(0x7fffffff)\n\n  typedef long long int int64_t;\n  typedef unsigned int uint32_t;\n  typedef signed char int8_t;\n  typedef unsigned char uint8_t;  // NOTE: this MUST be \"unsigned char\"! \"char\" is equivalent to \"signed char\"\n  typedef short int16_t;\n  static_assert(sizeof(int64_t) == 8, \"expected size does not match\");\n  static_assert(sizeof(uint32_t) == 4, \"expected size does not match\");\n  static_assert(sizeof(int8_t) == 1, \"expected size does not match\");\n  constexpr int num_threads = CUDA_OR_ROCM_NUM_THREADS;\n  constexpr int thread_work_size = 4; // TODO: make template substitution once we decide where those vars live\n  constexpr int block_work_size = thread_work_size * num_threads;\n\n  \n  \n  \n  namespace std {\n  \n  using ::signbit;\n  using ::isfinite;\n  using ::isinf;\n  using ::isnan;\n  \n  using ::abs;\n  \n  using ::acos;\n  using ::acosf;\n  using ::asin;\n  using ::asinf;\n  using ::atan;\n  using ::atanf;\n  using ::atan2;\n  using ::atan2f;\n  using ::ceil;\n  using ::ceilf;\n  using ::cos;\n  using ::cosf;\n  using ::cosh;\n  using ::coshf;\n  \n  using ::exp;\n  using ::expf;\n  \n  using ::fabs;\n  using ::fabsf;\n  using ::floor;\n  using ::floorf;\n  \n  using ::fmod;\n  using ::fmodf;\n  \n  using ::frexp;\n  using ::frexpf;\n  using ::ldexp;\n  using ::ldexpf;\n  \n  using ::log;\n  using ::logf;\n  \n  using ::log10;\n  using ::log10f;\n  using ::modf;\n  using ::modff;\n  \n  using ::pow;\n  using ::powf;\n  \n  using ::sin;\n  using ::sinf;\n  using ::sinh;\n  using ::sinhf;\n  \n  using ::sqrt;\n  using ::sqrtf;\n  using ::tan;\n  using ::tanf;\n  \n  using ::tanh;\n  using ::tanhf;\n  \n  using ::acosh;\n  using ::acoshf;\n  using ::asinh;\n  using ::asinhf;\n  using ::atanh;\n  using ::atanhf;\n  using ::cbrt;\n  using ::cbrtf;\n  \n  using ::copysign;\n  using ::copysignf;\n  \n  using ::erf;\n  using ::erff;\n  using ::erfc;\n  using ::erfcf;\n  using ::exp2;\n  using ::exp2f;\n  using ::expm1;\n  using ::expm1f;\n  using ::fdim;\n  using ::fdimf;\n  using ::fmaf;\n  using ::fma;\n  using ::fmax;\n  using ::fmaxf;\n  using ::fmin;\n  using ::fminf;\n  using ::hypot;\n  using ::hypotf;\n  using ::ilogb;\n  using ::ilogbf;\n  using ::lgamma;\n  using ::lgammaf;\n  using ::llrint;\n  using ::llrintf;\n  using ::llround;\n  using ::llroundf;\n  using ::log1p;\n  using ::log1pf;\n  using ::log2;\n  using ::log2f;\n  using ::logb;\n  using ::logbf;\n  using ::lrint;\n  using ::lrintf;\n  using ::lround;\n  using ::lroundf;\n  \n  using ::nan;\n  using ::nanf;\n  \n  using ::nearbyint;\n  using ::nearbyintf;\n  using ::nextafter;\n  using ::nextafterf;\n  using ::remainder;\n  using ::remainderf;\n  using ::remquo;\n  using ::remquof;\n  using ::rint;\n  using ::rintf;\n  using ::round;\n  using ::roundf;\n  using ::scalbln;\n  using ::scalblnf;\n  using ::scalbn;\n  using ::scalbnf;\n  using ::tgamma;\n  using ::tgammaf;\n  using ::trunc;\n  using ::truncf;\n  \n  } // namespace std\n  \n  \n\n  // NB: Order matters for this macro; it is relied upon in\n  // _promoteTypesLookup and the serialization format.\n  // Note, some types have ctype as void because we don't support them in codegen\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_) \\\n  _(uint8_t, Byte) /* 0 */                               \\\n  _(int8_t, Char) /* 1 */                                \\\n  _(int16_t, Short) /* 2 */                              \\\n  _(int, Int) /* 3 */                                    \\\n  _(int64_t, Long) /* 4 */                               \\\n  _(at::Half, Half) /* 5 */                                  \\\n  _(float, Float) /* 6 */                                \\\n  _(double, Double) /* 7 */                              \\\n  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \\\n  _(std::complex<float>, ComplexFloat) /* 9 */                          \\\n  _(std::complex<double>, ComplexDouble) /* 10 */                         \\\n  _(bool, Bool) /* 11 */                                 \\\n  _(void, QInt8) /* 12 */                          \\\n  _(void, QUInt8) /* 13 */                        \\\n  _(void, QInt32) /* 14 */                        \\\n  _(at::BFloat16, BFloat16) /* 15 */                             \\\n\n  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_QINT(_)       \\\n  _(uint8_t, Byte)                                                 \\\n  _(int8_t, Char)                                                  \\\n  _(int16_t, Short)                                                \\\n  _(int, Int)                                                      \\\n  _(int64_t, Long)                                                 \\\n  _(at::Half, Half)                                                \\\n  _(float, Float)                                                  \\\n  _(double, Double)                                                \\\n  _(std::complex<at::Half>, ComplexHalf)                           \\\n  _(std::complex<float>, ComplexFloat)                             \\\n  _(std::complex<double>, ComplexDouble)                           \\\n  _(bool, Bool)                                                    \\\n  _(at::BFloat16, BFloat16)\n\n\n  enum class ScalarType : int8_t {\n  #define DEFINE_ENUM(_1, n) n,\n  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ENUM)\n  #undef DEFINE_ENUM\n      Undefined,\n  NumOptions\n  };\n\n  template <typename T, int size>\n  struct Array {\n  T data[size];\n\n  __device__ T operator[](int i) const {\n      return data[i];\n  }\n  __device__ T& operator[](int i) {\n      return data[i];\n  }\n  Array() = default;\n  Array(const Array&) = default;\n  Array& operator=(const Array&) = default;\n  __device__ Array(T x) {\n    for (int i = 0; i < size; i++) {\n      data[i] = x;\n    }\n  }\n  };\n\n  \n  \n  \n  \n  \n\n\n\n  template <typename T>\n  struct DivMod {\n  T div;\n  T mod;\n\n  __device__ DivMod(T _div, T _mod) {\n      div = _div;\n      mod = _mod;\n  }\n  };\n\n  //<unsigned int>\n  struct IntDivider {\n  IntDivider() = default;\n\n  __device__ inline unsigned int div(unsigned int n) const {\n  unsigned int t = __umulhi(n, m1);\n  return (t + n) >> shift;\n  }\n\n  __device__ inline unsigned int mod(unsigned int n) const {\n  return n - div(n) * divisor;\n  }\n\n  __device__ inline DivMod<unsigned int> divmod(unsigned int n) const {\n  unsigned int q = div(n);\n  return DivMod<unsigned int>(q, n - q * divisor);\n  }\n\n  unsigned int divisor;  // d above.\n  unsigned int m1;  // Magic number: m' above.\n  unsigned int shift;  // Shift amounts.\n  };\n\n  template <int NARGS>\n  struct TrivialOffsetCalculator {\n    // The offset for each argument. Wrapper around fixed-size array.\n    // The offsets are in # of elements, not in bytes.\n    Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; arg++) {\n        offsets[arg] = linear_idx;\n      }\n      return offsets;\n    }\n  };\n\n  template<int NARGS>\n  struct OffsetCalculator {\n  OffsetCalculator() = default;\n  __device__ __forceinline__ Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n      Array<unsigned int, NARGS> offsets;\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n      offsets[arg] = 0;\n      }\n\n      #pragma unroll\n      for (int dim = 0; dim < MAX_DIMS; ++dim) {\n      if (dim == dims) {\n          break;\n      }\n\n      auto divmod = sizes_[dim].divmod(linear_idx);\n      linear_idx = divmod.div;\n\n      #pragma unroll\n      for (int arg = 0; arg < NARGS; ++arg) {\n          offsets[arg] += divmod.mod * strides_[dim][arg];\n      }\n      //printf(\"offset calc thread dim size stride offset %d %d %d %d %d %d %d %d\\n\",\n      //threadIdx.x, dim, sizes_[dim].divisor, strides_[dim][0], offsets[0], linear_idx, divmod.div, divmod.mod);\n      }\n      return offsets;\n  }\n\n    int dims;\n    IntDivider sizes_[MAX_DIMS];\n    // NOTE: this approach will not support nInputs == 0\n    unsigned int strides_[MAX_DIMS][NARGS];\n  };\n\n\n\n  #define C10_HOST_DEVICE __host__ __device__\n  #define C10_DEVICE __device__\n  #if defined(__clang__) && defined(__HIP__)\n  #ifndef __forceinline__\n  #define __forceinline__ inline __attribute__((always_inline))\n  #endif\n  // until ROCm support for kernel asserts is restored\n  #define assert(expr) (static_cast<void>(0))\n  #endif\n\n  template <typename T>\n  __device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n  #if defined(__clang__) && defined(__HIP__)\n    return __shfl_down(value, delta, width);\n  #else\n    return __shfl_down_sync(mask, value, delta, width);\n  #endif\n  }\n\n\n  #if 0\n  template <typename T>\n  __device__ __forceinline__ std::complex<T> WARP_SHFL_DOWN(std::complex<T> value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n  {\n    return std::complex<T>(\n  #if defined(__clang__) && defined(__HIP__)\n        __shfl_down(value.real(), delta, width),\n        __shfl_down(value.imag(), delta, width));\n  #else\n        __shfl_down_sync(mask, value.real(), delta, width),\n        __shfl_down_sync(mask, value.imag(), delta, width));\n  #endif\n  }\n  #endif\n\n  // aligned vector generates vectorized load/store on CUDA\n  template<typename scalar_t, int vec_size>\n  struct alignas(sizeof(scalar_t) * vec_size) aligned_vector {\n    scalar_t val[vec_size];\n  };\n\n\n  C10_HOST_DEVICE static void reduce_fraction(size_t &numerator, size_t &denominator) {\n    // get GCD of num and denom using Euclid's algorithm.\n    // Can replace this with std::gcd if we ever support c++17.\n    size_t a = denominator;\n    size_t b = numerator;\n    while (b != 0) {\n        a %= b;\n        // swap(a,b)\n        size_t tmp = a;\n        a = b;\n        b = tmp;\n    }\n\n    // a is now the GCD\n    numerator /= a;\n    denominator /= a;\n  }\n\n\n\n\n  struct ReduceConfig {\n  //has to match host-side ReduceConfig in the eager code\n  static constexpr int BLOCK_X = 0;\n  static constexpr int BLOCK_Y = 1;\n  static constexpr int CTA = 2;\n\n  static constexpr int input_vec_size = 4;\n  int element_size_bytes;\n  int num_inputs;\n  int num_outputs;\n  int step_input = 1;\n  int step_output = 1;\n  int ctas_per_output = 1;\n  int input_mult[3] = {0, 0, 0};\n  int output_mult[2] = {0, 0};\n\n  int block_width;\n  int block_height;\n  int num_threads;\n\n  bool vectorize_input = false;\n  int output_vec_size = 1;\n\n  C10_HOST_DEVICE bool should_block_x_reduce() const {\n    return input_mult[BLOCK_X] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_block_y_reduce() const {\n    return input_mult[BLOCK_Y] != 0;\n  }\n\n  C10_HOST_DEVICE bool should_global_reduce() const {\n    return input_mult[CTA] != 0;\n  }\n\n  C10_DEVICE bool should_store(int output_idx) const {\n    return output_idx < num_outputs &&\n      (!should_block_x_reduce() || threadIdx.x == 0) &&\n      (!should_block_y_reduce() || threadIdx.y == 0);\n  }\n\n  C10_DEVICE bool should_reduce_tail() const {\n    return (!should_block_y_reduce() || threadIdx.y == 0) &&\n      (!should_global_reduce() || blockIdx.y == 0);\n  }\n\n  C10_HOST_DEVICE int input_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta2 = blockIdx.y;\n    return (lane * input_mult[BLOCK_X] +\n            warp * input_mult[BLOCK_Y] +\n            cta2 * input_mult[CTA]);\n  }\n\n  template <int output_vec_size>\n  C10_HOST_DEVICE int output_idx() const {\n    int lane = threadIdx.x;\n    int warp = threadIdx.y;\n    int cta1 = blockIdx.x;\n    return (lane * output_mult[BLOCK_X] +\n            warp * output_mult[BLOCK_Y] +\n            cta1 * step_output) * output_vec_size;\n  }\n\n  C10_DEVICE int shared_memory_offset(int offset) const {\n    return threadIdx.x + (threadIdx.y + offset) * blockDim.x;\n  }\n\n  C10_DEVICE int staging_memory_offset(int cta2) const {\n    int offset = cta2 + blockIdx.x * gridDim.y;\n    if (!should_block_x_reduce()) {\n      offset = threadIdx.x + offset * blockDim.x;\n    }\n    return offset;\n  }\n\n\n  };\n\n\n//TODO this will need to be different for more generic reduction functions\nnamespace reducer {\n\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n\n  inline __device__ arg_t combine(arg_t a, arg_t b) { return a * b; }\n\n  inline __device__ out_scalar_t project(arg_t arg) {\n    return (out_scalar_t) arg;\n  }\n\n  inline __device__ arg_t warp_shfl_down(arg_t arg, int offset) {\n    return WARP_SHFL_DOWN(arg, offset);\n  }\n\n  inline __device__ arg_t translate_idx(arg_t acc, int64_t /*idx*/) {\n    return acc;\n  }\n\n  // wrap a normal reduction that ignores the index\n  inline __device__ arg_t reduce(arg_t acc, arg_t val, int64_t idx) {\n     return combine(acc, val);\n  }\n}\n\n\nstruct ReduceJitOp {\n  using scalar_t = int64_t;\n  using arg_t = int64_t;\n  using out_scalar_t = int64_t;\n\n  using InputCalculator = OffsetCalculator<1>;\n  using OutputCalculator = OffsetCalculator<2>;\n\n//   static constexpr bool can_accumulate_in_output =\n//     std::is_convertible<arg_t, out_scalar_t>::value\n//     && std::is_convertible<out_scalar_t, arg_t>::value;\n\n  static constexpr int input_vec_size = ReduceConfig::input_vec_size;\n\n  arg_t ident;\n  ReduceConfig config;\n  InputCalculator input_calc;\n  OutputCalculator output_calc;\n  const void* src;\n  const char* dst[2]; //it accepts at most two destinations\n  // acc_buf used for accumulation among sub Tensor Iterator when accumulation on\n  // output is not permissible\n  void* acc_buf;\n  // cta_buf used for accumulation between blocks during global reduction\n  void* cta_buf;\n  int* semaphores;\n  int64_t base_idx;\n  bool accumulate;\n  bool final_output;\n  int noutputs;\n\n\n  C10_DEVICE void run() const {\n    extern __shared__ char shared_memory[];\n    uint32_t output_idx = config.output_idx<1>();\n    uint32_t input_idx = config.input_idx();\n    auto base_offsets1 = output_calc.get(output_idx)[1];\n\n    using arg_vec_t = Array<arg_t, 1>;\n    arg_vec_t value;\n\n    if (output_idx < config.num_outputs && input_idx < config.num_inputs) {\n      const scalar_t* input_slice = (const scalar_t*)((const char*)src + base_offsets1);\n\n      value = thread_reduce<1>(input_slice);\n    }\n\n    if (config.should_block_y_reduce()) {\n      value = block_y_reduce<1>(value, shared_memory);\n    }\n    if (config.should_block_x_reduce()) {\n      value = block_x_reduce<1>(value, shared_memory);\n    }\n\n    using out_ptr_vec_t = Array<out_scalar_t*, 1>;\n    using offset_vec_t = Array<uint32_t, 1>;\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < 1; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    arg_vec_t* acc = nullptr;\n    if (acc_buf != nullptr) {\n      size_t numerator = sizeof(arg_t);\n      size_t denominator = sizeof(out_scalar_t);\n      reduce_fraction(numerator, denominator);\n      acc = (arg_vec_t*)((char*)acc_buf + (base_offsets[0] * numerator / denominator));\n    }\n\n    if (config.should_global_reduce()) {\n      value = global_reduce<1>(value, acc, shared_memory);\n    } else if (config.should_store(output_idx)) {\n      if (accumulate) {\n        #pragma unroll\n        for (int i = 0; i < 1; i++) {\n          value[i] = reducer::translate_idx(value[i], base_idx);\n        }\n      }\n\n      if (acc == nullptr) {\n        if (accumulate) {\n          value = accumulate_in_output<1>(out, value);\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            *(out[i]) = get_accumulated_output(out[i], value[i]);\n          }\n        }\n      } else {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < 1; i++) {\n            value[i] = reducer::combine((*acc)[i], value[i]);\n          }\n        }\n        if (final_output) {\n          set_results_to_output<1>(value, base_offsets);\n        } else {\n          *acc = value;\n        }\n      }\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce(const scalar_t* data) const {\n    if (config.vectorize_input) {\n      assert(output_vec_size == 1);\n      // reduce at the header of input_slice where memory is not aligned,\n      // so that thread_reduce will have an aligned memory to work on.\n      return {input_vectorized_thread_reduce_impl(data)};\n    } else {\n      uint32_t element_stride = input_calc.strides_[0][0] / sizeof(scalar_t);\n      bool is_contiguous = (input_calc.dims == 1 && element_stride == 1);\n      if (is_contiguous) {\n        return thread_reduce_impl<output_vec_size>(data, [](uint32_t idx) { return idx; });\n      } else if (input_calc.dims == 1) {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return idx * element_stride; });\n      } else {\n        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return input_calc.get(idx)[0] / sizeof(scalar_t); });\n      }\n    }\n  }\n\n  C10_DEVICE arg_t input_vectorized_thread_reduce_impl(const scalar_t* data) const {\n    uint32_t end = config.num_inputs;\n\n    // Handle the head of input slice where data is not aligned\n    arg_t value = ident;\n    constexpr int align_bytes = alignof(aligned_vector<scalar_t, input_vec_size>);\n    constexpr int align_elements = align_bytes / sizeof(scalar_t);\n    int shift = ((int64_t)data) % align_bytes / sizeof(scalar_t);\n    if (shift > 0) {\n      data -= shift;\n      end += shift;\n      if(threadIdx.x >= shift && threadIdx.x < align_elements && config.should_reduce_tail()){\n        value = reducer::reduce(value, data[threadIdx.x], threadIdx.x - shift);\n      }\n      end -= align_elements;\n      data += align_elements;\n      shift = align_elements - shift;\n    }\n\n    // Do the vectorized reduction\n    using load_t = aligned_vector<scalar_t, input_vec_size>;\n\n    uint32_t idx = config.input_idx();\n    const uint32_t stride = config.step_input;\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_t value_list[input_vec_size];\n    value_list[0] = value;\n\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[i] = ident;\n    }\n\n    scalar_t values[input_vec_size];\n\n    load_t *values_vector = reinterpret_cast<load_t*>(&values[0]);\n\n    while (idx * input_vec_size + input_vec_size - 1 < end) {\n      *values_vector = reinterpret_cast<const load_t*>(data)[idx];\n      #pragma unroll\n      for (uint32_t i = 0; i < input_vec_size; i++) {\n        value_list[i] = reducer::reduce(value_list[i], values[i], shift + idx * input_vec_size + i);\n      }\n      idx += stride;\n    }\n\n    // tail\n    uint32_t tail_start = end - end % input_vec_size;\n    if (config.should_reduce_tail()) {\n      int idx = tail_start + threadIdx.x;\n      if (idx < end) {\n        value_list[0] = reducer::reduce(value_list[0], data[idx], idx + shift);\n      }\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < input_vec_size; i++) {\n      value_list[0] = reducer::combine(value_list[0], value_list[i]);\n    }\n    return value_list[0];\n  }\n\n  template <int output_vec_size, typename offset_calc_t>\n  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce_impl(const scalar_t* data_, offset_calc_t calc) const {\n    uint32_t idx = config.input_idx();\n    const uint32_t end = config.num_inputs;\n    const uint32_t stride = config.step_input;\n    const int vt0=4;\n\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using load_t = aligned_vector<scalar_t, output_vec_size>;\n    const load_t* data = reinterpret_cast<const load_t*>(data_);\n\n    // Multiple accumulators to remove dependency between unrolled loops.\n    arg_vec_t value_list[vt0];\n\n    #pragma unroll\n    for (int i = 0; i < vt0; i++) {\n      #pragma unroll\n      for (int j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = ident;\n      }\n    }\n\n    load_t values[vt0];\n\n    while (idx + (vt0 - 1) * stride < end) {\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        values[i] = data[calc(idx + i * stride) / output_vec_size];\n      }\n      #pragma unroll\n      for (uint32_t i = 0; i < vt0; i++) {\n        #pragma unroll\n        for (uint32_t j = 0; j < output_vec_size; j++) {\n          value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx + i * stride);\n        }\n      }\n      idx += stride * vt0;\n    }\n\n    // tail\n    int idx_ = idx;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      values[i] = data[calc(idx) / output_vec_size];\n      idx += stride;\n    }\n    idx = idx_;\n    #pragma unroll\n    for (uint32_t i = 0; i < vt0; i++) {\n      if (idx >= end) {\n        break;\n      }\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx);\n      }\n      idx += stride;\n    }\n\n    // combine accumulators\n    #pragma unroll\n    for (int i = 1; i < vt0; i++) {\n      #pragma unroll\n      for (uint32_t j = 0; j < output_vec_size; j++) {\n        value_list[0][j] = reducer::combine(value_list[0][j], value_list[i][j]);\n      }\n    }\n    return value_list[0];\n  }\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_x_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    int dim_x = blockDim.x;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    if (dim_x > warpSize) {\n      int address_base = threadIdx.x + threadIdx.y*blockDim.x;\n      shared[address_base] = value;\n      for (int offset = dim_x/2; offset >= warpSize; offset >>= 1) {\n        __syncthreads();\n        if (threadIdx.x < offset && threadIdx.x + offset < blockDim.x) {\n          args_vec_t other = shared[address_base + offset];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], other[i]);\n          }\n          shared[address_base] = value;\n        }\n      }\n      dim_x = warpSize;\n    }\n\n    __syncthreads();\n\n    for (int offset = 1; offset < dim_x; offset <<= 1) {\n      #pragma unroll\n      for (int i = 0; i < output_vec_size; i++) {\n        arg_t other = reducer::warp_shfl_down(value[i], offset);\n        value[i] = reducer::combine(value[i], other);\n      }\n    }\n    return value;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> block_y_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n    using args_vec_t = Array<arg_t, output_vec_size>;\n    args_vec_t* shared = (args_vec_t*)shared_memory;\n    shared[config.shared_memory_offset(0)] = value;\n    for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) {\n      __syncthreads();\n      if (threadIdx.y < offset && threadIdx.y + offset < blockDim.y) {\n        args_vec_t other = shared[config.shared_memory_offset(offset)];\n        #pragma unroll\n        for (int i = 0; i < output_vec_size; i++) {\n          value[i] = reducer::combine(value[i], other[i]);\n        }\n        shared[config.shared_memory_offset(0)] = value;\n      }\n    }\n    return value;\n  }\n  \n\n  C10_DEVICE bool mark_block_finished() const {\n    __shared__ bool is_last_block_done_shared;\n\n    __syncthreads();\n    if (threadIdx.x == 0 && threadIdx.y == 0) {\n      int prev_blocks_finished = atomicAdd(&semaphores[blockIdx.x], 1);\n      is_last_block_done_shared = (prev_blocks_finished == gridDim.y - 1);\n    }\n\n    __syncthreads();\n\n    return is_last_block_done_shared;\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> accumulate_in_output(\n    Array<out_scalar_t*, output_vec_size> out,\n    Array<arg_t, output_vec_size> value\n  ) const {\n    Array<arg_t, output_vec_size> ret;\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      ret[i] = reducer::combine(*(out[i]), value[i]);\n    }\n    return ret;\n  }\n\n\n  C10_DEVICE out_scalar_t get_accumulated_output(\n    out_scalar_t* out, arg_t value\n  ) const {\n    assert(!final_output);\n    return (out_scalar_t)value;\n  }\n\n  template<class T>\n  C10_DEVICE void set_results(const T x, const uint32_t base_offset) const {\n    assert(noutputs == 1);\n    auto res = (out_scalar_t*)((char*)dst[0] + base_offset);\n    *res = x;\n  }\n\n//TODO - multi-output reduction - we won't be able to use thrust::pair\n//just explicitly specify typed output reads/writes\n//Currently implemented for max of two outputs\n//   template<class T1, class T2>\n//   C10_DEVICE void set_results(const thrust::pair<T1, T2> x, const index_t base_offset) const {\n//     if (noutputs >= 1) {\n//       auto res0 = (T1*)((char*)dst[0] + base_offset);\n//       *res0 = x.first;\n//     }\n//     if (noutputs >= 2) {\n//       // base offset is computed assuming element size being sizeof(T1), so we need to make a\n//       // correction to obtain the correct base offset\n//       auto res1 = (T2*) ((char *) dst[1] + base_offset / sizeof(T1) * sizeof(T2));\n//       *res1 = x.second;\n//     }\n//   }\n\n  template <int output_vec_size>\n  C10_DEVICE void set_results_to_output(Array<arg_t, output_vec_size> value, Array<uint32_t, output_vec_size> base_offset) const {\n    assert(final_output);\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      set_results(reducer::project(value[i]), base_offset[i]);\n    }\n  }\n\n  template <int output_vec_size>\n  C10_DEVICE Array<arg_t, output_vec_size> global_reduce(Array<arg_t, output_vec_size> value, Array<arg_t, output_vec_size> *acc, char* shared_memory) const {\n    using arg_vec_t = Array<arg_t, output_vec_size>;\n    using out_ptr_vec_t = Array<out_scalar_t*, output_vec_size>;\n    using offset_vec_t = Array<uint32_t, output_vec_size>;\n\n    arg_vec_t* reduce_buffer = (arg_vec_t*)cta_buf;\n    uint32_t output_idx = config.output_idx<output_vec_size>();\n    offset_vec_t base_offsets;\n    out_ptr_vec_t out;\n\n    #pragma unroll\n    for (int i = 0; i < output_vec_size; i++) {\n      base_offsets[i] = output_calc.get(output_idx + i)[0];\n      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n    }\n\n    bool should_store = config.should_store(output_idx);\n    if (should_store) {\n      uint32_t offset = config.staging_memory_offset(blockIdx.y);\n      reduce_buffer[offset] = value;\n    }\n\n    __threadfence(); // make sure writes are globally visible\n    __syncthreads(); // if multiple warps in this block wrote to staging, make sure they're all done\n    bool is_last_block_done = mark_block_finished();\n\n    if (is_last_block_done) {\n      value = ident;\n      if (config.should_block_x_reduce()) {\n        uint32_t input_offset = threadIdx.x + threadIdx.y * blockDim.x;\n        uint32_t step = blockDim.x * blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      } else {\n        uint32_t input_offset = threadIdx.y;\n        uint32_t step = blockDim.y;\n        for (; input_offset < config.ctas_per_output; input_offset += step) {\n          uint32_t idx = config.staging_memory_offset(input_offset);\n          arg_vec_t next = reduce_buffer[idx];\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::combine(value[i], next[i]);\n          }\n        }\n      }\n      value = block_y_reduce(value, shared_memory);\n      if (config.should_block_x_reduce()) {\n        value = block_x_reduce<output_vec_size>(value, shared_memory);\n      }\n      if (should_store) {\n        if (accumulate) {\n          #pragma unroll\n          for (int i = 0; i < output_vec_size; i++) {\n            value[i] = reducer::translate_idx(value[i], base_idx);\n          }\n        }\n\n        if (acc == nullptr) {\n          if (accumulate) {\n            value = accumulate_in_output<output_vec_size>(out, value);\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              *(out[i]) = get_accumulated_output(out[i], value[i]);\n            }\n          }\n        } else {\n          if (accumulate) {\n            #pragma unroll\n            for (int i = 0; i < output_vec_size; i++) {\n              value[i] = reducer::combine((*acc)[i], value[i]);\n            }\n          }\n          if (final_output) {\n            set_results_to_output<output_vec_size>(value, base_offsets);\n          } else {\n            *acc = value;\n          }\n        }\n      }\n    }\n\n    return value;\n  }\n};\n\nextern \"C\"\n__launch_bounds__(512, 4)\n__global__ void reduction_prod_kernel(ReduceJitOp r){\n  r.run();\n}\nnvrtc: error: failed to open libnvrtc-builtins.so.12.1.\n  Make sure that libnvrtc-builtins.so.12.1 is installed correctly."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwvQ5PXU4cNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}