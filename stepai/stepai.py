# -*- coding: utf-8 -*-
"""StepAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HaHAtj4d1hGPa4QcV9eOd6A6c_NfNtGz
"""

import nltk
import numpy as np
from sentence_transformers import SentenceTransformer, util

# Load a pre-trained SentenceTransformer model
nltk.download('punkt')
model = SentenceTransformer('all-MiniLM-L6-v2')

def chunk_data(text):
    sentences = nltk.sent_tokenize(text)
    embeddings = model.encode(sentences, convert_to_tensor=True)

    # Group sentences based on semantic similarity
    clusters = util.community_detection(embeddings, min_community_size=2)

    chunks = []
    for cluster in clusters:
        chunk = " ".join([sentences[idx] for idx in cluster])
        chunks.append(chunk)

    return chunks


# with open('merged_texts.txt', 'r',encoding="utf8") as file:
#     text = file.read()
# chunks = chunk_data(text)

import pickle
with open('chunks.pkl', 'rb') as f:
    chunks = pickle.load(f)

from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection

# Connect to Milvus
connections.connect(host='localhost', port='19530')

# Define collection schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="metadata", dtype=DataType.STRING)
]
schema = CollectionSchema(fields, description="chunk collection")

# Create collection
collection = Collection("chunks", schema)

# Insert data
embeddings = model.encode(chunks)
metadata = ["chunk metadata" for _ in chunks]
entities = [
    [i for i in range(len(chunks))],
    embeddings,
    metadata
]
collection.insert(entities)

from rank_bm25 import BM25Okapi
from transformers import DPRContextEncoder, DPRQuestionEncoder, DPRReader

# BM25 Retrieval
bm25 = BM25Okapi([nltk.word_tokenize(chunk) for chunk in chunks])

# DPR Retrieval/
context_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")

def retrieve(query):
    question_embedding = question_encoder(query).pooler_output
    bm25_scores = bm25.get_scores(nltk.word_tokenize(query))

    dpr_scores = []
    for chunk in chunks:
        chunk_embedding = context_encoder(chunk).pooler_output
        score = util.pytorch_cos_sim(question_embedding, chunk_embedding)
        dpr_scores.append(score)

    hybrid_scores = [bm25_scores[i] + dpr_scores[i] for i in range(len(chunks))]
    ranked_indices = np.argsort(hybrid_scores)[::-1]
    return [chunks[idx] for idx in ranked_indices[:10]]

from transformers import pipeline

qa_pipeline = pipeline("question-answering")

def answer_question(query):
    retrieved_chunks = retrieve(query)
    answers = [qa_pipeline(question=query, context=chunk) for chunk in retrieved_chunks]
    return sorted(answers, key=lambda x: x['score'], reverse=True)[0]['answer']

input1="What is navidia?"
answer_question(input1)

# import streamlit as st

# st.title("Question Answering System")

# query = st.text_input("Enter your query:")
# if query:
#     answer = answer_question(query)
#     st.write("Answer:", answer)