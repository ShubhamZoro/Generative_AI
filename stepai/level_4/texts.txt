







CUDA Installation Guide for Linux












































1. Introduction
1.1. System Requirements
1.2. OS Support Policy

1.3. Host Compiler Support Policy
1.3.1. Supported C++ Dialects


1.4. About This Document



2. Pre-installation Actions
2.1. Verify You Have a CUDA-Capable GPU
2.2. Verify You Have a Supported Version of Linux
2.3. Verify the System Has gcc Installed
2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed
2.5. Install GPUDirect Storage
2.6. Choose an Installation Method
2.7. Download the NVIDIA CUDA Toolkit
2.8. Address Custom xorg.conf, If Applicable
2.9. Handle Conflicting Installation Methods



3. Package Manager Installation
3.1. Overview

3.2. RHEL 8 / Rocky 8
3.2.1. Prepare RHEL 8 / Rocky 8
3.2.2. Local Repo Installation for RHEL 8 / Rocky 8
3.2.3. Network Repo Installation for RHEL 8 / Rocky 8
3.2.4. Common Instructions for RHEL 8 / Rocky 8



3.3. RHEL 9 / Rocky 9
3.3.1. Prepare RHEL 9 / Rocky 9
3.3.2. Local Repo Installation for RHEL 9 / Rocky 9
3.3.3. Network Repo Installation for RHEL 9 / Rocky 9
3.3.4. Common Instructions for RHEL 9 / Rocky 9



3.4. KylinOS 10
3.4.1. Prepare KylinOS 10
3.4.2. Local Repo Installation for KylinOS
3.4.3. Network Repo Installation for KylinOS
3.4.4. Common Instructions for KylinOS 10



3.5. Fedora
3.5.1. Prepare Fedora
3.5.2. Local Repo Installation for Fedora
3.5.3. Network Repo Installation for Fedora
3.5.4. Common Installation Instructions for Fedora



3.6. SLES
3.6.1. Prepare SLES
3.6.2. Local Repo Installation for SLES
3.6.3. Network Repo Installation for SLES
3.6.4. Common Installation Instructions for SLES



3.7. OpenSUSE
3.7.1. Prepare OpenSUSE
3.7.2. Local Repo Installation for OpenSUSE
3.7.3. Network Repo Installation for OpenSUSE
3.7.4. Common Installation Instructions for OpenSUSE



3.8. WSL
3.8.1. Prepare WSL
3.8.2. Local Repo Installation for WSL
3.8.3. Network Repo Installation for WSL
3.8.4. Common Installation Instructions for WSL



3.9. Ubuntu
3.9.1. Prepare Ubuntu
3.9.2. Local Repo Installation for Ubuntu
3.9.3. Network Repo Installation for Ubuntu
3.9.4. Common Installation Instructions for Ubuntu



3.10. Debian
3.10.1. Prepare Debian
3.10.2. Local Repo Installation for Debian
3.10.3. Network Repo Installation for Debian
3.10.4. Common Installation Instructions for Debian



3.11. Amazon Linux 2023
3.11.1. Prepare Amazon Linux 2023
3.11.2. Local Repo Installation for Amazon Linux
3.11.3. Network Repo Installation for Amazon Linux
3.11.4. Common Installation Instructions for Amazon Linux



3.12. Additional Package Manager Capabilities
3.12.1. Available Packages
3.12.2. Meta Packages
3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpm
3.12.4. Package Upgrades




4. Driver Installation

5. NVIDIA Open GPU Kernel Modules
5.1. CUDA Runfile
5.2. Debian
5.3. Fedora
5.4. KylinOS 10
5.5. RHEL 9 and Rocky 9
5.6. RHEL 8 and Rocky 8
5.7. OpenSUSE and SLES
5.8. Ubuntu



6. Precompiled Streams
6.1. Precompiled Streams Support Matrix
6.2. Modularity Profiles



7. Kickstart Installation
7.1. RHEL 8 / Rocky Linux 8
7.2. RHEL 9 / Rocky Linux 9



8. Runfile Installation
8.1. Runfile Overview
8.2. Installation

8.3. Disabling Nouveau
8.3.1. Fedora
8.3.2. RHEL / Rocky and KylinOS
8.3.3. OpenSUSE
8.3.4. SLES
8.3.5. WSL
8.3.6. Ubuntu
8.3.7. Debian


8.4. Device Node Verification
8.5. Advanced Options
8.6. Uninstallation



9. Conda Installation
9.1. Conda Overview
9.2. Installing CUDA Using Conda
9.3. Uninstalling CUDA Using Conda
9.4. Installing Previous CUDA Releases
9.5. Upgrading from cudatoolkit Package


10. Pip Wheels

11. Tarball and Zip Archive Deliverables
11.1. Parsing Redistrib JSON
11.2. Importing Tarballs into CMake
11.3. Importing Tarballs into Bazel



12. CUDA Cross-Platform Environment
12.1. CUDA Cross-Platform Installation
12.2. CUDA Cross-Platform Samples



13. Post-installation Actions

13.1. Mandatory Actions
13.1.1. Environment Setup



13.2. Recommended Actions
13.2.1. Install Persistence Daemon
13.2.2. Install Writable Samples

13.2.3. Verify the Installation
13.2.3.1. Verify the Driver Version
13.2.3.2. Running the Binaries


13.2.4. Install Nsight Eclipse Plugins
13.2.5. Local Repo Removal



13.3. Optional Actions
13.3.1. Install Third-party Libraries
13.3.2. Install the Source Code for cuda-gdb
13.3.3. Select the Active Version of CUDA




14. Advanced Setup

15. Frequently Asked Questions
15.1. How do I install the Toolkit in a different location?
15.2. Why do I see ânvcc: No such file or directoryâ when I try to build a CUDA application?
15.3. Why do I see âerror while loading shared libraries: <lib name>: cannot open shared object file: No such file or directoryâ when I try to run a CUDA application that uses a CUDA library?
15.4. Why do I see multiple â404 Not Foundâ errors when updating my repository meta-data on Ubuntu?
15.5. How can I tell X to ignore a GPU for compute-only use?
15.6. Why doesnât the cuda-repo package install the CUDA Toolkit and Drivers?
15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04?
15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update?
15.9. How do I install a CUDA driver with a version less than 367 using a network repo?
15.10. How do I install an older CUDA version using a network repo?
15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency?
15.12. How do I handle âErrors were encountered while processing: glx-diversionsâ?


16. Additional Considerations
17. Switching between Driver Module Flavors
18. Removing CUDA Toolkit and Driver

19. Notices
19.1. Notice
19.2. OpenCL
19.3. Trademarks


20. Copyright






Installation Guide for Linux






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






NVIDIA CUDA Installation Guide for Linux
The installation instructions for the CUDA Toolkit on Linux.


1. Introductionï

CUDAÂ® is a parallel computing platform and programming model invented by NVIDIAÂ®. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind:

Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/Cï»¿+ï»¿+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.

CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools.


1.1. System Requirementsï

To use NVIDIA CUDA on your system, you will need the following installed:

CUDA-capable GPU
A supported version of Linux with a gcc compiler and toolchain
CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads)

The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release.
The following table lists the supported Linux distributions. Please review the footnotes associated with the table.


Table 1 Native Linux Distribution Support in CUDA 12.5 Update 1ï









Distribution
Kernel1
Default GCC
GLIBC




x86_64


RHEL 9.y (y <= 4)
5.14.0-427
11.4.1
2.34


RHEL 8.y (y <= 10)
4.18.0-553
8.5.0
2.28


OpenSUSE Leap 15.y (y <= 5)
5.14.21-150500
7.5.0
2.31


Rocky Linux 8.y (y<=10)
4.18.0-553
8.5.0
2.28


Rocky Linux 9.y (y<=4)
5.14.0-427
11.4.1
2.34


SUSE SLES 15.y (y <= 5)
5.14.21-150500
7.5.0
2.31


Ubuntu 24.04 LTS
6.8.0-31
13.2.0
2.39


Ubuntu 22.04.z (z <= 4) LTS
6.5.0-27
12.3.0
2.35


Ubuntu 20.04.z (z <= 6) LTS
5.15.0-67
9.4.0
2.31


Debian 12.x (x<=5)
6.1.76-1
12.2.0
2.36


Debian 11.y (y<=9)
5.10.209-2
10.2.1
2.31


Debian 10.z (z<=13)
4.19.0-21
8.3.0
2.28


Fedora 39
6.5.6-300
13.2.1
2.38


KylinOS V10 SP2
4.19.90-25.14.v2101.ky10
7.3.0
2.28


Amazon Linux 2023
6.1.82-99.168
11.4.1
2.34


Arm64 sbsa


RHEL 9.y (y <= 4)
5.14.0-427
11.4.1
2.34


RHEL 8.y (y <= 10)
4.18.0-553
8.5.0
2.28


SUSE SLES 15.y (y <= 5)
5.14.21-150500
7.5.0
2.32


Ubuntu 24.04 LTS
6.8.0-31
13.2.0
2.39


Ubuntu 22.04 LTS (z <= 5) LTS
5.15.0-102
11.4.0
2.35


Ubuntu 20.04.z (z <= 5) LTS
5.4.0-174
9.4.0
2.31


Arm64 sbsa Jetson (dGPU)


20.04.06 LTS Rel35 JP 5.x
5.10.192-tegra
9.4.0
2.31


22.04.4 LTS Rel36 - JP6.x
5.15.136-tegra
11.4.0
2.35


Aarch64 Jetson (iGPU)


L4T Ubuntu 22.04 Rel36 - JP6.x
6.1.80-tegra
11.4.0
2.3.5




The following notes apply to the kernel versions supported by CUDA:




For specific kernel versions supported on Red Hat Enterprise Linux (RHEL), visit https://access.redhat.com/articles/3078.
A list of kernel versions including the release dates for SUSE Linux Enterprise Server (SLES) is available at https://www.suse.com/support/kb/doc/?id=000019587.




L4T provides a Linux kernel and a sample root filesystem derived from Ubuntu 20.04. For more details, visit https://developer.nvidia.com/embedded/jetson-linux.




1.2. OS Support Policyï


CUDA support for Ubuntu 20.04.x, Ubuntu 22.04.x, RHEL 8.x, RHEL 9.x, Rocky Linux 8.x, Rocky Linux 9.x, SUSE SLES 15.x and OpenSUSE Leap 15.x will be until the standard EOSS as defined for each OS. Please refer to the support lifecycle for these OSes to know their support timelines.
CUDA supports the latest Fedora release version. For Fedora release timelines, visit https://docs.fedoraproject.org/en-US/releases/.
CUDA supports a single KylinOS release version. For details, visit https://www.kylinos.cn/.

Refer to the support lifecycle for these supported OSes to know their support timelines and plan to move to newer releases accordingly.



1.3. Host Compiler Support Policyï

In order to compile the CPU âHostâ code in the CUDA source, the CUDA compiler NVCC requires a compatible host compiler to be installed on the system. The version of the host compiler supported on Linux platforms is tabulated as below. NVCC performs a version check on the host compilerâs major version and so newer minor versions of the compilers listed below will be supported, but major versions falling outside the range will not be supported.


Table 2 Supported Compilersï












Distribution
GCC
Clang
NVHPC
XLC
ArmC/C++
ICC




x86_64
6.x - 13.2
7.x - 17.0
23.x
No
No
2021.7


Arm64 sbsa
6.x - 13.2
7.x - 17.0
22.x
No
23.04.1
No



For GCC and Clang, the preceding table indicates the minimum version and the latest version supported. If you are on a Linux distribution that may use an older version of GCC toolchain as default than what is listed above, it is recommended to upgrade to a newer toolchain CUDA 11.0 or later toolkit. Newer GCC toolchains are available with the Red Hat Developer Toolset for example. For platforms that ship a compiler version older than GCC 6 by default, linking to static or dynamic libraries that are shipped with the CUDA Toolkit is not supported. We only support libstdc++ (GCCâs implementation) for all the supported host compilers for the platforms listed above.


1.3.1. Supported C++ Dialectsï

NVCC and NVRTC (CUDA Runtime Compiler) support the following C++ dialect: C++11, C++14, C++17, C++20 on supported host compilers. The default C++ dialect of NVCC  is determined by the default dialect of the host compiler used for compilation. Refer to host compiler documentation and the CUDA Programming Guide for more details on language support.
C++20 is supported with the following flavors of host compiler in both host and device code.










Distribution
GCC
Clang
NVHPC
Arm C/C++




x86_64
>=10.x
>=11.x
>=22.x
22.x







1.4. About This Documentï

This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line. You do not need previous experience with CUDA or experience with parallel computation. Note: This guide covers installation only on systems with X Windows installed.

Note
Many commands in this document might require superuser privileges. On most distributions of Linux, this will require you to log in as root. For systems that have enabled the sudo package, use the sudo prefix for all necessary commands.





2. Pre-installation Actionsï

Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux:

Verify the system has a CUDA-capable GPU.
Verify the system is running a supported version of Linux.
Verify the system has gcc installed.
Verify the system has the correct kernel headers and development packages installed.
Download the NVIDIA CUDA Toolkit.
Handle conflicting installation methods.


Note
You can override the install-time prerequisite checks by running the installer with the -override flag. Remember that the prerequisites will still be required to use the NVIDIA CUDA Toolkit.



2.1. Verify You Have a CUDA-Capable GPUï

To verify that your GPU is CUDA-capable, go to your distributionâs equivalent of System Properties, or, from the command line, enter:

lspci | grep -i nvidia


If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin) at the command line and rerun the previous lspci command.
If your graphics card is from NVIDIA and it is listed in https://developer.nvidia.com/cuda-gpus, your GPU is CUDA-capable.
The Release Notes for the CUDA Toolkit also contain a list of supported products.



2.2. Verify You Have a Supported Version of Linuxï

The CUDA Development Tools are only supported on some specific distributions of Linux. These are listed in the CUDA Toolkit release notes.
To determine which distribution and release number youâre running, type the following at the command line:

uname -m && cat /etc/*release


You should see output similar to the following, modified for your particular system:

x86_64
Red Hat Enterprise Linux Workstation release 6.0 (Santiago)


The x86_64 line indicates you are running on a 64-bit system. The remainder gives information about your distribution.



2.3. Verify the System Has gcc Installedï

The gcc compiler is required for development using the CUDA Toolkit. It is not required for running CUDA applications. It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly.
To verify the version of gcc installed on your system, type the following on the command line:

gcc --version


If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web.



2.4. Verify the System has the Correct Kernel Headers and Development Packages Installedï

The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. For example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed.
While the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed. However, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using. Therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version.
The version of the kernel your system is running can be found by running the following command:

uname -r


This is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers. This command will be used multiple times below to specify the version of the packages to install. Note that below are the common-case scenarios for kernel usage. More advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running.

Note
If you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed. Otherwise, the CUDA Driver will fail to work with the new kernel.




2.5. Install GPUDirect Storageï

If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package.
GDS packages can be installed using the CUDA packaging guide. Follow the instructions in MLNX_OFED Requirements and Installation.
GDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode. Installation instructions for them differ slightly. Compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations.
Full GDS support is restricted to the following Linux distros:

Ubuntu 20.04, Ubuntu 22.04
RHEL 8.3, RHEL 8.4, RHEL 9.0

Starting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver. Follow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages.



2.6. Choose an Installation Methodï

The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages).
The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distributionâs native package management system. The distribution-specific packages interface with the distributionâs native package management system. It is recommended to use the distribution-specific packages, where possible.

Note
For both native as well as cross development, the toolkit must be installed using the distribution-specific installer. See the CUDA Cross-Platform Installation section for more details.




2.7. Download the NVIDIA CUDA Toolkitï

The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads.
Choose the platform you are using and download the NVIDIA CUDA Toolkit.
The CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification
The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.
To calculate the MD5 checksum of the downloaded file, run the following:

md5sum <file>





2.8. Address Custom xorg.conf, If Applicableï

The driver relies on an automatically generated xorg.conf file at /etc/X11/xorg.conf. If a custom-built xorg.conf file is present, this functionality will be disabled and the driver may not work. You can try removing the existing xorg.conf file, or adding the contents of /etc/X11/xorg.conf.d/00-nvidia.conf to the xorg.conf file. The xorg.conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration.



2.9. Handle Conflicting Installation Methodsï

Before installing CUDA, any previous installations that could conflict should be uninstalled. This will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs. Runfile). See the following charts for specifics.


Table 3 CUDA Toolkit Installation Compatibility Matrixï












Installed Toolkit Version == X.Y
Installed Toolkit Version != X.Y


RPM/Deb
run
RPM/Deb
run


Installing Toolkit Version X.Y
RPM/Deb
No Action
Uninstall Run
No Action
No Action


run
Uninstall RPM/Deb
Uninstall Run
No Action
No Action





Table 4 NVIDIA Driver Installation Compatibility Matrixï












Installed Driver Version == X.Y
Installed Driver Version != X.Y


RPM/Deb
run
RPM/Deb
run


Installing Driver Version X.Y
RPM/Deb
No Action
Uninstall Run
No Action
Uninstall Run


run
Uninstall RPM/Deb
No Action
Uninstall RPM/Deb
No Action



Use the following command to uninstall a Toolkit runfile installation:

sudo /usr/local/cuda-X.Y/bin/cuda-uninstaller


Use the following command to uninstall a Driver runfile installation:

sudo /usr/bin/nvidia-uninstall


Use the following commands to uninstall an RPM/Deb installation:

sudo dnf remove <package_name>                      # RHEL 8 / Rocky Linux 8



sudo dnf remove <package_name>                      # Fedora



sudo zypper remove <package_name>                   # OpenSUSE / SLES



sudo apt-get --purge remove <package_name>          # Ubuntu






3. Package Manager Installationï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.


3.1. Overviewï

Installation using RPM or Debian packages interfaces with your systemâs package management system. When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in /var/. Such a package only informs the package manager where to find the actual installation packages, but will not install them.
If the online network repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper.
Distribution-specific instructions detail how to install CUDA:

RHEL 8 / Rocky Linux 8
RHEL 9 / Rocky Linux 9
KylinOS 10
Fedora
SLES
OpenSUSE
WSL
Ubuntu
Debian
Amazon Linux 2023

Finally, some helpful package manager capabilities are detailed.
These instructions are for native development only. For cross-platform development, see the CUDA Cross-Platform Environment section.

Note
Optional components such as nvidia-fs, libnvidia_nscq, and fabricmanager are not installed by default and will have to be installed separately as needed.




3.2. RHEL 8 / Rocky 8ï



3.2.1. Prepare RHEL 8 / Rocky 8ï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r)


If matching kernel-headers and kernel-devel packages are not available for the currently running kernel version, you may need to use the previously shipped version of these packages. See https://bugzilla.redhat.com/show_bug.cgi?id=1986132 for more information.


Satisfy third-party package dependency:


Satisfy DKMS dependency: The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau. Those packages are only available on third-party repositories, such as EPEL. Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding.
To enable EPEL:

sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm




Enable optional repos:
On RHEL 8 Linux only, execute the following steps to enable optional repositories.


On x86_64 systems:

subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms








Remove Outdated Signing Key:

sudo rpm --erase gpg-pubkey-7fa2af80*



Choose an installation method: local repo or network repo.




3.2.2. Local Repo Installation for RHEL 8 / Rocky 8ï


Install local repository on file system:


sudo rpm --install cuda-repo-<distro>-X-Y-local-<version>*.<arch>.rpm





3.2.3. Network Repo Installation for RHEL 8 / Rocky 8ï



Enable the network repo:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo


where $distro/$arch should be replaced by one of the following:

rhel8/cross-linux-sbsa
rhel8/sbsa
rhel8/x86_64



Install the new CUDA public GPG key:
The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685.
On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted.
For upgrades, you must also also fetch an updated .repo entry:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo




Clean Yum repository cache:

sudo dnf clean expire-cache







3.2.4. Common Instructions for RHEL 8 / Rocky 8ï

These instructions apply to both local and network installation.


Install CUDA SDK:

sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit




Install GPUDirect Filesystem:

sudo dnf install nvidia-gds




Add libcuda.so symbolic link, if necessary
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.


Reboot the system:

sudo reboot



Perform the post-installation actions.





3.3. RHEL 9 / Rocky 9ï



3.3.1. Prepare RHEL 9 / Rocky 9ï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r)




Satisfy third-party package dependency:


Satisfy DKMS dependency: The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau. Those packages are only available on third-party repositories, such as EPEL. Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding.
To enable EPEL:

sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm




Enable optional repos:
On RHEL 9 Linux only, execute the following steps to enable optional repositories.


On x86_64 systems:

subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms








Remove Outdated Signing Key:

sudo rpm --erase gpg-pubkey-7fa2af80*



Choose an installation method: local repo or network repo.




3.3.2. Local Repo Installation for RHEL 9 / Rocky 9ï



Install local repository on file system:

sudo rpm --install cuda-repo-<distro>-X-Y-local-<version>*.<arch>.rpm







3.3.3. Network Repo Installation for RHEL 9 / Rocky 9ï



Enable the network repo:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo


where $distro/$arch should be replaced by one of the following:

rhel9/cross-linux-sbsa
rhel9/sbsa
rhel9/x86_64



Install the new CUDA public GPG key:
The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685.
On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted.
For upgrades, you must also also fetch an updated .repo entry:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo




Clean Yum repository cache:

sudo dnf clean expire-cache







3.3.4. Common Instructions for RHEL 9 / Rocky 9ï

These instructions apply to both local and network installation.


Install CUDA SDK:

sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit




Install GPUDirect Filesystem:

sudo dnf install nvidia-gds




Add libcuda.so symbolic link, if necessary
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.


Reboot the system:

sudo reboot



Perform the post-installation actions.





3.4. KylinOS 10ï



3.4.1. Prepare KylinOS 10ï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r)



Choose an installation method: local repo or network repo.




3.4.2. Local Repo Installation for KylinOSï



Install local repository on file system:

sudo rpm --install cuda-repo-kylin10-X-Y-local-<version>*.<arch>.rpm







3.4.3. Network Repo Installation for KylinOSï



Enable the network repo:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/kylin10/x86_64/cuda-$distro.repo




Install the new CUDA public GPG key:
The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685.
On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted.


Clean Yum repository cache:

sudo dnf clean expire-cache







3.4.4. Common Instructions for KylinOS 10ï

These instructions apply to both local and network installation.


Install CUDA SDK:

sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit




Install GPUDirect Filesystem:

sudo dnf install nvidia-gds




Add libcuda.so symbolic link, if necessary
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.


Reboot the system:

sudo reboot



Perform the post-installation actions.





3.5. Fedoraï



3.5.1. Prepare Fedoraï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r)




Remove Outdated Signing Key:

sudo rpm --erase gpg-pubkey-7fa2af80*



Choose an installation method: local repo or network repo.




3.5.2. Local Repo Installation for Fedoraï



Install local repository on file system:

sudo rpm --install cuda-repo-<distro>-X-Y-local-<version>*.x86_64.rpm


where distro is fedora37 or fedora39, for example.





3.5.3. Network Repo Installation for Fedoraï



Enable the network repo:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo


where $distro should be replaced by one of the following:

fedora37
fedora39



Install the new CUDA public GPG key:
The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685.
On a fresh installation of Fedora, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted.
For upgrades, you must also fetch an updated .repo entry:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo




Clean DNF repository cache:

sudo dnf clean expire-cache







3.5.4. Common Installation Instructions for Fedoraï

These instructions apply to both local and network installation for Fedora.


Install CUDA SDK:

sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit



Note
The CUDA driver installation may fail if the RPMFusion non-free repository is enabled. In this case, CUDA installations should temporarily disable the RPMFusion non-free repository.


sudo dnf --disablerepo="rpmfusion-nonfree*" install cuda


It may be necessary to rebuild the grub configuration files, particularly if you use a non-default partition scheme. If so, then run this below command, and reboot the system:

sudo grub2-mkconfig -o /boot/grub2/grub.cfg




Reboot the system:

sudo reboot




Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.

Perform the post-installation actions.





3.6. SLESï



3.6.1. Prepare SLESï


Perform the pre-installation actions.

The kernel development packages for the currently running kernel can be installed with:

sudo zypper install -y kernel-<variant>-devel=<version>


To run the above command, you will need the variant and version of the currently running kernel. Use the output of the uname command to determine the currently running kernelâs variant and version:

$ uname -r
3.16.6-2-default


In the above example, the variant is default and version is 3.16.6-2.
The kernel development packages for the default kernel variant can be installed with:

sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\-default//')




The kernel headers and development packages for the currently running kernel can be installed with:

sudo zypper install -y kernel-<variant>-devel=<version>



On SLES12 SP4, install the Mesa-libgl-devel Linux packages before proceeding. See Mesa-libGL-devel.

Add the user to the video group:

sudo usermod -a -G video <username>




Remove Outdated Signing Key:

sudo rpm --erase gpg-pubkey-7fa2af80*



Choose an installation method: local repo or network repo.




3.6.2. Local Repo Installation for SLESï


Install local repository on file system:


sudo rpm --install cuda-repo-sles15-X-Y-local-<version>*.x86_64.rpm





3.6.3. Network Repo Installation for SLESï



Enable the network repo:

sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo


where $distro/$arch should be replaced by one of the following:

sles15/cross-linux-sbsa
sles15/sbsa
sles15/x86_64



Install the new CUDA public GPG key:
The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685.
On a fresh installation of SLES, the zypper package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted.
For upgrades, you must also also fetch an updated .repo entry:

sudo zypper removerepo cuda-$distro-$arch
sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo




Refresh Zypper repository cache:

sudo SUSEConnect --product PackageHub/15/<architecture>
sudo zypper refresh







3.6.4. Common Installation Instructions for SLESï

These instructions apply to both local and network installation for SLES.


Install CUDA SDK:

sudo zypper install cuda-toolkit




Install CUDA Samples GL dependencies:
Refer to CUDA Cross-Platform Samples.


Reboot the system:

sudo reboot



Perform the post-installation actions.





3.7. OpenSUSEï



3.7.1. Prepare OpenSUSEï


Perform the pre-installation actions.

The kernel development packages for the currently running kernel can be installed with:

sudo zypper install -y kernel-<variant>-devel=<version>


To run the above command, you will need the variant and version of the currently running kernel. Use the output of the uname command to determine the currently running kernelâs variant and version:

$ uname -r
3.16.6-2-default


In the above example, the variant is default and version is 3.16.6-2.
The kernel development packages for the default kernel variant can be installed with:

sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\-default//')




Add the user to the video group:

sudo usermod -a -G video <username>




Remove Outdated Signing Key:

sudo rpm --erase gpg-pubkey-7fa2af80*



Choose an installation method: local repo or network repo.




3.7.2. Local Repo Installation for OpenSUSEï



Install local repository on file system:

sudo rpm --install cuda-repo-opensuse15-<version>.x86_64.rpm







3.7.3. Network Repo Installation for OpenSUSEï



Enable the network repo:

sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo




Install the new CUDA public GPG key:
The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685. On fresh installation of openSUSE, the zypper package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted.
For upgrades, you must also also fetch an updated .repo entry:

sudo zypper removerepo cuda-opensuse15-x86_64
sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo




Refresh Zypper repository cache:

sudo zypper refresh







3.7.4. Common Installation Instructions for OpenSUSEï

These instructions apply to both local and network installation for OpenSUSE.


Install CUDA SDK:

sudo zypper install cuda-toolkit




Reboot the system:

sudo reboot



Perform the post-installation actions.





3.8. WSLï

These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case; it is important to not install the cuda-drivers packages within the WSL environment.


3.8.1. Prepare WSLï


Perform the pre-installation actions.

Remove Outdated Signing Key:

sudo apt-key del 7fa2af80



Choose an installation method: local repo or network repo.




3.8.2. Local Repo Installation for WSLï



Install local repositiry on file system:

sudo dpkg -i cuda-repo-wsl-ubuntu-X-Y-local_<version>*_x86_64.deb




Enroll ephemeral public GPG key:

sudo cp /var/cuda-repo-wsl-ubuntu-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/







3.8.3. Network Repo Installation for WSLï

The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc. This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended.


Install the newcuda-keyring package:

wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


Or if you are unable to install the cuda-keyring package, you can optionally:


Enroll the new signing key manually:

wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-archive-keyring.gpg
sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg




Enable the network repository:

echo "deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/ /" | sudo tee /etc/apt/sources.list.d/cuda-wsl-ubuntu-x86_64.list




Add pin file to prioritize CUDA repository:

wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600









3.8.4. Common Installation Instructions for WSLï

These instructions apply to both local and network installation for WSL.


Update the Apt repository cache:

sudo apt-get update




Install CUDA SDK:

sudo apt-get install cuda-toolkit



Perform the post-installation actions.





3.9. Ubuntuï



3.9.1. Prepare Ubuntuï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo apt-get install linux-headers-$(uname -r)




Remove Outdated Signing Key:

sudo apt-key del 7fa2af80



Choose an installation method: local repo or network repo.




3.9.2. Local Repo Installation for Ubuntuï



Install local repository on file system:

sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb




Enroll ephemeral public GPG key:

sudo cp /var/cuda-repo-<distro>-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/




Add pin file to prioritize CUDA repository:

wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/x86_64/cuda-<distro>.pin
 sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600







3.9.3. Network Repo Installation for Ubuntuï

The new GPG public key for the CUDA repository is 3bf863cc. This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended.


Install the new cuda-keyring package:

wget https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


where $distro/$arch should be replaced by one of the following:

ubuntu1604/x86_64
ubuntu1804/cross-linux-sbsa
ubuntu1804/sbsa
ubuntu1804/x86_64
ubuntu2004/cross-linux-aarch64
ubuntu2004/arm64
ubuntu2004/cross-linux-sbsa
ubuntu2004/sbsa
ubuntu2004/x86_64
ubuntu2204/sbsa
ubuntu2204/x86_64


Note
arm64-Jetson repos:

native: $distro/arm64
cross: $distro/cross-linux-aarch64



sudo dpkg -i cuda-keyring_1.1-1_all.deb




Or if you are unable to install the cuda-keyring package, you can optionally:


Enroll the new signing key manually:

wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-archive-keyring.gpg
sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg




Enable the network repository:

echo "deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/ /" | sudo tee /etc/apt/sources.list.d/cuda-<distro>-<arch>.list




Add pin file to prioritize CUDA repository:

wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-<distro>.pin
sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600









3.9.4. Common Installation Instructions for Ubuntuï

These instructions apply to both local and network installation for Ubuntu.


Update the Apt repository cache:

sudo apt-get update




Install CUDA SDK:

Note
These two commands must be executed separately.


sudo apt-get install cuda-toolkit


To include all GDS packages:

sudo apt-get install nvidia-gds




Reboot the system

sudo reboot



Perform the Post-installation Actions





3.10. Debianï



3.10.1. Prepare Debianï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo apt-get install linux-headers-$(uname -r)




Enable the contrib repository:

sudo add-apt-repository contrib




Remove Outdated Signing Key:

sudo apt-key del 7fa2af80



Choose an installation method: local repo or network repo.




3.10.2. Local Repo Installation for Debianï



Install local repository on file system:

sudo dpkg -i cuda-repo-<distro>-X-Y-local_<version>*_x86_64.deb




Enroll ephemeral public GPG key:

sudo cp /var/cuda-repo-<distro>-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/







3.10.3. Network Repo Installation for Debianï

The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc. This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended.


Install the new cuda-keyring package:

wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb


where $distro/$arch should be replaced by one of the following:

debian10/x86_64
debian11/x86_64


sudo dpkg -i cuda-keyring_1.1-1_all.deb




Or if you are unable to install the cuda-keyring package, you can optionally:


Enroll the new signing key manually:

wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/x86_64/cuda-archive-keyring.gpg
sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg




Enable the network repository:

echo "deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/<distro>/x86_64/ /" | sudo tee /etc/apt/sources.list.d/cuda-<distro>-x86_64.list









3.10.4. Common Installation Instructions for Debianï

These instructions apply to both local and network installation for Debian.


Update the Apt repository cache:

sudo apt-get update



Note
If you are using Debian 10, you may instead need to run:


sudo apt-get --allow-releaseinfo-change update




Install CUDA SDK:

sudo apt-get -y install cuda




Reboot the system:

sudo reboot



Perform the post-installation actions.





3.11. Amazon Linux 2023ï



3.11.1. Prepare Amazon Linux 2023ï


Perform the pre-installation actions.

The kernel headers and development packages for the currently running kernel can be installed with:

sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r)



Choose an installation method: local repo or network repo.




3.11.2. Local Repo Installation for Amazon Linuxï



Install local repository on file system:

sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm







3.11.3. Network Repo Installation for Amazon Linuxï



Enable the network repository:

sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo




Clean DNF repository cache:

sudo dnf clean expire-cache







3.11.4. Common Installation Instructions for Amazon Linuxï

These instructions apply to both local and network installation for Amazon Linux.


Install CUDA SDK:

sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit




Install GPUDirect Filesystem:

sudo dnf install nvidia-gds




Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.


Reboot the system:

sudo reboot



Perform the post-installation actions.





3.12. Additional Package Manager Capabilitiesï

Below are some additional capabilities of the package manager that users can take advantage of.


3.12.1. Available Packagesï

The recommended installation package is the cuda package. This package will install the full set of other CUDA packages required for native development and should cover most scenarios.
The cuda package installs all the available packages for native developments. That includes the compiler, the debugger, the profiler, the math libraries, and so on. For x86_64 platforms, this also includes Nsight Eclipse Edition and the visual profilers. It also includes the NVIDIA driver package.
On supported platforms, the cuda-cross-aarch64 and cuda-cross-sbsa packages install all the packages required for cross-platform development to arm64-Jetson and arm64-Server, respectively. The libraries and header files of the target architectureâs display driver package are also installed to enable the cross compilation of driver applications. The cuda-cross-<arch> packages do not install the native display driver.

Note
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running existing 32-bit applications on existing GPUs except Hopper. Hopper does not support 32-bit applications. Ada will be the last architecture with driver support for 32-bit applications.

The packages installed by the packages above can also be installed individually by specifying their names explicitly. The list of available packages be can obtained with:

yum --disablerepo="*" --enablerepo="cuda*" list available    # RedHat



dnf --disablerepo="*" --enablerepo="cuda*" list available    # Fedora



zypper packages -r cuda                                      # OpenSUSE & SLES



cat /var/lib/apt/lists/*cuda*Packages | grep "Package:"      # Ubuntu





3.12.2. Meta Packagesï

Meta packages are RPM/Deb/Conda packages which contain no (or few) files but have multiple dependencies. They are used to install many CUDA packages when you may not know the details of the packages you want. The following table lists the meta packages.


Table 5 Meta Packages Available for CUDA 12.4ï







Meta Package
Purpose




cuda
Installs all CUDA Toolkit and Driver packages. Handles upgrading to the next version of the cuda package when itâs released.


cuda-12-5
Installs all CUDA Toolkit and Driver packages. Remains at version 12.5 until an additional version of CUDA is installed.


cuda-toolkit-12-5
Installs all CUDA Toolkit packages required to develop CUDA applications. Does not include the driver.


cuda-toolkit-15
Installs all CUDA Toolkit packages required to develop applications. Will not upgrade beyond the 12.x series toolkits. Does not include the driver.


cuda-toolkit
Installs all CUDA Toolkit packages required to develop applications. Handles upgrading to the next 12.x version of CUDA when itâs released. Does not include the driver.


cuda-tools-12-5
Installs all CUDA command line and visual tools.


cuda-runtime-12-5
Installs all CUDA Toolkit packages required to run CUDA applications, as well as the Driver packages.


cuda-compiler-12-5
Installs all CUDA compiler packages.


cuda-libraries-12-5
Installs all runtime CUDA Library packages.


cuda-libraries-dev-12-5
Installs all development CUDA Library packages.


cuda-drivers
Installs all NVIDIA Driver packages with proprietary kernel modules. Handles upgrading to the next version of the Driver packages when theyâre released.


cuda-drivers-555
Installs all NVIDIA Driver packages with proprietary kernel modules. Will not upgrade beyond the 555 branch drivers.






3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpmï

These packages provide 32-bit driver libraries needed for things such as Steam (popular game app store/launcher), older video games, and some compute applications.
For Debian 10 and Debian 11:

sudo dpkg --add-architecture i386
sudo apt-get update
sudo apt-get install libcuda1-i386 nvidia-driver-libs-i386


For Debian 12:

sudo dpkg --add-architecture i386
sudo apt-get update
apt install nvidia-driver-libs:i386


For Ubuntu:

sudo dpkg --add-architecture i386
sudo apt-get update
sudo apt-get install libnvidia-compute-<branch>:i386 libnvidia-decode-<branch>:i386 \
 libnvidia-encode-<branch>:i386 libnvidia-extra-<branch>:i386 libnvidia-fbc1-<branch>:i386 \
 libnvidia-gl-<branch>:i386


Where <branch> is the driver version, for example 495.
For Fedora and RHEL8+:

sudo dnf install nvidia-driver-cuda-libs.i686 nvidia-driver-devel.i686 \
 nvidia-driver-libs.i686 nvidia-driver-NvFBCOpenGL.i686 nvidia-driver-NVML.i686



Note
There is no modularity profile support.

For openSUSE/SLES:

sudo zypper install nvidia-compute-G06-32bit nvidia-gl-G06-32bit nvidia-video-G06-32bit





3.12.4. Package Upgradesï

The cuda package points to the latest stable release of the CUDA Toolkit. When a new version is available, use the following commands to upgrade the toolkit and driver:

sudo dnf install cuda-toolkit                                # Fedora, RHEL9, RHEL8, and KylinOS



sudo zypper install cuda-toolkit                             # OpenSUSE and SLES



sudo apt-get install cuda-toolkit                            # Ubuntu and Debian


The cuda-cross-<arch> packages can also be upgraded in the same manner.
The cuda-drivers package points to the latest driver release available in the CUDA repository. When a new version is available, use the following commands to upgrade the driver:

sudo dnf module install nvidia-driver:latest-dkms             # Fedora, RHEL9, RHEL8, and KylinOS



sudo zypper install cuda-drivers nvidia-gfxG04-kmp-default   # OpenSUSE and SLES



sudo apt-get install cuda-drivers                            # Ubuntu and Debian


Some desktop environments, such as GNOME or KDE, will display a notification alert when new packages are available.
To avoid any automatic upgrade, and lock down the toolkit installation to the X.Y release, install the cuda-X-Y or cuda-cross-<arch>-X-Y package.
Side-by-side installations are supported. For instance, to install both the X.Y CUDA Toolkit and the X.Y+1 CUDA Toolkit, install the cuda-X.Y and cuda-X.Y+1 packages.





4. Driver Installationï

This section is for users who want to install a specific driver version.
For Debian and Ubuntu:

sudo apt-get install cuda-drivers-<driver_branch>


For example:

sudo apt-get install cuda-drivers-535


For OpenSUSE and SLES:

sudo zypper -v install cuda-drivers-<driver_branch>


For example:

sudo zypper -v install cuda-drivers-550


This allows you to get the highest version in the specified branch.
For Fedora and RHEL8+:

sudo dnf module install nvidia-driver:<stream>/<profile>


where profile by default is âdefaultâ and does not need to be specified.

Example dkms streams: 450-dkms or latest-dkms
Example precompiled streams: 450 or latest


Note
Precompiled streams are only supported on RHEL8 x86_64 and RHEL9 x86_64.

To uninstall or change streams on Fedora and RHEL8:

sudo dnf module remove --all nvidia-driver
sudo dnf module reset nvidia-driver





5. NVIDIA Open GPU Kernel Modulesï

The NVIDIA Linux GPU Driver contains several kernel modules:

nvidia.ko
nvidia-modeset.ko
nvidia-uvm.ko
nvidia-drm.ko
nvidia-peermem.ko

Starting in the 515 driver release series, two âflavorsâ of these kernel modules are provided:

Proprietary- this is the flavor that NVIDIA has historically shipped.
Open-source - published kernel modules that are dual licensed MIT/GPLv2. These are new starting in release 515. With every driver release, the source code to the open kernel modules will be published on https://github.com/NVIDIA/open-gpu-kernel-modules and a tarball will be provided on https://download.nvidia.com/XFree86/.

Verify that your NVIDIA GPU is at least Turing or newer generation.

lspci | grep VGA


Experimental support for GeForce and Quadro SKUs can be enabled with:

echo "options nvidia NVreg_OpenRmEnableUnsupportedGpus=1" | sudo tee /etc/modprobe.d/nvidia-gsp.conf


To install NVIDIA Open GPU Kernel Modules, follow the instructions below.


5.1. CUDA Runfileï

Pass the CLI argument to the CUDA runfile to opt in to NVIDIA Open GPU Kernel Modules:

sh cuda_<release>_<version>_linux.run -m=kernel-open





5.2. Debianï



Install the NVIDIA Open GPU Kernel Modules package:

sudo apt-get install nvidia-kernel-open-dkms




Install the rest of the NVIDIA driver packages:

sudo apt-get install cuda-drivers




OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package:

sudo apt-get install -v nvidia-kernel-open-dkms=<version>-1




Install the rest of the NVIDIA driver packages:

sudo apt-get install -v cuda-drivers-<driver_branch>


For example:

sudo apt-get install -v nvidia-kernel-open-dkms=550.90.07-1
sudo apt-get install -v cuda-drivers-550







5.3. Fedoraï



Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:open-dkms




OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:<driver_branch>-open







5.4. KylinOS 10ï



Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:open-dkms




OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:<driver_branch>-open







5.5. RHEL 9 and Rocky 9ï



Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:open-dkms




OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:<driver_branch>-open







5.6. RHEL 8 and Rocky 8ï



Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:open-dkms




OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages:

sudo dnf module install nvidia-driver:<driver_branch>-open







5.7. OpenSUSE and SLESï



Install the NVIDIA Open GPU Kernel Modules package:

sudo zypper install nvidia-open-driver-G06-kmp-default




Install the rest of the NVIDIA driver packages:

sudo zypper install cuda-drivers




OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package:

sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-<flavor> | sed 's| ||g' | awk -F '|' '/<driver_branch>/ {print $2"="$4}')




Install the rest of the NVIDIA driver packages:

sudo zypper -v install cuda-drivers-<driver_branch>


For example:

sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-default | sed 's| ||g' | awk -F '|' '/550/ {print $2"="$4}')
sudo zypper -v install cuda-drivers-550







5.8. Ubuntuï



Install the NVIDIA Open GPU Kernel Modules package:

sudo apt-get install nvidia-driver-<driver_branch>-open




Install the rest of the NVIDIA driver packages:

sudo apt-get install cuda-drivers-<driver_branch>





Note
End-users on Ubuntu should upgrade their NVIDIA Open GPU kernel modules using the following:

sudo apt-get install --verbose-versions nvidia-kernel-source-550-open cuda-drivers-550



OR to install a specific driver version


Install the NVIDIA Open GPU Kernel Modules package:

sudo apt-get install -v nvidia-driver-<driver_branch>-open




Install the rest of the NVIDIA driver packages:

sudo apt-get install -v cuda-drivers-<driver_branch>


For example:

sudo apt-get install -v nvidia-driver-550-open
sudo apt-get install -v cuda-drivers-550








6. Precompiled Streamsï

Precompiled streams offer an optional method of streamlining the installation process.
The advantages of precompiled streams:

Precompiled: faster boot up after driver and/or kernel updates
Pre-tested: kernel and driver combination has been validated
Removes gcc dependency: no compiler installation required
Removes dkms dependency: enabling EPEL repository not required
Removes kernel-devel and kernel-headers dependencies: no black screen if matching packages are missing

When using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale .ko files. To prevent system breakages, the NVIDIA dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists. This can delay the application of security fixes but ensures that a tested kernel and driver combination is always used. A warning is displayed by dnf during that upgrade situation:

NOTE:  Skipping kernel installation since no NVIDIA driver kernel module package
 kmod-nvidia-${driver}-${kernel} ... could be found


Packaging templates and instructions are provided on GitHub to allow you to maintain your own precompiled kernel module packages for custom kernels and derivative Linux distros: NVIDIA/yum-packaging-precompiled-kmod
To use the new driver packages on RHEL 8 or RHEL 9:


First, ensure that the Red Hat repositories are enabled:
RHEL 8:

subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms


or
RHEL 9:



subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms






Choose one of the four options below depending on the desired driver:


latest always updates to the highest versioned driver (precompiled):

sudo dnf module install nvidia-driver:latest




<id> locks the driver updates to the specified driver branch (precompiled):

sudo dnf module install nvidia-driver:<id>



Note
Replace <id> with the appropriate driver branch streams, for example 520, 515, 470, or 450.



latest-dkms always updates to the highest versioned driver (non-precompiled):

sudo dnf module install nvidia-driver:latest-dkms



Note
This is the default stream.



<id>-dkms locks the driver updates to the specified driver branch (non-precompiled):

sudo dnf module install nvidia-driver:<id>-dkms



Note
Valid streams include 520-dkms, 515-dkms, 470-dkms, and 450-dkms.







6.1. Precompiled Streams Support Matrixï

This table shows the supported precompiled and legacy DKMS streams for each driver.









NVIDIA Driver
Precompiled Stream
Legacy DKMS Stream
Open DKMS Stream




Highest version
latest
latest-dkms
open-dkms


Locked at 520.x
520
520-dkms
520-open


Locked at 515.x
515
515-dkms
515-open



Prior to switching between module streams, first reset:

sudo dnf module reset nvidia-driver



Note
This is also required for upgrading between branch locked streams.

Or alternatively:

sudo dnf module switch-to nvidia-driver:<stream>





6.2. Modularity Profilesï

Modularity profiles work with any supported modularity stream and allow for additional use cases. These modularity profiles are available on RHEL8+ and Fedora.


Table 6 Table 5. List of nvidia-driver Module Profilesï








Stream
Profile
Use Case




Default
/default
Installs all the driver packages in a stream.


Kickstart
/ks
Performs unattended Linux OS installation using a config file.


NVSwitch Fabric
/fm
Installs all the driver packages plus components required for bootstrapping an NVSwitch system (including the Fabric Manager and NSCQ telemetry).


Source
/src
Source headers for compilation (precompiled streams only).



For example:

sudo dnf module nvidia-driver:<stream>/default
sudo dnf module nvidia-driver:<stream>/ks
sudo dnf module nvidia-driver:<stream>/fm
sudo dnf module nvidia-driver:<stream>/src


You can install multiple modularity profiles using BASH curly brace expansion, for example:

sudo dnf module install nvidia-driver:latest/{default,src}


See https://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams in the Developer Blog and https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/precompiled/ for more information.




7. Kickstart Installationï



7.1. RHEL 8 / Rocky Linux 8ï



Enable the EPEL repository:

repo --name=epel --baseurl=http://download.fedoraproject.org/pub/epel/8/Everything/x86_64/




Enable the CUDA repository:

repo --name=cuda-rhel8 --baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/




In the packages section of the ks.cfg file, make sure you are using the /ks profile and :latest-dkms stream:

@nvidia-driver:latest-dkms/ks



Perform the post-installation actions.




7.2. RHEL 9 / Rocky Linux 9ï



Enable the EPEL repository:

repo --name=epel --baseurl=http://download.fedoraproject.org/pub/epel/9/Everything/x86_64/




Enable the CUDA repository:

repo --name=cuda-rhel9 --baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/




In the packages section of the ks.cfg file, make sure you are using the /ks profile and :latest-dkms stream:

@nvidia-driver:latest-dkms/ks



Perform the post-installation actions.





8. Runfile Installationï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.
This section describes the installation and configuration of CUDA when using the standalone installer. The standalone installer is a â.runâ file and is completely self-contained.


8.1. Runfile Overviewï

The Runfile installation installs the NVIDIA Driver and CUDA Toolkit via an interactive ncurses-based interface.
The installation steps are listed below. Distribution-specific instructions on disabling the Nouveau drivers as well as steps for verifying device node creation are also provided.
Finally, advanced options for the installer and uninstallation steps are detailed below.
The Runfile installation does not include support for cross-platform development. For cross-platform development, see the CUDA Cross-Platform Environment section.



8.2. Installationï


Perform the pre-installation actions.
Disable the Nouveau drivers.

Reboot into text mode (runlevel 3).
This can usually be accomplished by adding the number â3â to the end of the systemâs kernel boot parameters.
Since the NVIDIA drivers are not yet installed, the text terminals may not display correctly. Temporarily adding ânomodesetâ to the systemâs kernel boot parameters may fix this issue.
Consult your systemâs bootloader documentation for information on how to make the above boot parameter changes.
The reboot is required to completely unload the Nouveau drivers and prevent the graphical interface from loading. The CUDA driver cannot be installed while the Nouveau drivers are loaded or while the graphical interface is active.

Verify that the Nouveau drivers are not loaded. If the Nouveau drivers are still loaded, consult your distributionâs documentation to see if further steps are needed to disable Nouveau.

Run the installer and follow the on-screen prompts:

sudo sh cuda_<version>_linux.run


The installer will prompt for the following:

EULA Acceptance
CUDA Driver installation
CUDA Toolkit installation, location, and /usr/local/cuda symbolic link

The default installation location for the toolkit is /usr/local/cuda-12.4:
The /usr/local/cuda symbolic link points to the location where the CUDA Toolkit was installed. This link allows projects to use the latest CUDA Toolkit without any configuration file update.
The installer must be executed with sufficient privileges to perform some actions. When the current privileges are insufficient to perform an action, the installer will ask for the userâs password to attempt to install with root privileges. Actions that cause the installer to attempt to install with root privileges are:

installing the CUDA Driver
installing the CUDA Toolkit to a location the user does not have permission to write to
creating the /usr/local/cuda symbolic link

Running the installer with sudo, as shown above, will give permission to install to directories that require root permissions. Directories and files created while running the installer with sudo will have root ownership.
If installing the driver, the installer will also ask if the openGL libraries should be installed. If the GPU used for display is not an NVIDIA GPU, the NVIDIA openGL libraries should not be installed. Otherwise, the openGL libraries used by the graphics driver of the non-NVIDIA GPU will be overwritten and the GUI will not work. If performing a silent installation, the --no-opengl-libs option should be used to prevent the openGL libraries from being installed. See the Advanced Options section for more details.
If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf, may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information.

Note
Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries.



Reboot the system to reload the graphical interface:

sudo reboot



Verify the device nodes are created properly.
Perform the post-installation actions.




8.3. Disabling Nouveauï

To install the Display Driver, the Nouveau drivers must first be disabled. Each distribution of Linux has a different method for disabling Nouveau.
The Nouveau drivers are loaded if the following command prints anything:

lsmod | grep nouveau




8.3.1. Fedoraï



Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:

blacklist nouveau
options nouveau modeset=0




Regenerate the kernel initramfs:

sudo dracut --force




Run the following command:

sudo grub2-mkconfig -o /boot/grub2/grub.cfg



Reboot the system.




8.3.2. RHEL / Rocky and KylinOSï



Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:

blacklist nouveau
options nouveau modeset=0




Regenerate the kernel initramfs:

sudo dracut --force







8.3.3. OpenSUSEï



Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:

blacklist nouveau
options nouveau modeset=0




Regenerate the kernel initrd:

sudo /sbin/mkinitrd







8.3.4. SLESï

No actions to disable Nouveau are required as Nouveau is not installed on SLES.



8.3.5. WSLï

No actions to disable Nouveau are required as Nouveau is not installed on WSL.



8.3.6. Ubuntuï



Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:

blacklist nouveau
options nouveau modeset=0




Regenerate the kernel initramfs:

sudo update-initramfs -u







8.3.7. Debianï



Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:

blacklist nouveau
options nouveau modeset=0




Regenerate the kernel initramfs:

sudo update-initramfs -u








8.4. Device Node Verificationï

Check that the device files/dev/nvidia* exist and have the correct (0666) file permissions. These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver. Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuidnvidia-modprobe tool that is bundled with the NVIDIA Driver. However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below:

#!/bin/bash

/sbin/modprobe nvidia

if [ "$?" -eq 0 ]; then
  # Count the number of NVIDIA controllers found.
  NVDEVS=`lspci | grep -i NVIDIA`
  N3D=`echo "$NVDEVS" | grep "3D controller" | wc -l`
  NVGA=`echo "$NVDEVS" | grep "VGA compatible controller" | wc -l`

  N=`expr $N3D + $NVGA - 1`
  for i in `seq 0 $N`; do
    mknod -m 666 /dev/nvidia$i c 195 $i
  done

  mknod -m 666 /dev/nvidiactl c 195 255

else
  exit 1
fi

/sbin/modprobe nvidia-uvm

if [ "$?" -eq 0 ]; then
  # Find out the major device number used by the nvidia-uvm driver
  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`

  mknod -m 666 /dev/nvidia-uvm c $D 0
else
  exit 1
fi





8.5. Advanced Optionsï









Action
Options Used
Explanation




Silent Installation
--silent
Required for any silent installation. Performs an installation with no further user-input and minimal command-line output based on the options provided below. Silent installations are useful for scripting the installation of CUDA. Using this option implies acceptance of the EULA. The following flags can be used to customize the actions taken during installation. At least one of --driver, --uninstall, and --toolkit must be passed if running with non-root permissions.


--driver
Install the CUDA Driver.


--toolkit
Install the CUDA Toolkit.


--toolkitpath=<path>
Install the CUDA Toolkit to the <path> directory. If not provided, the default path of /usr/local/cuda-12.4 is used.


--defaultroot=<path>
Install libraries to the <path> directory. If the <path> is not provided, then the default path of your distribution is used. This only applies to the libraries installed outside of the CUDA Toolkit path.


Extraction
--extract=<path>

Extracts to the <path> the following: the driver runfile, the raw files of the toolkit to <path>.
This is especially useful when one wants to install the driver using one or more of the command-line options provided by the driver installer which are not exposed in this installer.



Overriding Installation Checks
--override
Ignores compiler, third-party library, and toolkit detection checks which would prevent the CUDA Toolkit from installing.


No OpenGL Libraries
--no-opengl-libs
Prevents the driver installation from installing NVIDIAâs GL libraries. Useful for systems where the display is driven by a non-NVIDIA GPU. In such systems, NVIDIAâs GL libraries could prevent X from loading properly.


No man pages
--no-man-page
Do not install the man pages under /usr/share/man.


Overriding Kernel Source
--kernel-source-path=<path>
Tells the driver installation to use <path> as the kernel source directory when building the NVIDIA kernel module. Required for systems where the kernel source is installed to a non-standard location.


Running nvidia-xconfig
--run-nvidia-xconfig
Tells the driver installation to run nvidia-xconfig to update the system X configuration file so that the NVIDIA X driver is used. The pre-existing X configuration file will be backed up.


No nvidia-drm kernel module
--no-drm
Do not install the nvidia-drm kernel module. This option should only be used to work around failures to build or install the nvidia-drm kernel module on systems that do not need the provided features.


Custom Temporary Directory Selection
--tmpdir=<path>
Performs any temporary actions within <path> instead of /tmp. Useful in cases where /tmp cannot be used (doesnât exist, is full, is mounted with ânoexecâ, etc.).


Show Installer Options
--help
Prints the list of command-line options to stdout.






8.6. Uninstallationï

To uninstall the CUDA Toolkit, run the uninstallation script provided in the bin directory of the toolkit. By default, it is located in /usr/local/cuda-12.4/bin:

sudo /usr/local/cuda-12.4/bin/cuda-uninstaller


To uninstall the NVIDIA Driver, run nvidia-uninstall:

sudo /usr/bin/nvidia-uninstall


To enable the Nouveau drivers, remove the blacklist file created in the Disabling Nouveau section, and regenerate the kernel initramfs/initrd again as described in that section.




9. Conda Installationï

This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia.


9.1. Conda Overviewï

The Conda installation installs the CUDA Toolkit. The installation steps are listed below.



9.2. Installing CUDA Using Condaï

To perform a basic install of all CUDA Toolkit components using Conda, run the following command:

conda install cuda -c nvidia





9.3. Uninstalling CUDA Using Condaï

To uninstall the CUDA Toolkit using Conda, run the following command:

conda remove cuda





9.4. Installing Previous CUDA Releasesï

All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as:

conda install cuda -c nvidia/label/cuda-11.3.0





9.5. Upgrading from cudatoolkit Packageï

If you had previously installed CUDA using the cudatoolkit package and want to maintain a similar install footprint, you can limit your installation to the following packages:

cuda-libraries-dev
cuda-nvcc
cuda-nvtx
cuda-cupti


Note
Some extra files, such as headers, will be included in this installation which were not included in the cudatoolkit package. If you need to reduce your installation further, replace cuda-libraries-dev with the specific libraries you need.





10. Pip Wheelsï

NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.

python3 -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.

python3 -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:

--extra-index-url https://pypi.org/simple


Procedure
Install the CUDA runtime package:

python3 -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:

python3 -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cccl-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-cuda-opencl-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cublas-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvfatbin-cu12
nvidia-nvjitlink-cu12
nvidia-nvjpeg-cu12
nvidia-nvml-dev-cu12
nvidia-nvtx-cu12

These metapackages install the following packages:

nvidia-cuda-runtime-cu125
nvidia-cuda-cccl-cu125
nvidia-cuda-cupti-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-opencl-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-cuda-nvrtc-cu125
nvidia-cufft-cu125
nvidia-curand-cu125
nvidia-cusolver-cu125
nvidia-cusparse-cu125
nvidia-npp-cu125
nvidia-nvfatbin-cu125
nvidia-nvjitlink-cu125
nvidia-nvjpeg-cu125
nvidia-nvml-dev-cu125
nvidia-nvtx-cu125




11. Tarball and Zip Archive Deliverablesï

In an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community CI/CD systems, tarball and zip archives are available for each component.
These tarball and zip archives, known as binary archives, are provided at https://developer.download.nvidia.com/compute/cuda/redist/.

These component .tar.xz and .zip binary archives do not replace existing packages such as .deb, .rpm, runfile, conda, etc. and are not meant for general consumption, as they are not installers. However this standardized approach will replace existing .txz archives.
For each release, a JSON manifest is provided such as redistrib_11.4.2.json, which corresponds to the CUDA 11.4.2 release label (CUDA 11.4 update 2) which includes the release date, the name of each component, license name, relative URL for each platform and checksums.
Package maintainers are advised to check the provided LICENSE for each component prior to redistribution. Instructions for developers using CMake and Bazel build systems are provided in the next sections.


11.1. Parsing Redistrib JSONï

The following example of a JSON manifest contains keys for each component: name, license, version, and a platform array which includes relative_path, sha256, md5, and size (bytes) for each archive.

{
    "release_date": "2021-09-07",
    "cuda_cudart": {
        "name": "CUDA Runtime (cudart)",
        "license": "CUDA Toolkit",
        "version": "11.4.108",
        "linux-x86_64": {
            "relative_path": "cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-11.4.108-archive.tar.xz",
            "sha256": "d08a1b731e5175aa3ae06a6d1c6b3059dd9ea13836d947018ea5e3ec2ca3d62b",
            "md5": "da198656b27a3559004c3b7f20e5d074",
            "size": "828300"
        },
        "linux-ppc64le": {
            "relative_path": "cuda_cudart/linux-ppc64le/cuda_cudart-linux-ppc64le-11.4.108-archive.tar.xz",
            "sha256": "831dffe062ae3ebda3d3c4010d0ee4e40a01fd5e6358098a87bb318ea7c79e0c",
            "md5": "ca73328e3f8e2bb5b1f2184c98c3a510",
            "size": "776840"
        },
        "linux-sbsa": {
            "relative_path": "cuda_cudart/linux-sbsa/cuda_cudart-linux-sbsa-11.4.108-archive.tar.xz",
            "sha256": "2ab9599bbaebdcf59add73d1f1a352ae619f8cb5ccec254093c98efd4c14553c",
            "md5": "aeb5c19661f06b6398741015ba368102",
            "size": "782372"
        },
        "windows-x86_64": {
            "relative_path": "cuda_cudart/windows-x86_64/cuda_cudart-windows-x86_64-11.4.108-archive.zip",
            "sha256": "b59756c27658d1ea87a17c06d064d1336576431cd64da5d1790d909e455d06d3",
            "md5": "7f6837a46b78198402429a3760ab28fc",
            "size": "2897751"
        }
    }
}


A JSON schema is provided at https://developer.download.nvidia.com/compute/redist/redistrib-v2.schema.json.
A sample script that parses these JSON manifests is available on GitHub:

Downloads each archive
Validates SHA256 checksums
Extracts archives
Flattens into a collapsed directory structure



Table 7 Available Tarball and Zip Archivesï







Product
Example




CUDA Toolkit
./parse_redist.py --product cuda --label 12.3.2


cuBLASMp
./parse_redist.py --product cublasmp --label 0.1.0


cuDNN
./parse_redist.py --product cudnn --label 8.9.6.50


cuDSS
./parse_redist.py --product cudss --label 0.1.0


cuQuantum
./parse_redist.py --product cuquantum --label 23.10.0


cuSPARSELt
./parse_redist.py --product cusparselt --label 0.5.2


cuTENSOR
./parse_redist.py --product cutensor --label 1.7.0


NVIDIA driver
./parse_redist.py --product nvidia-driver --label 535.129.03


nvJPEG2000
./parse_redist.py --product nvjpeg2000 --label 0.7.5


NVPL
./parse_redist.py --product nvpl --label 23.11


nvTIFF
./parse_redist.py --product nvtiff --label 0.3.0






11.2. Importing Tarballs into CMakeï

The recommended module for importing these tarballs into the CMake build system is via FindCUDAToolkit (3.17 and newer).

Note
The FindCUDA module is deprecated.

The path to the extraction location can be specified with the CUDAToolkit_ROOT environmental variable. For example CMakeLists.txt and commands, see cmake/1_FindCUDAToolkit/.
For older versions of CMake, the ExternalProject_Add module is an alternative method. For example CMakeLists.txt file and commands, see cmake/2_ExternalProject/.



11.3. Importing Tarballs into Bazelï

The recommended method of importing these tarballs into the Bazel build system is using http_archive and pkg_tar.
For an example, see bazel/1_pkg_tar/.




12. CUDA Cross-Platform Environmentï

Cross development for arm64-sbsa is supported on Ubuntu 20.04, Ubuntu 22.04, RHEL 8, RHEL 9, and SLES 15.
Cross development for arm64-Jetson is only supported on Ubuntu 20.04
We recommend selecting a host development environment that matches the supported cross-target environment. This selection helps prevent possible host/target incompatibilities, such as GCC or GLIBC version mismatches.


12.1. CUDA Cross-Platform Installationï

Some of the following steps may have already been performed as part of the native Ubuntu installation. Such steps can safely be skipped.
These steps should be performed on the x86_64 host system, rather than the target system. To install the native CUDA Toolkit on the target system, refer to the native Ubuntu installation section.

Perform the pre-installation actions.

Install repository meta-data package with:

sudo dpkg -i cuda-repo-cross-<identifier>_all.deb


where <identifier> indicates the operating system, architecture, and/or the version of the package.


Update the Apt repository cache:

sudo apt-get update




Install the appropriate cross-platform CUDA Toolkit:


For aarch64:

sudo apt-get install cuda-cross-aarch64




For QNX:

sudo apt-get install cuda-cross-qnx





Perform the post-installation actions.




12.2. CUDA Cross-Platform Samplesï

CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples.




13. Post-installation Actionsï

The post-installation actions must be manually performed. These actions are split into mandatory, recommended, and optional sections.


13.1. Mandatory Actionsï

Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used.


13.1.1. Environment Setupï

The PATH variable needs to include export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}}. Nsight Compute has moved to /opt/nvidia/nsight-compute/ only in rpm/deb installation method. When using .run installer it is still located under /usr/local/cuda-12.4/.
To add this path to the PATH variable:

export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}}


In addition, when using the runfile installation method, the LD_LIBRARY_PATH variable needs to contain /usr/local/cuda-12.4/lib64 on a 64-bit system, or /usr/local/cuda-12.4/lib on a 32-bit system


To change the environment variables for 64-bit operating systems:

export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}




To change the environment variables for 32-bit operating systems:

export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}




Note that the above paths change when using a custom install path with the runfile installation method.




13.2. Recommended Actionsï

Other actions are recommended to verify the integrity of the installation.


13.2.1. Install Persistence Daemonï

NVIDIA is providing a user-space daemon on Linux to support persistence of driver state across CUDA job runs. The daemon approach provides a more elegant and robust solution to this problem than persistence mode. For more details on the NVIDIA Persistence Daemon, see the documentation here.
The NVIDIA Persistence Daemon can be started as the root user by running:

/usr/bin/nvidia-persistenced --verbose


This command should be run on boot. Consult your Linux distributionâs init documentation for details on how to automate this.



13.2.2. Install Writable Samplesï

CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples.



13.2.3. Verify the Installationï

Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the sample programs, located in https://github.com/nvidia/cuda-samples.

Note
Ensure the PATH and, if using the runfile installation method, LD_LIBRARY_PATH variables are set correctly.



13.2.3.1. Verify the Driver Versionï

If you installed the driver, verify that the correct version of it is loaded. If you did not install the driver, or are using an operating system where the driver is not loaded via a kernel module, such as L4T, skip this step.
When the driver is loaded, the driver version can be found by executing the command

cat /proc/driver/nvidia/version


Note that this command will not work on an iGPU/dGPU system.



13.2.3.2. Running the Binariesï

After compilation, find and run deviceQueryfrom https://github.com/nvidia/cuda-samples. If the CUDA software is installed and configured correctly, the output for deviceQuery should look similar to that shown in Figure 1.



Figure 1 Figure 1. Valid Results from deviceQuery CUDA Sampleï


The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found (the first highlighted line), that the device matches the one on your system (the second highlighted line), and that the test passed (the final highlighted line).
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, this likely means that the /dev/nvidia* files are missing or have the wrong permissions.
On systems where SELinux is enabled, you might need to temporarily disable this security feature to run deviceQuery. To do this, type:

setenforce 0


from the command line as the superuser.
Running the bandwidthTest program ensures that the system and the CUDA-capable device are able to communicate correctly. Its output is shown in Figure 2.



Figure 2 Figure 2. Valid Results from bandwidthTest CUDA Sampleï


Note that the measurements for your CUDA-capable device description will vary from system to system. The important point is that you obtain measurements, and that the second-to-last line (in Figure 2) confirms that all necessary tests passed.
Should the tests not pass, make sure you have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
If you run into difficulties with the link step (such as libraries not being found), consult the Linux Release Notes found in https://github.com/nvidia/cuda-samples.




13.2.4. Install Nsight Eclipse Pluginsï

To install Nsight Eclipse plugins, an installation script is provided:

/usr/local/cuda-12.4/bin/nsight_ee_plugins_manage.sh install <eclipse-dir>


Refer to Nsight Eclipse Plugins Installation Guide for more details.



13.2.5. Local Repo Removalï

Removal of the local repo installer is recommended after installation of CUDA SDK.
Ubuntu and Debian

sudo apt-get remove --purge "cuda-repo-<distro>-X-Y-local*"


Fedora

sudo dnf remove "cuda-repo-<distro>-X-Y-local*"


RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8

sudo dnf remove "cuda-repo-<distro>-X-Y-local*"


openSUSE 15 and SLES 15

sudo zypper remove "cuda-repo-<distro>-X-Y-local*"


Removal of the local repo installer is recommended after installation of NVIDA driver.
Ubuntu and Debian

sudo apt-get remove --purge "nvidia-driver-local-repo-<distro>*"


Fedora

sudo dnf remove "nvidia-driver-local-repo-<distro>*"


RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8

sudo dnf remove "nvidia-driver-local-repo-<distro>*"


openSUSE 15 and SLES 15

sudo zypper remove "nvidia-driver-local-repo-<distro>*"






13.3. Optional Actionsï

Other options are not necessary to use the CUDA Toolkit, but are available to provide additional features.


13.3.1. Install Third-party Librariesï

Some CUDA samples use third-party libraries which may not be installed by default on your system. These samples attempt to detect any required libraries when building.
If a library is not detected, it waives itself and warns you which library is missing. To build and run these samples, you must install the missing libraries. In cases where these dependencies are not installed, follow the instructions below.
RHEL 8 / Rocky Linux 8

sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \
    make mesa-libGLU-devel freeimage-devel libglfw3-devel


RHEL 9 / Rocky Linux 9

sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \
                    make mesa-libGLU-devel freeimage-devel libglfw3-devel


KylinOS 10

sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \
                    make mesa-libGLU-devel freeimage-devel libglfw3-devel


Fedora

sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \
    make mesa-libGLU-devel freeimage-devel libglfw3-devel


SLES

sudo zypper install libglut3 libX11-devel libXi6 libXmu6 libGLU1 make


OpenSUSE

sudo zypper install freeglut-devel libX11-devel libXi-devel libXmu-devel \
    make Mesa-libGL-devel freeimage-devel


Ubuntu

sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \
    libxmu-dev libxi-dev libglu1-mesa-dev libfreeimage-dev libglfw3-dev


Debian

sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \
    libxmu-dev libxi-dev libglu1-mesa-dev libfreeimage-dev libglfw3-dev





13.3.2. Install the Source Code for cuda-gdbï

The cuda-gdb source must be explicitly selected for installation with the runfile installation method. During the installation, in the component selection page, expand the component âCUDA Tools 12.4â and select cuda-gdb-src for installation. It is unchecked by default.
To obtain a copy of the source code for cuda-gdb using the RPM and Debian installation methods, the cuda-gdb-src package must be installed.
The source code is installed as a tarball in the /usr/local/cuda-12.4/extras directory.



13.3.3. Select the Active Version of CUDAï

For applications that rely on the symlinks /usr/local/cuda and /usr/local/cuda-MAJOR, you may wish to change to a different installed version of CUDA using the provided alternatives.
To show the active version of CUDA and all available versions:

update-alternatives --display cuda


To show the active minor version of a given major CUDA release:

update-alternatives --display cuda-12


To update the active version of CUDA:

sudo update-alternatives --config cuda







14. Advanced Setupï

Below is information on some advanced setup scenarios which are not covered in the basic instructions above.


Table 8 Advanced Setup Scenarios when Installing CUDAï







Scenario
Instructions




Install CUDA using the Package Manager installation method without installing the NVIDIA GL libraries.

Fedora
Install CUDA using the following command:

sudo dnf install cuda-toolkit-12-4 \
    nvidia-driver-cuda akmod-nvidia


Follow the instructions here to ensure that Nouveau is disabled.
If performing an upgrade over a previous installation, the NVIDIA kernel module may need to be rebuilt by following the instructions here.
OpenSUSE/SLES
On some system configurations the NVIDIA GL libraries may need to be locked before installation using:

sudo zypper addlock nvidia-glG04


Install CUDA using the following command:

sudo zypper install --no-recommends cuda-toolkit-12-4 \
    nvidia-computeG04 \
    nvidia-gfxG04-kmp-default


Follow the instructions here to ensure that Nouveau is disabled.
Ubuntu
This functionality isnât supported on Ubuntu. Instead, the driver packages integrate with the Bumblebee framework to provide a solution for users who wish to control what applications the NVIDIA drivers are used for. See Ubuntuâs Bumblebee wiki for more information.




Upgrade from a RPM/Deb driver installation which includes the diagnostic driver packages to a
driver installation which does not include the diagnostic driver packages.


RHEL/CentOS
Remove diagnostic packages using the following command:

sudo yum remove cuda-drivers-diagnostic \
    xorg-x11-drv-nvidia-diagnostic


Follow the instructions here to continue installation as normal.
Fedora
Remove diagnostic packages using the following command:

sudo dnf remove cuda-drivers-diagnostic \
    xorg-x11-drv-nvidia-diagnostic


Follow the instructions here to continue installation as normal.
OpenSUSE/SLES
Remove diagnostic packages using the following command:

sudo zypper remove cuda-drivers-diagnostic \
    nvidia-diagnosticG04


Follow the instructions here to continue installation as normal.
Ubuntu
Remove diagnostic packages using the following command:

sudo apt-get purge cuda-drivers-diagnostic \
    nvidia-384-diagnostic


Follow the instructions here to continue installation as normal.



Use a specific GPU for rendering the display.

Add or replace a Device entry in your xorg.conf file, located at /etc/X11/xorg.conf. The Device entry should resemble the following:

Section "Device"
    Identifier    "Device0"
    Driver        "driver_name"
    VendorName    "vendor_name"
    BusID         "bus_id"
EndSection


The details will you will need to add differ on a case-by-case basis. For example, if you have two NVIDIA GPUs and you want the first GPU to be used for display, you would replace âdriver_nameâ with ânvidiaâ, âvendor_nameâ with âNVIDIA Corporationâ and âbus_idâ with the Bus ID of the GPU.
The Bus ID will resemble âPCI:00:02.0â and can be found by running lspci.



Install CUDA to a specific directory using the Package Manager installation method.

RPM
The RPM packages donât support custom install locations through the package managers (Yum and Zypper), but it is possible to install the RPM packages to a custom location using rpmâs --relocate parameter:

sudo rpm --install --relocate /usr/local/cuda-12.4=/new/toolkit package.rpm


You will need to install the packages in the correct dependency order; this task is normally taken care of by the package managers. For example, if package âfooâ has a dependency on package âbarâ, you should install package âbarâ first, and package âfooâ second. You can check the dependencies of a RPM package as follows:

rpm -qRp package.rpm


Note that the driver packages cannot be relocated.
Deb
The Deb packages do not support custom install locations. It is however possible to extract the contents of the Deb packages and move the files to the desired install location. See the next scenario for more details one xtracting Deb packages.



Extract the contents of the installers.

Runfile
The Runfile can be extracted into the standalone Toolkit and Driver Runfiles by using the --extract parameter. The Toolkit standalone Runfiles can be further extracted by running:

./runfile.run --tar mxvf


The Driver Runfile can be extracted by running:

./runfile.run -x


RPM
The RPM packages can be extracted by running:

rpm2cpio package.rpm | cpio -idmv


Deb
The Deb packages can be extracted by running:

dpkg-deb -x package.deb output_dir






Modify Ubuntuâs apt package manager to query specific architectures for specific repositories.
This is useful when a foreign architecture has been added, causing â404 Not Foundâ errors to appear when the repository meta-data is updated.


Each repository you wish to restrict to specific architectures must have its sources.list entry modified. This is done by modifying the /etc/apt/sources.list file and any files containing repositories you wish to restrict under the /etc/apt/sources.list.d/ directory. Normally, it is sufficient to modify only the entries in /etc/apt/sources.list
An architecture-restricted repository entry looks like:

deb [arch=<arch1>,<arch2>] <url>


For example, if you wanted to restrict a repository to only the amd64 and i386 architectures, it would look like:

deb [arch=amd64,i386] <url>


It is not necessary to restrict the deb-src repositories, as these repositories donât provide architecture-specific packages.
For more details, see the sources.list manpage.




The nvidia.ko kernel module fails to load, saying some symbols are unknown.
For example:

nvidia: Unknown symbol drm_open (err 0)




Check to see if there are any optionally installable modules that might provide these symbols which are not currently installed.
For the example of the drm_open symbol, check to see if there are any packages which provide drm_open and are not already installed. For instance, on Ubuntu 14.04, the linux-image-extra package provides the DRM kernel module (which provides drm_open). This package is optional even though the kernel headers reflect the availability of DRM regardless of whether this package is installed or not.



The runfile installer fails to extract due to limited space in the TMP directory.
This can occur on systems with limited storage in the TMP directory (usually /tmp), or on systems which use a tmpfs in memory to handle temporary storage. In this case, the --tmpdir command-line option should be used to instruct the runfile to use a directory with sufficient space to extract into. More information on this option can be found here.


Re-enable Wayland after installing the RPM driver on Fedora.

Wayland is disabled during installation of the Fedora driver RPM due to compatability issues. To re-enable Wayland, comment out this line in /etc/gdm/custom.conf:

WaylandEnable=false





In case of the error: E: Failed to fetch file:/var/cuda-repo File not found

Debian and Ubuntu
This can occur when installing CUDA after uninstalling a different version. Use the following command before installation:

sudo rm -v /var/lib/apt/lists/*cuda* /var/lib/apt/lists/*nvidia*





Verbose installation on Debian and Ubuntu

Use the --verbose-versions flag, for example:

sudo apt-get install --verbose-versions cuda









15. Frequently Asked Questionsï



15.1. How do I install the Toolkit in a different location?ï

The Runfile installation asks where you wish to install the Toolkit during an interactive install. If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location:

./runfile.run --silent \
                --toolkit --toolkitpath=/my/new/toolkit


The RPM and Deb packages cannot be installed to a custom install location directly using the package managers. See the âInstall CUDA to a specific directory using the Package Manager installation methodâ scenario in the Advanced Setup section for more information.



15.2. Why do I see ânvcc: No such file or directoryâ when I try to build a CUDA application?ï

Your PATH environment variable is not set up correctly. Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12.4/bin.

export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}}





15.3. Why do I see âerror while loading shared libraries: <lib name>: cannot open shared object file: No such file or directoryâ when I try to run a CUDA application that uses a CUDA library?ï

Your LD_LIBRARY_PATH environment variable is not set up correctly. Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12.4/lib{,64}:

export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}





15.4. Why do I see multiple â404 Not Foundâ errors when updating my repository meta-data on Ubuntu?ï

These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the systemâs sources.list file. Repositories that do not host packages for the newly added architecture will present this error. While noisy, the error itself does no harm. Please see the Advanced Setup section for details on how to modify your sources.list file to prevent these errors.



15.5. How can I tell X to ignore a GPU for compute-only use?ï

To make sure X doesnât use a certain GPU for display, you need to specify which other GPU to use for display. For more information, please refer to the âUse a specific GPU for rendering the displayâ scenario in the Advanced Setup section.



15.6. Why doesnât the cuda-repo package install the CUDA Toolkit and Drivers?ï

When using RPM or Deb, the downloaded package is a repository package. Such a package only informs the package manager where to find the actual installation packages, but will not install them.
See the Package Manager Installation section for more details.



15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04?ï

After installing CUDA, set the driver value for the intel device in /etc/X11/xorg.conf to âmodesettingâ as shown below:

Section "Device"
    Identifier "intel"
    Driver "modesetting"
    ...
EndSection


To prevent Ubuntu from reverting the change in xorg.conf, edit /etc/default/grub to add ânogpumanagerâ to GRUB_CMDLINE_LINUX_DEFAULT.
Run the following command to update grub before rebooting:

sudo update-grub





15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update?ï

System updates may include an updated Linux kernel. In many cases, a new Linux kernel will be installed without properly updating the required Linux kernel headers and development packages. To ensure the CUDA driver continues to work when performing a system update, rerun the commands in the Kernel Headers and Development Packages section.
Additionally, on Fedora, the Akmods framework will sometimes fail to correctly rebuild the NVIDIA kernel module packages when a new Linux kernel is installed. When this happens, it is usually sufficient to invoke Akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting:

sudo akmods --force
sudo depmod


You can reach a virtual console by hitting ctrl+alt+f2 at the same time.



15.9. How do I install a CUDA driver with a version less than 367 using a network repo?ï

To install a CUDA driver at a version earlier than 367 using a network repo, the required packages will need to be explicitly installed at the desired version. For example, to install 352.99, instead of installing the cuda-drivers metapackage at version 352.99, you will need to install all required packages of cuda-drivers at version 352.99.



15.10. How do I install an older CUDA version using a network repo?ï

Depending on your system configuration, you may not be able to install old versions of CUDA using the cuda metapackage. In order to install a specific version of CUDA, you may need to specify all of the packages that would normally be installed by the cuda metapackage at the version you want to install.
If you are using yum to install certain packages at an older version, the dependencies may not resolve as expected. In this case you may need to pass â--setopt=obsoletes=0â to yum to allow an install of packages which are obsoleted at a later version than you are trying to install.



15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency?ï

This dependency comes from the SUSE repositories and shouldnât affect the use of the NVIDIA driver or the CUDA Toolkit. To disable this dependency, you can lock that package with the following command:

sudo zypper al Mesa-dri-nouveau





15.12. How do I handle âErrors were encountered while processing: glx-diversionsâ?ï

This sometimes occurs when trying to uninstall CUDA after a clean .deb installation. Run the following commands:

sudo apt-get install glx-diversions --reinstall
sudo apt-get remove nvidia-alternative


Then re-run the commands from Removing CUDA Toolkit and Driver.




16. Additional Considerationsï

Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C++ Programming Guide, located in /usr/local/cuda-12.4/doc.
A number of helpful development tools are included in the CUDA Toolkit to assist you as you develop your CUDA programs, such as NVIDIAÂ® Nsightâ¢ Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK.
For technical support on programming questions, consult and participate in the developer forums at https://forums.developer.nvidia.com/c/accelerated-computing/cuda/206.



17. Switching between Driver Module Flavorsï

Use the following steps to switch between the NVIDIA driver legacy and open module flavors on your system.

Note
If switching to open module, experimental support for GeForce and Quadro SKUs can be enabled with:

echo "options nvidia NVreg_OpenRmEnableUnsupportedGpus=1" | sudo tee /etc/modprobe.d/nvidia-gsp.conf




Note
Replace XXX with the NVIDIA driver branch number such as 550.

Fedora, RHEL 9 / Rocky Linux 9, RHEL 8 / Rocky Linux 8
To switch between legacy and open: uninstall, then reinstall.
Kylin OS
To switch between legacy and open: uninstall, then reinstall.
Ubuntu
To switch from legacy to open:

sudo apt-get --purge remove nvidia-kernel-source-XXX
sudo apt-get install --verbose-versions nvidia-kernel-open-XXX
sudo apt-get install --verbose-versions cuda-drivers-XXX


To switch from open to legacy:

sudo apt-get remove --purge nvidia-kernel-open-XXX
sudo apt-get install --verbose-versions cuda-drivers-XXX


Debian
To switch from legacy to open:

sudo apt-get --purge remove nvidia-kernel-dkms
sudo apt-get install --verbose-versions nvidia-kernel-open-dkms
sudo apt-get install --verbose-versions cuda-drivers-XXX


To switch from open to legacy:

sudo apt-get remove --purge nvidia-kernel-open-dkms
sudo apt-get install --verbose-versions cuda-drivers-XXX


OpenSUSE
To switch from legacy to open:

sudo zypper remove nvidia-driver-G06-kmp-default
sudo zypper install --details nvidia-open-driver-G06-kmp-default
sudo zypper install --details cuda-drivers-XXX


To switch from open to legacy:

sudo zypper remove nvidia-open-driver-G06-kmp-default
sudo zypper install --details cuda-drivers-XXX


SLES
To switch from legacy to open:

sudo zypper remove nvidia-driver-G06-kmp-default nvidia-driver-G06-kmp-azure
sudo zypper install --details nvidia-open-driver-G06-kmp-default nvidia-open-driver-G06-kmp-azure
sudo zypper install --details cuda-drivers-XXX


To switch from open to legacy:

sudo zypper remove nvidia-open-driver-G06-kmp-default nvidia-driver-G06-open-kmp-azure
sudo zypper install --details cuda-drivers-XXX



Note
The Azure package is only available for SLES (x86_64).




18. Removing CUDA Toolkit and Driverï

Follow the below steps to properly uninstall the CUDA Toolkit and NVIDIA Drivers from your system. These steps will ensure that the uninstallation will be clean.
KylinOS 10
To remove CUDA Toolkit:

sudo dnf remove "cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
                    "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"


To remove NVIDIA Drivers:

sudo dnf module remove --all nvidia-driver


To reset the module stream:

sudo dnf module reset nvidia-driver


RHEL 9 / Rocky Linux 9
To remove CUDA Toolkit:

sudo dnf remove "cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"


To remove NVIDIA Drivers:

sudo dnf module remove --all nvidia-driver


To reset the module stream:

sudo dnf module reset nvidia-driver


RHEL 8 / Rocky Linux 8
To remove CUDA Toolkit:

sudo dnf remove "cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"


To remove NVIDIA Drivers:

sudo dnf module remove --all nvidia-driver


To reset the module stream:

sudo dnf module reset nvidia-driver


Fedora
To remove CUDA Toolkit:

sudo dnf remove "cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"


To remove NVIDIA Drivers:

sudo dnf module remove --all nvidia-driver


To reset the module stream:

sudo dnf module reset nvidia-driver


To remove 3rd party NVIDIA Drivers:

sudo dnf remove "*nvidia*"


OpenSUSE / SLES
To remove CUDA Toolkit:

sudo zypper remove "cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"


To remove NVIDIA Drivers:

sudo zypper remove "*nvidia*"


Ubuntu and Debian
To remove CUDA Toolkit:

sudo apt-get --purge remove "*cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" "*nvvm*"


To remove NVIDIA Drivers:

sudo apt-get --purge remove "*nvidia*" "libxnvctrl*"


To clean up the uninstall:

sudo apt-get autoremove





19. Noticesï



19.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



19.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



19.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.




20. Copyrightï

Â© 2009-2024 NVIDIA Corporation & affiliates. All rights reserved.
This product includes software developed by the Syncro Soft SRL (http://www.sync.ro/).









Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


Copyright Â© 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      



















CUDA C++ Programming Guide












































1. Introduction
1.1. The Benefits of Using GPUs
1.2. CUDAÂ®: A General-Purpose Parallel Computing Platform and Programming Model
1.3. A Scalable Programming Model
1.4. Document Structure



2. Programming Model
2.1. Kernels

2.2. Thread Hierarchy
2.2.1. Thread Block Clusters


2.3. Memory Hierarchy
2.4. Heterogeneous Programming

2.5. Asynchronous SIMT Programming Model
2.5.1. Asynchronous Operations


2.6. Compute Capability



3. Programming Interface

3.1. Compilation with NVCC

3.1.1. Compilation Workflow
3.1.1.1. Offline Compilation
3.1.1.2. Just-in-Time Compilation


3.1.2. Binary Compatibility
3.1.3. PTX Compatibility
3.1.4. Application Compatibility
3.1.5. C++ Compatibility
3.1.6. 64-Bit Compatibility



3.2. CUDA Runtime
3.2.1. Initialization
3.2.2. Device Memory

3.2.3. Device Memory L2 Access Management
3.2.3.1. L2 cache Set-Aside for Persisting Accesses
3.2.3.2. L2 Policy for Persisting Accesses
3.2.3.3. L2 Access Properties
3.2.3.4. L2 Persistence Example
3.2.3.5. Reset L2 Access to Normal
3.2.3.6. Manage Utilization of L2 set-aside cache
3.2.3.7. Query L2 cache Properties
3.2.3.8. Control L2 Cache Set-Aside Size for Persisting Memory Access


3.2.4. Shared Memory
3.2.5. Distributed Shared Memory

3.2.6. Page-Locked Host Memory
3.2.6.1. Portable Memory
3.2.6.2. Write-Combining Memory
3.2.6.3. Mapped Memory



3.2.7. Memory Synchronization Domains
3.2.7.1. Memory Fence Interference
3.2.7.2. Isolating Traffic with Domains
3.2.7.3. Using Domains in CUDA



3.2.8. Asynchronous Concurrent Execution
3.2.8.1. Concurrent Execution between Host and Device
3.2.8.2. Concurrent Kernel Execution
3.2.8.3. Overlap of Data Transfer and Kernel Execution
3.2.8.4. Concurrent Data Transfers

3.2.8.5. Streams
3.2.8.5.1. Creation and Destruction of Streams
3.2.8.5.2. Default Stream
3.2.8.5.3. Explicit Synchronization
3.2.8.5.4. Implicit Synchronization
3.2.8.5.5. Overlapping Behavior
3.2.8.5.6. Host Functions (Callbacks)
3.2.8.5.7. Stream Priorities



3.2.8.6. Programmatic Dependent Launch and Synchronization
3.2.8.6.1. Background
3.2.8.6.2. API Description
3.2.8.6.3. Use in CUDA Graphs



3.2.8.7. CUDA Graphs

3.2.8.7.1. Graph Structure
3.2.8.7.1.1. Node Types
3.2.8.7.1.2. Edge Data


3.2.8.7.2. Creating a Graph Using Graph APIs

3.2.8.7.3. Creating a Graph Using Stream Capture
3.2.8.7.3.1. Cross-stream Dependencies and Events
3.2.8.7.3.2. Prohibited and Unhandled Operations
3.2.8.7.3.3. Invalidation


3.2.8.7.4. CUDA User Objects

3.2.8.7.5. Updating Instantiated Graphs
3.2.8.7.5.1. Graph Update Limitations
3.2.8.7.5.2. Whole Graph Update
3.2.8.7.5.3. Individual node update
3.2.8.7.5.4. Individual node enable


3.2.8.7.6. Using Graph APIs

3.2.8.7.7. Device Graph Launch

3.2.8.7.7.1. Device Graph Creation
3.2.8.7.7.1.1. Device Graph Requirements
3.2.8.7.7.1.2. Device Graph Upload
3.2.8.7.7.1.3. Device Graph Update



3.2.8.7.7.2. Device Launch

3.2.8.7.7.2.1. Device Launch Modes
3.2.8.7.7.2.1.1. Fire and Forget Launch
3.2.8.7.7.2.1.2. Graph Execution Environments

3.2.8.7.7.2.1.3. Tail Launch
3.2.8.7.7.2.1.3.1. Tail Self-launch


3.2.8.7.7.2.1.4. Sibling Launch







3.2.8.7.8. Conditional Graph Nodes
3.2.8.7.8.1. Conditional Handles
3.2.8.7.8.2. Condtional Node Body Graph Requirements
3.2.8.7.8.3. Conditional IF Nodes
3.2.8.7.8.4. Conditional WHILE Nodes





3.2.8.8. Events
3.2.8.8.1. Creation and Destruction of Events
3.2.8.8.2. Elapsed Time


3.2.8.9. Synchronous Calls



3.2.9. Multi-Device System
3.2.9.1. Device Enumeration
3.2.9.2. Device Selection
3.2.9.3. Stream and Event Behavior

3.2.9.4. Peer-to-Peer Memory Access
3.2.9.4.1. IOMMU on Linux


3.2.9.5. Peer-to-Peer Memory Copy


3.2.10. Unified Virtual Address Space
3.2.11. Interprocess Communication
3.2.12. Error Checking
3.2.13. Call Stack

3.2.14. Texture and Surface Memory

3.2.14.1. Texture Memory
3.2.14.1.1. Texture Object API
3.2.14.1.2. 16-Bit Floating-Point Textures
3.2.14.1.3. Layered Textures
3.2.14.1.4. Cubemap Textures
3.2.14.1.5. Cubemap Layered Textures
3.2.14.1.6. Texture Gather



3.2.14.2. Surface Memory
3.2.14.2.1. Surface Object API
3.2.14.2.2. Cubemap Surfaces
3.2.14.2.3. Cubemap Layered Surfaces


3.2.14.3. CUDA Arrays
3.2.14.4. Read/Write Coherency



3.2.15. Graphics Interoperability
3.2.15.1. OpenGL Interoperability

3.2.15.2. Direct3D Interoperability
3.2.15.2.1. Direct3D 9 Version
3.2.15.2.2. Direct3D 10 Version
3.2.15.2.3. Direct3D 11 Version


3.2.15.3. SLI Interoperability



3.2.16. External Resource Interoperability

3.2.16.1. Vulkan Interoperability
3.2.16.1.1. Matching device UUIDs
3.2.16.1.2. Importing Memory Objects
3.2.16.1.3. Mapping Buffers onto Imported Memory Objects
3.2.16.1.4. Mapping Mipmapped Arrays onto Imported Memory Objects
3.2.16.1.5. Importing Synchronization Objects
3.2.16.1.6. Signaling/Waiting on Imported Synchronization Objects


3.2.16.2. OpenGL Interoperability

3.2.16.3. Direct3D 12 Interoperability
3.2.16.3.1. Matching Device LUIDs
3.2.16.3.2. Importing Memory Objects
3.2.16.3.3. Mapping Buffers onto Imported Memory Objects
3.2.16.3.4. Mapping Mipmapped Arrays onto Imported Memory Objects
3.2.16.3.5. Importing Synchronization Objects
3.2.16.3.6. Signaling/Waiting on Imported Synchronization Objects



3.2.16.4. Direct3D 11 Interoperability
3.2.16.4.1. Matching Device LUIDs
3.2.16.4.2. Importing Memory Objects
3.2.16.4.3. Mapping Buffers onto Imported Memory Objects
3.2.16.4.4. Mapping Mipmapped Arrays onto Imported Memory Objects
3.2.16.4.5. Importing Synchronization Objects
3.2.16.4.6. Signaling/Waiting on Imported Synchronization Objects



3.2.16.5. NVIDIA Software Communication Interface Interoperability (NVSCI)
3.2.16.5.1. Importing Memory Objects
3.2.16.5.2. Mapping Buffers onto Imported Memory Objects
3.2.16.5.3. Mapping Mipmapped Arrays onto Imported Memory Objects
3.2.16.5.4. Importing Synchronization Objects
3.2.16.5.5. Signaling/Waiting on Imported Synchronization Objects






3.3. Versioning and Compatibility
3.4. Compute Modes
3.5. Mode Switches
3.6. Tesla Compute Cluster Mode for Windows



4. Hardware Implementation
4.1. SIMT Architecture
4.2. Hardware Multithreading



5. Performance Guidelines
5.1. Overall Performance Optimization Strategies

5.2. Maximize Utilization
5.2.1. Application Level
5.2.2. Device Level

5.2.3. Multiprocessor Level
5.2.3.1. Occupancy Calculator





5.3. Maximize Memory Throughput
5.3.1. Data Transfer between Host and Device
5.3.2. Device Memory Accesses



5.4. Maximize Instruction Throughput
5.4.1. Arithmetic Instructions
5.4.2. Control Flow Instructions
5.4.3. Synchronization Instruction


5.5. Minimize Memory Thrashing


6. CUDA-Enabled GPUs

7. C++ Language Extensions

7.1. Function Execution Space Specifiers
7.1.1. __global__
7.1.2. __device__
7.1.3. __host__
7.1.4. Undefined behavior
7.1.5. __noinline__ and __forceinline__
7.1.6. __inline_hint__



7.2. Variable Memory Space Specifiers
7.2.1. __device__
7.2.2. __constant__
7.2.3. __shared__
7.2.4. __grid_constant__
7.2.5. __managed__
7.2.6. __restrict__



7.3. Built-in Vector Types
7.3.1. char, short, int, long, longlong, float, double
7.3.2. dim3



7.4. Built-in Variables
7.4.1. gridDim
7.4.2. blockIdx
7.4.3. blockDim
7.4.4. threadIdx
7.4.5. warpSize


7.5. Memory Fence Functions
7.6. Synchronization Functions
7.7. Mathematical Functions

7.8. Texture Functions

7.8.1. Texture Object API
7.8.1.1. tex1Dfetch()
7.8.1.2. tex1D()
7.8.1.3. tex1DLod()
7.8.1.4. tex1DGrad()
7.8.1.5. tex2D()
7.8.1.6. tex2D() for sparse CUDA arrays
7.8.1.7. tex2Dgather()
7.8.1.8. tex2Dgather() for sparse CUDA arrays
7.8.1.9. tex2DGrad()
7.8.1.10. tex2DGrad() for sparse CUDA arrays
7.8.1.11. tex2DLod()
7.8.1.12. tex2DLod() for sparse CUDA arrays
7.8.1.13. tex3D()
7.8.1.14. tex3D() for sparse CUDA arrays
7.8.1.15. tex3DLod()
7.8.1.16. tex3DLod() for sparse CUDA arrays
7.8.1.17. tex3DGrad()
7.8.1.18. tex3DGrad() for sparse CUDA arrays
7.8.1.19. tex1DLayered()
7.8.1.20. tex1DLayeredLod()
7.8.1.21. tex1DLayeredGrad()
7.8.1.22. tex2DLayered()
7.8.1.23. tex2DLayered() for sparse CUDA arrays
7.8.1.24. tex2DLayeredLod()
7.8.1.25. tex2DLayeredLod() for sparse CUDA arrays
7.8.1.26. tex2DLayeredGrad()
7.8.1.27. tex2DLayeredGrad() for sparse CUDA arrays
7.8.1.28. texCubemap()
7.8.1.29. texCubemapGrad()
7.8.1.30. texCubemapLod()
7.8.1.31. texCubemapLayered()
7.8.1.32. texCubemapLayeredGrad()
7.8.1.33. texCubemapLayeredLod()





7.9. Surface Functions

7.9.1. Surface Object API
7.9.1.1. surf1Dread()
7.9.1.2. surf1Dwrite
7.9.1.3. surf2Dread()
7.9.1.4. surf2Dwrite()
7.9.1.5. surf3Dread()
7.9.1.6. surf3Dwrite()
7.9.1.7. surf1DLayeredread()
7.9.1.8. surf1DLayeredwrite()
7.9.1.9. surf2DLayeredread()
7.9.1.10. surf2DLayeredwrite()
7.9.1.11. surfCubemapread()
7.9.1.12. surfCubemapwrite()
7.9.1.13. surfCubemapLayeredread()
7.9.1.14. surfCubemapLayeredwrite()




7.10. Read-Only Data Cache Load Function
7.11. Load Functions Using Cache Hints
7.12. Store Functions Using Cache Hints
7.13. Time Function

7.14. Atomic Functions

7.14.1. Arithmetic Functions
7.14.1.1. atomicAdd()
7.14.1.2. atomicSub()
7.14.1.3. atomicExch()
7.14.1.4. atomicMin()
7.14.1.5. atomicMax()
7.14.1.6. atomicInc()
7.14.1.7. atomicDec()
7.14.1.8. atomicCAS()



7.14.2. Bitwise Functions
7.14.2.1. atomicAnd()
7.14.2.2. atomicOr()
7.14.2.3. atomicXor()





7.15. Address Space Predicate Functions
7.15.1. __isGlobal()
7.15.2. __isShared()
7.15.3. __isConstant()
7.15.4. __isGridConstant()
7.15.5. __isLocal()



7.16. Address Space Conversion Functions
7.16.1. __cvta_generic_to_global()
7.16.2. __cvta_generic_to_shared()
7.16.3. __cvta_generic_to_constant()
7.16.4. __cvta_generic_to_local()
7.16.5. __cvta_global_to_generic()
7.16.6. __cvta_shared_to_generic()
7.16.7. __cvta_constant_to_generic()
7.16.8. __cvta_local_to_generic()



7.17. Alloca Function
7.17.1. Synopsis
7.17.2. Description
7.17.3. Example



7.18. Compiler Optimization Hint Functions
7.18.1. __builtin_assume_aligned()
7.18.2. __builtin_assume()
7.18.3. __assume()
7.18.4. __builtin_expect()
7.18.5. __builtin_unreachable()
7.18.6. Restrictions


7.19. Warp Vote Functions

7.20. Warp Match Functions
7.20.1. Synopsis
7.20.2. Description



7.21. Warp Reduce Functions
7.21.1. Synopsis
7.21.2. Description



7.22. Warp Shuffle Functions
7.22.1. Synopsis
7.22.2. Description

7.22.3. Examples
7.22.3.1. Broadcast of a single value across a warp
7.22.3.2. Inclusive plus-scan across sub-partitions of 8 threads
7.22.3.3. Reduction across a warp





7.23. Nanosleep Function
7.23.1. Synopsis
7.23.2. Description
7.23.3. Example



7.24. Warp Matrix Functions
7.24.1. Description
7.24.2. Alternate Floating Point
7.24.3. Double Precision
7.24.4. Sub-byte Operations
7.24.5. Restrictions
7.24.6. Element Types and Matrix Sizes
7.24.7. Example



7.25. DPX
7.25.1. Examples



7.26. Asynchronous Barrier
7.26.1. Simple Synchronization Pattern
7.26.2. Temporal Splitting and Five Stages of Synchronization
7.26.3. Bootstrap Initialization, Expected Arrival Count, and Participation
7.26.4. A Barrierâs Phase: Arrival, Countdown, Completion, and Reset
7.26.5. Spatial Partitioning (also known as Warp Specialization)
7.26.6. Early Exit (Dropping out of Participation)
7.26.7. Completion Function

7.26.8. Memory Barrier Primitives Interface
7.26.8.1. Data Types
7.26.8.2. Memory Barrier Primitives API





7.27. Asynchronous Data Copies
7.27.1. memcpy_async API
7.27.2. Copy and Compute Pattern - Staging Data Through Shared Memory
7.27.3. Without memcpy_async
7.27.4. With memcpy_async
7.27.5. Asynchronous Data Copies using cuda::barrier

7.27.6. Performance Guidance for memcpy_async
7.27.6.1. Alignment
7.27.6.2. Trivially copyable
7.27.6.3. Warp Entanglement - Commit
7.27.6.4. Warp Entanglement - Wait
7.27.6.5. Warp Entanglement - Arrive-On
7.27.6.6. Keep Commit and Arrive-On Operations Converged





7.28. Asynchronous Data Copies using cuda::pipeline
7.28.1. Single-Stage Asynchronous Data Copies using cuda::pipeline
7.28.2. Multi-Stage Asynchronous Data Copies using cuda::pipeline
7.28.3. Pipeline Interface

7.28.4. Pipeline Primitives Interface
7.28.4.1. memcpy_async Primitive
7.28.4.2. Commit Primitive
7.28.4.3. Wait Primitive
7.28.4.4. Arrive On Barrier Primitive





7.29. Asynchronous Data Copies using Tensor Memory Access (TMA)
7.29.1. Using TMA to transfer one-dimensional arrays

7.29.2. Using TMA to transfer multi-dimensional arrays
7.29.2.1. Multi-dimensional TMA PTX wrappers




7.30. Profiler Counter Function
7.31. Assertion
7.32. Trap function
7.33. Breakpoint Function

7.34. Formatted Output
7.34.1. Format Specifiers
7.34.2. Limitations
7.34.3. Associated Host-Side API
7.34.4. Examples



7.35. Dynamic Global Memory Allocation and Operations
7.35.1. Heap Memory Allocation
7.35.2. Interoperability with Host Memory API

7.35.3. Examples
7.35.3.1. Per Thread Allocation
7.35.3.2. Per Thread Block Allocation
7.35.3.3. Allocation Persisting Between Kernel Launches




7.36. Execution Configuration
7.37. Launch Bounds
7.38. Maximum Number of Registers per Thread
7.39. #pragma unroll
7.40. SIMD Video Instructions
7.41. Diagnostic Pragmas



8. Cooperative Groups
8.1. Introduction

8.2. Whatâs New in Cooperative Groups
8.2.1. CUDA 12.2
8.2.2. CUDA 12.1
8.2.3. CUDA 12.0



8.3. Programming Model Concept
8.3.1. Composition Example



8.4. Group Types

8.4.1. Implicit Groups
8.4.1.1. Thread Block Group
8.4.1.2. Cluster Group
8.4.1.3. Grid Group
8.4.1.4. Multi Grid Group



8.4.2. Explicit Groups

8.4.2.1. Thread Block Tile
8.4.2.1.1. Warp-Synchronous Code Pattern
8.4.2.1.2. Single thread group



8.4.2.2. Coalesced Groups
8.4.2.2.1. Discovery Pattern







8.5. Group Partitioning
8.5.1. tiled_partition
8.5.2. labeled_partition
8.5.3. binary_partition



8.6. Group Collectives

8.6.1. Synchronization
8.6.1.1. barrier_arrive and barrier_wait
8.6.1.2. sync



8.6.2. Data Transfer
8.6.2.1. memcpy_async
8.6.2.2. wait and wait_prior



8.6.3. Data Manipulation
8.6.3.1. reduce
8.6.3.2. Reduce Operators
8.6.3.3. inclusive_scan and exclusive_scan



8.6.4. Execution control
8.6.4.1. invoke_one and invoke_one_broadcast




8.7. Grid Synchronization
8.8. Multi-Device Synchronization



9. CUDA Dynamic Parallelism

9.1. Introduction
9.1.1. Overview
9.1.2. Glossary



9.2. Execution Environment and Memory Model

9.2.1. Execution Environment
9.2.1.1. Parent and Child Grids
9.2.1.2. Scope of CUDA Primitives
9.2.1.3. Synchronization
9.2.1.4. Streams and Events
9.2.1.5. Ordering and Concurrency
9.2.1.6. Device Management



9.2.2. Memory Model

9.2.2.1. Coherence and Consistency
9.2.2.1.1. Global Memory
9.2.2.1.2. Zero Copy Memory
9.2.2.1.3. Constant Memory
9.2.2.1.4. Shared and Local Memory
9.2.2.1.5. Local Memory
9.2.2.1.6. Texture Memory







9.3. Programming Interface

9.3.1. CUDA C++ Reference

9.3.1.1. Device-Side Kernel Launch
9.3.1.1.1. Launches are Asynchronous
9.3.1.1.2. Launch Environment Configuration



9.3.1.2. Streams
9.3.1.2.1. The Implicit (NULL) Stream
9.3.1.2.2. The Fire-and-Forget Stream
9.3.1.2.3. The Tail Launch Stream


9.3.1.3. Events
9.3.1.4. Synchronization
9.3.1.5. Device Management

9.3.1.6. Memory Declarations
9.3.1.6.1. Device and Constant Memory
9.3.1.6.2. Textures and Surfaces
9.3.1.6.3. Shared Memory Variable Declarations
9.3.1.6.4. Symbol Addresses



9.3.1.7. API Errors and Launch Failures
9.3.1.7.1. Launch Setup APIs


9.3.1.8. API Reference



9.3.2. Device-side Launch from PTX

9.3.2.1. Kernel Launch APIs
9.3.2.1.1. cudaLaunchDevice
9.3.2.1.2. cudaGetParameterBuffer


9.3.2.2. Parameter Buffer Layout



9.3.3. Toolkit Support for Dynamic Parallelism
9.3.3.1. Including Device Runtime API in CUDA Code
9.3.3.2. Compiling and Linking





9.4. Programming Guidelines
9.4.1. Basics

9.4.2. Performance
9.4.2.1. Dynamic-parallelism-enabled Kernel Overhead



9.4.3. Implementation Restrictions and Limitations

9.4.3.1. Runtime
9.4.3.1.1. Memory Footprint
9.4.3.1.2. Pending Kernel Launches
9.4.3.1.3. Configuration Options
9.4.3.1.4. Memory Allocation and Lifetime
9.4.3.1.5. SM Id and Warp Id
9.4.3.1.6. ECC Errors







9.5. CDP2 vs CDP1
9.5.1. Differences Between CDP1 and CDP2
9.5.2. Compatibility and Interoperability



9.6. Legacy CUDA Dynamic Parallelism (CDP1)

9.6.1. Execution Environment and Memory Model (CDP1)

9.6.1.1. Execution Environment (CDP1)
9.6.1.1.1. Parent and Child Grids (CDP1)
9.6.1.1.2. Scope of CUDA Primitives (CDP1)
9.6.1.1.3. Synchronization (CDP1)
9.6.1.1.4. Streams and Events (CDP1)
9.6.1.1.5. Ordering and Concurrency (CDP1)
9.6.1.1.6. Device Management (CDP1)



9.6.1.2. Memory Model (CDP1)

9.6.1.2.1. Coherence and Consistency (CDP1)
9.6.1.2.1.1. Global Memory (CDP1)
9.6.1.2.1.2. Zero Copy Memory (CDP1)
9.6.1.2.1.3. Constant Memory (CDP1)
9.6.1.2.1.4. Shared and Local Memory (CDP1)
9.6.1.2.1.5. Local Memory (CDP1)
9.6.1.2.1.6. Texture Memory (CDP1)







9.6.2. Programming Interface (CDP1)

9.6.2.1. CUDA C++ Reference (CDP1)

9.6.2.1.1. Device-Side Kernel Launch (CDP1)
9.6.2.1.1.1. Launches are Asynchronous (CDP1)
9.6.2.1.1.2. Launch Environment Configuration (CDP1)



9.6.2.1.2. Streams (CDP1)
9.6.2.1.2.1. The Implicit (NULL) Stream (CDP1)


9.6.2.1.3. Events (CDP1)

9.6.2.1.4. Synchronization (CDP1)
9.6.2.1.4.1. Block Wide Synchronization (CDP1)


9.6.2.1.5. Device Management (CDP1)

9.6.2.1.6. Memory Declarations (CDP1)
9.6.2.1.6.1. Device and Constant Memory (CDP1)
9.6.2.1.6.2. Textures and Surfaces (CDP1)
9.6.2.1.6.3. Shared Memory Variable Declarations (CDP1)
9.6.2.1.6.4. Symbol Addresses (CDP1)



9.6.2.1.7. API Errors and Launch Failures (CDP1)
9.6.2.1.7.1. Launch Setup APIs (CDP1)


9.6.2.1.8. API Reference (CDP1)



9.6.2.2. Device-side Launch from PTX (CDP1)

9.6.2.2.1. Kernel Launch APIs (CDP1)
9.6.2.2.1.1. cudaLaunchDevice (CDP1)
9.6.2.2.1.2. cudaGetParameterBuffer (CDP1)


9.6.2.2.2. Parameter Buffer Layout (CDP1)



9.6.2.3. Toolkit Support for Dynamic Parallelism (CDP1)
9.6.2.3.1. Including Device Runtime API in CUDA Code (CDP1)
9.6.2.3.2. Compiling and Linking (CDP1)





9.6.3. Programming Guidelines (CDP1)
9.6.3.1. Basics (CDP1)

9.6.3.2. Performance (CDP1)
9.6.3.2.1. Synchronization (CDP1)
9.6.3.2.2. Dynamic-parallelism-enabled Kernel Overhead (CDP1)



9.6.3.3. Implementation Restrictions and Limitations (CDP1)

9.6.3.3.1. Runtime (CDP1)
9.6.3.3.1.1. Memory Footprint (CDP1)
9.6.3.3.1.2. Nesting and Synchronization Depth (CDP1)
9.6.3.3.1.3. Pending Kernel Launches (CDP1)
9.6.3.3.1.4. Configuration Options (CDP1)
9.6.3.3.1.5. Memory Allocation and Lifetime (CDP1)
9.6.3.3.1.6. SM Id and Warp Id (CDP1)
9.6.3.3.1.7. ECC Errors (CDP1)











10. Virtual Memory Management
10.1. Introduction
10.2. Query for Support

10.3. Allocating Physical Memory
10.3.1. Shareable Memory Allocations

10.3.2. Memory Type
10.3.2.1. Compressible Memory




10.4. Reserving a Virtual Address Range
10.5. Virtual Aliasing Support
10.6. Mapping Memory
10.7. Controlling Access Rights



11. Stream Ordered Memory Allocator
11.1. Introduction
11.2. Query for Support
11.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync)
11.4. Memory Pools and the cudaMemPool_t
11.5. Default/Implicit Pools
11.6. Explicit Pools
11.7. Physical Page Caching Behavior
11.8. Resource Usage Statistics

11.9. Memory Reuse Policies
11.9.1. cudaMemPoolReuseFollowEventDependencies
11.9.2. cudaMemPoolReuseAllowOpportunistic
11.9.3. cudaMemPoolReuseAllowInternalDependencies
11.9.4. Disabling Reuse Policies


11.10. Device Accessibility for Multi-GPU Support

11.11. IPC Memory Pools
11.11.1. Creating and Sharing IPC Memory Pools
11.11.2. Set Access in the Importing Process
11.11.3. Creating and Sharing Allocations from an Exported Pool
11.11.4. IPC Export Pool Limitations
11.11.5. IPC Import Pool Limitations


11.12. Synchronization API Actions

11.13. Addendums
11.13.1. cudaMemcpyAsync Current Context/Device Sensitivity
11.13.2. cuPointerGetAttribute Query
11.13.3. cuGraphAddMemsetNode
11.13.4. Pointer Attributes





12. Graph Memory Nodes
12.1. Introduction
12.2. Support and Compatibility

12.3. API Fundamentals
12.3.1. Graph Node APIs
12.3.2. Stream Capture
12.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graph
12.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunch



12.4. Optimized Memory Reuse
12.4.1. Address Reuse within a Graph
12.4.2. Physical Memory Management and Sharing



12.5. Performance Considerations
12.5.1. First Launch / cudaGraphUpload


12.6. Physical Memory Footprint

12.7. Peer Access
12.7.1. Peer Access with Graph Node APIs
12.7.2. Peer Access with Stream Capture





13. Mathematical Functions
13.1. Standard Functions
13.2. Intrinsic Functions



14. C++ Language Support
14.1. C++11 Language Features
14.2. C++14 Language Features
14.3. C++17 Language Features
14.4. C++20 Language Features

14.5. Restrictions
14.5.1. Host Compiler Extensions

14.5.2. Preprocessor Symbols
14.5.2.1. __CUDA_ARCH__



14.5.3. Qualifiers
14.5.3.1. Device Memory Space Specifiers
14.5.3.2. __managed__ Memory Space Specifier
14.5.3.3. Volatile Qualifier


14.5.4. Pointers

14.5.5. Operators
14.5.5.1. Assignment Operator
14.5.5.2. Address Operator


14.5.6. Run Time Type Information (RTTI)
14.5.7. Exception Handling
14.5.8. Standard Library
14.5.9. Namespace Reservations

14.5.10. Functions
14.5.10.1. External Linkage
14.5.10.2. Implicitly-declared and explicitly-defaulted functions

14.5.10.3. Function Parameters
14.5.10.3.1. __global__ Function Argument Processing
14.5.10.3.2. Toolkit and Driver Compatibility
14.5.10.3.3. Link Compatibility across Toolkit Revisions


14.5.10.4. Static Variables within Function
14.5.10.5. Function Pointers
14.5.10.6. Function Recursion
14.5.10.7. Friend Functions
14.5.10.8. Operator Function
14.5.10.9. Allocation and Deallocation Functions



14.5.11. Classes
14.5.11.1. Data Members
14.5.11.2. Function Members
14.5.11.3. Virtual Functions
14.5.11.4. Virtual Base Classes
14.5.11.5. Anonymous Unions
14.5.11.6. Windows-Specific


14.5.12. Templates
14.5.13. Trigraphs and Digraphs
14.5.14. Const-qualified variables
14.5.15. Long Double
14.5.16. Deprecation Annotation
14.5.17. Noreturn Annotation
14.5.18. [[likely]] / [[unlikely]] Standard Attributes
14.5.19. const and pure GNU Attributes
14.5.20. __nv_pure__ Attribute
14.5.21. Intel Host Compiler Specific

14.5.22. C++11 Features
14.5.22.1. Lambda Expressions
14.5.22.2. std::initializer_list
14.5.22.3. Rvalue references
14.5.22.4. Constexpr functions and function templates
14.5.22.5. Constexpr variables

14.5.22.6. Inline namespaces
14.5.22.6.1. Inline unnamed namespaces


14.5.22.7. thread_local
14.5.22.8. __global__ functions and function templates
14.5.22.9. __managed__ and __shared__ variables
14.5.22.10. Defaulted functions



14.5.23. C++14 Features
14.5.23.1. Functions with deduced return type
14.5.23.2. Variable templates



14.5.24. C++17 Features
14.5.24.1. Inline Variable
14.5.24.2. Structured Binding



14.5.25. C++20 Features
14.5.25.1. Module support
14.5.25.2. Coroutine support
14.5.25.3. Three-way comparison operator
14.5.25.4. Consteval functions




14.6. Polymorphic Function Wrappers

14.7. Extended Lambdas
14.7.1. Extended Lambda Type Traits
14.7.2. Extended Lambda Restrictions
14.7.3. Notes on __host__ __device__ lambdas
14.7.4. *this Capture By Value
14.7.5. Additional Notes



14.8. Code Samples
14.8.1. Data Aggregation Class
14.8.2. Derived Class
14.8.3. Class Template
14.8.4. Function Template
14.8.5. Functor Class





15. Texture Fetching
15.1. Nearest-Point Sampling
15.2. Linear Filtering
15.3. Table Lookup



16. Compute Capabilities
16.1. Feature Availability
16.2. Features and Technical Specifications
16.3. Floating-Point Standard

16.4. Compute Capability 5.x
16.4.1. Architecture
16.4.2. Global Memory
16.4.3. Shared Memory



16.5. Compute Capability 6.x
16.5.1. Architecture
16.5.2. Global Memory
16.5.3. Shared Memory



16.6. Compute Capability 7.x
16.6.1. Architecture
16.6.2. Independent Thread Scheduling
16.6.3. Global Memory
16.6.4. Shared Memory



16.7. Compute Capability 8.x
16.7.1. Architecture
16.7.2. Global Memory
16.7.3. Shared Memory



16.8. Compute Capability 9.0
16.8.1. Architecture
16.8.2. Global Memory
16.8.3. Shared Memory
16.8.4. Features Accelerating Specialized Computations





17. Driver API
17.1. Context
17.2. Module
17.3. Kernel Execution
17.4. Interoperability between Runtime and Driver APIs

17.5. Driver Entry Point Access
17.5.1. Introduction
17.5.2. Driver Function Typedefs

17.5.3. Driver Function Retrieval
17.5.3.1. Using the driver API
17.5.3.2. Using the runtime API
17.5.3.3. Retrieve per-thread default stream versions
17.5.3.4. Access new CUDA features



17.5.4. Potential Implications with cuGetProcAddress
17.5.4.1. Implications with cuGetProcAddress vs Implicit Linking
17.5.4.2. Compile Time vs Runtime Version Usage in cuGetProcAddress
17.5.4.3. API Version Bumps with Explicit Version Checks
17.5.4.4. Issues with Runtime API Usage
17.5.4.5. Issues with Runtime API and Dynamic Versioning
17.5.4.6. Issues with Runtime API allowing CUDA Version
17.5.4.7. Implications to API/ABI


17.5.5. Determining cuGetProcAddress Failure Reasons




18. CUDA Environment Variables

19. Unified Memory Programming

19.1. Unified Memory Introduction
19.1.1. System Requirements for Unified Memory

19.1.2. Programming Model
19.1.2.1. Allocation APIs for System-Allocated Memory
19.1.2.2. Allocation API for CUDA Managed Memory: cudaMallocManaged()
19.1.2.3. Global-Scope Managed Variables Using __managed__
19.1.2.4. Difference between Unified Memory and Mapped Memory
19.1.2.5. Pointer Attributes
19.1.2.6. Runtime detection of Unified Memory Support Level
19.1.2.7. GPU Memory Oversubscription

19.1.2.8. Performance Hints
19.1.2.8.1. Data Prefetching
19.1.2.8.2. Data Usage Hints
19.1.2.8.3. Querying Data Usage Attributes on Managed Memory







19.2. Unified memory on devices with full CUDA Unified Memory support

19.2.1. System-Allocated Memory: in-depth examples
19.2.1.1. File-backed Unified Memory
19.2.1.2. Inter-Process Communication (IPC) with Unified Memory



19.2.2. Performance Tuning

19.2.2.1. Memory Paging and Page Sizes
19.2.2.1.1. Choosing the right page size
19.2.2.1.2. CPU and GPU page tables: hardware coherency vs. software coherency


19.2.2.2. Direct Unified Memory Access from host
19.2.2.3. Host Native Atomics





19.3. Unified memory on devices without full CUDA Unified Memory support
19.3.1. Unified memory on devices with only CUDA Managed Memory support

19.3.2. Unified memory on Windows or devices with compute capability 5.x
19.3.2.1. Data Migration and Coherency
19.3.2.2. GPU Memory Oversubscription
19.3.2.3. Multi-GPU

19.3.2.4. Coherency and Concurrency
19.3.2.4.1. GPU Exclusive Access To Managed Memory
19.3.2.4.2. Explicit Synchronization and Logical GPU Activity
19.3.2.4.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streams
19.3.2.4.4. Stream Association Examples
19.3.2.4.5. Stream Attach With Multithreaded Host Programs
19.3.2.4.6. Advanced Topic: Modular Programs and Data Access Constraints
19.3.2.4.7. Memcpy()/Memset() Behavior With Stream-associated Unified Memory









20. Lazy Loading
20.1. What is Lazy Loading?

20.2. Lazy Loading version support
20.2.1. Driver
20.2.2. Toolkit
20.2.3. Compiler



20.3. Triggering loading of kernels in lazy mode
20.3.1. CUDA Driver API
20.3.2. CUDA Runtime API


20.4. Querying whether Lazy Loading is turned on

20.5. Possible issues when adopting lazy loading
20.5.1. Concurrent execution
20.5.2. Allocators
20.5.3. Autotuning





21. Extended GPU Memory

21.1. Preliminaries
21.1.1. EGM Platforms: System topology
21.1.2. Socket Identifiers: What are they? How to access them?
21.1.3. Allocators and EGM support
21.1.4. Memory management extensions to current APIs



21.2. Using the EGM Interface
21.2.1. Single-Node, Single-GPU

21.2.2. Single-Node, Multi-GPU
21.2.2.1. Using VMM APIs
21.2.2.2. Using CUDA Memory Pool


21.2.3. Multi-Node, Single-GPU





22. Notices
22.1. Notice
22.2. OpenCL
22.3. Trademarks








CUDA C++ Programming Guide






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






CUDA C++ Programming Guide
The programming guide to the CUDA model and interface.
Changes from Version 12.4

Added section Asynchronous Data Copies using Tensor Memory Access (TMA).
Added Unified Memory Programming guide supporting Grace Hopper with Address Translation Service (ATS) and Heterogeneous Memory Management (HMM ) on x86.



1. Introductionï



1.1. The Benefits of Using GPUsï

The Graphics Processing Unit (GPU)1 provides much higher instruction throughput and memory bandwidth than the CPU within a similar price and power envelope. Many applications leverage these higher capabilities to run faster on the GPU than on the CPU (see GPU Applications). Other computing devices, like FPGAs, are also very energy efficient, but offer much less programming flexibility than GPUs.
This difference in capabilities between the GPU and the CPU exists because they are designed with different goals in mind. While the CPU is designed to excel at executing a sequence of operations, called a thread, as fast as possible and can execute a few tens of these threads in parallel, the GPU is designed to excel at executing thousands of them in parallel (amortizing the slower single-thread performance to achieve greater throughput).
The GPU is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control. The schematic Figure 1 shows an example distribution of chip resources for a CPU versus a GPU.



Figure 1 The GPU Devotes More Transistors to Data Processingï


Devoting more transistors to data processing, for example, floating-point computations, is beneficial for highly parallel computations; the GPU can hide memory access latencies with computation, instead of relying on large data caches and complex flow control to avoid long memory access latencies, both of which are expensive in terms of transistors.
In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance. Applications with a high degree of parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than on the CPU.



1.2. CUDAÂ®: A General-Purpose Parallel Computing Platform and Programming Modelï

In November 2006, NVIDIAÂ® introduced CUDAÂ®, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU.
CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. As illustrated by Figure 2, other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC.



Figure 2 GPU Computing Applications. CUDA is designed to support various languages and application programming interfaces.ï





1.3. A Scalable Programming Modelï

The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.
The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C.
At its core are three key abstractions â a hierarchy of thread groups, shared memories, and barrier synchronization â that are simply exposed to the programmer as a minimal set of language extensions.
These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block.
This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to know the physical multiprocessor count.
This scalable programming model allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions: from the high-performance enthusiast GeForce GPUs and professional Quadro and Tesla computing products to a variety of inexpensive, mainstream GeForce GPUs (see CUDA-Enabled GPUs for a list of all CUDA-enabled GPUs).



Figure 3 Automatic Scalabilityï


Note
A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors.







1.4. Document Structureï

This document is organized into the following sections:

Introduction is a general introduction to CUDA.
Programming Model outlines the CUDA programming model.
Programming Interface describes the programming interface.
Hardware Implementation describes the hardware implementation.
Performance Guidelines gives some guidance on how to achieve maximum performance.
CUDA-Enabled GPUs lists all CUDA-enabled devices.
C++ Language Extensions is a detailed description of all extensions to the C++ language.
Cooperative Groups describes synchronization primitives for various groups of CUDA threads.
CUDA Dynamic Parallelism describes how to launch and synchronize one kernel from another.
Virtual Memory Management describes how to manage the unified virtual address space.
Stream Ordered Memory Allocator describes how applications can order memory allocation and deallocation.
Graph Memory Nodes describes how graphs can create and own memory allocations.
Mathematical Functions lists the mathematical functions supported in CUDA.
C++ Language Support lists the C++ features supported in device code.
Texture Fetching gives more details on texture fetching.
Compute Capabilities gives the technical specifications of various devices, as well as more architectural details.
Driver API introduces the low-level driver API.
CUDA Environment Variables lists all the CUDA environment variables.
Unified Memory Programming introduces the Unified Memory programming model.


1

The graphics qualifier comes from the fact that when the GPU was originally created, two decades ago, it was designed as a specialized processor to accelerate graphics rendering. Driven by the insatiable market demand for real-time, high-definition, 3D graphics, it has evolved into a general processor used for many more workloads than just graphics rendering.






2. Programming Modelï

This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++.
An extensive description of CUDA C++ is given in Programming Interface.
Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample.


2.1. Kernelsï

CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions.
A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<...>>>execution configuration syntax (see C++ Language Extensions). Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables.
As an illustration, the following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C:

// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main()
{
    ...
    // Kernel invocation with N threads
    VecAdd<<<1, N>>>(A, B, C);
    ...
}


Here, each of the N threads that execute VecAdd() performs one pair-wise addition.



2.2. Thread Hierarchyï

For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.
The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).
As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C:

// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}


There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads.
However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks.
Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 4. The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system.



Figure 4 Grid of Thread Blocksï


The number of threads per block and the number of blocks per grid specified in the <<<...>>> syntax can be of type int or dim3. Two-dimensional blocks or grids can be specified as in the example above.
Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable.
Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows.

// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}


A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. The grid is created with enough blocks to have one thread per matrix element as before. For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case.
Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 3, enabling programmers to write code that scales with the number of cores.
Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed. Shared Memory gives an example of using shared memory. In addition to __syncthreads(), the Cooperative Groups API provides a rich set of thread-synchronization primitives.
For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight.


2.2.1. Thread Block Clustersï

With the introduction of NVIDIA Compute Capability 9.0, the CUDA programming model introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks. Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU Processing Cluster (GPC) in the GPU.
Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension as illustrated by Figure 5. The number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in CUDA.
Note that on GPU hardware or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly. Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using the cudaOccupancyMaxPotentialClusterSize API.



Figure 5 Grid of Thread Block Clustersï



Note
In a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes. The rank of a block in a cluster can be found using the Cluster Group API.

A thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using __cluster_dims__(X,Y,Z) or using the CUDA kernel launch API cudaLaunchKernelEx. The example below shows how to launch a cluster using compiler time kernel attribute. The cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the classical <<< , >>>. If a kernel uses compile-time cluster size, the cluster size cannot be modified when launching the kernel.

// Kernel definition
// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension
__global__ void __cluster_dims__(2, 1, 1) cluster_kernel(float *input, float* output)
{

}

int main()
{
    float *input, *output;
    // Kernel invocation with compile time cluster size
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // The grid dimension is not affected by cluster launch, and is still enumerated
    // using number of blocks.
    // The grid dimension must be a multiple of cluster size.
    cluster_kernel<<<numBlocks, threadsPerBlock>>>(input, output);
}


A thread block cluster size can also be set at runtime and the kernel can be launched using the CUDA kernel launch API cudaLaunchKernelEx. The code example below shows how to launch a cluster kernel using the extensible API.

// Kernel definition
// No compile time attribute attached to the kernel
__global__ void cluster_kernel(float *input, float* output)
{

}

int main()
{
    float *input, *output;
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);

    // Kernel invocation with runtime cluster size
    {
        cudaLaunchConfig_t config = {0};
        // The grid dimension is not affected by cluster launch, and is still enumerated
        // using number of blocks.
        // The grid dimension should be a multiple of cluster size.
        config.gridDim = numBlocks;
        config.blockDim = threadsPerBlock;

        cudaLaunchAttribute attribute[1];
        attribute[0].id = cudaLaunchAttributeClusterDimension;
        attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension
        attribute[0].val.clusterDim.y = 1;
        attribute[0].val.clusterDim.z = 1;
        config.attrs = attribute;
        config.numAttrs = 1;

        cudaLaunchKernelEx(&config, cluster_kernel, input, output);
    }
}


In GPUs with compute capability 9.0, all the thread blocks in the cluster are guaranteed to be co-scheduled on a single GPU Processing Cluster (GPC) and allow thread blocks in the cluster to perform hardware-supported synchronization using the Cluster Group API cluster.sync(). Cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks using num_threads() and num_blocks() API respectively. The rank of a thread or block in the cluster group can be queried using dim_threads() and dim_blocks() API respectively.
Thread blocks that belong to a cluster have access to the Distributed Shared Memory. Thread blocks in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared memory. Distributed Shared Memory gives an example of performing histograms in distributed shared memory.




2.3. Memory Hierarchyï

CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster can perform read, write, and atomics operations on each otherâs shared memory. All threads have access to the same global memory.
There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses). Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory).
The global, constant, and texture memory spaces are persistent across kernel launches by the same application.



Figure 6 Memory Hierarchyï





2.4. Heterogeneous Programmingï

As illustrated by Figure 7, the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program. This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU.
The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory, respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface). This includes device memory allocation and deallocation as well as data transfer between host and device memory.
Unified Memory provides managed memory to bridge the host and device memory spaces. Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. See Unified Memory Programming for an introduction to Unified Memory.



Figure 7 Heterogeneous Programmingï


Note
Serial code executes on the host while parallel code executes on the device.







2.5. Asynchronous SIMT Programming Modelï

In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation. Starting with devices based on the NVIDIA Ampere GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model. The asynchronous programming model defines the behavior of asynchronous operations with respect to CUDA threads.
The asynchronous programming model defines the behavior of Asynchronous Barrier for synchronization between CUDA threads. The model also explains and defines how cuda::memcpy_async can be used to move data asynchronously from global memory while computing in the GPU.


2.5.1. Asynchronous Operationsï

An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread. In a well formed program one or more CUDA threads synchronize with the asynchronous operation. The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads.
Such an asynchronous thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation. An asynchronous operation uses a synchronization object to synchronize the completion of the operation. Such a synchronization object can be explicitly managed by a user (e.g., cuda::memcpy_async) or implicitly managed within a library (e.g., cooperative_groups::memcpy_async).
A synchronization object could be a cuda::barrier or a cuda::pipeline. These objects are explained in detail in Asynchronous Barrier and Asynchronous Data Copies using cuda::pipeline. These synchronization objects can be used at different thread scopes. A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation. The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each.







Thread Scope
Description




cuda::thread_scope::thread_scope_thread
Only the CUDA thread which initiated asynchronous operations synchronizes.


cuda::thread_scope::thread_scope_block
All or any CUDA threads within the same thread block as the initiating thread synchronizes.


cuda::thread_scope::thread_scope_device
All or any CUDA threads in the same GPU device as the initiating thread synchronizes.


cuda::thread_scope::thread_scope_system
All or any CUDA or CPU threads in the same system as the initiating thread synchronizes.



These thread scopes are implemented as extensions to standard C++ in the CUDA Standard C++ library.




2.6. Compute Capabilityï

The compute capability of a device is represented by a version number, also sometimes called its âSM versionâ. This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU.
The compute capability comprises a major revision number X and a minor revision number Y and is denoted by X.Y.
Devices with the same major revision number are of the same core architecture. The major revision number is 9 for devices based on the NVIDIA Hopper GPU architecture, 8 for devices based on the NVIDIA Ampere GPU architecture, 7 for devices based on the Volta architecture, 6 for devices based on the Pascal architecture, 5 for devices based on the Maxwell architecture, and 3 for devices based on the Kepler architecture.
The minor revision number corresponds to an incremental improvement to the core architecture, possibly including new features.
Turing is the architecture for devices of compute capability 7.5, and is an incremental update based on the Volta architecture.
CUDA-Enabled GPUs lists of all CUDA-enabled devices along with their compute capability. Compute Capabilities gives the technical specifications of each compute capability.

Note
The compute capability version of a particular GPU should not be confused with the CUDA version (for example, CUDA 7.5, CUDA 8, CUDA 9), which is the version of the CUDA software platform. The CUDA platform is used by application developers to create applications that run on many generations of GPU architectures, including future GPU architectures yet to be invented. While new versions of the CUDA platform often add native support for a new GPU architecture by supporting the compute capability version of that architecture, new versions of the CUDA platform typically also include software features that are independent of hardware generation.

The Tesla and Fermi architectures are no longer supported starting with CUDA 7.0 and CUDA 9.0, respectively.




3. Programming Interfaceï

CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device.
It consists of a minimal set of extensions to the C++ language and a runtime library.
The core language extensions have been introduced in Programming Model. They allow programmers to define a kernel as a C++ function and use some new syntax to specify the grid and block dimension each time the function is called. A complete description of all extensions can be found in C++ Language Extensions. Any source file that contains some of these extensions must be compiled with nvcc as outlined in Compilation with NVCC.
The runtime is introduced in CUDA Runtime. It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual.
The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed. The driver API is introduced in Driver API and fully described in the reference manual.


3.1. Compilation with NVCCï

Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual. It is however usually more effective to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc to execute on the device.
nvcc is a compiler driver that simplifies the process of compiling C++ or PTX code: It provides simple and familiar command line options and executes them by invoking the collection of tools that implement the different compilation stages. This section gives an overview of nvcc workflow and command options. A complete description can be found in the nvcc user manual.


3.1.1. Compilation Workflowï



3.1.1.1. Offline Compilationï

Source files compiled with nvcc can include a mix of host code (i.e., code that executes on the host) and device code (i.e., code that executes on the device). nvccâs basic workflow consists in separating device code from host code and then:

compiling the device code into an assembly form (PTX code) and/or binary form (cubin object),
and modifying the host code by replacing the <<<...>>> syntax introduced in Kernels (and described in more details in Execution Configuration) by the necessary CUDA runtime function calls to load and launch each compiled kernel from the PTX code and/or cubin object.

The modified host code is output either as C++ code that is left to be compiled using another tool or as object code directly by letting nvcc invoke the host compiler during the last compilation stage.
Applications can then:

Either link to the compiled host code (this is the most common case),
Or ignore the modified host code (if any) and use the CUDA driver API (see Driver API) to load and execute the PTX code or cubin object.




3.1.1.2. Just-in-Time Compilationï

Any PTX code loaded by an application at runtime is compiled further to binary code by the device driver. This is called just-in-time compilation. Just-in-time compilation increases application load time, but allows the application to benefit from any new compiler improvements coming with each new device driver. It is also the only way for applications to run on devices that did not exist at the time the application was compiled, as detailed in Application Compatibility.
When the device driver just-in-time compiles some PTX code for some application, it automatically caches a copy of the generated binary code in order to avoid repeating the compilation in subsequent invocations of the application. The cache - referred to as compute cache - is automatically invalidated when the device driver is upgraded, so that applications can benefit from the improvements in the new just-in-time compiler built into the device driver.
Environment variables are available to control just-in-time compilation as described in CUDA Environment Variables
As an alternative to using nvcc to compile CUDA C++ device code, NVRTC can be used to compile CUDA C++ device code to PTX at runtime. NVRTC is a runtime compilation library for CUDA C++; more information can be found in the NVRTC User guide.




3.1.2. Binary Compatibilityï

Binary code is architecture-specific. A cubin object is generated using the compiler option -code that specifies the targeted architecture: For example, compiling with -code=sm_80 produces binary code for devices of compute capability 8.0. Binary compatibility is guaranteed from one minor revision to the next one, but not from one minor revision to the previous one or across major revisions. In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where zâ¥y.

Note
Binary compatibility is supported only for the desktop. It is not supported for Tegra. Also, the binary compatibility between desktop and Tegra is not supported.




3.1.3. PTX Compatibilityï

Some PTX instructions are only supported on devices of higher compute capabilities. For example, Warp Shuffle Functions are only supported on devices of compute capability 5.0 and above. The -arch compiler option specifies the compute capability that is assumed when compiling C++ to PTX code. So, code that contains warp shuffle, for example, must be compiled with -arch=compute_50 (or higher).
PTX code produced for some specific compute capability can always be compiled to binary code of greater or equal compute capability. Note that a binary compiled from an earlier PTX version may not make use of some hardware features. For example, a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal. As a result, the final binary may perform worse than would be possible if the binary were generated using the latest version of PTX.
PTX code compiled to target architecture conditional features only run on the exact same physical architecture and nowhere else. Arch conditional PTX code is not forward and backward compatible.
Example code compiled with sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible.



3.1.4. Application Compatibilityï

To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability as described in Binary Compatibility and PTX Compatibility. In particular, to be able to execute code on future architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled for these devices (see Just-in-Time Compilation).
Which PTX and binary code gets embedded in a CUDA C++ application is controlled by the -arch and -code compiler options or the -gencode compiler option as detailed in the nvcc user manual. For example,

nvcc x.cu
        -gencode arch=compute_50,code=sm_50
        -gencode arch=compute_60,code=sm_60
        -gencode arch=compute_70,code=\"compute_70,sm_70\"


embeds binary code compatible with compute capability 5.0 and 6.0 (first and second -gencode options) and PTX and binary code compatible with compute capability 7.0 (third -gencode option).
Host code is generated to automatically select at runtime the most appropriate code to load and execute, which, in the above example, will be:

5.0 binary code for devices with compute capability 5.0 and 5.2,
6.0 binary code for devices with compute capability 6.0 and 6.1,
7.0 binary code for devices with compute capability 7.0 and 7.5,
PTX code which is compiled to binary code at runtime for devices with compute capability 8.0 and 8.6.

x.cu can have an optimized code path that uses warp reduction operations, for example, which are only supported in devices of compute capability 8.0 and higher. The __CUDA_ARCH__ macro can be used to differentiate various code paths based on compute capability. It is only defined for device code. When compiling with -arch=compute_80 for example, __CUDA_ARCH__ is equal to 800.
If x.cu is compiled for architecture conditional features example with sm_90a or compute_90a, the code can only run on devices with compute capability 9.0.
Applications using the driver API must compile code to separate files and explicitly load and execute the most appropriate file at runtime.
The Volta architecture introduces Independent Thread Scheduling which changes the way threads are scheduled on the GPU. For code relying on specific behavior of SIMT scheduling in previous architectures, Independent Thread Scheduling may alter the set of participating threads, leading to incorrect results. To aid migration while implementing the corrective actions detailed in Independent Thread Scheduling, Volta developers can opt-in to Pascalâs thread scheduling with the compiler option combination -arch=compute_60 -code=sm_70.
The nvcc user manual lists various shorthands for the -arch, -code, and -gencode compiler options. For example, -arch=sm_70 is a shorthand for -arch=compute_70 -code=compute_70,sm_70 (which is the same as -gencode arch=compute_70,code=\"compute_70,sm_70\").



3.1.5. C++ Compatibilityï

The front end of the compiler processes CUDA source files according to C++ syntax rules. Full C++ is supported for the host code. However, only a subset of C++ is fully supported for the device code as described in C++ Language Support.



3.1.6. 64-Bit Compatibilityï

The 64-bit version of nvcc compiles device code in 64-bit mode (i.e., pointers are 64-bit). Device code compiled in 64-bit mode is only supported with host code compiled in 64-bit mode.




3.2. CUDA Runtimeï

The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a, or dynamically via cudart.dll or libcudart.so. Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime.
All its entry points are prefixed with cuda.
As mentioned in Heterogeneous Programming, the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory. Device Memory gives an overview of the runtime functions used to manage device memory.
Shared Memory illustrates the use of shared memory, introduced in Thread Hierarchy, to maximize performance.
Page-Locked Host Memory introduces page-locked host memory that is required to overlap kernel execution with data transfers between host and device memory.
Asynchronous Concurrent Execution describes the concepts and API used to enable asynchronous concurrent execution at various levels in the system.
Multi-Device System shows how the programming model extends to a system with multiple devices attached to the same host.
Error Checking describes how to properly check the errors generated by the runtime.
Call Stack mentions the runtime functions used to manage the CUDA C++ call stack.
Texture and Surface Memory presents the texture and surface memory spaces that provide another way to access device memory; they also expose a subset of the GPU texturing hardware.
Graphics Interoperability introduces the various functions the runtime provides to interoperate with the two main graphics APIs, OpenGL and Direct3D.


3.2.1. Initializationï

As of CUDA 12.0, the cudaInitDevice() and cudaSetDevice() calls initialize the runtime and the primary context associated with the specified device. Absent these calls, the runtime will implicitly use device 0 and self-initialize as needed to process other runtime API requests. One needs to keep this in mind when timing runtime function calls and when interpreting the error code from the first call into the runtime. Before 12.0, cudaSetDevice() would not initialize the runtime and applications would often use the no-op runtime call cudaFree(0) to isolate the runtime initialization from other api activity (both for the sake of timing and error handling).
The runtime creates a CUDA context for each device in the system (see Context for more details on CUDA contexts). This context is the primary context for this device and is initialized at the first runtime function which requires an active context on this device. It is shared among all the host threads of the application. As part of this context creation, the device code is just-in-time compiled if necessary (see Just-in-Time Compilation) and loaded into device memory. This all happens transparently. If needed, for example, for driver API interoperability, the primary context of a device can be accessed from the driver API as described in Interoperability between Runtime and Driver APIs.
When a host thread calls cudaDeviceReset(), this destroys the primary context of the device the host thread currently operates on (i.e., the current device as defined in Device Selection). The next runtime function call made by any host thread that has this device as current will create a new primary context for this device.

Note
The CUDA interfaces use global state that is initialized during host program initiation and destroyed during host program termination. The CUDA runtime and driver cannot detect if this state is invalid, so using any of these interfaces (implicitly or explicitly) during program initiation or termination after main) will result in undefined behavior.
As of CUDA 12.0, cudaSetDevice() will now explicitly initialize the runtime after changing the current device for the host thread. Previous versions of CUDA delayed runtime initialization on the new device until the first runtime call was made after cudaSetDevice(). This change means that it is now very important to check the return value of cudaSetDevice() for initialization errors.
The runtime functions from the error handling and version management sections of the reference manual do not initialize the runtime.




3.2.2. Device Memoryï

As mentioned in Heterogeneous Programming, the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory. Kernels operate out of device memory, so the runtime provides functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory.
Device memory can be allocated either as linear memory or as CUDA arrays.
CUDA arrays are opaque memory layouts optimized for texture fetching. They are described in Texture and Surface Memory.
Linear memory is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list. The size of the address space depends on the host system (CPU) and the compute capability of the used GPU:


Table 1 Linear Memory Address Spaceï










x86_64 (AMD64)
POWER (ppc64le)
ARM64




up to compute capability 5.3 (Maxwell)
40bit
40bit
40bit


compute capability 6.0 (Pascal) or newer
up to 47bit
up to 49bit
up to 48bit




Note
On devices of compute capability 5.3 (Maxwell) and earlier, the CUDA driver creates an uncommitted 40bit virtual address reservation to ensure that memory allocations (pointers) fall into the supported range. This reservation appears as reserved virtual memory, but does not occupy any physical memory until the program actually allocates memory.

Linear memory is typically allocated using cudaMalloc() and freed using cudaFree() and data transfer between host memory and device memory are typically done using cudaMemcpy(). In the vector addition code sample of Kernels, the vectors need to be copied from host memory to device memory:

// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < N)
        C[i] = A[i] + B[i];
}

// Host code
int main()
{
    int N = ...;
    size_t size = N * sizeof(float);

    // Allocate input vectors h_A and h_B in host memory
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);

    // Initialize input vectors
    ...

    // Allocate vectors in device memory
    float* d_A;
    cudaMalloc(&d_A, size);
    float* d_B;
    cudaMalloc(&d_B, size);
    float* d_C;
    cudaMalloc(&d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // Copy result from device memory to host memory
    // h_C contains the result in host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Free host memory
    ...
}


Linear memory can also be allocated through cudaMallocPitch() and cudaMalloc3D(). These functions are recommended for allocations of 2D or 3D arrays as it makes sure that the allocation is appropriately padded to meet the alignment requirements described in Device Memory Accesses, therefore ensuring best performance when accessing the row addresses or performing copies between 2D arrays and other regions of device memory (using the cudaMemcpy2D() and cudaMemcpy3D() functions). The returned pitch (or stride) must be used to access array elements. The following code sample allocates a width x height 2D array of floating-point values and shows how to loop over the array elements in device code:

// Host code
int width = 64, height = 64;
float* devPtr;
size_t pitch;
cudaMallocPitch(&devPtr, &pitch,
                width * sizeof(float), height);
MyKernel<<<100, 512>>>(devPtr, pitch, width, height);

// Device code
__global__ void MyKernel(float* devPtr,
                         size_t pitch, int width, int height)
{
    for (int r = 0; r < height; ++r) {
        float* row = (float*)((char*)devPtr + r * pitch);
        for (int c = 0; c < width; ++c) {
            float element = row[c];
        }
    }
}


The following code sample allocates a width x height x depth 3D array of floating-point values and shows how to loop over the array elements in device code:

// Host code
int width = 64, height = 64, depth = 64;
cudaExtent extent = make_cudaExtent(width * sizeof(float),
                                    height, depth);
cudaPitchedPtr devPitchedPtr;
cudaMalloc3D(&devPitchedPtr, extent);
MyKernel<<<100, 512>>>(devPitchedPtr, width, height, depth);

// Device code
__global__ void MyKernel(cudaPitchedPtr devPitchedPtr,
                         int width, int height, int depth)
{
    char* devPtr = devPitchedPtr.ptr;
    size_t pitch = devPitchedPtr.pitch;
    size_t slicePitch = pitch * height;
    for (int z = 0; z < depth; ++z) {
        char* slice = devPtr + z * slicePitch;
        for (int y = 0; y < height; ++y) {
            float* row = (float*)(slice + y * pitch);
            for (int x = 0; x < width; ++x) {
                float element = row[x];
            }
        }
    }
}



Note
To avoid allocating too much memory and thus impacting system-wide performance, request the allocation parameters from the user based on the problem size. If the allocation fails, you can fallback to other slower memory types (cudaMallocHost(), cudaHostRegister(), etc.), or return an error telling the user how much memory was needed that was denied. If your application cannot request the allocation parameters for some reason, we recommend using cudaMallocManaged() for platforms that support it.

The reference manual lists all the various functions used to copy memory between linear memory allocated with cudaMalloc(), linear memory allocated with cudaMallocPitch() or cudaMalloc3D(), CUDA arrays, and memory allocated for variables declared in global or constant memory space.
The following code sample illustrates various ways of accessing global variables via the runtime API:

__constant__ float constData[256];
float data[256];
cudaMemcpyToSymbol(constData, data, sizeof(data));
cudaMemcpyFromSymbol(data, constData, sizeof(data));

__device__ float devData;
float value = 3.14f;
cudaMemcpyToSymbol(devData, &value, sizeof(float));

__device__ float* devPointer;
float* ptr;
cudaMalloc(&ptr, 256 * sizeof(float));
cudaMemcpyToSymbol(devPointer, &ptr, sizeof(ptr));


cudaGetSymbolAddress() is used to retrieve the address pointing to the memory allocated for a variable declared in global memory space. The size of the allocated memory is obtained through cudaGetSymbolSize().



3.2.3. Device Memory L2 Access Managementï

When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting. On the other hand, if the data is only accessed once, such data accesses can be considered to be streaming.
Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache, potentially providing higher bandwidth and lower latency accesses to global memory.


3.2.3.1. L2 cache Set-Aside for Persisting Accessesï

A portion of the L2 cache can be set aside to be used for persisting data accesses to global memory. Persisting accesses have prioritized use of this set-aside portion of L2 cache, whereas normal or streaming, accesses to global memory can only utilize this portion of L2 when it is unused by persisting accesses.
The L2 cache set-aside size for persisting accesses may be adjusted, within limits:

cudaGetDeviceProperties(&prop, device_id);
size_t size = min(int(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); /* set-aside 3/4 of L2 cache for persisting accesses or the max allowed*/


When the GPU is configured in Multi-Instance GPU (MIG) mode, the L2 cache set-aside functionality is disabled.
When using the Multi-Process Service (MPS), the L2 cache set-aside size cannot be changed by cudaDeviceSetLimit. Instead, the set-aside size can only be specified at start up of MPS server through the environment variable CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT.



3.2.3.2. L2 Policy for Persisting Accessesï

An access policy window specifies a contiguous region of global memory and a persistence property in the L2 cache for accesses within that region.
The code example below shows how to set an L2 persisting access window using a CUDA Stream.
CUDA Stream Example

cudaStreamAttrValue stream_attribute;                                         // Stream level attributes data structure
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(ptr); // Global Memory data pointer
stream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    // Number of bytes for persistence access.
                                                                              // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)
stream_attribute.accessPolicyWindow.hitRatio  = 0.6;                          // Hint for cache hit ratio
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; // Type of access property on cache hit
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  // Type of access property on cache miss.

//Set the attributes to a CUDA stream of type cudaStream_t
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);


When a kernel subsequently executes in CUDA stream, memory accesses within the global memory extent [ptr..ptr+num_bytes) are more likely to persist in the L2 cache than accesses to other global memory locations.
L2 persistence can also be set for a CUDA Graph Kernel Node as shown in the example below:
CUDA GraphKernelNode Example

cudaKernelNodeAttrValue node_attribute;                                     // Kernel level attributes data structure
node_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(ptr); // Global Memory data pointer
node_attribute.accessPolicyWindow.num_bytes = num_bytes;                    // Number of bytes for persistence access.
                                                                            // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)
node_attribute.accessPolicyWindow.hitRatio  = 0.6;                          // Hint for cache hit ratio
node_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; // Type of access property on cache hit
node_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  // Type of access property on cache miss.

//Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t
cudaGraphKernelNodeSetAttribute(node, cudaKernelNodeAttributeAccessPolicyWindow, &node_attribute);


The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property. In both of the examples above, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property. Which specific memory accesses are classified as persisting (the hitProp) is random with a probability of approximately hitRatio; the probability distribution depends upon the hardware architecture and the memory extent.
For example, if the L2 set-aside cache size is 16KB and the num_bytes in the accessPolicyWindow is 32KB:

With a hitRatio of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area.
With a hitRatio of 1.0, the hardware will attempt to cache the whole 32KB window in the set-aside L2 cache area. Since the set-aside area is smaller than the window, cache lines will be evicted to keep the most recently used 16KB of the 32KB data in the set-aside portion of the L2 cache.

The hitRatio can therefore be used to avoid thrashing of cache lines and overall reduce the amount of data moved into and out of the L2 cache.
A hitRatio value below 1.0 can be used to manually control the amount of data different accessPolicyWindows from concurrent CUDA streams can cache in L2. For example, let the L2 set-aside cache size be 16KB; two concurrent kernels in two different CUDA streams, each with a 16KB accessPolicyWindow, and both with hitRatio value 1.0, might evict each othersâ cache lines when competing for the shared L2 resource. However, if both accessPolicyWindows have a hitRatio value of 0.5, they will be less likely to evict their own or each othersâ persisting cache lines.



3.2.3.3. L2 Access Propertiesï

Three types of access properties are defined for different global memory data accesses:

cudaAccessPropertyStreaming: Memory accesses that occur with the streaming property are less likely to persist in the L2 cache because these accesses are preferentially evicted.
cudaAccessPropertyPersisting: Memory accesses that occur with the persisting property are more likely to persist in the L2 cache because these accesses are preferentially retained in the set-aside portion of L2 cache.
cudaAccessPropertyNormal: This access property forcibly resets previously applied persisting access property to a normal status. Memory accesses with the persisting property from previous CUDA kernels may be retained in L2 cache long after their intended use. This persistence-after-use reduces the amount of L2 cache available to subsequent kernels that do not use the persisting property. Resetting an access property window with the cudaAccessPropertyNormal property removes the persisting (preferential retention) status of the prior access, as if the prior access had been without an access property.




3.2.3.4. L2 Persistence Exampleï

The following example shows how to set-aside L2 cache for persistent accesses, use the set-aside L2 cache in CUDA kernels via CUDA Stream and then reset the L2 cache.

cudaStream_t stream;
cudaStreamCreate(&stream);                                                                  // Create CUDA stream

cudaDeviceProp prop;                                                                        // CUDA device properties variable
cudaGetDeviceProperties( &prop, device_id);                                                 // Query GPU properties
size_t size = min( int(prop.l2CacheSize * 0.75) , prop.persistingL2CacheMaxSize );
cudaDeviceSetLimit( cudaLimitPersistingL2CacheSize, size);                                  // set-aside 3/4 of L2 cache for persisting accesses or the max allowed

size_t window_size = min(prop.accessPolicyMaxWindowSize, num_bytes);                        // Select minimum of user defined num_bytes and max window size.

cudaStreamAttrValue stream_attribute;                                                       // Stream level attributes data structure
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(data1);               // Global Memory data pointer
stream_attribute.accessPolicyWindow.num_bytes = window_size;                                // Number of bytes for persistence access
stream_attribute.accessPolicyWindow.hitRatio  = 0.6;                                        // Hint for cache hit ratio
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting;               // Persistence Property
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;                // Type of access property on cache miss

cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);   // Set the attributes to a CUDA Stream

for(int i = 0; i < 10; i++) {
    cuda_kernelA<<<grid_size,block_size,0,stream>>>(data1);                                 // This data1 is used by a kernel multiple times
}                                                                                           // [data1 + num_bytes) benefits from L2 persistence
cuda_kernelB<<<grid_size,block_size,0,stream>>>(data1);                                     // A different kernel in the same stream can also benefit
                                                                                            // from the persistence of data1

stream_attribute.accessPolicyWindow.num_bytes = 0;                                          // Setting the window size to 0 disable it
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);   // Overwrite the access policy attribute to a CUDA Stream
cudaCtxResetPersistingL2Cache();                                                            // Remove any persistent lines in L2

cuda_kernelC<<<grid_size,block_size,0,stream>>>(data2);                                     // data2 can now benefit from full L2 in normal mode





3.2.3.5. Reset L2 Access to Normalï

A persisting L2 cache line from a previous CUDA kernel may persist in L2 long after it has been used. Hence, a reset to normal for L2 cache is important for streaming or normal memory accesses to utilize the L2 cache with normal priority. There are three ways a persisting access can be reset to normal status.

Reset a previous persisting memory region with the access property, cudaAccessPropertyNormal.
Reset all persisting L2 cache lines to normal by calling cudaCtxResetPersistingL2Cache().
Eventually untouched lines are automatically reset to normal. Reliance on automatic reset is strongly discouraged because of the undetermined length of time required for automatic reset to occur.




3.2.3.6. Manage Utilization of L2 set-aside cacheï

Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams. However, the L2 set-aside cache portion is shared among all these concurrent CUDA kernels. As a result, the net utilization of this set-aside cache portion is the sum of all the concurrent kernelsâ individual use. The benefits of designating memory accesses as persisting diminish as the volume of persisting accesses exceeds the set-aside L2 cache capacity.
To manage utilization of the set-aside L2 cache portion, an application must consider the following:

Size of L2 set-aside cache.
CUDA kernels that may concurrently execute.
The access policy window for all the CUDA kernels that may concurrently execute.
When and how L2 reset is required to allow normal or streaming accesses to utilize the previously set-aside L2 cache with equal priority.




3.2.3.7. Query L2 cache Propertiesï

Properties related to L2 cache are a part of cudaDeviceProp struct and can be queried using CUDA runtime API cudaGetDeviceProperties
CUDA Device Properties include:

l2CacheSize: The amount of available L2 cache on the GPU.
persistingL2CacheMaxSize: The maximum amount of L2 cache that can be set-aside for persisting memory accesses.
accessPolicyMaxWindowSize: The maximum size of the access policy window.




3.2.3.8. Control L2 Cache Set-Aside Size for Persisting Memory Accessï

The L2 set-aside cache size for persisting memory accesses is queried using CUDA runtime API cudaDeviceGetLimit and set using CUDA runtime API cudaDeviceSetLimit as a cudaLimit. The maximum value for setting this limit is cudaDeviceProp::persistingL2CacheMaxSize.

enum cudaLimit {
    /* other fields not shown */
    cudaLimitPersistingL2CacheSize
};






3.2.4. Shared Memoryï

As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier.
Shared memory is expected to be much faster than global memory as mentioned in Thread Hierarchy and detailed in Shared Memory. It can be used as scratchpad memory (or software managed cache) to minimize global memory accesses from a CUDA block as illustrated by the following matrix multiplication example.
The following code sample is a straightforward implementation of matrix multiplication that does not take advantage of shared memory. Each thread reads one row of A and one column of B and computes the corresponding element of C as illustrated in Figure 8. A is therefore read B.width times from global memory and B is read A.height times.

// Matrices are stored in row-major order:
// M(row, col) = *(M.elements + row * M.width + col)
typedef struct {
    int width;
    int height;
    float* elements;
} Matrix;

// Thread block size
#define BLOCK_SIZE 16

// Forward declaration of the matrix multiplication kernel
__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);

// Matrix multiplication - Host code
// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
void MatMul(const Matrix A, const Matrix B, Matrix C)
{
    // Load A and B to device memory
    Matrix d_A;
    d_A.width = A.width; d_A.height = A.height;
    size_t size = A.width * A.height * sizeof(float);
    cudaMalloc(&d_A.elements, size);
    cudaMemcpy(d_A.elements, A.elements, size,
               cudaMemcpyHostToDevice);
    Matrix d_B;
    d_B.width = B.width; d_B.height = B.height;
    size = B.width * B.height * sizeof(float);
    cudaMalloc(&d_B.elements, size);
    cudaMemcpy(d_B.elements, B.elements, size,
               cudaMemcpyHostToDevice);

    // Allocate C in device memory
    Matrix d_C;
    d_C.width = C.width; d_C.height = C.height;
    size = C.width * C.height * sizeof(float);
    cudaMalloc(&d_C.elements, size);

    // Invoke kernel
    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
    MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);

    // Read C from device memory
    cudaMemcpy(C.elements, d_C.elements, size,
               cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A.elements);
    cudaFree(d_B.elements);
    cudaFree(d_C.elements);
}

// Matrix multiplication kernel called by MatMul()
__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
{
    // Each thread computes one element of C
    // by accumulating results into Cvalue
    float Cvalue = 0;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    for (int e = 0; e < A.width; ++e)
        Cvalue += A.elements[row * A.width + e]
                * B.elements[e * B.width + col];
    C.elements[row * C.width + col] = Cvalue;
}





Figure 8 Matrix Multiplication without Shared Memoryï


The following code sample is an implementation of matrix multiplication that does take advantage of shared memory. In this implementation, each thread block is responsible for computing one square sub-matrix Csub of C and each thread within the block is responsible for computing one element of Csub. As illustrated in Figure 9, Csub is equal to the product of two rectangular matrices: the sub-matrix of A of dimension (A.width, block_size) that has the same row indices as Csub, and the sub-matrix of B of dimension (block_size, A.width )that has the same column indices as Csub. In order to fit into the deviceâs resources, these two rectangular matrices are divided into as many square matrices of dimension block_size as necessary and Csub is computed as the sum of the products of these square matrices. Each of these products is performed by first loading the two corresponding square matrices from global memory to shared memory with one thread loading one element of each matrix, and then by having each thread compute one element of the product. Each thread accumulates the result of each of these products into a register and once done writes the result to global memory.
By blocking the computation this way, we take advantage of fast shared memory and save a lot of global memory bandwidth since A is only read (B.width / block_size) times from global memory and B is read (A.height / block_size) times.
The Matrix type from the previous code sample is augmented with a stride field, so that sub-matrices can be efficiently represented with the same type. __device__ functions are used to get and set elements and build any sub-matrix from a matrix.

// Matrices are stored in row-major order:
// M(row, col) = *(M.elements + row * M.stride + col)
typedef struct {
    int width;
    int height;
    int stride;
    float* elements;
} Matrix;
// Get a matrix element
__device__ float GetElement(const Matrix A, int row, int col)
{
    return A.elements[row * A.stride + col];
}
// Set a matrix element
__device__ void SetElement(Matrix A, int row, int col,
                           float value)
{
    A.elements[row * A.stride + col] = value;
}
// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is
// located col sub-matrices to the right and row sub-matrices down
// from the upper-left corner of A
 __device__ Matrix GetSubMatrix(Matrix A, int row, int col)
{
    Matrix Asub;
    Asub.width    = BLOCK_SIZE;
    Asub.height   = BLOCK_SIZE;
    Asub.stride   = A.stride;
    Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row
                                         + BLOCK_SIZE * col];
    return Asub;
}
// Thread block size
#define BLOCK_SIZE 16
// Forward declaration of the matrix multiplication kernel
__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);
// Matrix multiplication - Host code
// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
void MatMul(const Matrix A, const Matrix B, Matrix C)
{
    // Load A and B to device memory
    Matrix d_A;
    d_A.width = d_A.stride = A.width; d_A.height = A.height;
    size_t size = A.width * A.height * sizeof(float);
    cudaMalloc(&d_A.elements, size);
    cudaMemcpy(d_A.elements, A.elements, size,
               cudaMemcpyHostToDevice);
    Matrix d_B;
    d_B.width = d_B.stride = B.width; d_B.height = B.height;
    size = B.width * B.height * sizeof(float);
    cudaMalloc(&d_B.elements, size);
    cudaMemcpy(d_B.elements, B.elements, size,
    cudaMemcpyHostToDevice);
    // Allocate C in device memory
    Matrix d_C;
    d_C.width = d_C.stride = C.width; d_C.height = C.height;
    size = C.width * C.height * sizeof(float);
    cudaMalloc(&d_C.elements, size);
    // Invoke kernel
    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
    MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);
    // Read C from device memory
    cudaMemcpy(C.elements, d_C.elements, size,
               cudaMemcpyDeviceToHost);
    // Free device memory
    cudaFree(d_A.elements);
    cudaFree(d_B.elements);
    cudaFree(d_C.elements);
}
// Matrix multiplication kernel called by MatMul()
 __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
{
    // Block row and column
    int blockRow = blockIdx.y;
    int blockCol = blockIdx.x;
    // Each thread block computes one sub-matrix Csub of C
    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);
    // Each thread computes one element of Csub
    // by accumulating results into Cvalue
    float Cvalue = 0;
    // Thread row and column within Csub
    int row = threadIdx.y;
    int col = threadIdx.x;
    // Loop over all the sub-matrices of A and B that are
    // required to compute Csub
    // Multiply each pair of sub-matrices together
    // and accumulate the results
    for (int m = 0; m < (A.width / BLOCK_SIZE); ++m) {
        // Get sub-matrix Asub of A
        Matrix Asub = GetSubMatrix(A, blockRow, m);
        // Get sub-matrix Bsub of B
        Matrix Bsub = GetSubMatrix(B, m, blockCol);
        // Shared memory used to store Asub and Bsub respectively
        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
        // Load Asub and Bsub from device memory to shared memory
        // Each thread loads one element of each sub-matrix
        As[row][col] = GetElement(Asub, row, col);
        Bs[row][col] = GetElement(Bsub, row, col);
        // Synchronize to make sure the sub-matrices are loaded
        // before starting the computation
        __syncthreads();
        // Multiply Asub and Bsub together
        for (int e = 0; e < BLOCK_SIZE; ++e)
            Cvalue += As[row][e] * Bs[e][col];
        // Synchronize to make sure that the preceding
        // computation is done before loading two new
        // sub-matrices of A and B in the next iteration
        __syncthreads();
    }
    // Write Csub to device memory
    // Each thread writes one element
    SetElement(Csub, row, col, Cvalue);
}





Figure 9 Matrix Multiplication with Shared Memoryï





3.2.5. Distributed Shared Memoryï

Thread block clusters introduced in compute capability 9.0 provide the ability for threads in a thread block cluster to access shared memory of all the participating thread blocks in a cluster. This partitioned shared memory is called Distributed Shared Memory, and the corresponding address space is called Distributed shared memory address space. Threads that belong to a thread block cluster, can read, write or perform atomics in the distributed address space, regardless whether the address belongs to the local thread block or a remote thread block. Whether a kernel uses distributed shared memory or not, the shared memory size specifications, static or dynamic is still per thread block. The size of distributed shared memory is just the number of thread blocks per cluster multiplied by the size of shared memory per thread block.
Accessing data in distributed shared memory requires all the thread blocks to exist. A user can guarantee that all thread blocks have started executing using cluster.sync() from Cluster Group API.
The user also needs to ensure that all distributed shared memory operations happen before the exit of a thread block, e.g., if a remote thread block is trying to read a given thread blockâs shared memory, user needs to ensure that the shared memory read by remote thread block is completed before it can exit.
CUDA provides a mechanism to access to distributed shared memory, and applications can benefit from leveraging its capabilities. Lets look at a simple histogram computation and how to optimize it on the GPU using thread block cluster. A standard way of computing histograms is do the computation in the shared memory of each thread block and then perform global memory atomics. A limitation of this approach is the shared memory capacity. Once the histogram bins no longer fit in the shared memory, a user needs to directly compute histograms and hence the atomics in the global memory. With distributed shared memory, CUDA provides an intermediate step, where a depending on the histogram bins size, histogram can be computed in shared memory, distributed shared memory or global memory directly.
The CUDA kernel example below shows how to compute histograms in shared memory or distributed shared memory, depending on the number of histogram bins.

#include <cooperative_groups.h>

// Distributed Shared memory histogram kernel
__global__ void clusterHist_kernel(int *bins, const int nbins, const int bins_per_block, const int *__restrict__ input,
                                   size_t array_size)
{
  extern __shared__ int smem[];
  namespace cg = cooperative_groups;
  int tid = cg::this_grid().thread_rank();

  // Cluster initialization, size and calculating local bin offsets.
  cg::cluster_group cluster = cg::this_cluster();
  unsigned int clusterBlockRank = cluster.block_rank();
  int cluster_size = cluster.dim_blocks().x;

  for (int i = threadIdx.x; i < bins_per_block; i += blockDim.x)
  {
    smem[i] = 0; //Initialize shared memory histogram to zeros
  }

  // cluster synchronization ensures that shared memory is initialized to zero in
  // all thread blocks in the cluster. It also ensures that all thread blocks
  // have started executing and they exist concurrently.
  cluster.sync();

  for (int i = tid; i < array_size; i += blockDim.x * gridDim.x)
  {
    int ldata = input[i];

    //Find the right histogram bin.
    int binid = ldata;
    if (ldata < 0)
      binid = 0;
    else if (ldata >= nbins)
      binid = nbins - 1;

    //Find destination block rank and offset for computing
    //distributed shared memory histogram
    int dst_block_rank = (int)(binid / bins_per_block);
    int dst_offset = binid % bins_per_block;

    //Pointer to target block shared memory
    int *dst_smem = cluster.map_shared_rank(smem, dst_block_rank);

    //Perform atomic update of the histogram bin
    atomicAdd(dst_smem + dst_offset, 1);
  }

  // cluster synchronization is required to ensure all distributed shared
  // memory operations are completed and no thread block exits while
  // other thread blocks are still accessing distributed shared memory
  cluster.sync();

  // Perform global memory histogram, using the local distributed memory histogram
  int *lbins = bins + cluster.block_rank() * bins_per_block;
  for (int i = threadIdx.x; i < bins_per_block; i += blockDim.x)
  {
    atomicAdd(&lbins[i], smem[i]);
  }
}


The above kernel can be launched at runtime with a cluster size depending on the amount of distributed shared memory required. If histogram is small enough to fit in shared memory of just one block, user can launch kernel with cluster size 1. The code snippet below shows how to launch a cluster kernel dynamically based depending on shared memory requirements.

// Launch via extensible launch
{
  cudaLaunchConfig_t config = {0};
  config.gridDim = array_size / threads_per_block;
  config.blockDim = threads_per_block;

  // cluster_size depends on the histogram size.
  // ( cluster_size == 1 ) implies no distributed shared memory, just thread block local shared memory
  int cluster_size = 2; // size 2 is an example here
  int nbins_per_block = nbins / cluster_size;

  //dynamic shared memory size is per block.
  //Distributed shared memory size =  cluster_size * nbins_per_block * sizeof(int)
  config.dynamicSmemBytes = nbins_per_block * sizeof(int);

  CUDA_CHECK(::cudaFuncSetAttribute((void *)clusterHist_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, config.dynamicSmemBytes));

  cudaLaunchAttribute attribute[1];
  attribute[0].id = cudaLaunchAttributeClusterDimension;
  attribute[0].val.clusterDim.x = cluster_size;
  attribute[0].val.clusterDim.y = 1;
  attribute[0].val.clusterDim.z = 1;

  config.numAttrs = 1;
  config.attrs = attribute;

  cudaLaunchKernelEx(&config, clusterHist_kernel, bins, nbins, nbins_per_block, input, array_size);
}





3.2.6. Page-Locked Host Memoryï

The runtime provides functions to allow the use of page-locked (also known as pinned) host memory (as opposed to regular pageable host memory allocated by malloc()):

cudaHostAlloc() and cudaFreeHost() allocate and free page-locked host memory;
cudaHostRegister() page-locks a range of memory allocated by malloc() (see reference manual for limitations).

Using page-locked host memory has several benefits:

Copies between page-locked host memory and device memory can be performed concurrently with kernel execution for some devices as mentioned in Asynchronous Concurrent Execution.
On some devices, page-locked host memory can be mapped into the address space of the device, eliminating the need to copy it to or from device memory as detailed in Mapped Memory.
On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in Write-Combining Memory.


Note
Page-locked host memory is not cached on non I/O coherent Tegra devices. Also, cudaHostRegister() is not supported on non I/O coherent Tegra devices.

The simple zero-copy CUDA sample comes with a detailed document on the page-locked memory APIs.


3.2.6.1. Portable Memoryï

A block of page-locked memory can be used in conjunction with any device in the system (see Multi-Device System for more details on multi-device systems), but by default, the benefits of using page-locked memory described above are only available in conjunction with the device that was current when the block was allocated (and with all devices sharing the same unified address space, if any, as described in Unified Virtual Address Space). To make these advantages available to all devices, the block needs to be allocated by passing the flag cudaHostAllocPortable to cudaHostAlloc() or page-locked by passing the flag cudaHostRegisterPortable to cudaHostRegister().



3.2.6.2. Write-Combining Memoryï

By default page-locked host memory is allocated as cacheable. It can optionally be allocated as write-combining instead by passing flag cudaHostAllocWriteCombined to cudaHostAlloc(). Write-combining memory frees up the hostâs L1 and L2 cache resources, making more cache available to the rest of the application. In addition, write-combining memory is not snooped during transfers across the PCI Express bus, which can improve transfer performance by up to 40%.
Reading from write-combining memory from the host is prohibitively slow, so write-combining memory should in general be used for memory that the host only writes to.
Using CPU atomic instructions on WC memory should be avoided because not all CPU implementations guarantee that functionality.



3.2.6.3. Mapped Memoryï

A block of page-locked host memory can also be mapped into the address space of the device by passing flag cudaHostAllocMapped to cudaHostAlloc() or by passing flag cudaHostRegisterMapped to cudaHostRegister(). Such a block has therefore in general two addresses: one in host memory that is returned by cudaHostAlloc() or malloc(), and one in device memory that can be retrieved using cudaHostGetDevicePointer() and then used to access the block from within a kernel. The only exception is for pointers allocated with cudaHostAlloc() and when a unified address space is used for the host and the device as mentioned in Unified Virtual Address Space.
Accessing host memory directly from within a kernel does not provide the same bandwidth as device memory, but does have some advantages:

There is no need to allocate a block in device memory and copy data between this block and the block in host memory; data transfers are implicitly performed as needed by the kernel;
There is no need to use streams (see Concurrent Data Transfers) to overlap data transfers with kernel execution; the kernel-originated data transfers automatically overlap with kernel execution.

Since mapped page-locked memory is shared between host and device however, the application must synchronize memory accesses using streams or events (see Asynchronous Concurrent Execution) to avoid any potential read-after-write, write-after-read, or write-after-write hazards.
To be able to retrieve the device pointer to any mapped page-locked memory, page-locked memory mapping must be enabled by calling cudaSetDeviceFlags() with the cudaDeviceMapHost flag before any other CUDA call is performed. Otherwise, cudaHostGetDevicePointer() will return an error.
cudaHostGetDevicePointer() also returns an error if the device does not support mapped page-locked host memory. Applications may query this capability by checking the canMapHostMemory device property (see Device Enumeration), which is equal to 1 for devices that support mapped page-locked host memory.
Note that atomic functions (see Atomic Functions) operating on mapped page-locked memory are not atomic from the point of view of the host or other devices.
Also note that CUDA runtime requires that 1-byte, 2-byte, 4-byte, and 8-byte naturally aligned loads and stores to host memory initiated from the device are preserved as single accesses from the point of view of the host and other devices. On some platforms, atomics to memory may be broken by the hardware into separate load and store operations. These component load and store operations have the same requirements on preservation of naturally aligned accesses. As an example, the CUDA runtime does not support a PCI Express bus topology where a PCI Express bridge splits 8-byte naturally aligned writes into two 4-byte writes between the device and the host.




3.2.7. Memory Synchronization Domainsï



3.2.7.1. Memory Fence Interferenceï

Some CUDA applications may see degraded performance due to memory fence/flush operations waiting on more transactions than those necessitated by the CUDA memory consistency model.










__managed__ int x = 0;
__device__  cuda::atomic<int, cuda::thread_scope_device> a(0);
__managed__ cuda::atomic<int, cuda::thread_scope_system> b(0);








Thread 1 (SM)

x = 1;
a = 1;




Thread 2 (SM)

while (a != 1) ;
assert(x == 1);
b = 1;




Thread 3 (CPU)

while (b != 1) ;
assert(x == 1);






Consider the example above. The CUDA memory consistency model guarantees that the asserted condition will be true, so the write to x from thread 1 must be visible to thread 3, before the write to b from thread 2.
The memory ordering provided by the release and acquire of a is only sufficient to make x visible to thread 2, not thread 3, as it is a device-scope operation. The system-scope ordering provided by release and acquire of b, therefore, needs to ensure not only writes issued from thread 2 itself are visible to thread 3, but also writes from other threads that are visible to thread 2. This is known as cumulativity. As the GPU cannot know at the time of execution which writes have been guaranteed at the source level to be visible and which are visible only by chance timing, it must cast a conservatively wide net for in-flight memory operations.
This sometimes leads to interference: because the GPU is waiting on memory operations it is not required to at the source level, the fence/flush may take longer than necessary.
Note that fences may occur explicitly as intrinsics or atomics in code, like in the example, or implicitly to implement synchronizes-with relationships at task boundaries.
A common example is when a kernel is performing computation in local GPU memory, and a parallel kernel (e.g. from NCCL) is performing communications with a peer. Upon completion, the local kernel will implicitly flush its writes to satisfy any synchronizes-with relationships to downstream work. This may unnecessarily wait, fully or partially, on slower nvlink or PCIe writes from the communication kernel.



3.2.7.2. Isolating Traffic with Domainsï

Beginning with Hopper architecture GPUs and CUDA 12.0, the memory synchronization domains feature provides a way to alleviate such interference. In exchange for explicit assistance from code, the GPU can reduce the net cast by a fence operation. Each kernel launch is given a domain ID. Writes and fences are tagged with the ID, and a fence will only order writes matching the fenceâs domain. In the concurrent compute vs communication example, the communication kernels can be placed in a different domain.
When using domains, code must abide by the rule that ordering or synchronization between distinct domains on the same GPU requires system-scope fencing. Within a domain, device-scope fencing remains sufficient. This is necessary for cumulativity as one kernelâs writes will not be encompassed by a fence issued from a kernel in another domain. In essence, cumulativity is satisfied by ensuring that cross-domain traffic is flushed to the system scope ahead of time.
Note that this modifies the definition of thread_scope_device. However, because kernels will default to domain 0 as described below, backward compatibility is maintained.



3.2.7.3. Using Domains in CUDAï

Domains are accessible via the new launch attributes cudaLaunchAttributeMemSyncDomain and cudaLaunchAttributeMemSyncDomainMap. The former selects between logical domains cudaLaunchMemSyncDomainDefault and cudaLaunchMemSyncDomainRemote, and the latter provides a mapping from logical to physical domains. The remote domain is intended for kernels performing remote memory access in order to isolate their memory traffic from local kernels. Note, however, the selection of a particular domain does not affect what memory access a kernel may legally perform.
The domain count can be queried via device attribute cudaDevAttrMemSyncDomainCount. Hopper has 4 domains. To facilitate portable code, domains functionality can be used on all devices and CUDA will report a count of 1 prior to Hopper.
Having logical domains eases application composition. An individual kernel launch at a low level in the stack, such as from NCCL, can select a semantic logical domain without concern for the surrounding application architecture. Higher levels can steer logical domains using the mapping. The default value for the logical domain if it is not set is the default domain, and the default mapping is to map the default domain to 0 and the remote domain to 1 (on GPUs with more than 1 domain). Specific libraries may tag launches with the remote domain in CUDA 12.0 and later; for example, NCCL 2.16 will do so. Together, this provides a beneficial use pattern for common applications out of the box, with no code changes needed in other components, frameworks, or at application level. An alternative use pattern, for example in an application using nvshmem or with no clear separation of kernel types, could be to partition parallel streams. Stream A may map both logical domains to physical domain 0, stream B to 1, and so on.

// Example of launching a kernel with the remote logical domain
cudaLaunchAttribute domainAttr;
domainAttr.id = cudaLaunchAttrMemSyncDomain;
domainAttr.val = cudaLaunchMemSyncDomainRemote;
cudaLaunchConfig_t config;
// Fill out other config fields
config.attrs = &domainAttr;
config.numAttrs = 1;
cudaLaunchKernelEx(&config, myKernel, kernelArg1, kernelArg2...);



// Example of setting a mapping for a stream
// (This mapping is the default for streams starting on Hopper if not
// explicitly set, and provided for illustration)
cudaLaunchAttributeValue mapAttr;
mapAttr.memSyncDomainMap.default_ = 0;
mapAttr.memSyncDomainMap.remote = 1;
cudaStreamSetAttribute(stream, cudaLaunchAttributeMemSyncDomainMap, &mapAttr);



// Example of mapping different streams to different physical domains, ignoring
// logical domain settings
cudaLaunchAttributeValue mapAttr;
mapAttr.memSyncDomainMap.default_ = 0;
mapAttr.memSyncDomainMap.remote = 0;
cudaStreamSetAttribute(streamA, cudaLaunchAttributeMemSyncDomainMap, &mapAttr);
mapAttr.memSyncDomainMap.default_ = 1;
mapAttr.memSyncDomainMap.remote = 1;
cudaStreamSetAttribute(streamB, cudaLaunchAttributeMemSyncDomainMap, &mapAttr);


As with other launch attributes, these are exposed uniformly on CUDA streams, individual launches using cudaLaunchKernelEx, and kernel nodes in CUDA graphs. A typical use would set the mapping at stream level and the logical domain at launch level (or bracketing a section of stream use) as described above.
Both attributes are copied to graph nodes during stream capture. Graphs take both attributes from the node itself, essentially an indirect way of specifying a physical domain. Domain-related attributes set on the stream a graph is launched into are not used in execution of the graph.




3.2.8. Asynchronous Concurrent Executionï

CUDA exposes the following operations as independent tasks that can operate concurrently with one another:

Computation on the host;
Computation on the device;
Memory transfers from the host to the device;
Memory transfers from the device to the host;
Memory transfers within the memory of a given device;
Memory transfers among devices.

The level of concurrency achieved between these operations will depend on the feature set and compute capability of the device as described below.


3.2.8.1. Concurrent Execution between Host and Deviceï

Concurrent host execution is facilitated through asynchronous library functions that return control to the host thread before the device completes the requested task. Using asynchronous calls, many device operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available. This relieves the host thread of much of the responsibility to manage the device, leaving it free for other tasks. The following device operations are asynchronous with respect to the host:

Kernel launches;
Memory copies within a single deviceâs memory;
Memory copies from host to device of a memory block of 64 KB or less;
Memory copies performed by functions that are suffixed with Async;
Memory set function calls.

Programmers can globally disable asynchronicity of kernel launches for all CUDA applications running on a system by setting the CUDA_LAUNCH_BLOCKING environment variable to 1. This feature is provided for debugging purposes only and should not be used as a way to make production software run reliably.
Kernel launches are synchronous if hardware counters are collected via a profiler (Nsight, Visual Profiler) unless concurrent kernel profiling is enabled. Async memory copies might also be synchronous if they involve host memory that is not page-locked.



3.2.8.2. Concurrent Kernel Executionï

Some devices of compute capability 2.x and higher can execute multiple kernels concurrently. Applications may query this capability by checking the concurrentKernels device property (see Device Enumeration), which is equal to 1 for devices that support it.
The maximum number of kernel launches that a device can execute concurrently depends on its compute capability and is listed in Table 21.
A kernel from one CUDA context cannot execute concurrently with a kernel from another CUDA context. The GPU may time slice to provide forward progress to each context. If a user wants to run kernels from multiple process simultaneously on the SM, one must enable MPS.
Kernels that use many textures or a large amount of local memory are less likely to execute concurrently with other kernels.



3.2.8.3. Overlap of Data Transfer and Kernel Executionï

Some devices can perform an asynchronous memory copy to or from the GPU concurrently with kernel execution. Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration), which is greater than zero for devices that support it. If host memory is involved in the copy, it must be page-locked.
It is also possible to perform an intra-device copy simultaneously with kernel execution (on devices that support the concurrentKernels device property) and/or with copies to or from the device (for devices that support the asyncEngineCount property). Intra-device copies are initiated using the standard memory copy functions with destination and source addresses residing on the same device.



3.2.8.4. Concurrent Data Transfersï

Some devices of compute capability 2.x and higher can overlap copies to and from the device. Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration), which is equal to 2 for devices that support it. In order to be overlapped, any host memory involved in the transfers must be page-locked.



3.2.8.5. Streamsï

Applications manage the concurrent operations described above through streams. A stream is a sequence of commands (possibly issued by different host threads) that execute in order. Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently; this behavior is not guaranteed and should therefore not be relied upon for correctness (for example, inter-kernel communication is undefined). The commands issued on a stream may execute when all the dependencies of the command are met. The dependencies could be previously launched commands on same stream or dependencies from other streams. The successful completion of synchronize call guarantees that all the commands launched are completed.


3.2.8.5.1. Creation and Destruction of Streamsï

A stream is defined by creating a stream object and specifying it as the stream parameter to a sequence of kernel launches and host <-> device memory copies. The following code sample creates two streams and allocates an array hostPtr of float in page-locked memory.

cudaStream_t stream[2];
for (int i = 0; i < 2; ++i)
    cudaStreamCreate(&stream[i]);
float* hostPtr;
cudaMallocHost(&hostPtr, 2 * size);


Each of these streams is defined by the following code sample as a sequence of one memory copy from host to device, one kernel launch, and one memory copy from device to host:

for (int i = 0; i < 2; ++i) {
    cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel <<<100, 512, 0, stream[i]>>>
          (outputDevPtr + i * size, inputDevPtr + i * size, size);
    cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);
}


Each stream copies its portion of input array hostPtr to array inputDevPtr in device memory, processes inputDevPtr on the device by calling MyKernel(), and copies the result outputDevPtr back to the same portion of hostPtr. Overlapping Behavior describes how the streams overlap in this example depending on the capability of the device. Note that hostPtr must point to page-locked host memory for any overlap to occur.
Streams are released by calling cudaStreamDestroy().

for (int i = 0; i < 2; ++i)
    cudaStreamDestroy(stream[i]);


In case the device is still doing work in the stream when cudaStreamDestroy() is called, the function will return immediately and the resources associated with the stream will be released automatically once the device has completed all work in the stream.



3.2.8.5.2. Default Streamï

Kernel launches and host <-> device memory copies that do not specify any stream parameter, or equivalently that set the stream parameter to zero, are issued to the default stream. They are therefore executed in order.
For code that is compiled using the --default-stream per-thread compilation flag (or that defines the CUDA_API_PER_THREAD_DEFAULT_STREAM macro before including CUDA headers (cuda.h and cuda_runtime.h)), the default stream is a regular stream and each host thread has its own default stream.

Note
#define CUDA_API_PER_THREAD_DEFAULT_STREAM 1 cannot be used to enable this behavior when the code is compiled by nvcc as nvcc implicitly includes cuda_runtime.h at the top of the translation unit. In this case the --default-stream per-thread compilation flag needs to be used or the CUDA_API_PER_THREAD_DEFAULT_STREAM macro needs to be defined with the -DCUDA_API_PER_THREAD_DEFAULT_STREAM=1 compiler flag.

For code that is compiled using the --default-stream legacy compilation flag, the default stream is a special stream called the NULL stream and each device has a single NULL stream used for all host threads. The NULL stream is special as it causes implicit synchronization as described in Implicit Synchronization.
For code that is compiled without specifying a --default-stream compilation flag, --default-stream legacy is assumed as the default.



3.2.8.5.3. Explicit Synchronizationï

There are various ways to explicitly synchronize streams with each other.
cudaDeviceSynchronize() waits until all preceding commands in all streams of all host threads have completed.
cudaStreamSynchronize()takes a stream as a parameter and waits until all preceding commands in the given stream have completed. It can be used to synchronize the host with a specific stream, allowing other streams to continue executing on the device.
cudaStreamWaitEvent()takes a stream and an event as parameters (see Events for a description of events)and makes all the commands added to the given stream after the call to cudaStreamWaitEvent()delay their execution until the given event has completed.
cudaStreamQuery()provides applications with a way to know if all preceding commands in a stream have completed.



3.2.8.5.4. Implicit Synchronizationï

Two commands from different streams cannot run concurrently if any one of the following operations is issued in-between them by the host thread:

a page-locked host memory allocation,
a device memory allocation,
a device memory set,
a memory copy between two addresses to the same device memory,
any CUDA command to the NULL stream,
a switch between the L1/shared memory configurations described in Compute Capability 7.x.

Operations that require a dependency check include any other commands within the same stream as the launch being checked and any call to cudaStreamQuery() on that stream. Therefore, applications should follow these guidelines to improve their potential for concurrent kernel execution:

All independent operations should be issued before dependent operations,
Synchronization of any kind should be delayed as long as possible.




3.2.8.5.5. Overlapping Behaviorï

The amount of execution overlap between two streams depends on the order in which the commands are issued to each stream and whether or not the device supports overlap of data transfer and kernel execution (see Overlap of Data Transfer and Kernel Execution), concurrent kernel execution (see Concurrent Kernel Execution), and/or concurrent data transfers (see Concurrent Data Transfers).
For example, on devices that do not support concurrent data transfers, the two streams of the code sample of Creation and Destruction do not overlap at all because the memory copy from host to device is issued to stream[1] after the memory copy from device to host is issued to stream[0], so it can only start once the memory copy from device to host issued to stream[0] has completed. If the code is rewritten the following way (and assuming the device supports overlap of data transfer and kernel execution)

for (int i = 0; i < 2; ++i)
    cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
for (int i = 0; i < 2; ++i)
    MyKernel<<<100, 512, 0, stream[i]>>>
          (outputDevPtr + i * size, inputDevPtr + i * size, size);
for (int i = 0; i < 2; ++i)
    cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);


then the memory copy from host to device issued to stream[1] overlaps with the kernel launch issued to stream[0].
On devices that do support concurrent data transfers, the two streams of the code sample of Creation and Destruction do overlap: The memory copy from host to device issued to stream[1] overlaps with the memory copy from device to host issued to stream[0] and even with the kernel launch issued to stream[0] (assuming the device supports overlap of data transfer and kernel execution).



3.2.8.5.6. Host Functions (Callbacks)ï

The runtime provides a way to insert a CPU function call at any point into a stream via cudaLaunchHostFunc(). The provided function is executed on the host once all commands issued to the stream before the callback have completed.
The following code sample adds the host function MyCallback to each of two streams after issuing a host-to-device memory copy, a kernel launch and a device-to-host memory copy into each stream. The function will begin execution on the host after each of the device-to-host memory copies completes.

void CUDART_CB MyCallback(void *data){
    printf("Inside callback %d\n", (size_t)data);
}
...
for (size_t i = 0; i < 2; ++i) {
    cudaMemcpyAsync(devPtrIn[i], hostPtr[i], size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel<<<100, 512, 0, stream[i]>>>(devPtrOut[i], devPtrIn[i], size);
    cudaMemcpyAsync(hostPtr[i], devPtrOut[i], size, cudaMemcpyDeviceToHost, stream[i]);
    cudaLaunchHostFunc(stream[i], MyCallback, (void*)i);
}


The commands that are issued in a stream after a host function do not start executing before the function has completed.
A host function enqueued into a stream must not make CUDA API calls (directly or indirectly), as it might end up waiting on itself if it makes such a call leading to a deadlock.



3.2.8.5.7. Stream Prioritiesï

The relative priorities of streams can be specified at creation using cudaStreamCreateWithPriority(). The range of allowable priorities, ordered as [ highest priority, lowest priority ] can be obtained using the cudaDeviceGetStreamPriorityRange() function. At runtime, pending work in higher-priority streams takes preference over pending work in low-priority streams.
The following code sample obtains the allowable range of priorities for the current device, and creates streams with the highest and lowest available priorities.

// get the range of stream priorities for this device
int priority_high, priority_low;
cudaDeviceGetStreamPriorityRange(&priority_low, &priority_high);
// create streams with highest and lowest available priorities
cudaStream_t st_high, st_low;
cudaStreamCreateWithPriority(&st_high, cudaStreamNonBlocking, priority_high);
cudaStreamCreateWithPriority(&st_low, cudaStreamNonBlocking, priority_low);






3.2.8.6. Programmatic Dependent Launch and Synchronizationï

The Programmatic Dependent Launch mechanism allows for a dependent secondary kernel
to launch before the primary kernel it depends on in the same CUDA stream has finished executing.
Available starting with devices of compute capability 9.0, this technique can provide performance
benefits when the secondary kernel can complete significant work that does not depend on the results of the primary kernel.


3.2.8.6.1. Backgroundï

A CUDA application utilizes the GPU by launching and executing multiple kernels on it.
A typical GPU activity timeline is shown in Figure 10.



Figure 10 GPU activity timelineï


Here, secondary_kernel is launched after primary_kernel finishes its execution.
Serialized execution is usually necessary because secondary_kernel depends on result data
produced by primary_kernel. If secondary_kernel has no dependency on primary_kernel,
both of them can be launched concurrently by using CUDA streams.
Even if secondary_kernel is dependent on primary_kernel, there is some potential for
concurrent execution. For example, almost all the kernels have
some sort of preamble section during which tasks such as zeroing buffers or loading
constant values are performed.



Figure 11 Preamble section of secondary_kernelï


Figure 11 demonstrates the portion of secondary_kernel that could
be executed concurrently without impacting the application.
Note that concurrent launch also allows us to hide the launch latency of secondary_kernel behind
the execution of primary_kernel.



Figure 12 Concurrent execution of primary_kernel and secondary_kernelï


The concurrent launch and execution of secondary_kernel shown in Figure 12 is
achievable using Programmatic Dependent Launch.
Programmatic Dependent Launch introduces changes to the CUDA kernel launch APIs as explained in following section.
These APIs require at least compute capability 9.0 to provide overlapping execution.



3.2.8.6.2. API Descriptionï

In Programmatic Dependent Launch, a primary and a secondary kernel are launched in the same CUDA stream.
The primary kernel should execute cudaTriggerProgrammaticLaunchCompletion with all thread blocks when
itâs ready for the secondary kernel to launch. The secondary kernel must be launched using the extensible launch API as shown.

__global__ void primary_kernel() {
   // Initial work that should finish before starting secondary kernel

   // Trigger the secondary kernel
   cudaTriggerProgrammaticLaunchCompletion();

   // Work that can coincide with the secondary kernel
}

__global__ void secondary_kernel()
{
   // Independent work

   // Will block until all primary kernels the secondary kernel is dependent on have completed and flushed results to global memory
   cudaGridDependencySynchronize();

   // Dependent work
}

cudaLaunchAttribute attribute[1];
attribute[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;
attribute[0].val.programmaticStreamSerializationAllowed = 1;
configSecondary.attrs = attribute;
configSecondary.numAttrs = 1;

primary_kernel<<<grid_dim, block_dim, 0, stream>>>();
cudaLaunchKernelEx(&configSecondary, secondary_kernel);


When the secondary kernel is launched using the cudaLaunchAttributeProgrammaticStreamSerialization attribute,
the CUDA driver is safe to launch the secondary kernel early and not wait on the
completion and memory flush of the primary before launching the secondary.
The CUDA driver can launch the secondary kernel when all primary thread blocks have launched and executed
cudaTriggerProgrammaticLaunchCompletion.
If the primary kernel doesnât execute the trigger, it implicitly occurs after
all thread blocks in the primary kernel exit.
In either case, the secondary thread blocks might launch
before data written by the primary kernel is visible. As such, when the secondary kernel is configured with Programmatic Dependent Launch,
it must always use cudaGridDependencySynchronize
or other means to verify that the result data from the primary is available.
Please note that these methods provide the opportunity for the primary and secondary kernels to execute concurrently, however
this behavior is opportunistic and not guaranteed to lead to concurrent kernel execution.
Reliance on concurrent execution in this manner is unsafe and can lead to deadlock.



3.2.8.6.3. Use in CUDA Graphsï

Programmatic Dependent Launch can be used in CUDA Graphs via stream capture or directly via edge data. To program this feature in a CUDA Graph with edge data, use a cudaGraphDependencyType value of cudaGraphDependencyTypeProgrammatic on an edge connecting two kernel nodes. This edge type makes the upstream kernel visible to a cudaGridDependencySynchronize() in the downstream kernel. This type must be used with an outgoing port of either cudaGraphKernelNodePortLaunchCompletion or cudaGraphKernelNodePortProgrammatic.
The resulting graph equivalents for stream capture are as follows:







Stream code (abbreviated)
Resulting graph edge






cudaLaunchAttribute attribute;
attribute.id = cudaLaunchAttributeProgrammaticStreamSerialization;
attribute.val.programmaticStreamSerializationAllowed = 1;





cudaGraphEdgeData edgeData;
edgeData.type = cudaGraphDependencyTypeProgrammatic;
edgeData.from_port = cudaGraphKernelNodePortProgrammatic;







cudaLaunchAttribute attribute;
attribute.id = cudaLaunchAttributeProgrammaticEvent;
attribute.val.programmaticEvent.triggerAtBlockStart = 0;





cudaGraphEdgeData edgeData;
edgeData.type = cudaGraphDependencyTypeProgrammatic;
edgeData.from_port = cudaGraphKernelNodePortProgrammatic;







cudaLaunchAttribute attribute;
attribute.id = cudaLaunchAttributeProgrammaticEvent;
attribute.val.programmaticEvent.triggerAtBlockStart = 1;





cudaGraphEdgeData edgeData;
edgeData.type = cudaGraphDependencyTypeProgrammatic;
edgeData.from_port = cudaGraphKernelNodePortLaunchCompletion;










3.2.8.7. CUDA Graphsï

CUDA Graphs present a new model for work submission in CUDA. A graph is a series of operations, such as kernel launches, connected by dependencies, which is defined separately from its execution. This allows a graph to be defined once and then launched repeatedly. Separating out the definition of a graph from its execution enables a number of optimizations: first, CPU launch costs are reduced compared to streams, because much of the setup is done in advance; second, presenting the whole workflow to CUDA enables optimizations which might not be possible with the piecewise work submission mechanism of streams.
To see the optimizations possible with graphs, consider what happens in a stream: when you place a kernel into a stream, the host driver performs a sequence of operations in preparation for the execution of the kernel on the GPU. These operations, necessary for setting up and launching the kernel, are an overhead cost which must be paid for each kernel that is issued. For a GPU kernel with a short execution time, this overhead cost can be a significant fraction of the overall end-to-end execution time.
Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution.

During the definition phase, a program creates a description of the operations in the graph along with the dependencies between them.
Instantiation takes a snapshot of the graph template, validates it, and performs much of the setup and initialization of work with the aim of minimizing what needs to be done at launch. The resulting instance is known as an executable graph.
An executable graph may be launched into a stream, similar to any other CUDA work. It may be launched any number of times without repeating the instantiation.



3.2.8.7.1. Graph Structureï

An operation forms a node in a graph. The dependencies between the operations are the edges. These dependencies constrain the execution sequence of the operations.
An operation may be scheduled at any time once the nodes on which it depends are complete. Scheduling is left up to the CUDA system.


3.2.8.7.1.1. Node Typesï

A graph node can be one of:

kernel
CPU function call
memory copy
memset
empty node
waiting on an event
recording an event
signalling an external semaphore
waiting on an external semaphore
conditional node
child graph: To execute a separate nested graph, as shown in the following figure.




Figure 13 Child Graph Exampleï





3.2.8.7.1.2. Edge Dataï

CUDA 12.3 introduced edge data on CUDA Graphs. Edge data modifies a dependency specified by an edge and consists of three parts: an outgoing port, an incoming port, and a type. An outgoing port specifies when an associated edge is triggered. An incoming port specifies what portion of a node is dependent on an associated edge. A type modifies the relation between the endpoints.
Port values are specific to node type and direction, and edge types may be restricted to specific node types. In all cases, zero-initialized edge data represents default behavior. Outgoing port 0 waits on an entire task, incoming port 0 blocks an entire task, and edge type 0 is associated with a full dependency with memory synchronizing behavior.
Edge data is optionally specified in various graph APIs via a parallel array to the associated nodes. If it is omitted as an input parameter, zero-initialized data is used. If it is omitted as an output (query) parameter, the API accepts this if the edge data being ignored is all zero-initialized, and returns cudaErrorLossyQuery if the call would discard information.
Edge data is also available in some stream capture APIs: cudaStreamBeginCaptureToGraph(), cudaStreamGetCaptureInfo(), and cudaStreamUpdateCaptureDependencies(). In these cases, there is not yet a downstream node. The data is associated with a dangling edge (half edge) which will either be connected to a future captured node or discarded at termination of stream capture. Note that some edge types do not wait on full completion of the upstream node. These edges are ignored when considering if a stream capture has been fully rejoined to the origin stream, and cannot be discarded at the end of capture. See Creating a Graph Using Stream Capture.
Currently, no node types define additional incoming ports, and only kernel nodes define additional outgoing ports. There is one non-default dependency type, cudaGraphDependencyTypeProgrammatic, which enables Programmatic Dependent Launch between two kernel nodes.




3.2.8.7.2. Creating a Graph Using Graph APIsï

Graphs can be created via two mechanisms: explicit API and stream capture. The following is an example of creating and executing the below graph.



Figure 14 Creating a Graph Using Graph APIs Exampleï



// Create the graph - it starts out empty
cudaGraphCreate(&graph, 0);

// For the purpose of this example, we'll create
// the nodes separately from the dependencies to
// demonstrate that it can be done in two stages.
// Note that dependencies can also be specified
// at node creation.
cudaGraphAddKernelNode(&a, graph, NULL, 0, &nodeParams);
cudaGraphAddKernelNode(&b, graph, NULL, 0, &nodeParams);
cudaGraphAddKernelNode(&c, graph, NULL, 0, &nodeParams);
cudaGraphAddKernelNode(&d, graph, NULL, 0, &nodeParams);

// Now set up dependencies on each node
cudaGraphAddDependencies(graph, &a, &b, 1);     // A->B
cudaGraphAddDependencies(graph, &a, &c, 1);     // A->C
cudaGraphAddDependencies(graph, &b, &d, 1);     // B->D
cudaGraphAddDependencies(graph, &c, &d, 1);     // C->D





3.2.8.7.3. Creating a Graph Using Stream Captureï

Stream capture provides a mechanism to create a graph from existing stream-based APIs. A section of code which launches work into streams, including existing code, can be bracketed with calls to cudaStreamBeginCapture() and cudaStreamEndCapture(). See below.

cudaGraph_t graph;

cudaStreamBeginCapture(stream);

kernel_A<<< ..., stream >>>(...);
kernel_B<<< ..., stream >>>(...);
libraryCall(stream);
kernel_C<<< ..., stream >>>(...);

cudaStreamEndCapture(stream, &graph);


A call to cudaStreamBeginCapture() places a stream in capture mode. When a stream is being captured, work launched into the stream is not enqueued for execution. It is instead appended to an internal graph that is progressively being built up. This graph is then returned by calling cudaStreamEndCapture(), which also ends capture mode for the stream. A graph which is actively being constructed by stream capture is referred to as a capture graph.
Stream capture can be used on any CUDA stream except cudaStreamLegacy (the âNULL streamâ). Note that it can be used on cudaStreamPerThread. If a program is using the legacy stream, it may be possible to redefine stream 0 to be the per-thread stream with no functional change. See Default Stream.
Whether a stream is being captured can be queried with cudaStreamIsCapturing().
Work can be captured to an existing graph using cudaStreamBeginCaptureToGraph().  Instead of capturing to an internal graph, work is captured to a graph provided by the user.


3.2.8.7.3.1. Cross-stream Dependencies and Eventsï

Stream capture can handle cross-stream dependencies expressed with cudaEventRecord() and cudaStreamWaitEvent(), provided the event being waited upon was recorded into the same capture graph.
When an event is recorded in a stream that is in capture mode, it results in a captured event. A captured event represents a set of nodes in a capture graph.
When a captured event is waited on by a stream, it places the stream in capture mode if it is not already, and the next item in the stream will have additional dependencies on the nodes in the captured event. The two streams are then being captured to the same capture graph.
When cross-stream dependencies are present in stream capture, cudaStreamEndCapture() must still be called in the same stream where cudaStreamBeginCapture() was called; this is the origin stream. Any other streams which are being captured to the same capture graph, due to event-based dependencies, must also be joined back to the origin stream. This is illustrated below. All streams being captured to the same capture graph are taken out of capture mode upon cudaStreamEndCapture(). Failure to rejoin to the origin stream will result in failure of the overall capture operation.

// stream1 is the origin stream
cudaStreamBeginCapture(stream1);

kernel_A<<< ..., stream1 >>>(...);

// Fork into stream2
cudaEventRecord(event1, stream1);
cudaStreamWaitEvent(stream2, event1);

kernel_B<<< ..., stream1 >>>(...);
kernel_C<<< ..., stream2 >>>(...);

// Join stream2 back to origin stream (stream1)
cudaEventRecord(event2, stream2);
cudaStreamWaitEvent(stream1, event2);

kernel_D<<< ..., stream1 >>>(...);

// End capture in the origin stream
cudaStreamEndCapture(stream1, &graph);

// stream1 and stream2 no longer in capture mode


Graph returned by the above code is shown in Figure 14.

Note
When a stream is taken out of capture mode, the next non-captured item in the stream (if any) will still have a dependency on the most recent prior non-captured item, despite intermediate items having been removed.




3.2.8.7.3.2. Prohibited and Unhandled Operationsï

It is invalid to synchronize or query the execution status of a stream which is being captured or a captured event, because they do not represent items scheduled for execution. It is also invalid to query the execution status of or synchronize a broader handle which encompasses an active stream capture, such as a device or context handle when any associated stream is in capture mode.
When any stream in the same context is being captured, and it was not created with cudaStreamNonBlocking, any attempted use of the legacy stream is invalid. This is because the legacy stream handle at all times encompasses these other streams; enqueueing to the legacy stream would create a dependency on the streams being captured, and querying it or synchronizing it would query or synchronize the streams being captured.
It is therefore also invalid to call synchronous APIs in this case. Synchronous APIs, such as cudaMemcpy(), enqueue work to the legacy stream and synchronize it before returning.

Note
As a general rule, when a dependency relation would connect something that is captured with something that was not captured and instead enqueued for execution, CUDA prefers to return an error rather than ignore the dependency. An exception is made for placing a stream into or out of capture mode; this severs a dependency relation between items added to the stream immediately before and after the mode transition.

It is invalid to merge two separate capture graphs by waiting on a captured event from a stream which is being captured and is associated with a different capture graph than the event. It is invalid to wait on a non-captured event from a stream which is being captured without specifying the cudaEventWaitExternal flag.
A small number of APIs that enqueue asynchronous operations into streams are not currently supported in graphs and will return an error if called with a stream which is being captured, such as cudaStreamAttachMemAsync().



3.2.8.7.3.3. Invalidationï

When an invalid operation is attempted during stream capture, any associated capture graphs are invalidated. When a capture graph is invalidated, further use of any streams which are being captured or captured events associated with the graph is invalid and will return an error, until stream capture is ended with cudaStreamEndCapture(). This call will take the associated streams out of capture mode, but will also return an error value and a NULL graph.




3.2.8.7.4. CUDA User Objectsï

CUDA User Objects can be used to help manage the lifetime of resources used by asynchronous work in CUDA. In particular, this feature is useful for CUDA Graphs and stream capture.
Various resource management schemes are not compatible with CUDA graphs. Consider for example an event-based pool or a synchronous-create, asynchronous-destroy scheme.

// Library API with pool allocation
void libraryWork(cudaStream_t stream) {
    auto &resource = pool.claimTemporaryResource();
    resource.waitOnReadyEventInStream(stream);
    launchWork(stream, resource);
    resource.recordReadyEvent(stream);
}



// Library API with asynchronous resource deletion
void libraryWork(cudaStream_t stream) {
    Resource *resource = new Resource(...);
    launchWork(stream, resource);
    cudaStreamAddCallback(
        stream,
        [](cudaStream_t, cudaError_t, void *resource) {
            delete static_cast<Resource *>(resource);
        },
        resource,
        0);
    // Error handling considerations not shown
}


These schemes are difficult with CUDA graphs because of the non-fixed pointer or handle for the resource which requires indirection or graph update, and the synchronous CPU code needed each time the work is submitted. They also do not work with stream capture if these considerations are hidden from the caller of the library, and because of use of disallowed APIs during capture. Various solutions exist such as exposing the resource to the caller. CUDA user objects present another approach.
A CUDA user object associates a user-specified destructor callback with an internal refcount, similar to C++ shared_ptr. References may be owned by user code on the CPU and by CUDA graphs. Note that for user-owned references, unlike C++ smart pointers, there is no object representing the reference; users must track user-owned references manually. A typical use case would be to immediately move the sole user-owned reference to a CUDA graph after the user object is created.
When a reference is associated to a CUDA graph, CUDA will manage the graph operations automatically. A cloned cudaGraph_t retains a copy of every reference owned by the source cudaGraph_t, with the same multiplicity. An instantiated cudaGraphExec_t retains a copy of every reference in the source cudaGraph_t. When a cudaGraphExec_t is destroyed without being synchronized, the references are retained until the execution is completed.
Here is an example use.

cudaGraph_t graph;  // Preexisting graph

Object *object = new Object;  // C++ object with possibly nontrivial destructor
cudaUserObject_t cuObject;
cudaUserObjectCreate(
    &cuObject,
    object,  // Here we use a CUDA-provided template wrapper for this API,
             // which supplies a callback to delete the C++ object pointer
    1,  // Initial refcount
    cudaUserObjectNoDestructorSync  // Acknowledge that the callback cannot be
                                    // waited on via CUDA
);
cudaGraphRetainUserObject(
    graph,
    cuObject,
    1,  // Number of references
    cudaGraphUserObjectMove  // Transfer a reference owned by the caller (do
                             // not modify the total reference count)
);
// No more references owned by this thread; no need to call release API
cudaGraphExec_t graphExec;
cudaGraphInstantiate(&graphExec, graph, nullptr, nullptr, 0);  // Will retain a
                                                               // new reference
cudaGraphDestroy(graph);  // graphExec still owns a reference
cudaGraphLaunch(graphExec, 0);  // Async launch has access to the user objects
cudaGraphExecDestroy(graphExec);  // Launch is not synchronized; the release
                                  // will be deferred if needed
cudaStreamSynchronize(0);  // After the launch is synchronized, the remaining
                           // reference is released and the destructor will
                           // execute. Note this happens asynchronously.
// If the destructor callback had signaled a synchronization object, it would
// be safe to wait on it at this point.


References owned by graphs in child graph nodes are associated to the child graphs, not the parents. If a child graph is updated or deleted, the references change accordingly. If an executable graph or child graph is updated with cudaGraphExecUpdate or cudaGraphExecChildGraphNodeSetParams, the references in the new source graph are cloned and replace the references in the target graph. In either case, if previous launches are not synchronized, any references which would be released are held until the launches have finished executing.
There is not currently a mechanism to wait on user object destructors via a CUDA API. Users may signal a synchronization object manually from the destructor code. In addition, it is not legal to call CUDA APIs from the destructor, similar to the restriction on cudaLaunchHostFunc. This is to avoid blocking a CUDA internal shared thread and preventing forward progress. It is legal to signal another thread to perform an API call, if the dependency is one way and the thread doing the call cannot block forward progress of CUDA work.
User objects are created with cudaUserObjectCreate, which is a good starting point to browse related APIs.



3.2.8.7.5. Updating Instantiated Graphsï

Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution. In situations where the workflow is not changing, the overhead of definition and instantiation can be amortized over many executions, and graphs provide a clear advantage over streams.
A graph is a snapshot of a workflow, including kernels, parameters, and dependencies, in order to replay it as rapidly and efficiently as possible. In situations where the workflow changes the graph becomes out of date and must be modified. Major changes to graph structure such as topology or types of nodes will require re-instantiation of the source graph because various topology-related optimization techniques must be re-applied.
The cost of repeated instantiation can reduce the overall performance benefit from graph execution, but it is common for only node parameters, such as kernel parameters and cudaMemcpy addresses, to change while graph topology remains the same. For this case, CUDA provides a lightweight mechanism known as âGraph Update,â which allows certain node parameters to be modified in-place without having to rebuild the entire graph. This is much more efficient than re-instantiation.
Updates will take effect the next time the graph is launched, so they will not impact previous graph launches, even if they are running at the time of the update. A graph may be updated and relaunched repeatedly, so multiple updates/launches can be queued on a stream.
CUDA provides two mechanisms for updating instantiated graph parameters, whole graph update and individual node update. Whole graph update allows the user to supply a topologically identical cudaGraph_t object whose nodes contain updated parameters. Individual node update allows the user to explicitly update the parameters of individual nodes. Using an updated cudaGraph_t is more convenient when a large number of nodes are being updated, or when the graph topology is unknown to the caller (i.e., The graph resulted from stream capture of a library call). Using individual node update is preferred when the number of changes is small and the user has the handles to the nodes requiring updates. Individual node update skips the topology checks and comparisons for unchanged nodes, so it can be more efficient in many cases.
CUDA also provides a mechanism for enabling and disabling individual nodes without affecting their current parameters.
The following sections explain each approach in more detail.


3.2.8.7.5.1. Graph Update Limitationsï

Kernel nodes:

The owning context of the function cannot change.
A node whose function originally did not use CUDA dynamic parallelism cannot be updated to a function which uses CUDA dynamic parallelism.

cudaMemset and cudaMemcpy nodes:

The CUDA device(s) to which the operand(s) was allocated/mapped cannot change.
The source/destination memory must be allocated from the same context as the original source/destination memory.
Only 1D cudaMemset/cudaMemcpy nodes can be changed.

Additional memcpy node restrictions:

Changing either the source or destination memory type (i.e., cudaPitchedPtr, cudaArray_t, etc.), or the type of transfer (i.e., cudaMemcpyKind) is not supported.

External semaphore wait nodes and record nodes:

Changing the number of semaphores is not supported.

Conditional nodes:

The order of handle creation and assignment must match between the graphs.
Changing node parameters is not supported (i.e. number of graphs in the conditional, node context, etc).
Changing parameters of nodes within the conditional body graph is subject to the rules above.

There are no restrictions on updates to host nodes, event record nodes, or event wait nodes.



3.2.8.7.5.2. Whole Graph Updateï

cudaGraphExecUpdate() allows an instantiated graph (the âoriginal graphâ) to be updated with the parameters from a topologically identical graph (the âupdatingâ graph). The topology of the updating graph must be identical to the original graph used to instantiate the cudaGraphExec_t. In addition, the order in which the dependencies are specified must match. Finally, CUDA needs to consistently order the sink nodes (nodes with no dependencies). CUDA relies on the order of specific api calls to achieve consistent sink node ordering.
More explicitly, following the following rules will cause cudaGraphExecUpdate() to pair the nodes in the original graph and the updating graph deterministically:

For any capturing stream, the API calls operating on that stream must be made in the same order, including event wait and other api calls not directly corresponding to node creation.
The API calls which directly manipulate a given graph nodeâs incoming edges (including captured stream APIs, node add APIs, and edge addition / removal APIs) must be made in the same order. Moreover, when dependencies are specified in arrays to these APIs, the order in which the dependencies are specified inside those arrays must match.

Sink nodes must be consistently ordered. Sink nodes are nodes without dependent nodes / outgoing edges in the final graph at the time of the cudaGraphExecUpdate() invocation. The following operations affect sink node ordering (if present) and must (as a combined set) be made in the same order:

Node add APIs resulting in a sink node.
Edge removal resulting in a node becoming a sink node.
cudaStreamUpdateCaptureDependencies(), if it removes a sink node from a capturing streamâs dependency set.
cudaStreamEndCapture().



The following example shows how the API could be used to update an instantiated graph:

cudaGraphExec_t graphExec = NULL;

for (int i = 0; i < 10; i++) {
    cudaGraph_t graph;
    cudaGraphExecUpdateResult updateResult;
    cudaGraphNode_t errorNode;

    // In this example we use stream capture to create the graph.
    // You can also use the Graph API to produce a graph.
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);

    // Call a user-defined, stream based workload, for example
    do_cuda_work(stream);

    cudaStreamEndCapture(stream, &graph);

    // If we've already instantiated the graph, try to update it directly
    // and avoid the instantiation overhead
    if (graphExec != NULL) {
        // If the graph fails to update, errorNode will be set to the
        // node causing the failure and updateResult will be set to a
        // reason code.
        cudaGraphExecUpdate(graphExec, graph, &errorNode, &updateResult);
    }

    // Instantiate during the first iteration or whenever the update
    // fails for any reason
    if (graphExec == NULL || updateResult != cudaGraphExecUpdateSuccess) {

        // If a previous update failed, destroy the cudaGraphExec_t
        // before re-instantiating it
        if (graphExec != NULL) {
            cudaGraphExecDestroy(graphExec);
        }
        // Instantiate graphExec from graph. The error node and
        // error message parameters are unused here.
        cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);
    }

    cudaGraphDestroy(graph);
    cudaGraphLaunch(graphExec, stream);
    cudaStreamSynchronize(stream);
}


A typical workflow is to create the initial cudaGraph_t using either the stream capture or graph API. The cudaGraph_t is then instantiated and launched as normal. After the initial launch, a new cudaGraph_t is created using the same method as the initial graph and cudaGraphExecUpdate() is called. If the graph update is successful, indicated by the updateResult parameter in the above example, the updated cudaGraphExec_t is launched. If the update fails for any reason, the cudaGraphExecDestroy() and cudaGraphInstantiate() are called to destroy the original cudaGraphExec_t and instantiate a new one.
It is also possible to update the cudaGraph_t nodes directly (i.e., Using cudaGraphKernelNodeSetParams()) and subsequently update the cudaGraphExec_t, however it is more efficient to use the explicit node update APIs covered in the next section.
Conditional handle flags and default values are updated as part of the graph update.
Please see the Graph API for more information on usage and current limitations.



3.2.8.7.5.3. Individual node updateï

Instantiated graph node parameters can be updated directly. This eliminates the overhead of instantiation as well as the overhead of creating a new cudaGraph_t. If the number of nodes requiring update is small relative to the total number of nodes in the graph, it is better to update the nodes individually. The following methods are available for updating cudaGraphExec_t nodes:

cudaGraphExecKernelNodeSetParams()
cudaGraphExecMemcpyNodeSetParams()
cudaGraphExecMemsetNodeSetParams()
cudaGraphExecHostNodeSetParams()
cudaGraphExecChildGraphNodeSetParams()
cudaGraphExecEventRecordNodeSetEvent()
cudaGraphExecEventWaitNodeSetEvent()
cudaGraphExecExternalSemaphoresSignalNodeSetParams()
cudaGraphExecExternalSemaphoresWaitNodeSetParams()

Please see the Graph API for more information on usage and current limitations.



3.2.8.7.5.4. Individual node enableï

Kernel, memset and memcpy nodes in an instantiated graph can be enabled or disabled using the cudaGraphNodeSetEnabled() API. This allows the creation of a graph which contains a superset of the desired functionality which can be customized for each launch. The enable state of a node can be queried using the cudaGraphNodeGetEnabled() API.
A disabled node is functionally equivalent to empty node until it is reenabled. Node parameters are not affected by enabling/disabling a node. Enable state is unaffected by individual node update or whole graph update with cudaGraphExecUpdate(). Parameter updates while the node is disabled will take effect when the node is reenabled.
The following methods are available for enabling/disabling cudaGraphExec_t nodes, as well as querying their status :

cudaGraphNodeSetEnabled()
cudaGraphNodeGetEnabled()

Please see the Graph API for more information on usage and current limitations.




3.2.8.7.6. Using Graph APIsï

cudaGraph_t objects are not thread-safe. It is the responsibility of the user to ensure that multiple threads do not concurrently access the same cudaGraph_t.
A cudaGraphExec_t cannot run concurrently with itself. A launch of a cudaGraphExec_t will be ordered after previous launches of the same executable graph.
Graph execution is done in streams for ordering with other asynchronous work. However, the stream is for ordering only; it does not constrain the internal parallelism of the graph, nor does it affect where graph nodes execute.
See Graph API.



3.2.8.7.7. Device Graph Launchï

There are many workflows which need to make data-dependent decisions during runtime and execute different operations depending on those decisions. Rather than offloading this decision-making process to the host, which may require a round-trip from the device, users may prefer to perform it on the device. To that end, CUDA provides a mechanism to launch graphs from the device.
Device graph launch provides a convenient way to perform dynamic control flow from the device, be it something as simple as a loop or as complex as a device-side work scheduler. This functionality is only available on systems which support unified addressing.
Graphs which can be launched from the device will henceforth be referred to as device graphs, and graphs which cannot be launched from the device will be referred to as host graphs.
Device graphs can be launched from both the host and device, whereas host graphs can only be launched from the host. Unlike host launches, launching a device graph from the device while a previous launch of the graph is running will result in an error, returning cudaErrorInvalidValue; therefore, a device graph cannot be launched twice from the device at the same time. Launching a device graph from the host and device simultaneously will result in undefined behavior.


3.2.8.7.7.1. Device Graph Creationï

In order for a graph to be launched from the device, it must be instantiated explicitly for device launch. This is achieved by passing the cudaGraphInstantiateFlagDeviceLaunch flag to the cudaGraphInstantiate() call. As is the case for host graphs, device graph structure is fixed at time of instantiation and cannot be updated without re-instantiation, and instantiation can only be performed on the host. In order for a graph to be able to be instantiated for device launch, it must adhere to various requirements.

3.2.8.7.7.1.1. Device Graph Requirementsï
General requirements:

The graphâs nodes must all reside on a single device.
The graph can only contain kernel nodes, memcpy nodes, memset nodes, and child graph nodes.

Kernel nodes:

Use of CUDA Dynamic Parallelism by kernels in the graph is not permitted.
Cooperative launches are permitted so long as MPS is not in use.

Memcpy nodes:

Only copies involving device memory and/or pinned device-mapped host memory are permitted.
Copies involving CUDA arrays are not permitted.
Both operands must be accessible from the current device at time of instantiation. Note that the copy operation will be performed from the device on which the graph resides, even if it is targeting memory on another device.



3.2.8.7.7.1.2. Device Graph Uploadï
In order to launch a graph on the device, it must first be uploaded to the device to populate the necessary device resources. This can be achieved in one of two ways.
Firstly, the graph can be uploaded explicitly, either via cudaGraphUpload() or by requesting an upload as part of instantiation via cudaGraphInstantiateWithParams().
Alternatively, the graph can first be launched from the host, which will perform this upload step implicitly as part of the launch.
Examples of all three methods can be seen below:

// Explicit upload after instantiation
cudaGraphInstantiate(&deviceGraphExec1, deviceGraph1, cudaGraphInstantiateFlagDeviceLaunch);
cudaGraphUpload(deviceGraphExec1, stream);

// Explicit upload as part of instantiation
cudaGraphInstantiateParams instantiateParams = {0};
instantiateParams.flags = cudaGraphInstantiateFlagDeviceLaunch | cudaGraphInstantiateFlagUpload;
instantiateParams.uploadStream = stream;
cudaGraphInstantiateWithParams(&deviceGraphExec2, deviceGraph2, &instantiateParams);

// Implicit upload via host launch
cudaGraphInstantiate(&deviceGraphExec3, deviceGraph3, cudaGraphInstantiateFlagDeviceLaunch);
cudaGraphLaunch(deviceGraphExec3, stream);




3.2.8.7.7.1.3. Device Graph Updateï
Device graphs can only be updated from the host, and must be re-uploaded to the device upon executable graph update in order for the changes to take effect. This can be achieved using the same methods outlined in the previous section. Unlike host graphs, launching a device graph from the device while an update is being applied will result in undefined behavior.




3.2.8.7.7.2. Device Launchï

Device graphs can be launched from both the host and the device via cudaGraphLaunch(), which has the same signature on the device as on the host. Device graphs are launched via the same handle on the host and the device. Device graphs must be launched from another graph when launched from the device.
Device-side graph launch is per-thread and multiple launches may occur from different threads at the same time, so the user will need to select a single thread from which to launch a given graph.

3.2.8.7.7.2.1. Device Launch Modesï
Unlike host launch, device graphs cannot be launched into regular CUDA streams, and can only be launched into distinct named streams, which each denote a specific launch mode:


Table 2 Device-only Graph Launch Streamsï







Stream
Launch Mode




cudaStreamGraphFireAndForget
Fire and forget launch


cudaStreamGraphTailLaunch
Tail launch


cudaStreamGraphFireAndForgetAsSibling
Sibling launch




3.2.8.7.7.2.1.1. Fire and Forget Launchï
As the name suggests, a fire and forget launch is submitted to the GPU immediately, and it runs independently of the launching graph. In a fire-and-forget scenario, the launching graph is the parent, and the launched graph is the child.



Figure 15 Fire and forget launchï


The above diagram can be generated by the sample code below:

__global__ void launchFireAndForgetGraph(cudaGraphExec_t graph) {
    cudaGraphLaunch(graph, cudaStreamGraphFireAndForget);
}

void graphSetup() {
    cudaGraphExec_t gExec1, gExec2;
    cudaGraph_t g1, g2;

    // Create, instantiate, and upload the device graph.
    create_graph(&g2);
    cudaGraphInstantiate(&gExec2, g2, cudaGraphInstantiateFlagDeviceLaunch);
    cudaGraphUpload(gExec2, stream);

    // Create and instantiate the launching graph.
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);
    launchFireAndForgetGraph<<<1, 1, 0, stream>>>(gExec2);
    cudaStreamEndCapture(stream, &g1);
    cudaGraphInstantiate(&gExec1, g1);

    // Launch the host graph, which will in turn launch the device graph.
    cudaGraphLaunch(gExec1, stream);
}


A graph can have up to 120 total fire-and-forget graphs during the course of its execution. This total resets between launches of the same parent graph.


3.2.8.7.7.2.1.2. Graph Execution Environmentsï
In order to fully understand the device-side synchronization model, it is first necessary to understand the concept of an execution environment.
When a graph is launched from the device, it is launched into its own execution environment. The execution environment of a given graph encapsulates all work in the graph as well as all generated fire and forget work. The graph can be considered complete when it has completed execution and when all generated child work is complete.
The below diagram shows the environment encapsulation that would be generated by the fire-and-forget sample code in the previous section.



Figure 16 Fire and forget launch, with execution environmentsï


These environments are also hierarchical, so a graph environment can include multiple levels of child-environments from fire and forget launches.



Figure 17 Nested fire and forget environmentsï


When a graph is launched from the host, there exists a stream environment that parents the execution environment of the launched graph. The stream environment encapsulates all work generated as part of the overall launch. The stream launch is complete (i.e. downstream dependent work may now run) when the overall stream environment is marked as complete.



Figure 18 The stream environment, visualizedï




3.2.8.7.7.2.1.3. Tail Launchï
Unlike on the host, it is not possible to synchronize with device graphs from the GPU via traditional methods such as cudaDeviceSynchronize() or cudaStreamSynchronize(). Rather, in order to enable serial work dependencies, a different launch mode - tail launch - is offered, to provide similar functionality.
A tail launch executes when a graphâs environment is considered complete - ie, when the graph and all its children are complete. When a graph completes, the environment of the next graph in the tail launch list will replace the completed environment as a child of the parent environment. Like fire-and-forget launches, a graph can have multiple graphs enqueued for tail launch.



Figure 19 A simple tail launchï


The above execution flow can be generated by the code below:

__global__ void launchTailGraph(cudaGraphExec_t graph) {
    cudaGraphLaunch(graph, cudaStreamGraphTailLaunch);
}

void graphSetup() {
    cudaGraphExec_t gExec1, gExec2;
    cudaGraph_t g1, g2;

    // Create, instantiate, and upload the device graph.
    create_graph(&g2);
    cudaGraphInstantiate(&gExec2, g2, cudaGraphInstantiateFlagDeviceLaunch);
    cudaGraphUpload(gExec2, stream);

    // Create and instantiate the launching graph.
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);
    launchTailGraph<<<1, 1, 0, stream>>>(gExec2);
    cudaStreamEndCapture(stream, &g1);
    cudaGraphInstantiate(&gExec1, g1);

    // Launch the host graph, which will in turn launch the device graph.
    cudaGraphLaunch(gExec1, stream);
}


Tail launches enqueued by a given graph will execute one at a time, in order of when they were enqueued. So the first enqueued graph will run first, and then the second, and so on.



Figure 20 Tail launch orderingï


Tail launches enqueued by a tail graph will execute before tail launches enqueued by previous graphs in the tail launch list. These new tail launches will execute in the order they are enqueued.



Figure 21 Tail launch ordering when enqueued from multiple graphsï


A graph can have up to 255 pending tail launches.

3.2.8.7.7.2.1.3.1. Tail Self-launchï
It is possible for a device graph to enqueue itself for a tail launch, although a given graph can only have one self-launch enqueued at a time. In order to query the currently running device graph so that it can be relaunched, a new device-side function is added:

cudaGraphExec_t cudaGetCurrentGraphExec();


This function returns the handle of the currently running graph if it is a device graph. If the currently executing kernel is not a node within a device graph, this function will return NULL.
Below is sample code showing usage of this function for a relaunch loop:

__device__ int relaunchCount = 0;

__global__ void relaunchSelf() {
    int relaunchMax = 100;

    if (threadIdx.x == 0) {
        if (relaunchCount < relaunchMax) {
            cudaGraphLaunch(cudaGetCurrentGraphExec(), cudaStreamGraphTailLaunch);
        }

        relaunchCount++;
    }
}





3.2.8.7.7.2.1.4. Sibling Launchï
Sibling launch is a variation of fire-and-forget launch in which the graph is launched not as a child of the launching graphâs execution environment, but rather as a child of the launching graphâs parent environment. Sibling launch is equivalent to a fire-and-forget launch from the launching graphâs parent environment.



Figure 22 A simple sibling launchï


The above diagram can be generated by the sample code below:

__global__ void launchSiblingGraph(cudaGraphExec_t graph) {
    cudaGraphLaunch(graph, cudaStreamGraphFireAndForgetAsSibling);
}

void graphSetup() {
    cudaGraphExec_t gExec1, gExec2;
    cudaGraph_t g1, g2;

    // Create, instantiate, and upload the device graph.
    create_graph(&g2);
    cudaGraphInstantiate(&gExec2, g2, cudaGraphInstantiateFlagDeviceLaunch);
    cudaGraphUpload(gExec2, stream);

    // Create and instantiate the launching graph.
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);
    launchSiblingGraph<<<1, 1, 0, stream>>>(gExec2);
    cudaStreamEndCapture(stream, &g1);
    cudaGraphInstantiate(&gExec1, g1);

    // Launch the host graph, which will in turn launch the device graph.
    cudaGraphLaunch(gExec1, stream);
}


Since sibling launches are not launched into the launching graphâs execution environment, they will not gate tail launches enqueued by the launching graph.






3.2.8.7.8. Conditional Graph Nodesï

Conditional nodes allow conditional execution and looping of a graph contained within the conditional node. This allows dynamic and iterative workflows to be represented completely within a graph and frees up the host CPU to perform other work in parallel.
Evaluation of the condition value is performed on the device when the dependencies of the conditional node have been met. Conditional nodes can be one of the following types:

Conditional IF nodes execute their body graph once if the condition value is non-zero when the node is executed.
Conditional WHILE nodes execute their body graph if the condition value is non-zero when the node is executed and will continue to execute their body graph until the condition value is zero.

A condition value is accessed by a conditional handle , which must be created before the node. The condition value can be set by device code using cudaGraphSetConditional(). A default value, applied on each graph launch, can also be specified when the handle is created.
When the conditional node is created, an empty graph is created and the handle is returned to the user so that the graph can be populated.  This conditional body graph can be populated using either the graph APIs or cudaStreamBeginCaptureToGraph() .
Conditional nodes can be nested.


3.2.8.7.8.1. Conditional Handlesï

A condition value is represented by cudaGraphConditionalHandle and is created by cudaGraphConditionalHandleCreate().
The handle must be associated with a single conditional node. Handles cannot be destroyed.
If cudaGraphCondAssignDefault is specified when the handle is created, the condition value will be initialized to the specified default before every graph launch. If this flag is not provided, it is up to the user to initialize the condition value in a kernel upstream of the conditional node which tests it. If the condition value is not initialized by one of these methods, its value is undefined.
The default value and flags associated with a handle will be updated during whole graph update .



3.2.8.7.8.2. Condtional Node Body Graph Requirementsï

General requirements:

The graphâs nodes must all reside on a single device.
The graph can only contain kernel nodes, empty nodes, memcpy nodes, memset nodes, child graph nodes, and conditional nodes.

Kernel nodes:

Use of CUDA Dynamic Parallelism by kernels in the graph is not permitted.
Cooperative launches are permitted so long as MPS is not in use.

Memcpy/Memset nodes:

Only copies/memsets involving device memory and/or pinned device-mapped host memory are permitted.
Copies/memsets involving CUDA arrays are not permitted.
Both operands must be accessible from the current device at time of instantiation. Note that the copy operation will be performed from the device on which the graph resides, even if it is targeting memory on another device.




3.2.8.7.8.3. Conditional IF Nodesï

The body graph of an IF node will be executed once if the condition is non-zero when the node is executed.  The following diagram depicts a 3 node graph where the middle node, B, is a conditional node:



Figure 23 Conditional IF Nodeï


The following code illustrates the creation of a graph containing an IF conditional node. The default value of the condition is set using an upstream kernel. The body of the conditional is populated using the graph API .

__global__ void setHandle(cudaGraphConditionalHandle handle)
{
    ...
    cudaGraphSetConditional(handle, value);
    ...
}

void graphSetup() {
    cudaGraph_t graph;
    cudaGraphExec_t graphExec;
    cudaGraphNode_t node;
    void *kernelArgs[1];
    int value = 1;

    cudaGraphCreate(&graph, 0);

    cudaGraphConditionalHandle handle;
    cudaGraphConditionalHandleCreate(&handle, graph);

    // Use a kernel upstream of the conditional to set the handle value
    cudaGraphNodeParams params = { cudaGraphNodeTypeKernel };
    params.kernel.func = (void *)setHandle;
    params.kernel.gridDim.x = params.kernel.gridDim.y = params.kernel.gridDim.z = 1;
    params.kernel.blockDim.x = params.kernel.blockDim.y = params.kernel.blockDim.z = 1;
    params.kernel.kernelParams = kernelArgs;
    kernelArgs[0] = &handle;
    cudaGraphAddNode(&node, graph, NULL, 0, &params);

    cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional };
    cParams.conditional.handle = handle;
    cParams.conditional.type   = cudaGraphCondTypeIf;
    cParams.conditional.size   = 1;
    cudaGraphAddNode(&node, graph, &node, 1, &cParams);

    cudaGraph_t bodyGraph = cParams.conditional.phGraph_out[0];

    // Populate the body of the conditional node
    ...
    cudaGraphAddNode(&node, bodyGraph, NULL, 0, &params);

    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);
    cudaGraphLaunch(graphExec, 0);
    cudaDeviceSynchronize();

    cudaGraphExecDestroy(graphExec);
    cudaGraphDestroy(graph);
}





3.2.8.7.8.4. Conditional WHILE Nodesï

The body graph of a WHILE node will be executed until the condition is non-zero. The condition will be evaluated when the node is executed and after completion of the body graph. The following diagram depicts a 3 node graph where the middle node, B, is a conditional node:



Figure 24 Conditional WHILE Nodeï


The following code illustrates the creation of a graph containing a WHILE conditional node. The handle is created using cudaGraphCondAssignDefault to avoid the need for an upstream kernel. The body of the conditional is populated using the graph API .

__global__ void loopKernel(cudaGraphConditionalHandle handle)
{
    static int count = 10;
    cudaGraphSetConditional(handle, --count ? 1 : 0);
}

void graphSetup() {
    cudaGraph_t graph;
    cudaGraphExec_t graphExec;
    cudaGraphNode_t node;
    void *kernelArgs[1];

    cuGraphCreate(&graph, 0);

    cudaGraphConditionalHandle handle;
    cudaGraphConditionalHandleCreate(&handle, graph, 1, cudaGraphCondAssignDefault);

    cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional };
    cParams.conditional.handle = handle;
    cParams.conditional.type   = cudaGraphCondTypeWhile;
    cParams.conditional.size   = 1;
    cudaGraphAddNode(&node, graph, NULL, 0, &cParams);

    cudaGraph_t bodyGraph = cParams.conditional.phGraph_out[0];

    cudaGraphNodeParams params = { cudaGraphNodeTypeKernel };
    params.kernel.func = (void *)loopKernel;
    params.kernel.gridDim.x = params.kernel.gridDim.y = params.kernel.gridDim.z = 1;
    params.kernel.blockDim.x = params.kernel.blockDim.y = params.kernel.blockDim.z = 1;
    params.kernel.kernelParams = kernelArgs;
    kernelArgs[0] = &handle;
    cudaGraphAddNode(&node, bodyGraph, NULL, 0, &params);

    cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);
    cudaGraphLaunch(graphExec, 0);
    cudaDeviceSynchronize();

    cudaGraphExecDestroy(graphExec);
    cudaGraphDestroy(graph);
}







3.2.8.8. Eventsï

The runtime also provides a way to closely monitor the deviceâs progress, as well as perform accurate timing, by letting the application asynchronously record events at any point in the program, and query when these events are completed. An event has completed when all tasks - or optionally, all commands in a given stream - preceding the event have completed. Events in stream zero are completed after all preceding tasks and commands in all streams are completed.


3.2.8.8.1. Creation and Destruction of Eventsï

The following code sample creates two events:

cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);


They are destroyed this way:

cudaEventDestroy(start);
cudaEventDestroy(stop);





3.2.8.8.2. Elapsed Timeï

The events created in Creation and Destruction can be used to time the code sample of Creation and Destruction the following way:

cudaEventRecord(start, 0);
for (int i = 0; i < 2; ++i) {
    cudaMemcpyAsync(inputDev + i * size, inputHost + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel<<<100, 512, 0, stream[i]>>>
               (outputDev + i * size, inputDev + i * size, size);
    cudaMemcpyAsync(outputHost + i * size, outputDev + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);
}
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
float elapsedTime;
cudaEventElapsedTime(&elapsedTime, start, stop);






3.2.8.9. Synchronous Callsï

When a synchronous function is called, control is not returned to the host thread before the device has completed the requested task. Whether the host thread will then yield, block, or spin can be specified by calling cudaSetDeviceFlags()with some specific flags (see reference manual for details) before any other CUDA call is performed by the host thread.




3.2.9. Multi-Device Systemï



3.2.9.1. Device Enumerationï

A host system can have multiple devices. The following code sample shows how to enumerate these devices, query their properties, and determine the number of CUDA-enabled devices.

int deviceCount;
cudaGetDeviceCount(&deviceCount);
int device;
for (device = 0; device < deviceCount; ++device) {
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, device);
    printf("Device %d has compute capability %d.%d.\n",
           device, deviceProp.major, deviceProp.minor);
}





3.2.9.2. Device Selectionï

A host thread can set the device it operates on at any time by calling cudaSetDevice(). Device memory allocations and kernel launches are made on the currently set device; streams and events are created in association with the currently set device. If no call to cudaSetDevice() is made, the current device is device 0.
The following code sample illustrates how setting the current device affects memory allocation and kernel execution.

size_t size = 1024 * sizeof(float);
cudaSetDevice(0);            // Set device 0 as current
float* p0;
cudaMalloc(&p0, size);       // Allocate memory on device 0
MyKernel<<<1000, 128>>>(p0); // Launch kernel on device 0
cudaSetDevice(1);            // Set device 1 as current
float* p1;
cudaMalloc(&p1, size);       // Allocate memory on device 1
MyKernel<<<1000, 128>>>(p1); // Launch kernel on device 1





3.2.9.3. Stream and Event Behaviorï

A kernel launch will fail if it is issued to a stream that is not associated to the current device as illustrated in the following code sample.

cudaSetDevice(0);               // Set device 0 as current
cudaStream_t s0;
cudaStreamCreate(&s0);          // Create stream s0 on device 0
MyKernel<<<100, 64, 0, s0>>>(); // Launch kernel on device 0 in s0
cudaSetDevice(1);               // Set device 1 as current
cudaStream_t s1;
cudaStreamCreate(&s1);          // Create stream s1 on device 1
MyKernel<<<100, 64, 0, s1>>>(); // Launch kernel on device 1 in s1

// This kernel launch will fail:
MyKernel<<<100, 64, 0, s0>>>(); // Launch kernel on device 1 in s0


A memory copy will succeed even if it is issued to a stream that is not associated to the current device.
cudaEventRecord() will fail if the input event and input stream are associated to different devices.
cudaEventElapsedTime() will fail if the two input events are associated to different devices.
cudaEventSynchronize() and cudaEventQuery() will succeed even if the input event is associated to a device that is different from the current device.
cudaStreamWaitEvent() will succeed even if the input stream and input event are associated to different devices. cudaStreamWaitEvent() can therefore be used to synchronize multiple devices with each other.
Each device has its own default stream (see Default Stream), so commands issued to the default stream of a device may execute out of order or concurrently with respect to commands issued to the default stream of any other device.



3.2.9.4. Peer-to-Peer Memory Accessï

Depending on the system properties, specifically the PCIe and/or NVLINK topology, devices are able to address each otherâs memory (i.e., a kernel executing on one device can dereference a pointer to the memory of the other device). This peer-to-peer memory access feature is supported between two devices if cudaDeviceCanAccessPeer() returns true for these two devices.
Peer-to-peer memory access is only supported in 64-bit applications and must be enabled between two devices by calling cudaDeviceEnablePeerAccess() as illustrated in the following code sample. On non-NVSwitch enabled systems, each device can support a system-wide maximum of eight peer connections.
A unified address space is used for both devices (see Unified Virtual Address Space), so the same pointer can be used to address memory from both devices as shown in the code sample below.

cudaSetDevice(0);                   // Set device 0 as current
float* p0;
size_t size = 1024 * sizeof(float);
cudaMalloc(&p0, size);              // Allocate memory on device 0
MyKernel<<<1000, 128>>>(p0);        // Launch kernel on device 0
cudaSetDevice(1);                   // Set device 1 as current
cudaDeviceEnablePeerAccess(0, 0);   // Enable peer-to-peer access
                                    // with device 0

// Launch kernel on device 1
// This kernel launch can access memory on device 0 at address p0
MyKernel<<<1000, 128>>>(p0);




3.2.9.4.1. IOMMU on Linuxï

On Linux only, CUDA and the display driver does not support IOMMU-enabled bare-metal PCIe peer to peer memory copy. However, CUDA and the display driver does support IOMMU via VM pass through. As a consequence, users on Linux, when running on a native bare metal system, should disable the IOMMU. The IOMMU should be enabled and the VFIO driver be used as a PCIe pass through for virtual machines.
On Windows the above limitation does not exist.
See also Allocating DMA Buffers on 64-bit Platforms.




3.2.9.5. Peer-to-Peer Memory Copyï

Memory copies can be performed between the memories of two different devices.
When a unified address space is used for both devices (see Unified Virtual Address Space), this is done using the regular memory copy functions mentioned in Device Memory.
Otherwise, this is done using cudaMemcpyPeer(), cudaMemcpyPeerAsync(), cudaMemcpy3DPeer(), or cudaMemcpy3DPeerAsync() as illustrated in the following code sample.

cudaSetDevice(0);                   // Set device 0 as current
float* p0;
size_t size = 1024 * sizeof(float);
cudaMalloc(&p0, size);              // Allocate memory on device 0
cudaSetDevice(1);                   // Set device 1 as current
float* p1;
cudaMalloc(&p1, size);              // Allocate memory on device 1
cudaSetDevice(0);                   // Set device 0 as current
MyKernel<<<1000, 128>>>(p0);        // Launch kernel on device 0
cudaSetDevice(1);                   // Set device 1 as current
cudaMemcpyPeer(p1, 1, p0, 0, size); // Copy p0 to p1
MyKernel<<<1000, 128>>>(p1);        // Launch kernel on device 1


A copy (in the implicit NULL stream) between the memories of two different devices:

does not start until all commands previously issued to either device have completed and
runs to completion before any commands (see Asynchronous Concurrent Execution) issued after the copy to either device can start.

Consistent with the normal behavior of streams, an asynchronous copy between the memories of two devices may overlap with copies or kernels in another stream.
Note that if peer-to-peer access is enabled between two devices via cudaDeviceEnablePeerAccess() as described in Peer-to-Peer Memory Access, peer-to-peer memory copy between these two devices no longer needs to be staged through the host and is therefore faster.




3.2.10. Unified Virtual Address Spaceï

When the application is run as a 64-bit process, a single address space is used for the host and all the devices of compute capability 2.0 and higher. All host memory allocations made via CUDA API calls and all device memory allocations on supported devices are within this virtual address range. As a consequence:

The location of any memory on the host allocated through CUDA, or on any of the devices which use the unified address space, can be determined from the value of the pointer using cudaPointerGetAttributes().
When copying to or from the memory of any device which uses the unified address space, the cudaMemcpyKind parameter of cudaMemcpy*() can be set to cudaMemcpyDefault to determine locations from the pointers. This also works for host pointers not allocated through CUDA, as long as the current device uses unified addressing.
Allocations via cudaHostAlloc() are automatically portable (see Portable Memory) across all the devices for which the unified address space is used, and pointers returned by cudaHostAlloc() can be used directly from within kernels running on these devices (i.e., there is no need to obtain a device pointer via cudaHostGetDevicePointer() as described in Mapped Memory.

Applications may query if the unified address space is used for a particular device by checking that the unifiedAddressing device property (see Device Enumeration) is equal to 1.



3.2.11. Interprocess Communicationï

Any device memory pointer or event handle created by a host thread can be directly referenced by any other thread within the same process. It is not valid outside this process however, and therefore cannot be directly referenced by threads belonging to a different process.
To share device memory pointers and events across processes, an application must use the Inter Process Communication API, which is described in detail in the reference manual. The IPC API is only supported for 64-bit processes on Linux and for devices of compute capability 2.0 and higher. Note that the IPC API is not supported for cudaMallocManaged allocations.
Using this API, an application can get the IPC handle for a given device memory pointer using cudaIpcGetMemHandle(), pass it to another process using standard IPC mechanisms (for example, interprocess shared memory or files), and use cudaIpcOpenMemHandle() to retrieve a device pointer from the IPC handle that is a valid pointer within this other process. Event handles can be shared using similar entry points.
Note that allocations made by cudaMalloc() may be sub-allocated from a larger block of memory for performance reasons. In such case, CUDA IPC APIs will share the entire underlying memory block which may cause other sub-allocations to be shared, which can potentially lead to information disclosure between processes. To prevent this behavior, it is recommended to only share allocations with a 2MiB aligned size.
An example of using the IPC API is where a single primary process generates a batch of input data, making the data available to multiple secondary processes without requiring regeneration or copying.
Applications using CUDA IPC to communicate with each other should be compiled, linked, and run with the same CUDA driver and runtime.

Note
Since CUDA 11.5, only events-sharing IPC APIs are supported on L4T and embedded Linux Tegra devices with compute capability 7.x and higher. The memory-sharing IPC APIs are still not supported on Tegra platforms.




3.2.12. Error Checkingï

All runtime functions return an error code, but for an asynchronous function (see Asynchronous Concurrent Execution), this error code cannot possibly report any of the asynchronous errors that could occur on the device since the function returns before the device has completed the task; the error code only reports errors that occur on the host prior to executing the task, typically related to parameter validation; if an asynchronous error occurs, it will be reported by some subsequent unrelated runtime function call.
The only way to check for asynchronous errors just after some asynchronous function call is therefore to synchronize just after the call by calling cudaDeviceSynchronize() (or by using any other synchronization mechanisms described in Asynchronous Concurrent Execution) and checking the error code returned by cudaDeviceSynchronize().
The runtime maintains an error variable for each host thread that is initialized to cudaSuccess and is overwritten by the error code every time an error occurs (be it a parameter validation error or an asynchronous error). cudaPeekAtLastError() returns this variable. cudaGetLastError() returns this variable and resets it to cudaSuccess.
Kernel launches do not return any error code, so cudaPeekAtLastError() or cudaGetLastError() must be called just after the kernel launch to retrieve any pre-launch errors. To ensure that any error returned by cudaPeekAtLastError() or cudaGetLastError() does not originate from calls prior to the kernel launch, one has to make sure that the runtime error variable is set to cudaSuccess just before the kernel launch, for example, by calling cudaGetLastError() just before the kernel launch. Kernel launches are asynchronous, so to check for asynchronous errors, the application must synchronize in-between the kernel launch and the call to cudaPeekAtLastError() or cudaGetLastError().
Note that cudaErrorNotReady that may be returned by cudaStreamQuery() and cudaEventQuery() is not considered an error and is therefore not reported by cudaPeekAtLastError() or cudaGetLastError().



3.2.13. Call Stackï

On devices of compute capability 2.x and higher, the size of the call stack can be queried usingcudaDeviceGetLimit() and set using cudaDeviceSetLimit().
When the call stack overflows, the kernel call fails with a stack overflow error if the application is run via a CUDA debugger (CUDA-GDB, Nsight) or an unspecified launch error, otherwise.
When the compiler cannot determine the stack size, it issues a warning saying Stack size cannot be statically determined. This is usually the case with recursive functions.
Once this warning is issued, user will need to set stack size manually if default stack size is not sufficient.



3.2.14. Texture and Surface Memoryï

CUDA supports a subset of the texturing hardware that the GPU uses for graphics to access texture and surface memory. Reading data from texture or surface memory instead of global memory can have several performance benefits as described in Device Memory Accesses.


3.2.14.1. Texture Memoryï

Texture memory is read from kernels using the device functions described in Texture Functions. The process of reading a texture calling one of these functions is called a texture fetch. Each texture fetch specifies a parameter called a texture object for the texture object API.
The texture object specifies:

The texture, which is the piece of texture memory that is fetched. Texture objects are created at runtime and the texture is specified when creating the texture object as described in Texture Object API.
Its dimensionality that specifies whether the texture is addressed as a one dimensional array using one texture coordinate, a two-dimensional array using two texture coordinates, or a three-dimensional array using three texture coordinates. Elements of the array are called texels, short for texture elements. The texture width, height, and depth refer to the size of the array in each dimension. Table 21 lists the maximum texture width, height, and depth depending on the compute capability of the device.
The type of a texel, which is restricted to the basic integer and single-precision floating-point types and any of the 1-, 2-, and 4-component vector types defined in Built-in Vector Types that are derived from the basic integer and single-precision floating-point types.
The read mode, which is equal to cudaReadModeNormalizedFloat or cudaReadModeElementType. If it is cudaReadModeNormalizedFloat and the type of the texel is a 16-bit or 8-bit integer type, the value returned by the texture fetch is actually returned as floating-point type and the full range of the integer type is mapped to [0.0, 1.0] for unsigned integer type and [-1.0, 1.0] for signed integer type; for example, an unsigned 8-bit texture element with the value 0xff reads as 1. If it is cudaReadModeElementType, no conversion is performed.
Whether texture coordinates are normalized or not. By default, textures are referenced (by the functions of Texture Functions) using floating-point coordinates in the range [0, N-1] where N is the size of the texture in the dimension corresponding to the coordinate. For example, a texture that is 64x32 in size will be referenced with coordinates in the range [0, 63] and [0, 31] for the x and y dimensions, respectively. Normalized texture coordinates cause the coordinates to be specified in the range [0.0, 1.0-1/N] instead of [0, N-1], so the same 64x32 texture would be addressed by normalized coordinates in the range [0, 1-1/N] in both the x and y dimensions. Normalized texture coordinates are a natural fit to some applicationsâ requirements, if it is preferable for the texture coordinates to be independent of the texture size.
The addressing mode. It is valid to call the device functions of Section B.8 with coordinates that are out of range. The addressing mode defines what happens in that case. The default addressing mode is to clamp the coordinates to the valid range: [0, N) for non-normalized coordinates and [0.0, 1.0) for normalized coordinates. If the border mode is specified instead, texture fetches with out-of-range texture coordinates return zero. For normalized coordinates, the wrap mode and the mirror mode are also available. When using the wrap mode, each coordinate x is converted to frac(x)=x - floor(x) where floor(x) is the largest integer not greater than x. When using the mirror mode, each coordinate x is converted to frac(x) if floor(x) is even and 1-frac(x) if floor(x) is odd. The addressing mode is specified as an array of size three whose first, second, and third elements specify the addressing mode for the first, second, and third texture coordinates, respectively; the addressing mode are cudaAddressModeBorder, cudaAddressModeClamp, cudaAddressModeWrap, and cudaAddressModeMirror; cudaAddressModeWrap and cudaAddressModeMirror are only supported for normalized texture coordinates
The filtering mode which specifies how the value returned when fetching the texture is computed based on the input texture coordinates. Linear texture filtering may be done only for textures that are configured to return floating-point data. It performs low-precision interpolation between neighboring texels. When enabled, the texels surrounding a texture fetch location are read and the return value of the texture fetch is interpolated based on where the texture coordinates fell between the texels. Simple linear interpolation is performed for one-dimensional textures, bilinear interpolation for two-dimensional textures, and trilinear interpolation for three-dimensional textures. Texture Fetching gives more details on texture fetching. The filtering mode is equal to cudaFilterModePoint or cudaFilterModeLinear. If it is cudaFilterModePoint, the returned value is the texel whose texture coordinates are the closest to the input texture coordinates. If it is cudaFilterModeLinear, the returned value is the linear interpolation of the two (for a one-dimensional texture), four (for a two dimensional texture), or eight (for a three dimensional texture) texels whose texture coordinates are the closest to the input texture coordinates. cudaFilterModeLinear is only valid for returned values of floating-point type.

Texture Object API introduces the texture object API.
16-Bit Floating-Point Textures explains how to deal with 16-bit floating-point textures.
Textures can also be layered as described in Layered Textures.
Cubemap Textures and Cubemap Layered Textures describe a special type of texture, the cubemap texture.
Texture Gather describes a special texture fetch, texture gather.


3.2.14.1.1. Texture Object APIï

A texture object is created using cudaCreateTextureObject() from a resource description of type struct cudaResourceDesc, which specifies the texture, and from a texture description defined as such:

struct cudaTextureDesc
{
    enum cudaTextureAddressMode addressMode[3];
    enum cudaTextureFilterMode  filterMode;
    enum cudaTextureReadMode    readMode;
    int                         sRGB;
    int                         normalizedCoords;
    unsigned int                maxAnisotropy;
    enum cudaTextureFilterMode  mipmapFilterMode;
    float                       mipmapLevelBias;
    float                       minMipmapLevelClamp;
    float                       maxMipmapLevelClamp;
};



addressMode specifies the addressing mode;
filterMode specifies the filter mode;
readMode specifies the read mode;
normalizedCoords specifies whether texture coordinates are normalized or not;
See reference manual for sRGB, maxAnisotropy, mipmapFilterMode, mipmapLevelBias, minMipmapLevelClamp, and maxMipmapLevelClamp.

The following code sample applies some simple transformation kernel to a texture.

// Simple transformation kernel
__global__ void transformKernel(float* output,
                                cudaTextureObject_t texObj,
                                int width, int height,
                                float theta)
{
    // Calculate normalized texture coordinates
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;

    float u = x / (float)width;
    float v = y / (float)height;

    // Transform coordinates
    u -= 0.5f;
    v -= 0.5f;
    float tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
    float tv = v * cosf(theta) + u * sinf(theta) + 0.5f;

    // Read from texture and write to global memory
    output[y * width + x] = tex2D<float>(texObj, tu, tv);
}



// Host code
int main()
{
    const int height = 1024;
    const int width = 1024;
    float angle = 0.5;

    // Allocate and set some host data
    float *h_data = (float *)std::malloc(sizeof(float) * width * height);
    for (int i = 0; i < height * width; ++i)
        h_data[i] = i;

    // Allocate CUDA array in device memory
    cudaChannelFormatDesc channelDesc =
        cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindFloat);
    cudaArray_t cuArray;
    cudaMallocArray(&cuArray, &channelDesc, width, height);

    // Set pitch of the source (the width in memory in bytes of the 2D array pointed
    // to by src, including padding), we dont have any padding
    const size_t spitch = width * sizeof(float);
    // Copy data located at address h_data in host memory to device memory
    cudaMemcpy2DToArray(cuArray, 0, 0, h_data, spitch, width * sizeof(float),
                        height, cudaMemcpyHostToDevice);

    // Specify texture
    struct cudaResourceDesc resDesc;
    memset(&resDesc, 0, sizeof(resDesc));
    resDesc.resType = cudaResourceTypeArray;
    resDesc.res.array.array = cuArray;

    // Specify texture object parameters
    struct cudaTextureDesc texDesc;
    memset(&texDesc, 0, sizeof(texDesc));
    texDesc.addressMode[0] = cudaAddressModeWrap;
    texDesc.addressMode[1] = cudaAddressModeWrap;
    texDesc.filterMode = cudaFilterModeLinear;
    texDesc.readMode = cudaReadModeElementType;
    texDesc.normalizedCoords = 1;

    // Create texture object
    cudaTextureObject_t texObj = 0;
    cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);

    // Allocate result of transformation in device memory
    float *output;
    cudaMalloc(&output, width * height * sizeof(float));

    // Invoke kernel
    dim3 threadsperBlock(16, 16);
    dim3 numBlocks((width + threadsperBlock.x - 1) / threadsperBlock.x,
                    (height + threadsperBlock.y - 1) / threadsperBlock.y);
    transformKernel<<<numBlocks, threadsperBlock>>>(output, texObj, width, height,
                                                    angle);
    // Copy data from device back to host
    cudaMemcpy(h_data, output, width * height * sizeof(float),
                cudaMemcpyDeviceToHost);

    // Destroy texture object
    cudaDestroyTextureObject(texObj);

    // Free device memory
    cudaFreeArray(cuArray);
    cudaFree(output);

    // Free host memory
    free(h_data);

    return 0;
}





3.2.14.1.2. 16-Bit Floating-Point Texturesï

The 16-bit floating-point or half format supported by CUDA arrays is the same as the IEEE 754-2008 binary2 format.
CUDA C++ does not support a matching data type, but provides intrinsic functions to convert to and from the 32-bit floating-point format via the unsigned short type: __float2half_rn(float) and __half2float(unsigned short). These functions are only supported in device code. Equivalent functions for the host code can be found in the OpenEXR library, for example.
16-bit floating-point components are promoted to 32 bit float during texture fetching before any filtering is performed.
A channel description for the 16-bit floating-point format can be created by calling one of the cudaCreateChannelDescHalf*() functions.



3.2.14.1.3. Layered Texturesï

A one-dimensional or two-dimensional layered texture (also known as texture array in Direct3D and array texture in OpenGL) is a texture made up of a sequence of layers, all of which are regular textures of same dimensionality, size, and data type.
A one-dimensional layered texture is addressed using an integer index and a floating-point texture coordinate; the index denotes a layer within the sequence and the coordinate addresses a texel within that layer. A two-dimensional layered texture is addressed using an integer index and two floating-point texture coordinates; the index denotes a layer within the sequence and the coordinates address a texel within that layer.
A layered texture can only be a CUDA array by calling cudaMalloc3DArray() with the cudaArrayLayered flag (and a height of zero for one-dimensional layered texture).
Layered textures are fetched using the device functions described in tex1DLayered() and tex2DLayered(). Texture filtering (see Texture Fetching) is done only within a layer, not across layers.
Layered textures are only supported on devices of compute capability 2.0 and higher.



3.2.14.1.4. Cubemap Texturesï

A cubemap texture is a special type of two-dimensional layered texture that has six layers representing the faces of a cube:

The width of a layer is equal to its height.
The cubemap is addressed using three texture coordinates x, y, and z that are interpreted as a direction vector emanating from the center of the cube and pointing to one face of the cube and a texel within the layer corresponding to that face. More specifically, the face is selected by the coordinate with largest magnitude m and the corresponding layer is addressed using coordinates (s/m+1)/2 and (t/m+1)/2 where s and t are defined in Table 3.



Table 3 Cubemap Fetchï












face
m
s
t




|x| > |y| and |x| > |z|
x â¥ 0
0
x
-z
-y


x < 0
1
-x
z
-y


|y| > |x| and |y| > |z|
y â¥ 0
2
y
x
z


y < 0
3
-y
x
-z


|z| > |x| and |z| > |y|
z â¥ 0
4
z
x
-y


z < 0
5
-z
-x
-y



A cubemap texture can only be a CUDA array by calling cudaMalloc3DArray() with the cudaArrayCubemap flag.
Cubemap textures are fetched using the device function described in texCubemap().
Cubemap textures are only supported on devices of compute capability 2.0 and higher.



3.2.14.1.5. Cubemap Layered Texturesï

A cubemap layered texture is a layered texture whose layers are cubemaps of same dimension.
A cubemap layered texture is addressed using an integer index and three floating-point texture coordinates; the index denotes a cubemap within the sequence and the coordinates address a texel within that cubemap.
A cubemap layered texture can only be a CUDA array by calling cudaMalloc3DArray() with the cudaArrayLayered and cudaArrayCubemap flags.
Cubemap layered textures are fetched using the device function described in texCubemapLayered(). Texture filtering (see Texture Fetching) is done only within a layer, not across layers.
Cubemap layered textures are only supported on devices of compute capability 2.0 and higher.



3.2.14.1.6. Texture Gatherï

Texture gather is a special texture fetch that is available for two-dimensional textures only. It is performed by the tex2Dgather() function, which has the same parameters as tex2D(), plus an additional comp parameter equal to 0, 1, 2, or 3 (see tex2Dgather()). It returns four 32-bit numbers that correspond to the value of the component comp of each of the four texels that would have been used for bilinear filtering during a regular texture fetch. For example, if these texels are of values (253, 20, 31, 255), (250, 25, 29, 254), (249, 16, 37, 253), (251, 22, 30, 250), and comp is 2, tex2Dgather() returns (31, 29, 37, 30).
Note that texture coordinates are computed with only 8 bits of fractional precision. tex2Dgather() may therefore return unexpected results for cases where tex2D() would use 1.0 for one of its weights (Î± or Î², see Linear Filtering). For example, with an x texture coordinate of 2.49805: xB=x-0.5=1.99805, however the fractional part of xB is stored in an 8-bit fixed-point format. Since 0.99805 is closer to 256.f/256.f than it is to 255.f/256.f, xB has the value 2. A tex2Dgather() in this case would therefore return indices 2 and 3 in x, instead of indices 1 and 2.
Texture gather is only supported for CUDA arrays created with the cudaArrayTextureGather flag and of width and height less than the maximum specified in Table 21 for texture gather, which is smaller than for regular texture fetch.
Texture gather is only supported on devices of compute capability 2.0 and higher.




3.2.14.2. Surface Memoryï

For devices of compute capability 2.0 and higher, a CUDA array (described in Cubemap Surfaces), created with the cudaArraySurfaceLoadStore flag, can be read and written via a surface object using the functions described in Surface Functions.
Table 21 lists the maximum surface width, height, and depth depending on the compute capability of the device.


3.2.14.2.1. Surface Object APIï

A surface object is created using cudaCreateSurfaceObject() from a resource description of type struct cudaResourceDesc. Unlike texture memory, surface memory uses byte addressing. This means that the x-coordinate used to access a texture element via texture functions needs to be multiplied by the byte size of the element to access the same element via a surface function. For example, the element at texture coordinate x of a one-dimensional floating-point CUDA array bound to a texture object texObj and a surface object surfObj is read using tex1d(texObj, x) via texObj, but surf1Dread(surfObj, 4*x) via surfObj. Similarly, the element at texture coordinate x and y of a two-dimensional floating-point CUDA array bound to a texture object texObj and a surface object surfObj is accessed using tex2d(texObj, x, y) via texObj, but surf2Dread(surfObj, 4*x, y) via surObj (the byte offset of the y-coordinate is internally calculated from the underlying line pitch of the CUDA array).
The following code sample applies some simple transformation kernel to a surface.

// Simple copy kernel
__global__ void copyKernel(cudaSurfaceObject_t inputSurfObj,
                           cudaSurfaceObject_t outputSurfObj,
                           int width, int height)
{
    // Calculate surface coordinates
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
    if (x < width && y < height) {
        uchar4 data;
        // Read from input surface
        surf2Dread(&data,  inputSurfObj, x * 4, y);
        // Write to output surface
        surf2Dwrite(data, outputSurfObj, x * 4, y);
    }
}

// Host code
int main()
{
    const int height = 1024;
    const int width = 1024;

    // Allocate and set some host data
    unsigned char *h_data =
        (unsigned char *)std::malloc(sizeof(unsigned char) * width * height * 4);
    for (int i = 0; i < height * width * 4; ++i)
        h_data[i] = i;

    // Allocate CUDA arrays in device memory
    cudaChannelFormatDesc channelDesc =
        cudaCreateChannelDesc(8, 8, 8, 8, cudaChannelFormatKindUnsigned);
    cudaArray_t cuInputArray;
    cudaMallocArray(&cuInputArray, &channelDesc, width, height,
                    cudaArraySurfaceLoadStore);
    cudaArray_t cuOutputArray;
    cudaMallocArray(&cuOutputArray, &channelDesc, width, height,
                    cudaArraySurfaceLoadStore);

    // Set pitch of the source (the width in memory in bytes of the 2D array
    // pointed to by src, including padding), we dont have any padding
    const size_t spitch = 4 * width * sizeof(unsigned char);
    // Copy data located at address h_data in host memory to device memory
    cudaMemcpy2DToArray(cuInputArray, 0, 0, h_data, spitch,
                        4 * width * sizeof(unsigned char), height,
                        cudaMemcpyHostToDevice);

    // Specify surface
    struct cudaResourceDesc resDesc;
    memset(&resDesc, 0, sizeof(resDesc));
    resDesc.resType = cudaResourceTypeArray;

    // Create the surface objects
    resDesc.res.array.array = cuInputArray;
    cudaSurfaceObject_t inputSurfObj = 0;
    cudaCreateSurfaceObject(&inputSurfObj, &resDesc);
    resDesc.res.array.array = cuOutputArray;
    cudaSurfaceObject_t outputSurfObj = 0;
    cudaCreateSurfaceObject(&outputSurfObj, &resDesc);

    // Invoke kernel
    dim3 threadsperBlock(16, 16);
    dim3 numBlocks((width + threadsperBlock.x - 1) / threadsperBlock.x,
                    (height + threadsperBlock.y - 1) / threadsperBlock.y);
    copyKernel<<<numBlocks, threadsperBlock>>>(inputSurfObj, outputSurfObj, width,
                                                height);

    // Copy data from device back to host
    cudaMemcpy2DFromArray(h_data, spitch, cuOutputArray, 0, 0,
                            4 * width * sizeof(unsigned char), height,
                            cudaMemcpyDeviceToHost);

    // Destroy surface objects
    cudaDestroySurfaceObject(inputSurfObj);
    cudaDestroySurfaceObject(outputSurfObj);

    // Free device memory
    cudaFreeArray(cuInputArray);
    cudaFreeArray(cuOutputArray);

    // Free host memory
    free(h_data);

  return 0;
}





3.2.14.2.2. Cubemap Surfacesï

Cubemap surfaces are accessed usingsurfCubemapread() and surfCubemapwrite() (surfCubemapread and surfCubemapwrite) as a two-dimensional layered surface, i.e., using an integer index denoting a face and two floating-point texture coordinates addressing a texel within the layer corresponding to this face. Faces are ordered as indicated in Table 3.



3.2.14.2.3. Cubemap Layered Surfacesï

Cubemap layered surfaces are accessed using surfCubemapLayeredread() and surfCubemapLayeredwrite() (surfCubemapLayeredread() and surfCubemapLayeredwrite()) as a two-dimensional layered surface, i.e., using an integer index denoting a face of one of the cubemaps and two floating-point texture coordinates addressing a texel within the layer corresponding to this face. Faces are ordered as indicated in Table 3, so index ((2 * 6) + 3), for example, accesses the fourth face of the third cubemap.




3.2.14.3. CUDA Arraysï

CUDA arrays are opaque memory layouts optimized for texture fetching. They are one dimensional, two dimensional, or three-dimensional and composed of elements, each of which has 1, 2 or 4 components that may be signed or unsigned 8-, 16-, or 32-bit integers, 16-bit floats, or 32-bit floats. CUDA arrays are only accessible by kernels through texture fetching as described in Texture Memory or surface reading and writing as described in Surface Memory.



3.2.14.4. Read/Write Coherencyï

The texture and surface memory is cached (see Device Memory Accesses) and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global write or a surface write in the same kernel call returns undefined data. In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call.




3.2.15. Graphics Interoperabilityï

Some resources from OpenGL and Direct3D may be mapped into the address space of CUDA, either to enable CUDA to read data written by OpenGL or Direct3D, or to enable CUDA to write data for consumption by OpenGL or Direct3D.
A resource must be registered to CUDA before it can be mapped using the functions mentioned in OpenGL Interoperability and Direct3D Interoperability. These functions return a pointer to a CUDA graphics resource of type struct cudaGraphicsResource. Registering a resource is potentially high-overhead and therefore typically called only once per resource. A CUDA graphics resource is unregistered using cudaGraphicsUnregisterResource(). Each CUDA context which intends to use the resource is required to register it separately.
Once a resource is registered to CUDA, it can be mapped and unmapped as many times as necessary using cudaGraphicsMapResources() and cudaGraphicsUnmapResources(). cudaGraphicsResourceSetMapFlags() can be called to specify usage hints (write-only, read-only) that the CUDA driver can use to optimize resource management.
A mapped resource can be read from or written to by kernels using the device memory address returned by cudaGraphicsResourceGetMappedPointer() for buffers andcudaGraphicsSubResourceGetMappedArray() for CUDA arrays.
Accessing a resource through OpenGL, Direct3D, or another CUDA context while it is mapped produces undefined results. OpenGL Interoperability and Direct3D Interoperability give specifics for each graphics API and some code samples. SLI Interoperability gives specifics for when the system is in SLI mode.


3.2.15.1. OpenGL Interoperabilityï

The OpenGL resources that may be mapped into the address space of CUDA are OpenGL buffer, texture, and renderbuffer objects.
A buffer object is registered using cudaGraphicsGLRegisterBuffer(). In CUDA, it appears as a device pointer and can therefore be read and written by kernels or via cudaMemcpy() calls.
A texture or renderbuffer object is registered using cudaGraphicsGLRegisterImage(). In CUDA, it appears as a CUDA array. Kernels can read from the array by binding it to a texture or surface reference. They can also write to it via the surface write functions if the resource has been registered with the cudaGraphicsRegisterFlagsSurfaceLoadStore flag. The array can also be read and written via cudaMemcpy2D() calls. cudaGraphicsGLRegisterImage() supports all texture formats with 1, 2, or 4 components and an internal type of float (for example, GL_RGBA_FLOAT32), normalized integer (for example, GL_RGBA8, GL_INTENSITY16), and unnormalized integer (for example, GL_RGBA8UI) (please note that since unnormalized integer formats require OpenGL 3.0, they can only be written by shaders, not the fixed function pipeline).
The OpenGL context whose resources are being shared has to be current to the host thread making any OpenGL interoperability API calls.
Please note: When an OpenGL texture is made bindless (say for example by requesting an image or texture handle using the glGetTextureHandle*/glGetImageHandle* APIs) it cannot be registered with CUDA. The application needs to register the texture for interop before requesting an image or texture handle.
The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object:

GLuint positionsVBO;
struct cudaGraphicsResource* positionsVBO_CUDA;

int main()
{
    // Initialize OpenGL and GLUT for device 0
    // and make the OpenGL context current
    ...
    glutDisplayFunc(display);

    // Explicitly set device 0
    cudaSetDevice(0);

    // Create buffer object and register it with CUDA
    glGenBuffers(1, &positionsVBO);
    glBindBuffer(GL_ARRAY_BUFFER, positionsVBO);
    unsigned int size = width * height * 4 * sizeof(float);
    glBufferData(GL_ARRAY_BUFFER, size, 0, GL_DYNAMIC_DRAW);
    glBindBuffer(GL_ARRAY_BUFFER, 0);
    cudaGraphicsGLRegisterBuffer(&positionsVBO_CUDA,
                                 positionsVBO,
                                 cudaGraphicsMapFlagsWriteDiscard);

    // Launch rendering loop
    glutMainLoop();

    ...
}

void display()
{
    // Map buffer object for writing from CUDA
    float4* positions;
    cudaGraphicsMapResources(1, &positionsVBO_CUDA, 0);
    size_t num_bytes;
    cudaGraphicsResourceGetMappedPointer((void**)&positions,
                                         &num_bytes,
                                         positionsVBO_CUDA));

    // Execute kernel
    dim3 dimBlock(16, 16, 1);
    dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<<<dimGrid, dimBlock>>>(positions, time,
                                          width, height);

    // Unmap buffer object
    cudaGraphicsUnmapResources(1, &positionsVBO_CUDA, 0);

    // Render from buffer object
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
    glBindBuffer(GL_ARRAY_BUFFER, positionsVBO);
    glVertexPointer(4, GL_FLOAT, 0, 0);
    glEnableClientState(GL_VERTEX_ARRAY);
    glDrawArrays(GL_POINTS, 0, width * height);
    glDisableClientState(GL_VERTEX_ARRAY);

    // Swap buffers
    glutSwapBuffers();
    glutPostRedisplay();
}



void deleteVBO()
{
    cudaGraphicsUnregisterResource(positionsVBO_CUDA);
    glDeleteBuffers(1, &positionsVBO);
}

__global__ void createVertices(float4* positions, float time,
                               unsigned int width, unsigned int height)
{
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;

    // Calculate uv coordinates
    float u = x / (float)width;
    float v = y / (float)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    // calculate simple sine wave pattern
    float freq = 4.0f;
    float w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    // Write positions
    positions[y * width + x] = make_float4(u, w, v, 1.0f);
}


On Windows and for Quadro GPUs, cudaWGLGetDevice() can be used to retrieve the CUDA device associated to the handle returned by wglEnumGpusNV(). Quadro GPUs offer higher performance OpenGL interoperability than GeForce and Tesla GPUs in a multi-GPU configuration where OpenGL rendering is performed on the Quadro GPU and CUDA computations are performed on other GPUs in the system.



3.2.15.2. Direct3D Interoperabilityï

Direct3D interoperability is supported for Direct3D 9Ex, Direct3D 10, and Direct3D 11.
A CUDA context may interoperate only with Direct3D devices that fulfill the following criteria: Direct3D 9Ex devices must be created with DeviceType set to D3DDEVTYPE_HAL and BehaviorFlags with the D3DCREATE_HARDWARE_VERTEXPROCESSING flag; Direct3D 10 and Direct3D 11 devices must be created with DriverType set to D3D_DRIVER_TYPE_HARDWARE.
The Direct3D resources that may be mapped into the address space of CUDA are Direct3D buffers, textures, and surfaces. These resources are registered using cudaGraphicsD3D9RegisterResource(), cudaGraphicsD3D10RegisterResource(), and cudaGraphicsD3D11RegisterResource().
The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object.


3.2.15.2.1. Direct3D 9 Versionï


IDirect3D9* D3D;
IDirect3DDevice9* device;
struct CUSTOMVERTEX {
    FLOAT x, y, z;
    DWORD color;
};
IDirect3DVertexBuffer9* positionsVB;
struct cudaGraphicsResource* positionsVB_CUDA;

int main()
{
    int dev;
    // Initialize Direct3D
    D3D = Direct3DCreate9Ex(D3D_SDK_VERSION);

    // Get a CUDA-enabled adapter
    unsigned int adapter = 0;
    for (; adapter < g_pD3D->GetAdapterCount(); adapter++) {
        D3DADAPTER_IDENTIFIER9 adapterId;
        g_pD3D->GetAdapterIdentifier(adapter, 0, &adapterId);
        if (cudaD3D9GetDevice(&dev, adapterId.DeviceName)
            == cudaSuccess)
            break;
    }

     // Create device
    ...
    D3D->CreateDeviceEx(adapter, D3DDEVTYPE_HAL, hWnd,
                        D3DCREATE_HARDWARE_VERTEXPROCESSING,
                        &params, NULL, &device);

    // Use the same device
    cudaSetDevice(dev);

    // Create vertex buffer and register it with CUDA
    unsigned int size = width * height * sizeof(CUSTOMVERTEX);
    device->CreateVertexBuffer(size, 0, D3DFVF_CUSTOMVERTEX,
                               D3DPOOL_DEFAULT, &positionsVB, 0);
    cudaGraphicsD3D9RegisterResource(&positionsVB_CUDA,
                                     positionsVB,
                                     cudaGraphicsRegisterFlagsNone);
    cudaGraphicsResourceSetMapFlags(positionsVB_CUDA,
                                    cudaGraphicsMapFlagsWriteDiscard);

    // Launch rendering loop
    while (...) {
        ...
        Render();
        ...
    }
    ...
}



void Render()
{
    // Map vertex buffer for writing from CUDA
    float4* positions;
    cudaGraphicsMapResources(1, &positionsVB_CUDA, 0);
    size_t num_bytes;
    cudaGraphicsResourceGetMappedPointer((void**)&positions,
                                         &num_bytes,
                                         positionsVB_CUDA));

    // Execute kernel
    dim3 dimBlock(16, 16, 1);
    dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<<<dimGrid, dimBlock>>>(positions, time,
                                          width, height);

    // Unmap vertex buffer
    cudaGraphicsUnmapResources(1, &positionsVB_CUDA, 0);

    // Draw and present
    ...
}

void releaseVB()
{
    cudaGraphicsUnregisterResource(positionsVB_CUDA);
    positionsVB->Release();
}

__global__ void createVertices(float4* positions, float time,
                               unsigned int width, unsigned int height)
{
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;

    // Calculate uv coordinates
    float u = x / (float)width;
    float v = y / (float)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    // Calculate simple sine wave pattern
    float freq = 4.0f;
    float w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    // Write positions
    positions[y * width + x] =
                make_float4(u, w, v, __int_as_float(0xff00ff00));
}





3.2.15.2.2. Direct3D 10 Versionï


ID3D10Device* device;
struct CUSTOMVERTEX {
    FLOAT x, y, z;
    DWORD color;
};
ID3D10Buffer* positionsVB;
struct cudaGraphicsResource* positionsVB_CUDA;

int main()
{
    int dev;
    // Get a CUDA-enabled adapter
    IDXGIFactory* factory;
    CreateDXGIFactory(__uuidof(IDXGIFactory), (void**)&factory);
    IDXGIAdapter* adapter = 0;
    for (unsigned int i = 0; !adapter; ++i) {
        if (FAILED(factory->EnumAdapters(i, &adapter))
            break;
        if (cudaD3D10GetDevice(&dev, adapter) == cudaSuccess)
            break;
        adapter->Release();
    }
    factory->Release();

    // Create swap chain and device
    ...
    D3D10CreateDeviceAndSwapChain(adapter,
                                  D3D10_DRIVER_TYPE_HARDWARE, 0,
                                  D3D10_CREATE_DEVICE_DEBUG,
                                  D3D10_SDK_VERSION,
                                  &swapChainDesc, &swapChain,
                                  &device);
    adapter->Release();

    // Use the same device
    cudaSetDevice(dev);

    // Create vertex buffer and register it with CUDA
    unsigned int size = width * height * sizeof(CUSTOMVERTEX);
    D3D10_BUFFER_DESC bufferDesc;
    bufferDesc.Usage          = D3D10_USAGE_DEFAULT;
    bufferDesc.ByteWidth      = size;
    bufferDesc.BindFlags      = D3D10_BIND_VERTEX_BUFFER;
    bufferDesc.CPUAccessFlags = 0;
    bufferDesc.MiscFlags      = 0;
    device->CreateBuffer(&bufferDesc, 0, &positionsVB);
    cudaGraphicsD3D10RegisterResource(&positionsVB_CUDA,
                                      positionsVB,
                                      cudaGraphicsRegisterFlagsNone);
                                      cudaGraphicsResourceSetMapFlags(positionsVB_CUDA,
                                      cudaGraphicsMapFlagsWriteDiscard);

    // Launch rendering loop
    while (...) {
        ...
        Render();
        ...
    }
    ...
}



void Render()
{
    // Map vertex buffer for writing from CUDA
    float4* positions;
    cudaGraphicsMapResources(1, &positionsVB_CUDA, 0);
    size_t num_bytes;
    cudaGraphicsResourceGetMappedPointer((void**)&positions,
                                         &num_bytes,
                                         positionsVB_CUDA));

    // Execute kernel
    dim3 dimBlock(16, 16, 1);
    dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<<<dimGrid, dimBlock>>>(positions, time,
                                          width, height);

    // Unmap vertex buffer
    cudaGraphicsUnmapResources(1, &positionsVB_CUDA, 0);

    // Draw and present
    ...
}

void releaseVB()
{
    cudaGraphicsUnregisterResource(positionsVB_CUDA);
    positionsVB->Release();
}

__global__ void createVertices(float4* positions, float time,
                               unsigned int width, unsigned int height)
{
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;

    // Calculate uv coordinates
    float u = x / (float)width;
    float v = y / (float)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    // Calculate simple sine wave pattern
    float freq = 4.0f;
    float w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    // Write positions
    positions[y * width + x] =
                make_float4(u, w, v, __int_as_float(0xff00ff00));
}





3.2.15.2.3. Direct3D 11 Versionï


ID3D11Device* device;
struct CUSTOMVERTEX {
    FLOAT x, y, z;
    DWORD color;
};
ID3D11Buffer* positionsVB;
struct cudaGraphicsResource* positionsVB_CUDA;

int main()
{
    int dev;
    // Get a CUDA-enabled adapter
    IDXGIFactory* factory;
    CreateDXGIFactory(__uuidof(IDXGIFactory), (void**)&factory);
    IDXGIAdapter* adapter = 0;
    for (unsigned int i = 0; !adapter; ++i) {
        if (FAILED(factory->EnumAdapters(i, &adapter))
            break;
        if (cudaD3D11GetDevice(&dev, adapter) == cudaSuccess)
            break;
        adapter->Release();
    }
    factory->Release();

    // Create swap chain and device
    ...
    sFnPtr_D3D11CreateDeviceAndSwapChain(adapter,
                                         D3D11_DRIVER_TYPE_HARDWARE,
                                         0,
                                         D3D11_CREATE_DEVICE_DEBUG,
                                         featureLevels, 3,
                                         D3D11_SDK_VERSION,
                                         &swapChainDesc, &swapChain,
                                         &device,
                                         &featureLevel,
                                         &deviceContext);
    adapter->Release();

    // Use the same device
    cudaSetDevice(dev);

    // Create vertex buffer and register it with CUDA
    unsigned int size = width * height * sizeof(CUSTOMVERTEX);
    D3D11_BUFFER_DESC bufferDesc;
    bufferDesc.Usage          = D3D11_USAGE_DEFAULT;
    bufferDesc.ByteWidth      = size;
    bufferDesc.BindFlags      = D3D11_BIND_VERTEX_BUFFER;
    bufferDesc.CPUAccessFlags = 0;
    bufferDesc.MiscFlags      = 0;
    device->CreateBuffer(&bufferDesc, 0, &positionsVB);
    cudaGraphicsD3D11RegisterResource(&positionsVB_CUDA,
                                      positionsVB,
                                      cudaGraphicsRegisterFlagsNone);
    cudaGraphicsResourceSetMapFlags(positionsVB_CUDA,
                                    cudaGraphicsMapFlagsWriteDiscard);

    // Launch rendering loop
    while (...) {
        ...
        Render();
        ...
    }
    ...
}



void Render()
{
    // Map vertex buffer for writing from CUDA
    float4* positions;
    cudaGraphicsMapResources(1, &positionsVB_CUDA, 0);
    size_t num_bytes;
    cudaGraphicsResourceGetMappedPointer((void**)&positions,
                                         &num_bytes,
                                         positionsVB_CUDA));

    // Execute kernel
    dim3 dimBlock(16, 16, 1);
    dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
    createVertices<<<dimGrid, dimBlock>>>(positions, time,
                                          width, height);

    // Unmap vertex buffer
    cudaGraphicsUnmapResources(1, &positionsVB_CUDA, 0);

    // Draw and present
    ...
}

void releaseVB()
{
    cudaGraphicsUnregisterResource(positionsVB_CUDA);
    positionsVB->Release();
}

    __global__ void createVertices(float4* positions, float time,
                          unsigned int width, unsigned int height)
{
    unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;

// Calculate uv coordinates
    float u = x / (float)width;
    float v = y / (float)height;
    u = u * 2.0f - 1.0f;
    v = v * 2.0f - 1.0f;

    // Calculate simple sine wave pattern
    float freq = 4.0f;
    float w = sinf(u * freq + time)
            * cosf(v * freq + time) * 0.5f;

    // Write positions
    positions[y * width + x] =
                make_float4(u, w, v, __int_as_float(0xff00ff00));
}






3.2.15.3. SLI Interoperabilityï

In a system with multiple GPUs, all CUDA-enabled GPUs are accessible via the CUDA driver and runtime as separate devices. There are however special considerations as described below when the system is in SLI mode.
First, an allocation in one CUDA device on one GPU will consume memory on other GPUs that are part of the SLI configuration of the Direct3D or OpenGL device. Because of this, allocations may fail earlier than otherwise expected.
Second, applications should create multiple CUDA contexts, one for each GPU in the SLI configuration. While this is not a strict requirement, it avoids unnecessary data transfers between devices. The application can use the cudaD3D[9|10|11]GetDevices() for Direct3D and cudaGLGetDevices() for OpenGL set of calls to identify the CUDA device handle(s) for the device(s) that are performing the rendering in the current and next frame. Given this information the application will typically choose the appropriate device and map Direct3D or OpenGL resources to the CUDA device returned by cudaD3D[9|10|11]GetDevices() or cudaGLGetDevices() when the deviceList parameter is set to cudaD3D[9|10|11]DeviceListCurrentFrame or cudaGLDeviceListCurrentFrame.
Please note that resource returned from cudaGraphicsD9D[9|10|11]RegisterResource and cudaGraphicsGLRegister[Buffer|Image] must be only used on device the registration happened. Therefore on SLI configurations when data for different frames is computed on different CUDA devices it is necessary to register the resources for each separately.
See Direct3D Interoperability and OpenGL Interoperability for details on how the CUDA runtime interoperate with Direct3D and OpenGL, respectively.




3.2.16. External Resource Interoperabilityï

External resource interoperability allows CUDA to import certain resources that are explicitly exported by other APIs. These objects are typically exported by other APIs using handles native to the Operating System, like file descriptors on Linux or NT handles on Windows. They could also be exported using other unified interfaces such as the NVIDIA Software Communication Interface. There are two types of resources that can be imported: memory objects and synchronization objects.
Memory objects can be imported into CUDA using cudaImportExternalMemory(). An imported memory object can be accessed from within kernels using device pointers mapped onto the memory object via cudaExternalMemoryGetMappedBuffer()or CUDA mipmapped arrays mapped via cudaExternalMemoryGetMappedMipmappedArray(). Depending on the type of memory object, it may be possible for more than one mapping to be setup on a single memory object. The mappings must match the mappings setup in the exporting API. Any mismatched mappings result in undefined behavior. Imported memory objects must be freed using cudaDestroyExternalMemory(). Freeing a memory object does not free any mappings to that object. Therefore, any device pointers mapped onto that object must be explicitly freed using cudaFree() and any CUDA mipmapped arrays mapped onto that object must be explicitly freed using cudaFreeMipmappedArray(). It is illegal to access mappings to an object after it has been destroyed.
Synchronization objects can be imported into CUDA using cudaImportExternalSemaphore(). An imported synchronization object can then be signaled using cudaSignalExternalSemaphoresAsync() and waited on using cudaWaitExternalSemaphoresAsync(). It is illegal to issue a wait before the corresponding signal has been issued. Also, depending on the type of the imported synchronization object, there may be additional constraints imposed on how they can be signaled and waited on, as described in subsequent sections. Imported semaphore objects must be freed using cudaDestroyExternalSemaphore(). All outstanding signals and waits must have completed before the semaphore object is destroyed.


3.2.16.1. Vulkan Interoperabilityï



3.2.16.1.1. Matching device UUIDsï

When importing memory and synchronization objects exported by Vulkan, they must be imported and mapped on the same device as they were created on. The CUDA device that corresponds to the Vulkan physical device on which the objects were created can be determined by comparing the UUID of a CUDA device with that of the Vulkan physical device, as shown in the following code sample. Note that the Vulkan physical device should not be part of a device group that contains more than one Vulkan physical device. The device group as returned by vkEnumeratePhysicalDeviceGroups that contains the given Vulkan physical device must have a physical device count of 1.

int getCudaDeviceForVulkanPhysicalDevice(VkPhysicalDevice vkPhysicalDevice) {
    VkPhysicalDeviceIDProperties vkPhysicalDeviceIDProperties = {};
    vkPhysicalDeviceIDProperties.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_ID_PROPERTIES;
    vkPhysicalDeviceIDProperties.pNext = NULL;

    VkPhysicalDeviceProperties2 vkPhysicalDeviceProperties2 = {};
    vkPhysicalDeviceProperties2.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2;
    vkPhysicalDeviceProperties2.pNext = &vkPhysicalDeviceIDProperties;

    vkGetPhysicalDeviceProperties2(vkPhysicalDevice, &vkPhysicalDeviceProperties2);

    int cudaDeviceCount;
    cudaGetDeviceCount(&cudaDeviceCount);

    for (int cudaDevice = 0; cudaDevice < cudaDeviceCount; cudaDevice++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&deviceProp, cudaDevice);
        if (!memcmp(&deviceProp.uuid, vkPhysicalDeviceIDProperties.deviceUUID, VK_UUID_SIZE)) {
            return cudaDevice;
        }
    }
    return cudaInvalidDeviceId;
}





3.2.16.1.2. Importing Memory Objectsï

On Linux and Windows 10, both dedicated and non-dedicated memory objects exported by Vulkan can be imported into CUDA. On Windows 7, only dedicated memory objects can be imported. When importing a Vulkan dedicated memory object, the flag cudaExternalMemoryDedicated must be set.
A Vulkan memory object exported using VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT can be imported into CUDA using the file descriptor associated with that object as shown below. Note that CUDA assumes ownership of the file descriptor once it is imported. Using the file descriptor after a successful import results in undefined behavior.

cudaExternalMemory_t importVulkanMemoryObjectFromFileDescriptor(int fd, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueFd;
    desc.handle.fd = fd;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&extMem, &desc);

    // Input parameter 'fd' should not be used beyond this point as CUDA has assumed ownership of it

    return extMem;
}


A Vulkan memory object exported using VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT can be imported into CUDA using the NT handle associated with that object as shown below. Note that CUDA does not assume ownership of the NT handle and it is the applicationâs responsibility to close the handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying memory can be freed.

cudaExternalMemory_t importVulkanMemoryObjectFromNTHandle(HANDLE handle, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;
    desc.handle.win32.handle = handle;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&extMem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}


A Vulkan memory object exported using VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT can also be imported using a named handle if one exists as shown below.

cudaExternalMemory_t importVulkanMemoryObjectFromNamedNTHandle(LPCWSTR name, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueWin32;
    desc.handle.win32.name = (void *)name;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&extMem, &desc);

    return extMem;
}


A Vulkan memory object exported using VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT can be imported into CUDA using the globally shared D3DKMT handle associated with that object as shown below. Since a globally shared D3DKMT handle does not hold a reference to the underlying memory it is automatically destroyed when all other references to the resource are destroyed.

cudaExternalMemory_t importVulkanMemoryObjectFromKMTHandle(HANDLE handle, unsigned long long size, bool isDedicated) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeOpaqueWin32Kmt;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    if (isDedicated) {
        desc.flags |= cudaExternalMemoryDedicated;
    }

    cudaImportExternalMemory(&extMem, &desc);

    return extMem;
}





3.2.16.1.3. Mapping Buffers onto Imported Memory Objectsï

A device pointer can be mapped onto an imported memory object as shown below. The offset and size of the mapping must match that specified when creating the mapping using the corresponding Vulkan API. All mapped device pointers must be freed using cudaFree().

void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {

    void *ptr = NULL;

    cudaExternalMemoryBufferDesc desc = {};



    memset(&desc, 0, sizeof(desc));



    desc.offset = offset;

    desc.size = size;



    cudaExternalMemoryGetMappedBuffer(&ptr, extMem, &desc);



    // Note: âptrâ must eventually be freed using cudaFree()

    return ptr;

}





3.2.16.1.4. Mapping Mipmapped Arrays onto Imported Memory Objectsï

A CUDA mipmapped array can be mapped onto an imported memory object as shown below. The offset, dimensions, format and number of mip levels must match that specified when creating the mapping using the corresponding Vulkan API. Additionally, if the mipmapped array is bound as a color target in Vulkan, the flagcudaArrayColorAttachment must be set. All mapped mipmapped arrays must be freed using cudaFreeMipmappedArray(). The following code sample shows how to convert Vulkan parameters into the corresponding CUDA parameters when mapping mipmapped arrays onto imported memory objects.

cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: 'mipmap' must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&mipmap, extMem, &desc);

    return mipmap;
}

cudaChannelFormatDesc getCudaChannelFormatDescForVulkanFormat(VkFormat format)
{
    cudaChannelFormatDesc d;

    memset(&d, 0, sizeof(d));

    switch (format) {
    case VK_FORMAT_R8_UINT:             d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R8_SINT:             d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R8G8_UINT:           d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R8G8_SINT:           d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R8G8B8A8_UINT:       d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R8G8B8A8_SINT:       d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R16_UINT:            d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R16_SINT:            d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R16G16_UINT:         d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R16G16_SINT:         d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R16G16B16A16_UINT:   d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R16G16B16A16_SINT:   d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32_UINT:            d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R32_SINT:            d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32_SFLOAT:          d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case VK_FORMAT_R32G32_UINT:         d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R32G32_SINT:         d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32G32_SFLOAT:       d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case VK_FORMAT_R32G32B32A32_UINT:   d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindUnsigned; break;
    case VK_FORMAT_R32G32B32A32_SINT:   d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindSigned;   break;
    case VK_FORMAT_R32G32B32A32_SFLOAT: d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindFloat;    break;
    default: assert(0);
    }



    return d;
}

cudaExtent getCudaExtentForVulkanExtent(VkExtent3D vkExt, uint32_t arrayLayers, VkImageViewType vkImageViewType) {
    cudaExtent e = { 0, 0, 0 };

    switch (vkImageViewType) {
    case VK_IMAGE_VIEW_TYPE_1D:         e.width = vkExt.width; e.height = 0;            e.depth = 0;           break;
    case VK_IMAGE_VIEW_TYPE_2D:         e.width = vkExt.width; e.height = vkExt.height; e.depth = 0;           break;
    case VK_IMAGE_VIEW_TYPE_3D:         e.width = vkExt.width; e.height = vkExt.height; e.depth = vkExt.depth; break;
    case VK_IMAGE_VIEW_TYPE_CUBE:       e.width = vkExt.width; e.height = vkExt.height; e.depth = arrayLayers; break;
    case VK_IMAGE_VIEW_TYPE_1D_ARRAY:   e.width = vkExt.width; e.height = 0;            e.depth = arrayLayers; break;
    case VK_IMAGE_VIEW_TYPE_2D_ARRAY:   e.width = vkExt.width; e.height = vkExt.height; e.depth = arrayLayers; break;
    case VK_IMAGE_VIEW_TYPE_CUBE_ARRAY: e.width = vkExt.width; e.height = vkExt.height; e.depth = arrayLayers; break;
    default: assert(0);
    }

    return e;
}

unsigned int getCudaMipmappedArrayFlagsForVulkanImage(VkImageViewType vkImageViewType, VkImageUsageFlags vkImageUsageFlags, bool allowSurfaceLoadStore) {
    unsigned int flags = 0;

    switch (vkImageViewType) {
    case VK_IMAGE_VIEW_TYPE_CUBE:       flags |= cudaArrayCubemap;                    break;
    case VK_IMAGE_VIEW_TYPE_CUBE_ARRAY: flags |= cudaArrayCubemap | cudaArrayLayered; break;
    case VK_IMAGE_VIEW_TYPE_1D_ARRAY:   flags |= cudaArrayLayered;                    break;
    case VK_IMAGE_VIEW_TYPE_2D_ARRAY:   flags |= cudaArrayLayered;                    break;
    default: break;
    }

    if (vkImageUsageFlags & VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT) {
        flags |= cudaArrayColorAttachment;
    }

    if (allowSurfaceLoadStore) {
        flags |= cudaArraySurfaceLoadStore;
    }
    return flags;
}





3.2.16.1.5. Importing Synchronization Objectsï

A Vulkan semaphore object exported using VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BITcan be imported into CUDA using the file descriptor associated with that object as shown below. Note that CUDA assumes ownership of the file descriptor once it is imported. Using the file descriptor after a successful import results in undefined behavior.

cudaExternalSemaphore_t importVulkanSemaphoreObjectFromFileDescriptor(int fd) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueFd;
    desc.handle.fd = fd;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Input parameter 'fd' should not be used beyond this point as CUDA has assumed ownership of it

    return extSem;
}


A Vulkan semaphore object exported using VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT can be imported into CUDA using the NT handle associated with that object as shown below. Note that CUDA does not assume ownership of the NT handle and it is the applicationâs responsibility to close the handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying semaphore can be freed.

cudaExternalSemaphore_t importVulkanSemaphoreObjectFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}


A Vulkan semaphore object exported using VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT can also be imported using a named handle if one exists as shown below.

cudaExternalSemaphore_t importVulkanSemaphoreObjectFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&extSem, &desc);

    return extSem;
}


A Vulkan semaphore object exported using VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT can be imported into CUDA using the globally shared D3DKMT handle associated with that object as shown below. Since a globally shared D3DKMT handle does not hold a reference to the underlying semaphore it is automatically destroyed when all other references to the resource are destroyed.

cudaExternalSemaphore_t importVulkanSemaphoreObjectFromKMTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt;
    desc.handle.win32.handle = (void *)handle;

    cudaImportExternalSemaphore(&extSem, &desc);

    return extSem;
}





3.2.16.1.6. Signaling/Waiting on Imported Synchronization Objectsï

An imported Vulkan semaphore object can be signaled as shown below. Signaling such a semaphore object sets it to the signaled state. The corresponding wait that waits on this signal must be issued in Vulkan. Additionally, the wait that waits on this signal must be issued after this signal has been issued.

void signalExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&params, 0, sizeof(params));

    cudaSignalExternalSemaphoresAsync(&extSem, &params, 1, stream);
}


An imported Vulkan semaphore object can be waited on as shown below. Waiting on such a semaphore object waits until it reaches the signaled state and then resets it back to the unsignaled state. The corresponding signal that this wait is waiting on must be issued in Vulkan. Additionally, the signal must be issued before this wait can be issued.

void waitExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&params, 0, sizeof(params));

    cudaWaitExternalSemaphoresAsync(&extSem, &params, 1, stream);
}






3.2.16.2. OpenGL Interoperabilityï

Traditional OpenGL-CUDA interop as outlined in OpenGL Interoperability works by CUDA directly consuming handles created in OpenGL. However, since OpenGL can also consume memory and synchronization objects created in Vulkan, there exists an alternative approach to doing OpenGL-CUDA interop. Essentially, memory and synchronization objects exported by Vulkan could be imported into both, OpenGL and CUDA, and then used to coordinate memory accesses between OpenGL and CUDA. Please refer to the following OpenGL extensions for further details on how to import memory and synchronization objects exported by Vulkan:

GL_EXT_memory_object
GL_EXT_memory_object_fd
GL_EXT_memory_object_win32
GL_EXT_semaphore
GL_EXT_semaphore_fd
GL_EXT_semaphore_win32




3.2.16.3. Direct3D 12 Interoperabilityï



3.2.16.3.1. Matching Device LUIDsï

When importing memory and synchronization objects exported by Direct3D 12, they must be imported and mapped on the same device as they were created on. The CUDA device that corresponds to the Direct3D 12 device on which the objects were created can be determined by comparing the LUID of a CUDA device with that of the Direct3D 12 device, as shown in the following code sample. Note that the Direct3D 12 device must not be created on a linked node adapter. I.e. the node count as returned by ID3D12Device::GetNodeCount must be 1.

int getCudaDeviceForD3D12Device(ID3D12Device *d3d12Device) {
    LUID d3d12Luid = d3d12Device->GetAdapterLuid();

    int cudaDeviceCount;
    cudaGetDeviceCount(&cudaDeviceCount);

    for (int cudaDevice = 0; cudaDevice < cudaDeviceCount; cudaDevice++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&deviceProp, cudaDevice);
        char *cudaLuid = deviceProp.luid;

        if (!memcmp(&d3d12Luid.LowPart, cudaLuid, sizeof(d3d12Luid.LowPart)) &&
            !memcmp(&d3d12Luid.HighPart, cudaLuid + sizeof(d3d12Luid.LowPart), sizeof(d3d12Luid.HighPart))) {
            return cudaDevice;
        }
    }
    return cudaInvalidDeviceId;
}





3.2.16.3.2. Importing Memory Objectsï

A shareable Direct3D 12 heap memory object, created by setting the flag D3D12_HEAP_FLAG_SHARED in the call to ID3D12Device::CreateHeap, can be imported into CUDA using the NT handle associated with that object as shown below. Note that it is the applicationâs responsibility to close the NT handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying memory can be freed.

cudaExternalMemory_t importD3D12HeapFromNTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Heap;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;

    cudaImportExternalMemory(&extMem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}


A shareable Direct3D 12 heap memory object can also be imported using a named handle if one exists as shown below.

cudaExternalMemory_t importD3D12HeapFromNamedNTHandle(LPCWSTR name, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Heap;
    desc.handle.win32.name = (void *)name;
    desc.size = size;

    cudaImportExternalMemory(&extMem, &desc);

    return extMem;
}


A shareable Direct3D 12 committed resource, created by setting the flag D3D12_HEAP_FLAG_SHARED in the call to D3D12Device::CreateCommittedResource, can be imported into CUDA using the NT handle associated with that object as shown below. When importing a Direct3D 12 committed resource, the flag cudaExternalMemoryDedicated must be set. Note that it is the applicationâs responsibility to close the NT handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying memory can be freed.

cudaExternalMemory_t importD3D12CommittedResourceFromNTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Resource;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&extMem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}


A shareable Direct3D 12 committed resource can also be imported using a named handle if one exists as shown below.

cudaExternalMemory_t importD3D12CommittedResourceFromNamedNTHandle(LPCWSTR name, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D12Resource;
    desc.handle.win32.name = (void *)name;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&extMem, &desc);

    return extMem;
}





3.2.16.3.3. Mapping Buffers onto Imported Memory Objectsï

A device pointer can be mapped onto an imported memory object as shown below. The offset and size of the mapping must match that specified when creating the mapping using the corresponding Direct3D 12 API. All mapped device pointers must be freed using cudaFree().

void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {
    void *ptr = NULL;
    cudaExternalMemoryBufferDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.size = size;

    cudaExternalMemoryGetMappedBuffer(&ptr, extMem, &desc);

    // Note: 'ptr' must eventually be freed using cudaFree()
    return ptr;
}





3.2.16.3.4. Mapping Mipmapped Arrays onto Imported Memory Objectsï

A CUDA mipmapped array can be mapped onto an imported memory object as shown below. The offset, dimensions, format and number of mip levels must match that specified when creating the mapping using the corresponding Direct3D 12 API. Additionally, if the mipmapped array can be bound as a render target in Direct3D 12, the flag cudaArrayColorAttachment must be set. All mapped mipmapped arrays must be freed using cudaFreeMipmappedArray(). The following code sample shows how to convert Vulkan parameters into the corresponding CUDA parameters when mapping mipmapped arrays onto imported memory objects.

cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: 'mipmap' must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&mipmap, extMem, &desc);

    return mipmap;
}

cudaChannelFormatDesc getCudaChannelFormatDescForDxgiFormat(DXGI_FORMAT dxgiFormat)
{
    cudaChannelFormatDesc d;

    memset(&d, 0, sizeof(d));

    switch (dxgiFormat) {
    case DXGI_FORMAT_R8_UINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8_SINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8_UINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8_SINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8B8A8_UINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8B8A8_SINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16_UINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16_SINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16_UINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16_SINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16B16A16_UINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16B16A16_SINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_UINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32_SINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_FLOAT:          d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32_UINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32_SINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32_FLOAT:       d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32B32A32_UINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32B32A32_SINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32B32A32_FLOAT: d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindFloat;    break;
    default: assert(0);



    }

    return d;
}

cudaExtent getCudaExtentForD3D12Extent(UINT64 width, UINT height, UINT16 depthOrArraySize, D3D12_SRV_DIMENSION d3d12SRVDimension) {
    cudaExtent e = { 0, 0, 0 };

    switch (d3d12SRVDimension) {
    case D3D12_SRV_DIMENSION_TEXTURE1D:        e.width = width; e.height = 0;      e.depth = 0;                break;
    case D3D12_SRV_DIMENSION_TEXTURE2D:        e.width = width; e.height = height; e.depth = 0;                break;
    case D3D12_SRV_DIMENSION_TEXTURE3D:        e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURECUBE:      e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURE1DARRAY:   e.width = width; e.height = 0;      e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURE2DARRAY:   e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D12_SRV_DIMENSION_TEXTURECUBEARRAY: e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    default: assert(0);
    }

    return e;
}

unsigned int getCudaMipmappedArrayFlagsForD3D12Resource(D3D12_SRV_DIMENSION d3d12SRVDimension, D3D12_RESOURCE_FLAGS d3d12ResourceFlags, bool allowSurfaceLoadStore) {
    unsigned int flags = 0;

    switch (d3d12SRVDimension) {
    case D3D12_SRV_DIMENSION_TEXTURECUBE:      flags |= cudaArrayCubemap;                    break;
    case D3D12_SRV_DIMENSION_TEXTURECUBEARRAY: flags |= cudaArrayCubemap | cudaArrayLayered; break;
    case D3D12_SRV_DIMENSION_TEXTURE1DARRAY:   flags |= cudaArrayLayered;                    break;
    case D3D12_SRV_DIMENSION_TEXTURE2DARRAY:   flags |= cudaArrayLayered;                    break;
    default: break;
    }

    if (d3d12ResourceFlags & D3D12_RESOURCE_FLAG_ALLOW_RENDER_TARGET) {
        flags |= cudaArrayColorAttachment;
    }
    if (allowSurfaceLoadStore) {
        flags |= cudaArraySurfaceLoadStore;
    }

    return flags;
}





3.2.16.3.5. Importing Synchronization Objectsï

A shareable Direct3D 12 fence object, created by setting the flag D3D12_FENCE_FLAG_SHARED in the call to ID3D12Device::CreateFence, can be imported into CUDA using the NT handle associated with that object as shown below. Note that it is the applicationâs responsibility to close the handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying semaphore can be freed.

cudaExternalSemaphore_t importD3D12FenceFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D12Fence;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}


A shareable Direct3D 12 fence object can also be imported using a named handle if one exists as shown below.

cudaExternalSemaphore_t importD3D12FenceFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D12Fence;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&extSem, &desc);

    return extSem;
}





3.2.16.3.6. Signaling/Waiting on Imported Synchronization Objectsï

An imported Direct3D 12 fence object can be signaled as shown below. Signaling such a fence object sets its value to the one specified. The corresponding wait that waits on this signal must be issued in Direct3D 12. Additionally, the wait that waits on this signal must be issued after this signal has been issued.

void signalExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaSignalExternalSemaphoresAsync(&extSem, &params, 1, stream);
}


An imported Direct3D 12 fence object can be waited on as shown below. Waiting on such a fence object waits until its value becomes greater than or equal to the specified value. The corresponding signal that this wait is waiting on must be issued in Direct3D 12. Additionally, the signal must be issued before this wait can be issued.

void waitExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaWaitExternalSemaphoresAsync(&extSem, &params, 1, stream);
}






3.2.16.4. Direct3D 11 Interoperabilityï



3.2.16.4.1. Matching Device LUIDsï

When importing memory and synchronization objects exported by Direct3D 11, they must be imported and mapped on the same device as they were created on. The CUDA device that corresponds to the Direct3D 11 device on which the objects were created can be determined by comparing the LUID of a CUDA device with that of the Direct3D 11 device, as shown in the following code sample.

int getCudaDeviceForD3D11Device(ID3D11Device *d3d11Device) {
    IDXGIDevice *dxgiDevice;
    d3d11Device->QueryInterface(__uuidof(IDXGIDevice), (void **)&dxgiDevice);

    IDXGIAdapter *dxgiAdapter;
    dxgiDevice->GetAdapter(&dxgiAdapter);

    DXGI_ADAPTER_DESC dxgiAdapterDesc;
    dxgiAdapter->GetDesc(&dxgiAdapterDesc);

    LUID d3d11Luid = dxgiAdapterDesc.AdapterLuid;

    int cudaDeviceCount;
    cudaGetDeviceCount(&cudaDeviceCount);

    for (int cudaDevice = 0; cudaDevice < cudaDeviceCount; cudaDevice++) {
        cudaDeviceProp deviceProp;
        cudaGetDeviceProperties(&deviceProp, cudaDevice);
        char *cudaLuid = deviceProp.luid;

        if (!memcmp(&d3d11Luid.LowPart, cudaLuid, sizeof(d3d11Luid.LowPart)) &&
            !memcmp(&d3d11Luid.HighPart, cudaLuid + sizeof(d3d11Luid.LowPart), sizeof(d3d11Luid.HighPart))) {
            return cudaDevice;
        }
    }
    return cudaInvalidDeviceId;
}





3.2.16.4.2. Importing Memory Objectsï

A shareable Direct3D 11 texture resource, viz, ID3D11Texture1D, ID3D11Texture2D or ID3D11Texture3D, can be created by setting either the D3D11_RESOURCE_MISC_SHARED or D3D11_RESOURCE_MISC_SHARED_KEYEDMUTEX (on Windows 7) or D3D11_RESOURCE_MISC_SHARED_NTHANDLE (on Windows 10) when calling ID3D11Device:CreateTexture1D, ID3D11Device:CreateTexture2D or ID3D11Device:CreateTexture3D respectively. A shareable Direct3D 11 buffer resource, ID3D11Buffer, can be created by specifying either of the above flags when calling ID3D11Device::CreateBuffer. A shareable resource created by specifying the D3D11_RESOURCE_MISC_SHARED_NTHANDLE can be imported into CUDA using the NT handle associated with that object as shown below. Note that it is the applicationâs responsibility to close the NT handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying memory can be freed. When importing a Direct3D 11 resource, the flag cudaExternalMemoryDedicated must be set.

cudaExternalMemory_t importD3D11ResourceFromNTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D11Resource;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&extMem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extMem;
}


A shareable Direct3D 11 resource can also be imported using a named handle if one exists as shown below.

cudaExternalMemory_t importD3D11ResourceFromNamedNTHandle(LPCWSTR name, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D11Resource;
    desc.handle.win32.name = (void *)name;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&extMem, &desc);

    return extMem;
}


A shareable Direct3D 11 resource, created by specifying the D3D11_RESOURCE_MISC_SHARED or D3D11_RESOURCE_MISC_SHARED_KEYEDMUTEX, can be imported into CUDA using the globally shared D3DKMT handle associated with that object as shown below. Since a globally shared D3DKMT handle does not hold a reference to the underlying memory it is automatically destroyed when all other references to the resource are destroyed.

cudaExternalMemory_t importD3D11ResourceFromKMTHandle(HANDLE handle, unsigned long long size) {
    cudaExternalMemory_t extMem = NULL;
    cudaExternalMemoryHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalMemoryHandleTypeD3D11ResourceKmt;
    desc.handle.win32.handle = (void *)handle;
    desc.size = size;
    desc.flags |= cudaExternalMemoryDedicated;

    cudaImportExternalMemory(&extMem, &desc);

    return extMem;
}





3.2.16.4.3. Mapping Buffers onto Imported Memory Objectsï

A device pointer can be mapped onto an imported memory object as shown below. The offset and size of the mapping must match that specified when creating the mapping using the corresponding Direct3D 11 API. All mapped device pointers must be freed using cudaFree().

void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {
    void *ptr = NULL;
    cudaExternalMemoryBufferDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.size = size;

    cudaExternalMemoryGetMappedBuffer(&ptr, extMem, &desc);

    // Note: âptrâ must eventually be freed using cudaFree()
    return ptr;
}





3.2.16.4.4. Mapping Mipmapped Arrays onto Imported Memory Objectsï

A CUDA mipmapped array can be mapped onto an imported memory object as shown below. The offset, dimensions, format and number of mip levels must match that specified when creating the mapping using the corresponding Direct3D 11 API. Additionally, if the mipmapped array can be bound as a render target in Direct3D 12, the flag cudaArrayColorAttachment must be set. All mapped mipmapped arrays must be freed using cudaFreeMipmappedArray(). The following code sample shows how to convert Direct3D 11 parameters into the corresponding CUDA parameters when mapping mipmapped arrays onto imported memory objects.

cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: 'mipmap' must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&mipmap, extMem, &desc);

    return mipmap;
}

cudaChannelFormatDesc getCudaChannelFormatDescForDxgiFormat(DXGI_FORMAT dxgiFormat)
{
    cudaChannelFormatDesc d;
    memset(&d, 0, sizeof(d));
    switch (dxgiFormat) {
    case DXGI_FORMAT_R8_UINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8_SINT:            d.x = 8;  d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8_UINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8_SINT:          d.x = 8;  d.y = 8;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R8G8B8A8_UINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R8G8B8A8_SINT:      d.x = 8;  d.y = 8;  d.z = 8;  d.w = 8;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16_UINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16_SINT:           d.x = 16; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16_UINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16_SINT:        d.x = 16; d.y = 16; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R16G16B16A16_UINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R16G16B16A16_SINT:  d.x = 16; d.y = 16; d.z = 16; d.w = 16; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_UINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32_SINT:           d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32_FLOAT:          d.x = 32; d.y = 0;  d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32_UINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32_SINT:        d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32_FLOAT:       d.x = 32; d.y = 32; d.z = 0;  d.w = 0;  d.f = cudaChannelFormatKindFloat;    break;
    case DXGI_FORMAT_R32G32B32A32_UINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindUnsigned; break;
    case DXGI_FORMAT_R32G32B32A32_SINT:  d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindSigned;   break;
    case DXGI_FORMAT_R32G32B32A32_FLOAT: d.x = 32; d.y = 32; d.z = 32; d.w = 32; d.f = cudaChannelFormatKindFloat;    break;
    default: assert(0);
    }



    return d;
}

cudaExtent getCudaExtentForD3D11Extent(UINT64 width, UINT height, UINT16 depthOrArraySize, D3D12_SRV_DIMENSION d3d11SRVDimension) {
    cudaExtent e = { 0, 0, 0 };

    switch (d3d11SRVDimension) {
    case D3D11_SRV_DIMENSION_TEXTURE1D:        e.width = width; e.height = 0;      e.depth = 0;                break;
    case D3D11_SRV_DIMENSION_TEXTURE2D:        e.width = width; e.height = height; e.depth = 0;                break;
    case D3D11_SRV_DIMENSION_TEXTURE3D:        e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURECUBE:      e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURE1DARRAY:   e.width = width; e.height = 0;      e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURE2DARRAY:   e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    case D3D11_SRV_DIMENSION_TEXTURECUBEARRAY: e.width = width; e.height = height; e.depth = depthOrArraySize; break;
    default: assert(0);
    }
    return e;
}

unsigned int getCudaMipmappedArrayFlagsForD3D12Resource(D3D11_SRV_DIMENSION d3d11SRVDimension, D3D11_BIND_FLAG d3d11BindFlags, bool allowSurfaceLoadStore) {
    unsigned int flags = 0;

    switch (d3d11SRVDimension) {
    case D3D11_SRV_DIMENSION_TEXTURECUBE:      flags |= cudaArrayCubemap;                    break;
    case D3D11_SRV_DIMENSION_TEXTURECUBEARRAY: flags |= cudaArrayCubemap | cudaArrayLayered; break;
    case D3D11_SRV_DIMENSION_TEXTURE1DARRAY:   flags |= cudaArrayLayered;                    break;
    case D3D11_SRV_DIMENSION_TEXTURE2DARRAY:   flags |= cudaArrayLayered;                    break;
    default: break;
    }

    if (d3d11BindFlags & D3D11_BIND_RENDER_TARGET) {
        flags |= cudaArrayColorAttachment;
    }

    if (allowSurfaceLoadStore) {
        flags |= cudaArraySurfaceLoadStore;
    }

    return flags;
}





3.2.16.4.5. Importing Synchronization Objectsï

A shareable Direct3D 11 fence object, created by setting the flag D3D11_FENCE_FLAG_SHARED in the call to ID3D11Device5::CreateFence, can be imported into CUDA using the NT handle associated with that object as shown below. Note that it is the applicationâs responsibility to close the handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying semaphore can be freed.

cudaExternalSemaphore_t importD3D11FenceFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D11Fence;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}


A shareable Direct3D 11 fence object can also be imported using a named handle if one exists as shown below.

cudaExternalSemaphore_t importD3D11FenceFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeD3D11Fence;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&extSem, &desc);

    return extSem;
}


A shareable Direct3D 11 keyed mutex object associated with a shareable Direct3D 11 resource, viz, IDXGIKeyedMutex, created by setting the flag D3D11_RESOURCE_MISC_SHARED_KEYEDMUTEX, can be imported into CUDA using the NT handle associated with that object as shown below. Note that it is the applicationâs responsibility to close the handle when it is not required anymore. The NT handle holds a reference to the resource, so it must be explicitly freed before the underlying semaphore can be freed.

cudaExternalSemaphore_t importD3D11KeyedMutexFromNTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeKeyedMutex;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}


A shareable Direct3D 11 keyed mutex object can also be imported using a named handle if one exists as shown below.

cudaExternalSemaphore_t importD3D11KeyedMutexFromNamedNTHandle(LPCWSTR name) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeKeyedMutex;
    desc.handle.win32.name = (void *)name;

    cudaImportExternalSemaphore(&extSem, &desc);

    return extSem;
}


A shareable Direct3D 11 keyed mutex object can be imported into CUDA using the globally shared D3DKMT handle associated with that object as shown below. Since a globally shared D3DKMT handle does not hold a reference to the underlying memory it is automatically destroyed when all other references to the resource are destroyed.

cudaExternalSemaphore_t importD3D11FenceFromKMTHandle(HANDLE handle) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeKeyedMutexKmt;
    desc.handle.win32.handle = handle;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Input parameter 'handle' should be closed if it's not needed anymore
    CloseHandle(handle);

    return extSem;
}





3.2.16.4.6. Signaling/Waiting on Imported Synchronization Objectsï

An imported Direct3D 11 fence object can be signaled as shown below. Signaling such a fence object sets its value to the one specified. The corresponding wait that waits on this signal must be issued in Direct3D 11. Additionally, the wait that waits on this signal must be issued after this signal has been issued.

void signalExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaSignalExternalSemaphoresAsync(&extSem, &params, 1, stream);
}


An imported Direct3D 11 fence object can be waited on as shown below. Waiting on such a fence object waits until its value becomes greater than or equal to the specified value. The corresponding signal that this wait is waiting on must be issued in Direct3D 11. Additionally, the signal must be issued before this wait can be issued.

void waitExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long value, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&params, 0, sizeof(params));

    params.params.fence.value = value;

    cudaWaitExternalSemaphoresAsync(&extSem, &params, 1, stream);
}


An imported Direct3D 11 keyed mutex object can be signaled as shown below. Signaling such a keyed mutex object by specifying a key value releases the keyed mutex for that value. The corresponding wait that waits on this signal must be issued in Direct3D 11 with the same key value. Additionally, the Direct3D 11 wait must be issued after this signal has been issued.

void signalExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long key, cudaStream_t stream) {
    cudaExternalSemaphoreSignalParams params = {};

    memset(&params, 0, sizeof(params));

    params.params.keyedmutex.key = key;

    cudaSignalExternalSemaphoresAsync(&extSem, &params, 1, stream);
}


An imported Direct3D 11 keyed mutex object can be waited on as shown below. A timeout value in milliseconds is needed when waiting on such a keyed mutex. The wait operation waits until the keyed mutex value is equal to the specified key value or until the timeout has elapsed. The timeout interval can also be an infinite value. In case an infinite value is specified the timeout never elapses. The windows INFINITE macro must be used to specify an infinite timeout. The corresponding signal that this wait is waiting on must be issued in Direct3D 11. Additionally, the Direct3D 11 signal must be issued before this wait can be issued.

void waitExternalSemaphore(cudaExternalSemaphore_t extSem, unsigned long long key, unsigned int timeoutMs, cudaStream_t stream) {
    cudaExternalSemaphoreWaitParams params = {};

    memset(&params, 0, sizeof(params));

    params.params.keyedmutex.key = key;
    params.params.keyedmutex.timeoutMs = timeoutMs;

    cudaWaitExternalSemaphoresAsync(&extSem, &params, 1, stream);
}






3.2.16.5. NVIDIA Software Communication Interface Interoperability (NVSCI)ï

NvSciBuf and NvSciSync are interfaces developed for serving the following purposes:

NvSciBuf: Allows applications to allocate and exchange buffers in memory
NvSciSync: Allows applications to manage synchronization objects at operation boundaries

More details on these interfaces are available at: https://docs.nvidia.com/drive.


3.2.16.5.1. Importing Memory Objectsï

For allocating an NvSciBuf object compatible with a given CUDA device, the corresponding GPU id must be set with NvSciBufGeneralAttrKey_GpuId in the NvSciBuf attribute list as shown below. Optionally, applications can specify the following attributes -

NvSciBufGeneralAttrKey_NeedCpuAccess: Specifies if CPU access is required for the buffer
NvSciBufRawBufferAttrKey_Align: Specifies the alignment requirement of NvSciBufType_RawBuffer
NvSciBufGeneralAttrKey_RequiredPerm: Different access permissions can be configured for different UMDs per NvSciBuf memory object instance. For example, to provide the GPU with read-only access permissions to the buffer, create a duplicate NvSciBuf object using NvSciBufObjDupWithReducePerm() with NvSciBufAccessPerm_Readonly as the input parameter. Then import this newly created duplicate object with reduced permission into CUDA as shown
NvSciBufGeneralAttrKey_EnableGpuCache: To control GPU L2 cacheability
NvSciBufGeneralAttrKey_EnableGpuCompression: To specify GPU compression


Note
For more details on these attributes and their valid input options, refer to NvSciBuf Documentation.

The following code snippet illustrates their sample usage.

NvSciBufObj createNvSciBufObject() {
   // Raw Buffer Attributes for CUDA
    NvSciBufType bufType = NvSciBufType_RawBuffer;
    uint64_t rawsize = SIZE;
    uint64_t align = 0;
    bool cpuaccess_flag = true;
    NvSciBufAttrValAccessPerm perm = NvSciBufAccessPerm_ReadWrite;

    NvSciRmGpuId gpuid[] ={};
    CUuuid uuid;
    cuDeviceGetUuid(&uuid, dev));

    memcpy(&gpuid[0].bytes, &uuid.bytes, sizeof(uuid.bytes));
    // Disable cache on dev
    NvSciBufAttrValGpuCache gpuCache[] = {{gpuid[0], false}};
    NvSciBufAttrValGpuCompression gpuCompression[] = {{gpuid[0], NvSciBufCompressionType_GenericCompressible}};
    // Fill in values
    NvSciBufAttrKeyValuePair rawbuffattrs[] = {
         { NvSciBufGeneralAttrKey_Types, &bufType, sizeof(bufType) },
         { NvSciBufRawBufferAttrKey_Size, &rawsize, sizeof(rawsize) },
         { NvSciBufRawBufferAttrKey_Align, &align, sizeof(align) },
         { NvSciBufGeneralAttrKey_NeedCpuAccess, &cpuaccess_flag, sizeof(cpuaccess_flag) },
         { NvSciBufGeneralAttrKey_RequiredPerm, &perm, sizeof(perm) },
         { NvSciBufGeneralAttrKey_GpuId, &gpuid, sizeof(gpuid) },
         { NvSciBufGeneralAttrKey_EnableGpuCache &gpuCache, sizeof(gpuCache) },
         { NvSciBufGeneralAttrKey_EnableGpuCompression &gpuCompression, sizeof(gpuCompression) }
    };

    // Create list by setting attributes
    err = NvSciBufAttrListSetAttrs(attrListBuffer, rawbuffattrs,
            sizeof(rawbuffattrs)/sizeof(NvSciBufAttrKeyValuePair));

    NvSciBufAttrListCreate(NvSciBufModule, &attrListBuffer);

    // Reconcile And Allocate
    NvSciBufAttrListReconcile(&attrListBuffer, 1, &attrListReconciledBuffer,
                       &attrListConflictBuffer)
    NvSciBufObjAlloc(attrListReconciledBuffer, &bufferObjRaw);
    return bufferObjRaw;
}



NvSciBufObj bufferObjRo; // Readonly NvSciBuf memory obj
// Create a duplicate handle to the same memory buffer with reduced permissions
NvSciBufObjDupWithReducePerm(bufferObjRaw, NvSciBufAccessPerm_Readonly, &bufferObjRo);
return bufferObjRo;


The allocated NvSciBuf memory object can be imported in CUDA using the NvSciBufObj handle as shown below. Application should query the allocated NvSciBufObj for attributes required for filling CUDA External Memory Descriptor. Note that the attribute list and NvSciBuf objects should be maintained by the application. If the NvSciBuf object imported into CUDA is also mapped by other drivers, then based on NvSciBufGeneralAttrKey_GpuSwNeedCacheCoherency output attribute value the application must use NvSciSync objects (Refer Importing Synchronization Objects) as appropriate barriers to maintain coherence between CUDA and the other drivers.

Note
For more details on how to allocate and maintain NvSciBuf objects refer to NvSciBuf API Documentation.


cudaExternalMemory_t importNvSciBufObject (NvSciBufObj bufferObjRaw) {

    /*************** Query NvSciBuf Object **************/
    NvSciBufAttrKeyValuePair bufattrs[] = {
                { NvSciBufRawBufferAttrKey_Size, NULL, 0 },
                { NvSciBufGeneralAttrKey_GpuSwNeedCacheCoherency, NULL, 0 },
                { NvSciBufGeneralAttrKey_EnableGpuCompression, NULL, 0 }
    };
    NvSciBufAttrListGetAttrs(retList, bufattrs,
        sizeof(bufattrs)/sizeof(NvSciBufAttrKeyValuePair)));
                ret_size = *(static_cast<const uint64_t*>(bufattrs[0].value));

    // Note cache and compression are per GPU attributes, so read values for specific gpu by comparing UUID
    // Read cacheability granted by NvSciBuf
    int numGpus = bufattrs[1].len / sizeof(NvSciBufAttrValGpuCache);
    NvSciBufAttrValGpuCache[] cacheVal = (NvSciBufAttrValGpuCache *)bufattrs[1].value;
    bool ret_cacheVal;
    for (int i = 0; i < numGpus; i++) {
        if (memcmp(gpuid[0].bytes, cacheVal[i].gpuId.bytes, sizeof(CUuuid)) == 0) {
            ret_cacheVal = cacheVal[i].cacheability);
        }
    }

    // Read compression granted by NvSciBuf
    numGpus = bufattrs[2].len / sizeof(NvSciBufAttrValGpuCompression);
    NvSciBufAttrValGpuCompression[] compVal = (NvSciBufAttrValGpuCompression *)bufattrs[2].value;
    NvSciBufCompressionType ret_compVal;
    for (int i = 0; i < numGpus; i++) {
        if (memcmp(gpuid[0].bytes, compVal[i].gpuId.bytes, sizeof(CUuuid)) == 0) {
            ret_compVal = compVal[i].compressionType);
        }
    }

    /*************** NvSciBuf Registration With CUDA **************/

    // Fill up CUDA_EXTERNAL_MEMORY_HANDLE_DESC
    cudaExternalMemoryHandleDesc memHandleDesc;
    memset(&memHandleDesc, 0, sizeof(memHandleDesc));
    memHandleDesc.type = cudaExternalMemoryHandleTypeNvSciBuf;
    memHandleDesc.handle.nvSciBufObject = bufferObjRaw;
    // Set the NvSciBuf object with required access permissions in this step
    memHandleDesc.handle.nvSciBufObject = bufferObjRo;
    memHandleDesc.size = ret_size;
    cudaImportExternalMemory(&extMemBuffer, &memHandleDesc);
    return extMemBuffer;
 }





3.2.16.5.2. Mapping Buffers onto Imported Memory Objectsï

A device pointer can be mapped onto an imported memory object as shown below. The offset and size of the mapping can be filled as per the attributes of the allocated NvSciBufObj. All mapped device pointers must be freed using cudaFree().

void * mapBufferOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, unsigned long long size) {
    void *ptr = NULL;
    cudaExternalMemoryBufferDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.size = size;

    cudaExternalMemoryGetMappedBuffer(&ptr, extMem, &desc);

    // Note: 'ptr' must eventually be freed using cudaFree()
    return ptr;
}





3.2.16.5.3. Mapping Mipmapped Arrays onto Imported Memory Objectsï

A CUDA mipmapped array can be mapped onto an imported memory object as shown below. The offset, dimensions and format can be filled as per the attributes of the allocated NvSciBufObj. All mapped mipmapped arrays must be freed using cudaFreeMipmappedArray(). The following code sample shows how to convert NvSciBuf attributes into the corresponding CUDA parameters when mapping mipmapped arrays onto imported memory objects.

Note
The number of mip levels must be 1.


cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory(cudaExternalMemory_t extMem, unsigned long long offset, cudaChannelFormatDesc *formatDesc, cudaExtent *extent, unsigned int flags, unsigned int numLevels) {
    cudaMipmappedArray_t mipmap = NULL;
    cudaExternalMemoryMipmappedArrayDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.offset = offset;
    desc.formatDesc = *formatDesc;
    desc.extent = *extent;
    desc.flags = flags;
    desc.numLevels = numLevels;

    // Note: 'mipmap' must eventually be freed using cudaFreeMipmappedArray()
    cudaExternalMemoryGetMappedMipmappedArray(&mipmap, extMem, &desc);

    return mipmap;
}





3.2.16.5.4. Importing Synchronization Objectsï

NvSciSync attributes that are compatible with a given CUDA device can be generated using cudaDeviceGetNvSciSyncAttributes(). The returned attribute list can be used to create a NvSciSyncObj that is guaranteed compatibility with a given CUDA device.

NvSciSyncObj createNvSciSyncObject() {
    NvSciSyncObj nvSciSyncObj
    int cudaDev0 = 0;
    int cudaDev1 = 1;
    NvSciSyncAttrList signalerAttrList = NULL;
    NvSciSyncAttrList waiterAttrList = NULL;
    NvSciSyncAttrList reconciledList = NULL;
    NvSciSyncAttrList newConflictList = NULL;

    NvSciSyncAttrListCreate(module, &signalerAttrList);
    NvSciSyncAttrListCreate(module, &waiterAttrList);
    NvSciSyncAttrList unreconciledList[2] = {NULL, NULL};
    unreconciledList[0] = signalerAttrList;
    unreconciledList[1] = waiterAttrList;

    cudaDeviceGetNvSciSyncAttributes(signalerAttrList, cudaDev0, CUDA_NVSCISYNC_ATTR_SIGNAL);
    cudaDeviceGetNvSciSyncAttributes(waiterAttrList, cudaDev1, CUDA_NVSCISYNC_ATTR_WAIT);

    NvSciSyncAttrListReconcile(unreconciledList, 2, &reconciledList, &newConflictList);

    NvSciSyncObjAlloc(reconciledList, &nvSciSyncObj);

    return nvSciSyncObj;
}


An NvSciSync object (created as above) can be imported into CUDA using the NvSciSyncObj handle as shown below. Note that ownership of the NvSciSyncObj handle continues to lie with the application even after it is imported.

cudaExternalSemaphore_t importNvSciSyncObject(void* nvSciSyncObj) {
    cudaExternalSemaphore_t extSem = NULL;
    cudaExternalSemaphoreHandleDesc desc = {};

    memset(&desc, 0, sizeof(desc));

    desc.type = cudaExternalSemaphoreHandleTypeNvSciSync;
    desc.handle.nvSciSyncObj = nvSciSyncObj;

    cudaImportExternalSemaphore(&extSem, &desc);

    // Deleting/Freeing the nvSciSyncObj beyond this point will lead to undefined behavior in CUDA

    return extSem;
}





3.2.16.5.5. Signaling/Waiting on Imported Synchronization Objectsï

An imported NvSciSyncObj object can be signaled as outlined below. Signaling NvSciSync backed semaphore object initializes the fence parameter passed as input. This fence parameter is waited upon by a wait operation that corresponds to the aforementioned signal. Additionally, the wait that waits on this signal must be issued after this signal has been issued. If the flags are set to cudaExternalSemaphoreSignalSkipNvSciBufMemSync then memory synchronization operations (over all the imported NvSciBuf in this process) that are executed as a part of the signal operation by default are skipped. When NvsciBufGeneralAttrKey_GpuSwNeedCacheCoherency is FALSE, this flag should be set.

void signalExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream, void *fence) {
    cudaExternalSemaphoreSignalParams signalParams = {};

    memset(&signalParams, 0, sizeof(signalParams));

    signalParams.params.nvSciSync.fence = (void*)fence;
    signalParams.flags = 0; //OR cudaExternalSemaphoreSignalSkipNvSciBufMemSync

    cudaSignalExternalSemaphoresAsync(&extSem, &signalParams, 1, stream);

}


An imported NvSciSyncObj object can be waited upon as outlined below. Waiting on NvSciSync backed semaphore object waits until the input fence parameter is signaled by the corresponding signaler. Additionally, the signal must be issued before the wait can be issued. If the flags are set to cudaExternalSemaphoreWaitSkipNvSciBufMemSync then memory synchronization operations (over all the imported NvSciBuf in this process) that are executed as a part of the signal operation by default are skipped. When NvsciBufGeneralAttrKey_GpuSwNeedCacheCoherency is FALSE, this flag should be set.

void waitExternalSemaphore(cudaExternalSemaphore_t extSem, cudaStream_t stream, void *fence) {
     cudaExternalSemaphoreWaitParams waitParams = {};

    memset(&waitParams, 0, sizeof(waitParams));

    waitParams.params.nvSciSync.fence = (void*)fence;
    waitParams.flags = 0; //OR cudaExternalSemaphoreWaitSkipNvSciBufMemSync

    cudaWaitExternalSemaphoresAsync(&extSem, &waitParams, 1, stream);
}








3.3. Versioning and Compatibilityï

There are two version numbers that developers should care about when developing a CUDA application: The compute capability that describes the general specifications and features of the compute device (see Compute Capability) and the version of the CUDA driver API that describes the features supported by the driver API and runtime.
The version of the driver API is defined in the driver header file as CUDA_VERSION. It allows developers to check whether their application requires a newer device driver than the one currently installed. This is important, because the driver API is backward compatible, meaning that applications, plug-ins, and libraries (including the CUDA runtime) compiled against a particular version of the driver API will continue to work on subsequent device driver releases as illustrated in Figure 12. The driver API is not forward compatible, which means that applications, plug-ins, and libraries (including the CUDA runtime) compiled against a particular version of the driver API will not work on previous versions of the device driver.
It is important to note that there are limitations on the mixing and matching of versions that is supported:

Since only one version of the CUDA Driver can be installed at a time on a system, the installed driver must be of the same or higher version than the maximum Driver API version against which any application, plug-ins, or libraries that must run on that system were built.
All plug-ins and libraries used by an application must use the same version of the CUDA Runtime unless they statically link to the Runtime, in which case multiple versions of the runtime can coexist in the same process space. Note that if nvcc is used to link the application, the static version of the CUDA Runtime library will be used by default, and all CUDA Toolkit libraries are statically linked against the CUDA Runtime.
All plug-ins and libraries used by an application must use the same version of any libraries that use the runtime (such as cuFFT, cuBLAS, â¦) unless statically linking to those libraries.




Figure 25 The Driver API Is Backward but Not Forward Compatibleï


For Tesla GPU products, CUDA 10 introduced a new forward-compatible upgrade path for the user-mode components of the CUDA Driver. This feature is described in CUDA Compatibility. The requirements on the CUDA Driver version described here apply to the version of the user-mode components.



3.4. Compute Modesï

On Tesla solutions running Windows Server 2008 and later or Linux, one can set any device in a system in one of the three following modes using NVIDIAâs System Management Interface (nvidia-smi), which is a tool distributed as part of the driver:

Default compute mode: Multiple host threads can use the device (by calling cudaSetDevice() on this device, when using the runtime API, or by making current a context associated to the device, when using the driver API) at the same time.
Exclusive-process compute mode: Only one CUDA context may be created on the device across all processes in the system. The context may be current to as many threads as desired within the process that created that context.
Prohibited compute mode: No CUDA context can be created on the device.

This means, in particular, that a host thread using the runtime API without explicitly calling cudaSetDevice() might be associated with a device other than device 0 if device 0 turns out to be in prohibited mode or in exclusive-process mode and used by another process. cudaSetValidDevices() can be used to set a device from a prioritized list of devices.
Note also that, for devices featuring the Pascal architecture onwards (compute capability with major revision number 6 and higher), there exists support for Compute Preemption. This allows compute tasks to be preempted at instruction-level granularity, rather than thread block granularity as in prior Maxwell and Kepler GPU architecture, with the benefit that applications with long-running kernels can be prevented from either monopolizing the system or timing out. However, there will be context switch overheads associated with Compute Preemption, which is automatically enabled on those devices for which support exists. The individual attribute query function cudaDeviceGetAttribute() with the attribute cudaDevAttrComputePreemptionSupported can be used to determine if the device in use supports Compute Preemption. Users wishing to avoid context switch overheads associated with different processes can ensure that only one process is active on the GPU by selecting exclusive-process mode.
Applications may query the compute mode of a device by checking the computeMode device property (see Device Enumeration).



3.5. Mode Switchesï

GPUs that have a display output dedicate some DRAM memory to the so-called primary surface, which is used to refresh the display device whose output is viewed by the user. When users initiate a mode switch of the display by changing the resolution or bit depth of the display (using NVIDIA control panel or the Display control panel on Windows), the amount of memory needed for the primary surface changes. For example, if the user changes the display resolution from 1280x1024x32-bit to 1600x1200x32-bit, the system must dedicate 7.68 MB to the primary surface rather than 5.24 MB. (Full-screen graphics applications running with anti-aliasing enabled may require much more display memory for the primary surface.) On Windows, other events that may initiate display mode switches include launching a full-screen DirectX application, hitting Alt+Tab to task switch away from a full-screen DirectX application, or hitting Ctrl+Alt+Del to lock the computer.
If a mode switch increases the amount of memory needed for the primary surface, the system may have to cannibalize memory allocations dedicated to CUDA applications. Therefore, a mode switch results in any call to the CUDA runtime to fail and return an invalid context error.



3.6. Tesla Compute Cluster Mode for Windowsï

Using NVIDIAâs System Management Interface (nvidia-smi), the Windows device driver can be put in TCC (Tesla Compute Cluster) mode for devices of the Tesla and Quadro Series.
TCC mode removes support for any graphics functionality.




4. Hardware Implementationï

The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs). When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.
A multiprocessor is designed to execute hundreds of threads concurrently. To manage such a large number of threads, it employs a unique architecture called SIMT (Single-Instruction, Multiple-Thread) that is described in SIMT Architecture. The instructions are pipelined, leveraging instruction-level parallelism within a single thread, as well as extensive thread-level parallelism through simultaneous hardware multithreading as detailed in Hardware Multithreading. Unlike CPU cores, they are issued in order and there is no branch prediction or speculative execution.
SIMT Architecture and Hardware Multithreading describe the architecture features of the streaming multiprocessor that are common to all devices. Compute Capability 5.x, Compute Capability 6.x, and Compute Capability 7.x provide the specifics for devices of compute capabilities 5.x, 6.x, and 7.x respectively.
The NVIDIA GPU architecture uses a little-endian representation.


4.1. SIMT Architectureï

The multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel threads called warps. Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently. The term warp originates from weaving, the first parallel thread technology. A half-warp is either the first or second half of a warp. A quarter-warp is either the first, second, third, or fourth quarter of a warp.
When a multiprocessor is given one or more thread blocks to execute, it partitions them into warps and each warp gets scheduled by a warp scheduler for execution. The way a block is partitioned into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0. Thread Hierarchy describes how thread IDs relate to thread indices in the block.
A warp executes one common instruction at a time, so full efficiency is realized when all 32 threads of a warp agree on their execution path. If threads of a warp diverge via a data-dependent conditional branch, the warp executes each branch path taken, disabling threads that are not on that path. Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjoint code paths.
The SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements. A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads. For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance. Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually.
Prior to NVIDIA Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp. As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from.
Starting with the NVIDIA Volta architecture, Independent Thread Scheduling allows full concurrency between threads, regardless of warp. With Independent Thread Scheduling, the GPU maintains execution state per thread, including a program counter and call stack, and can yield execution at a per-thread granularity, either to make better use of execution resources or to allow one thread to wait for data to be produced by another. A schedule optimizer determines how to group active threads from the same warp together into SIMT units. This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity.
Independent Thread Scheduling can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity2 of previous hardware architectures. In particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions) should be revisited to ensure compatibility with NVIDIA Volta and beyond. See Compute Capability 7.x for further details.

Note
The threads of a warp that are participating in the current instruction are called the active threads, whereas threads not on the current instruction are inactive (disabled). Threads can be inactive for a variety of reasons including having exited earlier than other threads of their warp, having taken a different branch path than the branch path currently executed by the warp, or being the last threads of a block whose number of threads is not a multiple of the warp size.
If a non-atomic instruction executed by a warp writes to the same location in global or shared memory for more than one of the threads of the warp, the number of serialized writes that occur to that location varies depending on the compute capability of the device (see Compute Capability 5.x, Compute Capability 6.x, and Compute Capability 7.x), and which thread performs the final write is undefined.
If an atomic instruction executed by a warp reads, modifies, and writes to the same location in global memory for more than one of the threads of the warp, each read/modify/write to that location occurs and they are all serialized, but the order in which they occur is undefined.




4.2. Hardware Multithreadingï

The execution context (program counters, registers, and so on) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one execution context to another has no cost, and at every instruction issue time, a warp scheduler selects a warp that has threads ready to execute its next instruction (the active threads of the warp) and issues the instruction to those threads.
In particular, each multiprocessor has a set of 32-bit registers that are partitioned among the warps, and a parallel data cache or shared memory that is partitioned among the thread blocks.
The number of blocks and warps that can reside and be processed together on the multiprocessor for a given kernel depends on the amount of registers and shared memory used by the kernel and the amount of registers and shared memory available on the multiprocessor. There are also a maximum number of resident blocks and a maximum number of resident warps per multiprocessor. These limits as well the amount of registers and shared memory available on the multiprocessor are a function of the compute capability of the device and are given in Compute Capabilities. If there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel will fail to launch.
The total number of warps in a block is as follows:
\(\text{ceil}\left( \frac{T}{W_{size}},1 \right)\)

T is the number of threads per block,
Wsize is the warp size, which is equal to 32,
ceil(x, y) is equal to x rounded up to the nearest multiple of y.

The total number of registers and total amount of shared memory allocated for a block are documented in the CUDA Occupancy Calculator provided in the CUDA Toolkit.

2

The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction.






5. Performance Guidelinesï



5.1. Overall Performance Optimization Strategiesï

Performance optimization revolves around four basic strategies:

Maximize parallel execution to achieve maximum utilization;
Optimize memory usage to achieve maximum memory throughput;
Optimize instruction usage to achieve maximum instruction throughput;
Minimize memory thrashing.

Which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion; optimizing instruction usage of a kernel that is mostly limited by memory accesses will not yield any significant performance gain, for example. Optimization efforts should therefore be constantly directed by measuring and monitoring the performance limiters, for example using the CUDA profiler. Also, comparing the floating-point operation throughput or memory throughputâwhichever makes more senseâof a particular kernel to the corresponding peak theoretical throughput of the device indicates how much room for improvement there is for the kernel.



5.2. Maximize Utilizationï

To maximize utilization the application should be structured in a way that it exposes as much parallelism as possible and efficiently maps this parallelism to the various components of the system to keep them busy most of the time.


5.2.1. Application Levelï

At a high level, the application should maximize parallel execution between the host, the devices, and the bus connecting the host to the devices, by using asynchronous functions calls and streams as described in Asynchronous Concurrent Execution. It should assign to each processor the type of work it does best: serial workloads to the host; parallel workloads to the devices.
For the parallel workloads, at points in the algorithm where parallelism is broken because some threads need to synchronize in order to share data with each other, there are two cases: Either these threads belong to the same block, in which case they should use __syncthreads() and share data through shared memory within the same kernel invocation, or they belong to different blocks, in which case they must share data through global memory using two separate kernel invocations, one for writing to and one for reading from global memory. The second case is much less optimal since it adds the overhead of extra kernel invocations and global memory traffic. Its occurrence should therefore be minimized by mapping the algorithm to the CUDA programming model in such a way that the computations that require inter-thread communication are performed within a single thread block as much as possible.



5.2.2. Device Levelï

At a lower level, the application should maximize parallel execution between the multiprocessors of a device.
Multiple kernels can execute concurrently on a device, so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently as described in Asynchronous Concurrent Execution.



5.2.3. Multiprocessor Levelï

At an even lower level, the application should maximize parallel execution between the various functional units within a multiprocessor.
As described in Hardware Multithreading, a GPU multiprocessor primarily relies on thread-level parallelism to maximize utilization of its functional units. Utilization is therefore directly linked to the number of resident warps. At every instruction issue time, a warp scheduler selects an instruction that is ready to execute. This instruction can be another independent instruction of the same warp, exploiting instruction-level parallelism, or more commonly an instruction of another warp, exploiting thread-level parallelism. If a ready to execute instruction is selected it is issued to the active threads of the warp. The number of clock cycles it takes for a warp to be ready to execute its next instruction is called the latency, and full utilization is achieved when all warp schedulers always have some instruction to issue for some warp at every clock cycle during that latency period, or in other words, when latency is completely âhiddenâ. The number of instructions required to hide a latency of L clock cycles depends on the respective throughputs of these instructions (see Arithmetic Instructions for the throughputs of various arithmetic instructions). If we assume instructions with maximum throughput, it is equal to:

4L for devices of compute capability 5.x, 6.1, 6.2, 7.x and 8.x since for these devices, a multiprocessor issues one instruction per warp over one clock cycle for four warps at a time, as mentioned in Compute Capabilities.
2L for devices of compute capability 6.0 since for these devices, the two instructions issued every cycle are one instruction for two different warps.

The most common reason a warp is not ready to execute its next instruction is that the instructionâs input operands are not available yet.
If all input operands are registers, latency is caused by register dependencies, i.e., some of the input operands are written by some previous instruction(s) whose execution has not completed yet. In this case, the latency is equal to the execution time of the previous instruction and the warp schedulers must schedule instructions of other warps during that time. Execution time varies depending on the instruction. On devices of compute capability 7.x, for most arithmetic instructions, it is typically 4 clock cycles. This means that 16 active warps per multiprocessor (4 cycles, 4 warp schedulers) are required to hide arithmetic instruction latencies (assuming that warps execute instructions with maximum throughput, otherwise fewer warps are needed). If the individual warps exhibit instruction-level parallelism, i.e. have multiple independent instructions in their instruction stream, fewer warps are needed because multiple independent instructions from a single warp can be issued back to back.
If some input operand resides in off-chip memory, the latency is much higher: typically hundreds of clock cycles. The number of warps required to keep the warp schedulers busy during such high latency periods depends on the kernel code and its degree of instruction-level parallelism. In general, more warps are required if the ratio of the number of instructions with no off-chip memory operands (i.e., arithmetic instructions most of the time) to the number of instructions with off-chip memory operands is low (this ratio is commonly called the arithmetic intensity of the program).
Another reason a warp is not ready to execute its next instruction is that it is waiting at some memory fence (Memory Fence Functions) or synchronization point (Synchronization Functions). A synchronization point can force the multiprocessor to idle as more and more warps wait for other warps in the same block to complete execution of instructions prior to the synchronization point. Having multiple resident blocks per multiprocessor can help reduce idling in this case, as warps from different blocks do not need to wait for each other at synchronization points.
The number of blocks and warps residing on each multiprocessor for a given kernel call depends on the execution configuration of the call (Execution Configuration), the memory resources of the multiprocessor, and the resource requirements of the kernel as described in Hardware Multithreading. Register and shared memory usage are reported by the compiler when compiling with the --ptxas-options=-v option.
The total amount of shared memory required for a block is equal to the sum of the amount of statically allocated shared memory and the amount of dynamically allocated shared memory.
The number of registers used by a kernel can have a significant impact on the number of resident warps. For example, for devices of compute capability 6.x, if a kernel uses 64
registers and each block has 512 threads and requires very little shared memory, then two blocks (i.e., 32 warps) can reside on the multiprocessor since they require 2x512x64
registers, which exactly matches the number of registers available on the multiprocessor. But as soon as the kernel uses one more register, only one block (i.e., 16 warps) can be
resident since two blocks would require 2x512x65 registers, which are more registers than are available on the multiprocessor. Therefore, the compiler attempts to minimize register
usage while keeping register spilling (see Device Memory Accesses) and the number of instructions to a minimum. Register usage can be
controlled using the maxrregcount compiler option, the __launch_bounds__() qualifier as described in Launch Bounds, or the __maxnreg__()
qualifier as described in Maximum Number of Registers per Thread.
The register file is organized as 32-bit registers. So, each variable stored in a register needs at least one 32-bit register, for example, a double variable uses two 32-bit registers.
The effect of execution configuration on performance for a given kernel call generally depends on the kernel code. Experimentation is therefore recommended. Applications can also parametrize execution configurations based on register file size and shared memory size, which depends on the compute capability of the device, as well as on the number of multiprocessors and memory bandwidth of the device, all of which can be queried using the runtime (see reference manual).
The number of threads per block should be chosen as a multiple of the warp size to avoid wasting computing resources with under-populated warps as much as possible.


5.2.3.1. Occupancy Calculatorï

Several API functions exist to assist programmers in choosing thread block size and cluster size based on register and shared memory requirements.


The occupancy calculator API, cudaOccupancyMaxActiveBlocksPerMultiprocessor, can provide an occupancy prediction based on the block size and shared memory usage of a kernel. This function reports occupancy in terms of the number of concurrent thread blocks per multiprocessor.

Note that this value can be converted to other metrics. Multiplying by the number of warps per block yields the number of concurrent warps per multiprocessor; further dividing concurrent warps by max warps per multiprocessor gives the occupancy as a percentage.


The occupancy-based launch configurator APIs, cudaOccupancyMaxPotentialBlockSize and cudaOccupancyMaxPotentialBlockSizeVariableSMem, heuristically calculate an execution configuration that achieves the maximum multiprocessor-level occupancy.
The occupancy calculator API, cudaOccupancyMaxActiveClusters, can provided occupancy prediction based on the cluster size, block size and shared memory usage of a kernel. This function reports occupancy in terms of number of max active clusters of a given size on the GPU present in the system.

The following code sample calculates the occupancy of MyKernel. It then reports the occupancy level with the ratio between concurrent warps versus maximum warps per multiprocessor.

// Device code
__global__ void MyKernel(int *d, int *a, int *b)
{
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    d[idx] = a[idx] * b[idx];
}

// Host code
int main()
{
    int numBlocks;        // Occupancy in terms of active blocks
    int blockSize = 32;

    // These variables are used to convert occupancy to warps
    int device;
    cudaDeviceProp prop;
    int activeWarps;
    int maxWarps;

    cudaGetDevice(&device);
    cudaGetDeviceProperties(&prop, device);

    cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &numBlocks,
        MyKernel,
        blockSize,
        0);

    activeWarps = numBlocks * blockSize / prop.warpSize;
    maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;

    std::cout << "Occupancy: " << (double)activeWarps / maxWarps * 100 << "%" << std::endl;

    return 0;
}


The following code sample configures an occupancy-based kernel launch of MyKernel according to the user input.

// Device code
__global__ void MyKernel(int *array, int arrayCount)
{
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < arrayCount) {
        array[idx] *= array[idx];
    }
}

// Host code
int launchMyKernel(int *array, int arrayCount)
{
    int blockSize;      // The launch configurator returned block size
    int minGridSize;    // The minimum grid size needed to achieve the
                        // maximum occupancy for a full device
                        // launch
    int gridSize;       // The actual grid size needed, based on input
                        // size

    cudaOccupancyMaxPotentialBlockSize(
        &minGridSize,
        &blockSize,
        (void*)MyKernel,
        0,
        arrayCount);

    // Round up according to array size
    gridSize = (arrayCount + blockSize - 1) / blockSize;

    MyKernel<<<gridSize, blockSize>>>(array, arrayCount);
    cudaDeviceSynchronize();

    // If interested, the occupancy can be calculated with
    // cudaOccupancyMaxActiveBlocksPerMultiprocessor

    return 0;
}


The following code sample shows how to use the cluster occupancy API to find the max number of active clusters of a given size. Example code below calucaltes occupancy for cluster of size 2 and 128 threads per block.
Cluster size of 8 is forward compatible starting compute capability 9.0, except on GPU hardware or MIG configurations which are too small to support 8 multiprocessors in which case the maximum cluster size will be reduced. But it is recommended that the users query the maximum cluster size before launching a cluster kernel. Max cluster size can be queried using cudaOccupancyMaxPotentialClusterSize API.

{
  cudaLaunchConfig_t config = {0};
  config.gridDim = number_of_blocks;
  config.blockDim = 128; // threads_per_block = 128
  config.dynamicSmemBytes = dynamic_shared_memory_size;

  cudaLaunchAttribute attribute[1];
  attribute[0].id = cudaLaunchAttributeClusterDimension;
  attribute[0].val.clusterDim.x = 2; // cluster_size = 2
  attribute[0].val.clusterDim.y = 1;
  attribute[0].val.clusterDim.z = 1;
  config.attrs = attribute;
  config.numAttrs = 1;

  int max_cluster_size = 0;
  cudaOccupancyMaxPotentialClusterSize(&max_cluster_size, (void *)kernel, &config);

  int max_active_clusters = 0;
  cudaOccupancyMaxActiveClusters(&max_active_clusters, (void *)kernel, &config);

  std::cout << "Max Active Clusters of size 2: " << max_active_clusters << std::endl;
}


The CUDA Nsight Compute User Interface also provides a standalone occupancy calculator and launch configurator implementation in <CUDA_Toolkit_Path>/include/cuda_occupancy.h for any use cases that cannot depend on the CUDA software stack. The Nsight Compute version of the occupancy calculator is particularly useful as a learning tool that visualizes the impact of changes to the parameters that affect occupancy (block size, registers per thread, and shared memory per thread).





5.3. Maximize Memory Throughputï

The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth.
That means minimizing data transfers between the host and the device, as detailed in Data Transfer between Host and Device, since these have much lower bandwidth than data transfers between global memory and the device.
That also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i.e., L1 cache and L2 cache available on devices of compute capability 2.x and higher, texture cache and constant cache available on all devices).
Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it. As illustrated in CUDA Runtime, a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each thread of a block:

Load data from device memory to shared memory,
Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were populated by different threads,
Process the data in shared memory,
Synchronize again if necessary to make sure that shared memory has been updated with the results,
Write the results back to device memory.

For some applications (for example, for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality. As mentioned in Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0, for devices of compute capability 7.x, 8.x and 9.0, the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus shared memory is configurable for each kernel call.
The throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory. The next step in maximizing memory throughput is therefore to organize memory accesses as optimally as possible based on the optimal memory access patterns described in Device Memory Accesses. This optimization is especially important for global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact on performance.


5.3.1. Data Transfer between Host and Deviceï

Applications should strive to minimize data transfer between the host and the device. One way to accomplish this is to move more code from the host to the device, even if that means running kernels that do not expose enough parallelism to execute on the device with full efficiency. Intermediate data structures may be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.
Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately.
On systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked host memory as described in Page-Locked Host Memory.
In addition, when using mapped page-locked memory (Mapped Memory), there is no need to allocate any device memory and explicitly copy data between device and host memory. Data transfers are implicitly performed each time the kernel accesses the mapped memory. For maximum performance, these memory accesses must be coalesced as with accesses to global memory (see Device Memory Accesses). Assuming that they are and that the mapped memory is read or written only once, using mapped page-locked memory instead of explicit copies between device and host memory can be a win for performance.
On integrated systems where device memory and host memory are physically the same, any copy between host and device memory is superfluous and mapped page-locked memory should be used instead. Applications may query a device is integrated by checking that the integrated device property (see Device Enumeration) is equal to 1.



5.3.2. Device Memory Accessesï

An instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture memory) might need to be re-issued multiple times depending on the distribution of the memory addresses across the threads within the warp. How the distribution affects the instruction throughput this way is specific to each type of memory and described in the following sections. For example, for global memory, as a general rule, the more scattered the addresses are, the more reduced the throughput is.
Global Memory
Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions. These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions.
When a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads. In general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly. For example, if a 32-byte memory transaction is generated for each threadâs 4-byte access, throughput is divided by 8.
How many transactions are necessary and how much throughput is ultimately affected varies with the compute capability of the device. Compute Capability 5.x, Compute Capability 6.x, Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0 give more details on how global memory accesses are handled for various compute capabilities.
To maximize global memory throughput, it is therefore important to maximize coalescing by:

Following the most optimal access patterns based on Compute Capability 5.x, Compute Capability 6.x, Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0
Using data types that meet the size and alignment requirement detailed in the section Size and Alignment Requirement below,
Padding data in some cases, for example, when accessing a two-dimensional array as described in the section Two-Dimensional Arrays below.

Size and Alignment Requirement
Global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes. Any access (via a variable or a pointer) to data residing in global memory compiles to a single global memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is naturally aligned (i.e., its address is a multiple of that size).
If this size and alignment requirement is not fulfilled, the access compiles to multiple instructions with interleaved access patterns that prevent these instructions from fully coalescing. It is therefore recommended to use types that meet this requirement for data that resides in global memory.
The alignment requirement is automatically fulfilled for the Built-in Vector Types.
For structures, the size and alignment requirements can be enforced by the compiler using the alignment specifiers__align__(8) orÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  __align__(16), such as

struct __align__(8) {
    float x;
    float y;
};


or

struct __align__(16) {
    float x;
    float y;
    float z;
};


Any address of a variable residing in global memory or returned by one of the memory allocation routines from the driver or runtime API is always aligned to at least 256 bytes.
Reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words), so special care must be taken to maintain alignment of the starting address of any value or array of values of these types. A typical case where this might be easily overlooked is when using some custom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls to cudaMalloc() or cuMemAlloc()) is replaced by the allocation of a single large block of memory partitioned into multiple arrays, in which case the starting address of each array is offset from the blockâs starting address.
Two-Dimensional Arrays
A common global memory access pattern is when each thread of index (tx,ty) uses the following address to access one element of a 2D array of width width, located at address BaseAddress of type type* (where type meets the requirement described in Maximize Utilization):

BaseAddress + width * ty + tx


For these accesses to be fully coalesced, both the width of the thread block and the width of the array must be a multiple of the warp size.
In particular, this means that an array whose width is not a multiple of this size will be accessed much more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size and its rows padded accordingly. The cudaMallocPitch() and cuMemAllocPitch() functions and associated memory copy functions described in the reference manual enable programmers to write non-hardware-dependent code to allocate arrays that conform to these constraints.
Local Memory
Local memory accesses only occur for some automatic variables as mentioned in Variable Memory Space Specifiers. Automatic variables that the compiler is likely to place in local memory are:

Arrays for which it cannot determine that they are indexed with constant quantities,
Large structures or arrays that would consume too much register space,
Any variable if the kernel uses more registers than available (this is also known as register spilling).

Inspection of the PTX assembly code (obtained by compiling with the -ptx or-keep option) will tell if a variable has been placed in local memory during the first compilation phases as it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics. Even if it has not, subsequent compilation phases might still decide otherwise though if they find it consumes too much register space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if this is the case. Also, the compiler reports total local memory usage per kernel (lmem) when compiling with the --ptxas-options=-v option. Note that some mathematical functions have implementation paths that might access local memory.
The local memory space resides in device memory, so local memory accesses have the same high latency and low bandwidth as global memory accesses and are subject to the same requirements for memory coalescing as described in Device Memory Accesses. Local memory is however organized such that consecutive 32-bit words are accessed by consecutive thread IDs. Accesses are therefore fully coalesced as long as all threads in a warp access the same relative address (for example, same index in an array variable, same member in a structure variable).
On devices of compute capability 5.x onwards, local memory accesses are always cached in L2 in the same way as global memory accesses (see Compute Capability 5.x and Compute Capability 6.x).
Shared Memory
Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory.
To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously. Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is n times as high as the bandwidth of a single module.
However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized. The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests. If the number of separate memory requests is n, the initial memory request is said to cause n-way bank conflicts.
To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts. This is described in Compute Capability 5.x, Compute Capability 6.x, Compute Capability 7.x, Compute Capability 8.x, and Compute Capability 9.0 for devices of compute capability 5.x, 6.x, 7.x, 8.x, and 9.0 respectively.
Constant Memory
The constant memory space resides in device memory and is cached in the constant cache.
A request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests.
The resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise.
Texture and Surface Memory
The texture and surface memory spaces reside in device memory and are cached in texture cache, so a texture fetch or surface read costs one memory read from device memory only on a cache miss, otherwise it just costs one read from texture cache. The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture or surface addresses that are close together in 2D will achieve best performance. Also, it is designed for streaming fetches with a constant latency; a cache hit reduces DRAM bandwidth demand but not fetch latency.
Reading device memory through texture or surface fetching present some benefits that can make it an advantageous alternative to reading device memory from global or constant memory:

If the memory reads do not follow the access patterns that global or constant memory reads must follow to get good performance, higher bandwidth can be achieved providing that there is locality in the texture fetches or surface reads;
Addressing calculations are performed outside the kernel by dedicated units;
Packed data may be broadcast to separate variables in a single operation;
8-bit and 16-bit integer input data may be optionally converted to 32 bit floating-point values in the range [0.0, 1.0] or [-1.0, 1.0] (see Texture Memory).





5.4. Maximize Instruction Throughputï

To maximize instruction throughput the application should:

Minimize the use of arithmetic instructions with low throughput; this includes trading precision for speed when it does not affect the end result, such as using intrinsic instead of regular functions (intrinsic functions are listed in Intrinsic Functions), single-precision instead of double-precision, or flushing denormalized numbers to zero;
Minimize divergent warps caused by control flow instructions as detailed in Control Flow Instructions
Reduce the number of instructions, for example, by optimizing out synchronization points whenever possible as described in Synchronization Instruction or by using restricted pointers as described in __restrict__.

In this section, throughputs are given in number of operations per clock cycle per multiprocessor. For a warp size of 32, one instruction corresponds to 32 operations, so if N is the number of operations per clock cycle, the instruction throughput is N/32 instructions per clock cycle.
All throughputs are for one multiprocessor. They must be multiplied by the number of multiprocessors in the device to get throughput for the whole device.


5.4.1. Arithmetic Instructionsï

The following table gives the throughputs of the arithmetic instructions that are natively supported in hardware for devices of various compute capabilities.


Table 4 Throughput of Native Arithmetic Instructions. (Number of Results per Clock Cycle per Multiprocessor)ï
















Compute Capability
5.0, 5.2
5.3
6.0
6.1
6.2
7.x
8.0
8.6
8.9
9.0




16-bit floating-point add, multiply, multiply-add
N/A
256
128
2
256
128
2563
128
256


32-bit floating-point add, multiply, multiply-add
128
64
128
64
128


64-bit floating-point add, multiply, multiply-add
4
32
4
325
32
2
64


32-bit floating-point reciprocal, reciprocal square root, base-2 logarithm (__log2f), base 2 exponential (exp2f), sine (__sinf), cosine (__cosf)
32
16
32
16


32-bit integer add, extended-precision add, subtract, extended-precision subtract
128
64
128
64


32-bit integer multiply, multiply-add, extended-precision multiply-add
Multiple instruct.
646


24-bit integer multiply (__[u]mul24)
Multiple instruct.


32-bit integer shift
64
32
64


compare, minimum, maximum
64
32
64


32-bit integer bit reverse
64
32
64
16


Bit field extract/insert
64
32
64
Multiple Instruct.
64


32-bit bitwise AND, OR, XOR
128
64
128
64


count of leading zeros, most significant non-sign bit
32
16
32
16


population count
32
16
32
16


warp shuffle
32
328
32


warp reduce
Multiple instruct.
16


warp vote
64


sum of absolute difference
64
32
64


SIMD video instructions vabsdiff2
Multiple instruct.


SIMD video instructions vabsdiff4
Multiple instruct.
64


All other SIMD video instructions
Multiple instruct.


Type conversions from 8-bit and 16-bit integer to 32-bit integer types
32
16
32
64


Type conversions from and to 64-bit types
4
16
4
1610
16
2
2
16


All other type conversions
32
16
32
16


16-bit DPX
Multiple instruct.
128


32-bit DPX
Multiple instruct.
64



Other instructions and functions are implemented on top of the native instructions. The implementation may be different for devices of different compute capabilities, and the number of native instructions after compilation may fluctuate with every compiler version. For complicated functions, there can be multiple code paths depending on input. cuobjdump can be used to inspect a particular implementation in a cubin object.
The implementation of some functions are readily available on the CUDA header files (math_functions.h, device_functions.h, â¦).
In general, code compiled with -ftz=true (denormalized numbers are flushed to zero) tends to have higher performance than code compiled with -ftz=false. Similarly, code compiled with -prec-div=false (less precise division) tends to have higher performance code than code compiled with -prec-div=true, and code compiled with -prec-sqrt=false (less precise square root) tends to have higher performance than code compiled with -prec-sqrt=true. The nvcc user manual describes these compilation flags in more details.
Single-Precision Floating-Point Division
__fdividef(x, y) (see Intrinsic Functions) provides faster single-precision floating-point division than the division operator.
Single-Precision Floating-Point Reciprocal Square Root
To preserve IEEE-754 semantics the compiler can optimize 1.0/sqrtf() into rsqrtf() only when both reciprocal and square root are approximate, (i.e., with -prec-div=false and -prec-sqrt=false). It is therefore recommended to invoke rsqrtf() directly where desired.
Single-Precision Floating-Point Square Root
Single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity.
Sine and Cosine
sinf(x), cosf(x), tanf(x), sincosf(x), and corresponding double-precision instructions are much more expensive and even more so if the argument x is large in magnitude.
More precisely, the argument reduction code (see Mathematical Functions for implementation) comprises two code paths referred to as the fast path and the slow path, respectively.
The fast path is used for arguments sufficiently small in magnitude and essentially consists of a few multiply-add operations. The slow path is used for arguments large in magnitude and consists of lengthy computations required to achieve correct results over the entire argument range.
At present, the argument reduction code for the trigonometric functions selects the fast path for arguments whose magnitude is less than 105615.0f for the single-precision functions, and less than 2147483648.0 for the double-precision functions.
As the slow path requires more registers than the fast path, an attempt has been made to reduce register pressure in the slow path by storing some intermediate variables in local memory, which may affect performance because of local memory high latency and bandwidth (see Device Memory Accesses). At present, 28 bytes of local memory are used by single-precision functions, and 44 bytes are used by double-precision functions. However, the exact amount is subject to change.
Due to the lengthy computations and use of local memory in the slow path, the throughput of these trigonometric functions is lower by one order of magnitude when the slow path reduction is required as opposed to the fast path reduction.
Integer Arithmetic
Integer division and modulo operation are costly as they compile to up to 20 instructions. They can be replaced with bitwise operations in some cases: If n is a power of 2, (i/n) is equivalent to (i>>log2(n)) and (i%n) is equivalent to (i&(n-1)); the compiler will perform these conversions if n is literal.
__brev and __popc map to a single instruction and __brevll and __popcll to a few instructions.
__[u]mul24 are legacy intrinsic functions that no longer have any reason to be used.
Half Precision Arithmetic
In order to achieve good performance for 16-bit precision floating-point add, multiply or multiply-add, it is recommended that the half2 datatype is used for half precision and __nv_bfloat162 be used for __nv_bfloat16 precision. Vector intrinsics (for example, __hadd2, __hsub2, __hmul2, __hfma2) can then be used to do two operations in a single instruction. Using half2 or __nv_bfloat162 in place of two calls using half or __nv_bfloat16 may also help performance of other intrinsics, such as warp shuffles.
The intrinsic __halves2half2 is provided to convert two half precision values to the half2 datatype.
The intrinsic __halves2bfloat162 is provided to convert two __nv_bfloat precision values to the __nv_bfloat162 datatype.
Type Conversion
Sometimes, the compiler must insert conversion instructions, introducing additional execution cycles. This is the case for:

Functions operating on variables of type char or short whose operands generally need to be converted to int,
Double-precision floating-point constants (i.e., those constants defined without any type suffix) used as input to single-precision floating-point computations (as mandated by C/C++ standards).

This last case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3.141592653589793f, 1.0f, 0.5f.



5.4.2. Control Flow Instructionsï

Any flow control instruction (if, switch, do, for, while) can significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i.e., to follow different execution paths). If this happens, the different executions paths have to be serialized, increasing the total number of instructions executed for this warp.
To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps. This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture. A trivial example is when the controlling condition only depends on (threadIdx / warpSize) where warpSize is the warp size. In this case, no warp diverges since the controlling condition is perfectly aligned with the warps.
Sometimes, the compiler may unroll loops or it may optimize out short if or switch blocks by using branch predication instead, as detailed below. In these cases, no warp can ever diverge. The programmer can also control loop unrolling using the #pragma unroll directive (see #pragma unroll).
When using branch predication none of the instructions whose execution depends on the controlling condition gets skipped. Instead, each of them is associated with a per-thread condition code or predicate that is set to true or false based on the controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true predicate are actually executed. Instructions with a false predicate do not write results, and also do not evaluate addresses or read operands.



5.4.3. Synchronization Instructionï

Throughput for __syncthreads() is 32 operations per clock cycle for devices of compute capability 6.0, 16 operations per clock cycle for devices of compute capability 7.x as well as 8.x and 64 operations per clock cycle for devices of compute capability 5.x, 6.1 and 6.2.
Note that __syncthreads() can impact performance by forcing the multiprocessor to idle as detailed in Device Memory Accesses.




5.5. Minimize Memory Thrashingï

Applications that constantly allocate and free memory too often may find that the allocation calls tend to get slower over time up to a limit. This is typically expected due to the nature of releasing memory back to the operating system for its own use. For best performance in this regard, we recommend the following:

Try to size your allocation to the problem at hand. Donât try to allocate all available memory with cudaMalloc / cudaMallocHost / cuMemCreate, as this forces memory to be resident immediately and prevents other applications from being able to use that memory. This can put more pressure on operating system schedulers, or just prevent other applications using the same GPU from running entirely.
Try to allocate memory in appropriately sized allocations early in the application and allocations only when the application does not have any use for it. Reduce the number of cudaMalloc+cudaFree calls in the application, especially in performance-critical regions.
If an application cannot allocate enough device memory, consider falling back on other memory types such as cudaMallocHost or cudaMallocManaged, which may not be as performant, but will enable the application to make progress.
For platforms that support the feature, cudaMallocManaged allows for oversubscription, and with the correct cudaMemAdvise policies enabled, will allow the application to retain most if not all the performance of cudaMalloc. cudaMallocManaged also wonât force an allocation to be resident until it is needed or prefetched, reducing the overall pressure on the operating system schedulers and better enabling multi-tenet use cases.


3

128 for __nv_bfloat16

4

8 for GeForce GPUs, except for Titan GPUs

5

2 for compute capability 7.5 GPUs

6

32 for extended-precision

7

32 for GeForce GPUs, except for Titan GPUs

8

16 for compute capabilities 7.5 GPUs

9

8 for GeForce GPUs, except for Titan GPUs

10

2 for compute capabilities 7.5 GPUs






6. CUDA-Enabled GPUsï

https://developer.nvidia.com/cuda-gpus lists all CUDA-enabled devices with their compute capability.
The compute capability, number of multiprocessors, clock frequency, total amount of device memory, and other properties can be queried using the runtime (see reference manual).



7. C++ Language Extensionsï



7.1. Function Execution Space Specifiersï

Function execution space specifiers denote whether a function executes on the host or on the device and whether it is callable from the host or from the device.


7.1.1. __global__ï

The __global__ execution space specifier declares a function as being a kernel. Such a function is:

Executed on the device,
Callable from the host,
Callable from the device for devices of compute capability 5.0 or higher (see CUDA Dynamic Parallelism for more details).

A __global__ function must have void return type, and cannot be a member of a class.
Any call to a __global__ function must specify its execution configuration as described in Execution Configuration.
A call to a __global__ function is asynchronous, meaning it returns before the device has completed its execution.



7.1.2. __device__ï

The __device__ execution space specifier declares a function that is:

Executed on the device,
Callable from the device only.

The __global__ and __device__ execution space specifiers cannot be used together.



7.1.3. __host__ï

The __host__ execution space specifier declares a function that is:

Executed on the host,
Callable from the host only.

It is equivalent to declare a function with only the __host__ execution space specifier or to declare it without any of the __host__, __device__, or __global__ execution space specifier; in either case the function is compiled for the host only.
The __global__ and __host__ execution space specifiers cannot be used together.
The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device:

__host__ __device__ func()
{
#if __CUDA_ARCH__ >= 800
   // Device code path for compute capability 8.x
#elif __CUDA_ARCH__ >= 700
   // Device code path for compute capability 7.x
#elif __CUDA_ARCH__ >= 600
   // Device code path for compute capability 6.x
#elif __CUDA_ARCH__ >= 500
   // Device code path for compute capability 5.x
#elif !defined(__CUDA_ARCH__)
   // Host code path
#endif
}





7.1.4. Undefined behaviorï

A âcross-execution spaceâ call has undefined behavior when:

__CUDA_ARCH__ is defined, a call from within a __global__, __device__ or __host__ __device__ function to a __host__ function.
__CUDA_ARCH__ is undefined, a call from within a __host__ function to a __device__ function. 9




7.1.5. __noinline__ and __forceinline__ï

The compiler inlines any __device__ function when deemed appropriate.
The __noinline__ function qualifier can be used as a hint for the compiler not to inline the function if possible.
The __forceinline__ function qualifier can be used to force the compiler to inline the function.
The __noinline__ and __forceinline__ function qualifiers cannot be used together, and neither function qualifier can be applied to an inline function.



7.1.6. __inline_hint__ï

The __inline_hint__ qualifier enables more aggressive inlining in the compiler. Unlike __forceinline__, it does not imply that the function is inline. It can be used to improve inlining across modules when using LTO.
Neither the __noinline__ nor the __forceinline__ function qualifier can be used with the __inline_hint__ function qualifier.




7.2. Variable Memory Space Specifiersï

Variable memory space specifiers denote the memory location on the device of a variable.
An automatic variable declared in device code without any of the __device__, __shared__ and __constant__ memory space specifiers described in this section generally resides in a register. However in some cases the compiler might choose to place it in local memory, which can have adverse performance consequences as detailed in Device Memory Accesses.


7.2.1. __device__ï

The __device__ memory space specifier declares a variable that resides on the device.
At most one of the other memory space specifiers defined in the next three sections may be used together with __device__ to further denote which memory space the variable belongs to. If none of them is present, the variable:

Resides in global memory space,
Has the lifetime of the CUDA context in which it is created,
Has a distinct object per device,
Is accessible from all the threads within the grid and from the host through the runtime library (cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol()).




7.2.2. __constant__ï

The __constant__ memory space specifier, optionally used together with __device__, declares a variable that:

Resides in constant memory space,
Has the lifetime of the CUDA context in which it is created,
Has a distinct object per device,
Is accessible from all the threads within the grid and from the host through the runtime library (cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol()).

The behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point of this gridâs lifetime is undefined.



7.2.3. __shared__ï

The __shared__ memory space specifier, optionally used together with __device__, declares a variable that:

Resides in the shared memory space of a thread block,
Has the lifetime of the block,
Has a distinct object per block,
Is only accessible from all the threads within the block,
Does not have a constant address.

When declaring a variable in shared memory as an external array such as

extern __shared__ float shared[];


the size of the array is determined at launch time (see Execution Configuration). All variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the array must be explicitly managed through offsets. For example, if one wants the equivalent of

short array0[128];
float array1[64];
int   array2[256];


in dynamically allocated shared memory, one could declare and initialize the arrays the following way:

extern __shared__ float array[];
__device__ void func()      // __device__ or __global__ function
{
    short* array0 = (short*)array;
    float* array1 = (float*)&array0[128];
    int*   array2 =   (int*)&array1[64];
}


Note that pointers need to be aligned to the type they point to, so the following code, for example, does not work since array1 is not aligned to 4 bytes.

extern __shared__ float array[];
__device__ void func()      // __device__ or __global__ function
{
    short* array0 = (short*)array;
    float* array1 = (float*)&array0[127];
}


Alignment requirements for the built-in vector types are listed in Table 5.



7.2.4. __grid_constant__ï

The __grid_constant__ annotation for compute architectures greater or equal to 7.0 annotates a const-qualified __global__ function parameter of non-reference type that:

Has the lifetime of the grid,
Is private to the grid, i.e., the object is not accessible to host threads and threads from other grids, including sub-grids,
Has a distinct object per grid, i.e., all threads in the grid see the same address,
Is read-only, i.e., modifying a __grid_constant__ object or any of its sub-objects is undefined behavior, including mutable members.

Requirements:

Kernel parameters annotated with __grid_constant__ must have const-qualified non-reference types.
All function declarations must match with respect to any __grid_constant_ parameters.
A function template specialization must match the primary template declaration with respect to any __grid_constant__ parameters.
A function template instantiation directive must match the primary template declaration with respect to any __grid_constant__ parameters.

If the address of a __global__ function parameter is taken, the compiler will ordinarily make a copy of the kernel parameter in thread local memory and use the address of the copy, to partially support C++ semantics, which allow each thread to modify its own local copy of function parameters. Annotating a __global__ function parameter with __grid_constant__ ensures that the compiler will not create a copy of the kernel parameter in thread local memory, but will instead use the generic address of the parameter itself. Avoiding the local copy may result in improved performance.

__device__ void unknown_function(S const&);
__global__ void kernel(const __grid_constant__ S s) {
   s.x += threadIdx.x;  // Undefined Behavior: tried to modify read-only memory

   // Compiler will _not_ create a per-thread thread local copy of "s":
   unknown_function(s);
}





7.2.5. __managed__ï

The __managed__ memory space specifier, optionally used together with __device__, declares a variable that:

Can be referenced from both device and host code, for example, its address can be taken or it can be read or written directly from a device or host function.
Has the lifetime of an application.

See __managed__ Memory Space Specifier for more details.



7.2.6. __restrict__ï

nvcc supports restricted pointers via the __restrict__ keyword.
Restricted pointers were introduced in C99 to alleviate the aliasing problem that exists in C-type languages, and which inhibits all kind of optimization from code re-ordering to common sub-expression elimination.
Here is an example subject to the aliasing issue, where use of restricted pointer can help the compiler to reduce the number of instructions:

void foo(const float* a,
         const float* b,
         float* c)
{
    c[0] = a[0] * b[0];
    c[1] = a[0] * b[0];
    c[2] = a[0] * b[0] * a[1];
    c[3] = a[0] * a[1];
    c[4] = a[0] * b[0];
    c[5] = b[0];
    ...
}


In C-type languages, the pointers a, b, and c may be aliased, so any write through c could modify elements of a or b. This means that to guarantee functional correctness, the compiler cannot load a[0] and b[0] into registers, multiply them, and store the result to both c[0] and c[1], because the results would differ from the abstract execution model if, say, a[0] is really the same location as c[0]. So the compiler cannot take advantage of the common sub-expression. Likewise, the compiler cannot just reorder the computation of c[4] into the proximity of the computation of c[0] and c[1] because the preceding write to c[3] could change the inputs to the computation of c[4].
By making a, b, and c restricted pointers, the programmer asserts to the compiler that the pointers are in fact not aliased, which in this case means writes through c would never overwrite elements of a or b. This changes the function prototype as follows:

void foo(const float* __restrict__ a,
         const float* __restrict__ b,
         float* __restrict__ c);


Note that all pointer arguments need to be made restricted for the compiler optimizer to derive any benefit. With the __restrict__ keywords added, the compiler can now reorder and do common sub-expression elimination at will, while retaining functionality identical with the abstract execution model:

void foo(const float* __restrict__ a,
         const float* __restrict__ b,
         float* __restrict__ c)
{
    float t0 = a[0];
    float t1 = b[0];
    float t2 = t0 * t1;
    float t3 = a[1];
    c[0] = t2;
    c[1] = t2;
    c[4] = t2;
    c[2] = t2 * t3;
    c[3] = t0 * t3;
    c[5] = t1;
    ...
}


The effects here are a reduced number of memory accesses and reduced number of computations. This is balanced by an increase in register pressure due to âcachedâ loads and common sub-expressions.
Since register pressure is a critical issue in many CUDA codes, use of restricted pointers can have negative performance impact on CUDA code, due to reduced occupancy.




7.3. Built-in Vector Typesï



7.3.1. char, short, int, long, longlong, float, doubleï

These are vector types derived from the basic integer and floating-point types. They are structures and the 1st, 2nd, 3rd, and 4th components are accessible through the fields x, y, z, and w, respectively. They all come with a constructor function of the form make_<type name>; for example,

int2 make_int2(int x, int y);


which creates a vector of type int2 with value(x, y).
The alignment requirements of the vector types are detailed in the following table.


Table 5 Alignment Requirementsï







Type
Alignment




char1, uchar1
1


char2, uchar2
2


char3, uchar3
1


char4, uchar4
4


short1, ushort1
2


short2, ushort2
4


short3, ushort3
2


short4, ushort4
8


int1, uint1
4


int2, uint2
8


int3, uint3
4


int4, uint4
16


long1, ulong1
4 if sizeof(long) is equal to sizeof(int) 8, otherwise


long2, ulong2
8 if sizeof(long) is equal to sizeof(int), 16, otherwise


long3, ulong3
4 if sizeof(long) is equal to sizeof(int), 8, otherwise


long4, ulong4
16


longlong1, ulonglong1
8


longlong2, ulonglong2
16


longlong3, ulonglong3
8


longlong4, ulonglong4
16


float1
4


float2
8


float3
4


float4
16


double1
8


double2
16


double3
8


double4
16






7.3.2. dim3ï

This type is an integer vector type based on uint3 that is used to specify dimensions. When defining a variable of type dim3, any component left unspecified is initialized to 1.




7.4. Built-in Variablesï

Built-in variables specify the grid and block dimensions and the block and thread indices. They are only valid within functions that are executed on the device.


7.4.1. gridDimï

This variable is of type dim3 (see dim3) and contains the dimensions of the grid.



7.4.2. blockIdxï

This variable is of type uint3 (see char, short, int, long, longlong, float, double) and contains the block index within the grid.



7.4.3. blockDimï

This variable is of type dim3 (see dim3) and contains the dimensions of the block.



7.4.4. threadIdxï

This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the thread index within the block.



7.4.5. warpSizeï

This variable is of type int and contains the warp size in threads (see SIMT Architecture for the definition of a warp).




7.5. Memory Fence Functionsï

The CUDA programming model assumes a device with a weakly-ordered memory model, that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another CUDA or host thread. It is undefined behavior for two threads to read from or write to the same memory location without synchronization.
In the following example, thread 1 executes writeXY(), while thread 2 executes readXY().

__device__ int X = 1, Y = 2;

__device__ void writeXY()
{
    X = 10;
    Y = 20;
}

__device__ void readXY()
{
    int B = Y;
    int A = X;
}


The two threads read and write from the same memory locations X and Y simultaneously. Any data-race is undefined behavior, and has no defined semantics. The resulting values for A and B can be anything.
Memory fence functions can be used to enforce a sequentially-consistent ordering on memory accesses. The memory fence functions differ in the scope in which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device).

void __threadfence_block();


is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_block) and ensures that:

All writes to all memory made by the calling thread before the call to __threadfence_block() are observed by all threads in the block of the calling thread as occurring before all writes to all memory made by the calling thread after the call to __threadfence_block();
All reads from all memory made by the calling thread before the call to __threadfence_block() are ordered before all reads from all memory made by the calling thread after the call to __threadfence_block().


void __threadfence();


is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_device) and ensures that no writes to all memory made by the calling thread after the call to __threadfence() are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the call to __threadfence().

void __threadfence_system();


is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_system) and ensures that all writes to all memory made by the calling thread before the call to __threadfence_system() are observed by all threads in the device, host threads, and all threads in peer devices as occurring before all writes to all memory made by the calling thread after the call to __threadfence_system().
__threadfence_system() is only supported by devices of compute capability 2.x and higher.
In the previous code sample, we can insert fences in the codes as follows:

__device__ int X = 1, Y = 2;

__device__ void writeXY()
{
    X = 10;
    __threadfence();
    Y = 20;
}

__device__ void readXY()
{
    int B = Y;
    __threadfence();
    int A = X;
}


For this code, the following outcomes can be observed:

A equal to 1 and B equal to 2,
A equal to 10 and B equal to 2,
A equal to 10 and B equal to 20.

The fourth outcome is not possible, because the first write must be visible before the second write. If thread 1 and 2 belong to the same block, it is enough to use __threadfence_block(). If thread 1 and 2 do not belong to the same block, __threadfence() must be used if they are CUDA threads from the same device and __threadfence_system() must be used if they are CUDA threads from two different devices.
A common use case is when threads consume some data produced by other threads as illustrated by the following code sample of a kernel that computes the sum of an array of N numbers in one call. Each block first sums a subset of the array and stores the result in global memory. When all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result. In order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (see Atomic Functions about atomic functions). The last block is the one that receives the counter value equal to gridDim.x-1. If no fence is placed between storing the partial sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reach gridDim.x-1 and let the last block start reading partial sums before they have been actually updated in memory.
Memory fence functions only affect the ordering of memory operations by a thread; they do not, by themselves, ensure that these memory operations are visible to other threads (like __syncthreads() does for threads within a block (see Synchronization Functions)). In the code sample below, the visibility of memory operations on the result variable is ensured by declaring it as volatile (see Volatile Qualifier).

__device__ unsigned int count = 0;
__shared__ bool isLastBlockDone;
__global__ void sum(const float* array, unsigned int N,
                    volatile float* result)
{
    // Each block sums a subset of the input array.
    float partialSum = calculatePartialSum(array, N);

    if (threadIdx.x == 0) {

        // Thread 0 of each block stores the partial sum
        // to global memory. The compiler will use
        // a store operation that bypasses the L1 cache
        // since the "result" variable is declared as
        // volatile. This ensures that the threads of
        // the last block will read the correct partial
        // sums computed by all other blocks.
        result[blockIdx.x] = partialSum;

        // Thread 0 makes sure that the incrementing
        // of the "count" variable is only performed after
        // the partial sum has been written to global memory.
        __threadfence();

        // Thread 0 signals that it is done.
        unsigned int value = atomicInc(&count, gridDim.x);

        // Thread 0 determines if its block is the last
        // block to be done.
        isLastBlockDone = (value == (gridDim.x - 1));
    }

    // Synchronize to make sure that each thread reads
    // the correct value of isLastBlockDone.
    __syncthreads();

    if (isLastBlockDone) {

        // The last block sums the partial sums
        // stored in result[0 .. gridDim.x-1]
        float totalSum = calculateTotalSum(result);

        if (threadIdx.x == 0) {

            // Thread 0 of last block stores the total sum
            // to global memory and resets the count
            // variable, so that the next kernel call
            // works properly.
            result[0] = totalSum;
            count = 0;
        }
    }
}





7.6. Synchronization Functionsï


void __syncthreads();


waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to __syncthreads() are visible to all threads in the block.
__syncthreads() is used to coordinate communication between the threads of the same block. When some threads within a block access the same addresses in shared or global memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses. These data hazards can be avoided by synchronizing threads in-between these accesses.
__syncthreads() is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects.
Devices of compute capability 2.x and higher support three variations of __syncthreads() described below.

int __syncthreads_count(int predicate);


is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero.

int __syncthreads_and(int predicate);


is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them.

int __syncthreads_or(int predicate);


is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them.

void __syncwarp(unsigned mask=0xffffffff);


will cause the executing thread to wait until all warp lanes named in mask have executed a __syncwarp() (with the same mask) before resuming execution. Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute a corresponding __syncwarp() with the same mask, or the result is undefined.
Executing __syncwarp() guarantees memory ordering among threads participating in the barrier. Thus, threads within a warp that wish to communicate via memory can store to memory, execute __syncwarp(), and then safely read values stored by other threads in the warp.

Note
For .target sm_6x or below, all threads in mask must execute the same __syncwarp() in convergence, and the union of all values in mask must be equal to the active mask. Otherwise, the behavior is undefined.




7.7. Mathematical Functionsï

The reference manual lists all C/C++ standard library mathematical functions that are supported in device code and all intrinsic functions that are only supported in device code.
Mathematical Functions provides accuracy information for some of these functions when relevant.



7.8. Texture Functionsï

Texture objects are described in Texture Object API
Texture fetching is described in Texture Fetching.


7.8.1. Texture Object APIï



7.8.1.1. tex1Dfetch()ï


template<class T>
T tex1Dfetch(cudaTextureObject_t texObj, int x);


fetches from the region of linear memory specified by the one-dimensional texture object texObj using integer texture coordinate x. tex1Dfetch() only works with non-normalized coordinates, so only the border and clamp addressing modes are supported. It does not perform any texture filtering. For integer types, it may optionally promote the integer to single-precision floating point.



7.8.1.2. tex1D()ï


template<class T>
T tex1D(cudaTextureObject_t texObj, float x);


fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x.



7.8.1.3. tex1DLod()ï


template<class T>
T tex1DLod(cudaTextureObject_t texObj, float x, float level);


fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x at the level-of-detail level.



7.8.1.4. tex1DGrad()ï


template<class T>
T tex1DGrad(cudaTextureObject_t texObj, float x, float dx, float dy);


fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x. The level-of-detail is derived from the X-gradient dx and Y-gradient dy.



7.8.1.5. tex2D()ï


template<class T>
T tex2D(cudaTextureObject_t texObj, float x, float y);


fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y).



7.8.1.6. tex2D() for sparse CUDA arraysï


                template<class T>
T tex2D(cudaTextureObject_t texObj, float x, float y, bool* isResident);


fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y). Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.7. tex2Dgather()ï


template<class T>
T tex2Dgather(cudaTextureObject_t texObj,
              float x, float y, int comp = 0);


fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather.



7.8.1.8. tex2Dgather() for sparse CUDA arraysï


                template<class T>
T tex2Dgather(cudaTextureObject_t texObj,
            float x, float y, bool* isResident, int comp = 0);


fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.9. tex2DGrad()ï


template<class T>
T tex2DGrad(cudaTextureObject_t texObj, float x, float y,
            float2 dx, float2 dy);


fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y). The level-of-detail is derived from the dx and dy gradients.



7.8.1.10. tex2DGrad() for sparse CUDA arraysï


                template<class T>
T tex2DGrad(cudaTextureObject_t texObj, float x, float y,
        float2 dx, float2 dy, bool* isResident);


fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y). The level-of-detail is derived from the dx and dy gradients. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.11. tex2DLod()ï


template<class T>
tex2DLod(cudaTextureObject_t texObj, float x, float y, float level);


fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level.



7.8.1.12. tex2DLod() for sparse CUDA arraysï


        template<class T>
tex2DLod(cudaTextureObject_t texObj, float x, float y, float level, bool* isResident);


fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.13. tex3D()ï


template<class T>
T tex3D(cudaTextureObject_t texObj, float x, float y, float z);


fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z).



7.8.1.14. tex3D() for sparse CUDA arraysï


                template<class T>
T tex3D(cudaTextureObject_t texObj, float x, float y, float z, bool* isResident);


fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z). Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.15. tex3DLod()ï


template<class T>
T tex3DLod(cudaTextureObject_t texObj, float x, float y, float z, float level);


fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level.



7.8.1.16. tex3DLod() for sparse CUDA arraysï


                template<class T>
T tex3DLod(cudaTextureObject_t texObj, float x, float y, float z, float level, bool* isResident);


fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.17. tex3DGrad()ï


template<class T>
T tex3DGrad(cudaTextureObject_t texObj, float x, float y, float z,
            float4 dx, float4 dy);


fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy.



7.8.1.18. tex3DGrad() for sparse CUDA arraysï


                template<class T>
T tex3DGrad(cudaTextureObject_t texObj, float x, float y, float z,
        float4 dx, float4 dy, bool* isResident);


fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.19. tex1DLayered()ï


template<class T>
T tex1DLayered(cudaTextureObject_t texObj, float x, int layer);


fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x and index layer, as described in Layered Textures



7.8.1.20. tex1DLayeredLod()ï


template<class T>
T tex1DLayeredLod(cudaTextureObject_t texObj, float x, int layer, float level);


fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and level-of-detail level.



7.8.1.21. tex1DLayeredGrad()ï


template<class T>
T tex1DLayeredGrad(cudaTextureObject_t texObj, float x, int layer,
                   float dx, float dy);


fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and a level-of-detail derived from the dx and dy gradients.



7.8.1.22. tex2DLayered()ï


template<class T>
T tex2DLayered(cudaTextureObject_t texObj,
               float x, float y, int layer);


fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer, as described in Layered Textures.



7.8.1.23. tex2DLayered() for sparse CUDA arraysï


                template<class T>
T tex2DLayered(cudaTextureObject_t texObj,
            float x, float y, int layer, bool* isResident);


fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer, as described in Layered Textures. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.24. tex2DLayeredLod()ï


template<class T>
T tex2DLayeredLod(cudaTextureObject_t texObj, float x, float y, int layer,
                  float level);


fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y).



7.8.1.25. tex2DLayeredLod() for sparse CUDA arraysï


                template<class T>
T tex2DLayeredLod(cudaTextureObject_t texObj, float x, float y, int layer,
                float level, bool* isResident);


fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y). Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.26. tex2DLayeredGrad()ï


template<class T>
T tex2DLayeredGrad(cudaTextureObject_t texObj, float x, float y, int layer,
                   float2 dx, float2 dy);


fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients.



7.8.1.27. tex2DLayeredGrad() for sparse CUDA arraysï


                template<class T>
T tex2DLayeredGrad(cudaTextureObject_t texObj, float x, float y, int layer,
                float2 dx, float2 dy, bool* isResident);


fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients. Also returns whether the texel is resident in memory via isResident pointer. If not, the values fetched will be zeros.



7.8.1.28. texCubemap()ï


template<class T>
T texCubemap(cudaTextureObject_t texObj, float x, float y, float z);


fetches the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z), as described in Cubemap Textures.



7.8.1.29. texCubemapGrad()ï


template<class T>
T texCubemapGrad(cudaTextureObject_t texObj, float x, float, y, float z,
                float4 dx, float4 dy);


fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures. The level-of-detail used is derived from the dx and dy gradients.



7.8.1.30. texCubemapLod()ï


template<class T>
T texCubemapLod(cudaTextureObject_t texObj, float x, float, y, float z,
                float level);


fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures. The level-of-detail used is given by level.



7.8.1.31. texCubemapLayered()ï


template<class T>
T texCubemapLayered(cudaTextureObject_t texObj,
                    float x, float y, float z, int layer);


fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinates (x,y,z), and index layer, as described in Cubemap Layered Textures.



7.8.1.32. texCubemapLayeredGrad()ï


template<class T>
T texCubemapLayeredGrad(cudaTextureObject_t texObj, float x, float y, float z,
                       int layer, float4 dx, float4 dy);


fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer, as described in Cubemap Layered Textures, at level-of-detail derived from the dx and dy gradients.



7.8.1.33. texCubemapLayeredLod()ï


template<class T>
T texCubemapLayeredLod(cudaTextureObject_t texObj, float x, float y, float z,
                       int layer, float level);


fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer, as described in Cubemap Layered Textures, at level-of-detail level level.





7.9. Surface Functionsï

Surface functions are only supported by devices of compute capability 2.0 and higher.
Surface objects are described in described in Surface Object API
In the sections below, boundaryMode specifies the boundary mode, that is how out-of-range surface coordinates are handled; it is equal to either cudaBoundaryModeClamp, in which case out-of-range coordinates are clamped to the valid range, or cudaBoundaryModeZero, in which case out-of-range reads return zero and out-of-range writes are ignored, or cudaBoundaryModeTrap, in which case out-of-range accesses cause the kernel execution to fail.


7.9.1. Surface Object APIï



7.9.1.1. surf1Dread()ï


template<class T>
T surf1Dread(cudaSurfaceObject_t surfObj, int x,
               boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the one-dimensional surface object surfObj using byte coordinate x.



7.9.1.2. surf1Dwriteï


template<class T>
void surf1Dwrite(T data,
                  cudaSurfaceObject_t surfObj,
                  int x,
                  boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the one-dimensional surface object surfObj at byte coordinate x.



7.9.1.3. surf2Dread()ï


template<class T>
T surf2Dread(cudaSurfaceObject_t surfObj,
              int x, int y,
              boundaryMode = cudaBoundaryModeTrap);
template<class T>
void surf2Dread(T* data,
                 cudaSurfaceObject_t surfObj,
                 int x, int y,
                 boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the two-dimensional surface object surfObj using byte coordinates x and y.



7.9.1.4. surf2Dwrite()ï


template<class T>
void surf2Dwrite(T data,
                  cudaSurfaceObject_t surfObj,
                  int x, int y,
                  boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the two-dimensional surface object surfObj at byte coordinate x and y.



7.9.1.5. surf3Dread()ï


template<class T>
T surf3Dread(cudaSurfaceObject_t surfObj,
              int x, int y, int z,
              boundaryMode = cudaBoundaryModeTrap);
template<class T>
void surf3Dread(T* data,
                 cudaSurfaceObject_t surfObj,
                 int x, int y, int z,
                 boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the three-dimensional surface object surfObj using byte coordinates x, y, and z.



7.9.1.6. surf3Dwrite()ï


template<class T>
void surf3Dwrite(T data,
                  cudaSurfaceObject_t surfObj,
                  int x, int y, int z,
                  boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the three-dimensional object surfObj at byte coordinate x, y, and z.



7.9.1.7. surf1DLayeredread()ï


template<class T>
T surf1DLayeredread(
                 cudaSurfaceObject_t surfObj,
                 int x, int layer,
                 boundaryMode = cudaBoundaryModeTrap);
template<class T>
void surf1DLayeredread(T data,
                 cudaSurfaceObject_t surfObj,
                 int x, int layer,
                 boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the one-dimensional layered surface object surfObj using byte coordinate x and index layer.



7.9.1.8. surf1DLayeredwrite()ï


template<class Type>
void surf1DLayeredwrite(T data,
                 cudaSurfaceObject_t surfObj,
                 int x, int layer,
                 boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the two-dimensional layered surface object surfObj at byte coordinate x and index layer.



7.9.1.9. surf2DLayeredread()ï


template<class T>
T surf2DLayeredread(
                 cudaSurfaceObject_t surfObj,
                 int x, int y, int layer,
                 boundaryMode = cudaBoundaryModeTrap);
template<class T>
void surf2DLayeredread(T data,
                         cudaSurfaceObject_t surfObj,
                         int x, int y, int layer,
                         boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the two-dimensional layered surface object surfObj using byte coordinate x and y, and index layer.



7.9.1.10. surf2DLayeredwrite()ï


template<class T>
void surf2DLayeredwrite(T data,
                          cudaSurfaceObject_t surfObj,
                          int x, int y, int layer,
                          boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the one-dimensional layered surface object surfObj at byte coordinate x and y, and index layer.



7.9.1.11. surfCubemapread()ï


template<class T>
T surfCubemapread(
                 cudaSurfaceObject_t surfObj,
                 int x, int y, int face,
                 boundaryMode = cudaBoundaryModeTrap);
template<class T>
void surfCubemapread(T data,
                 cudaSurfaceObject_t surfObj,
                 int x, int y, int face,
                 boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the cubemap surface object surfObj using byte coordinate x and y, and face index face.



7.9.1.12. surfCubemapwrite()ï


template<class T>
void surfCubemapwrite(T data,
                 cudaSurfaceObject_t surfObj,
                 int x, int y, int face,
                 boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the cubemap object surfObj at byte coordinate x and y, and face index face.



7.9.1.13. surfCubemapLayeredread()ï


template<class T>
T surfCubemapLayeredread(
             cudaSurfaceObject_t surfObj,
             int x, int y, int layerFace,
             boundaryMode = cudaBoundaryModeTrap);
template<class T>
void surfCubemapLayeredread(T data,
             cudaSurfaceObject_t surfObj,
             int x, int y, int layerFace,
             boundaryMode = cudaBoundaryModeTrap);


reads the CUDA array specified by the cubemap layered surface object surfObj using byte coordinate x and y, and index layerFace.



7.9.1.14. surfCubemapLayeredwrite()ï


template<class T>
void surfCubemapLayeredwrite(T data,
             cudaSurfaceObject_t surfObj,
             int x, int y, int layerFace,
             boundaryMode = cudaBoundaryModeTrap);


writes value data to the CUDA array specified by the cubemap layered object surfObj at byte coordinate x and y, and index layerFace.





7.10. Read-Only Data Cache Load Functionï

The read-only data cache load function is only supported by devices of compute capability 5.0 and higher.

T __ldg(const T* address);


returns the data of type T located at address address, where T is char, signed char, short, int, long, long longunsigned char, unsigned short, unsigned int, unsigned long, unsigned long long, char2, char4, short2, short4, int2, int4, longlong2uchar2, uchar4, ushort2, ushort4, uint2, uint4, ulonglong2float, float2, float4, double, or double2. With the cuda_fp16.h header included, T can be __half or __half2. Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162. The operation is cached in the read-only data cache (see Global Memory).



7.11. Load Functions Using Cache Hintsï

These load functions are only supported by devices of compute capability 5.0 and higher.

T __ldcg(const T* address);
T __ldca(const T* address);
T __ldcs(const T* address);
T __ldlu(const T* address);
T __ldcv(const T* address);


returns the data of type T located at address address, where T is char, signed char, short, int, long, long longunsigned char, unsigned short, unsigned int, unsigned long, unsigned long long, char2, char4, short2, short4, int2, int4, longlong2uchar2, uchar4, ushort2, ushort4, uint2, uint4, ulonglong2float, float2, float4, double, or double2. With the cuda_fp16.h header included, T can be __half or __half2. Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162. The operation is using the corresponding cache operator (see PTX ISA)



7.12. Store Functions Using Cache Hintsï

These store functions are only supported by devices of compute capability 5.0 and higher.

void __stwb(T* address, T value);
void __stcg(T* address, T value);
void __stcs(T* address, T value);
void __stwt(T* address, T value);


stores the value argument of type T to the location at address address, where T is char, signed char, short, int, long, long longunsigned char, unsigned short, unsigned int, unsigned long, unsigned long long, char2, char4, short2, short4, int2, int4, longlong2uchar2, uchar4, ushort2, ushort4, uint2, uint4, ulonglong2float, float2, float4, double, or double2. With the cuda_fp16.h header included, T can be __half or __half2. Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162. The operation is using the corresponding cache operator (see PTX ISA )



7.13. Time Functionï


clock_t clock();
long long int clock64();


when executed in device code, returns the value of a per-multiprocessor counter that is incremented every clock cycle. Sampling this counter at the beginning and at the end of a kernel, taking the difference of the two samples, and recording the result per thread provides a measure for each thread of the number of clock cycles taken by the device to completely execute the thread, but not of the number of clock cycles the device actually spent executing thread instructions. The former number is greater than the latter since threads are time sliced.



7.14. Atomic Functionsï

An atomic function performs a read-modify-write atomic operation on one 32-bit, 64-bit, or 128-bit word residing in global or shared memory. In the case of float2 or float4, the read-modify-write operation is performed on each element of the vector residing in global memory. For example, atomicAdd() reads a word at some address in global or shared memory, adds a number to it, and writes the result back to the same address. Atomic functions can only be used in device functions.
The atomic functions described in this section have ordering cuda::memory_order_relaxed and are only atomic at a particular scope:

Atomic APIs with _system suffix (example: atomicAdd_system) are atomic at scope cuda::thread_scope_system if they meet particular conditions.
Atomic APIs without a suffix (example: atomicAdd) are atomic at scope cuda::thread_scope_device.
Atomic APIs with _block suffix (example: atomicAdd_block) are atomic at scope cuda::thread_scope_block.

In the following example both the CPU and the GPU atomically update an integer value at address addr:

__global__ void mykernel(int *addr) {
  atomicAdd_system(addr, 10);       // only available on devices with compute capability 6.x
}

void foo() {
  int *addr;
  cudaMallocManaged(&addr, 4);
  *addr = 0;

   mykernel<<<...>>>(addr);
   __sync_fetch_and_add(addr, 10);  // CPU atomic operation
}


Note that any atomic operation can be implemented based on atomicCAS() (Compare And Swap). For example, atomicAdd() for double-precision floating-point numbers is not available on devices with compute capability lower than 6.0 but it can be implemented as follows:

#if __CUDA_ARCH__ < 600
__device__ double atomicAdd(double* address, double val)
{
    unsigned long long int* address_as_ull =
                              (unsigned long long int*)address;
    unsigned long long int old = *address_as_ull, assumed;

    do {
        assumed = old;
        old = atomicCAS(address_as_ull, assumed,
                        __double_as_longlong(val +
                               __longlong_as_double(assumed)));

    // Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN)
    } while (assumed != old);

    return __longlong_as_double(old);
}
#endif


There are system-wide and block-wide variants of the following device-wide atomic APIs, with the following exceptions:

Devices with compute capability less than 6.0 only support device-wide atomic operations,
Tegra devices with compute capability less than 7.2 do not support system-wide atomic operations.



7.14.1. Arithmetic Functionsï



7.14.1.1. atomicAdd()ï


int atomicAdd(int* address, int val);
unsigned int atomicAdd(unsigned int* address,
                       unsigned int val);
unsigned long long int atomicAdd(unsigned long long int* address,
                                 unsigned long long int val);
float atomicAdd(float* address, float val);
double atomicAdd(double* address, double val);
__half2 atomicAdd(__half2 *address, __half2 val);
__half atomicAdd(__half *address, __half val);
__nv_bfloat162 atomicAdd(__nv_bfloat162 *address, __nv_bfloat162 val);
__nv_bfloat16 atomicAdd(__nv_bfloat16 *address, __nv_bfloat16 val);
float2 atomicAdd(float2* address, float2 val);
float4 atomicAdd(float4* address, float4 val);


reads the 16-bit, 32-bit or 64-bit old located at the address address in global or shared memory, computes (old + val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.
The 32-bit floating-point version of atomicAdd() is only supported by devices of compute capability 2.x and higher.
The 64-bit floating-point version of atomicAdd() is only supported by devices of compute capability 6.x and higher.
The 32-bit __half2 floating-point version of atomicAdd() is only supported by devices of compute capability 6.x and higher. The atomicity of the __half2 or __nv_bfloat162 add operation is guaranteed separately for each of the two __half or __nv_bfloat16 elements; the entire __half2 or __nv_bfloat162 is not guaranteed to be atomic as a single 32-bit access.
The float2 and float4 floating-point vector versions of atomicAdd() are only supported by devices of compute capability 9.x and higher. The atomicity of the float2 or float4 add operation is guaranteed separately for each of the two or four float elements; the entire float2 or float4 is not guaranteed to be atomic as a single 64-bit or 128-bit access.
The 16-bit __half floating-point version of atomicAdd() is only supported by devices of compute capability 7.x and higher.
The 16-bit __nv_bfloat16 floating-point version of atomicAdd() is only supported by devices of compute capability 8.x and higher.
The float2 and float4 floating-point vector versions of atomicAdd() are only supported by devices of compute capability 9.x and higher.
The float2 and float4 floating-point vector versions of atomicAdd() are only supported for global memory addresses.



7.14.1.2. atomicSub()ï


int atomicSub(int* address, int val);
unsigned int atomicSub(unsigned int* address,
                       unsigned int val);


reads the 32-bit word old located at the address address in global or shared memory, computes (old - val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.



7.14.1.3. atomicExch()ï


int atomicExch(int* address, int val);
unsigned int atomicExch(unsigned int* address,
                        unsigned int val);
unsigned long long int atomicExch(unsigned long long int* address,
                                  unsigned long long int val);
float atomicExch(float* address, float val);


reads the 32-bit or 64-bit word old located at the address address in global or shared memory and stores val back to memory at the same address. These two operations are performed in one atomic transaction. The function returns old.

template<typename T> T atomicExch(T* address, T val);


reads the 128-bit word old located at the address address in global or shared memory and stores val back to memory at the same address. These two operations are performed in one atomic transaction. The function returns old. The type T must meet the following requirements:

sizeof(T) == 16
alignof(T) >= 16
std::is_trivially_copyable<T>::value == true
// for C++03 and older
std::is_default_constructible<T>::value == true


So, T must be 128-bit and properly aligned, be trivially copyable, and on C++03 or older, it must also be default constructible.
The 128-bit atomicExch() is only supported by devices of compute capability 9.x and higher.



7.14.1.4. atomicMin()ï


int atomicMin(int* address, int val);
unsigned int atomicMin(unsigned int* address,
                       unsigned int val);
unsigned long long int atomicMin(unsigned long long int* address,
                                 unsigned long long int val);
long long int atomicMin(long long int* address,
                                long long int val);


reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes the minimum of old and val, and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.
The 64-bit version of atomicMin() is only supported by devices of compute capability 5.0 and higher.



7.14.1.5. atomicMax()ï


int atomicMax(int* address, int val);
unsigned int atomicMax(unsigned int* address,
                       unsigned int val);
unsigned long long int atomicMax(unsigned long long int* address,
                                 unsigned long long int val);
long long int atomicMax(long long int* address,
                                 long long int val);


reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes the maximum of old and val, and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.
The 64-bit version of atomicMax() is only supported by devices of compute capability 5.0 and higher.



7.14.1.6. atomicInc()ï


unsigned int atomicInc(unsigned int* address,
                       unsigned int val);


reads the 32-bit word old located at the address address in global or shared memory, computes ((old >= val) ? 0 : (old+1)), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.



7.14.1.7. atomicDec()ï


unsigned int atomicDec(unsigned int* address,
                       unsigned int val);


reads the 32-bit word old located at the address address in global or shared memory, computes (((old == 0) || (old > val)) ? val : (old-1) ), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.



7.14.1.8. atomicCAS()ï


int atomicCAS(int* address, int compare, int val);
unsigned int atomicCAS(unsigned int* address,
                       unsigned int compare,
                       unsigned int val);
unsigned long long int atomicCAS(unsigned long long int* address,
                                 unsigned long long int compare,
                                 unsigned long long int val);
unsigned short int atomicCAS(unsigned short int *address,
                             unsigned short int compare,
                             unsigned short int val);


reads the 16-bit, 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old == compare ? val : old), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old (Compare And Swap).

template<typename T> T atomicCAS(T* address, T compare, T val);


reads the 128-bit word old located at the address address in global or shared memory, computes (old == compare ? val : old), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old (Compare And Swap). The type T must meet the following requirements:

sizeof(T) == 16
alignof(T) >= 16
std::is_trivially_copyable<T>::value == true
// for C++03 and older
std::is_default_constructible<T>::value == true


So, T must be 128-bit and properly aligned, be trivially copyable, and on C++03 or older, it must also be default constructible.
The 128-bit atomicCAS() is only supported by devices of compute capability 9.x and higher.




7.14.2. Bitwise Functionsï



7.14.2.1. atomicAnd()ï


int atomicAnd(int* address, int val);
unsigned int atomicAnd(unsigned int* address,
                       unsigned int val);
unsigned long long int atomicAnd(unsigned long long int* address,
                                 unsigned long long int val);


reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old & val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.
The 64-bit version of atomicAnd() is only supported by devices of compute capability 5.0 and higher.



7.14.2.2. atomicOr()ï


int atomicOr(int* address, int val);
unsigned int atomicOr(unsigned int* address,
                      unsigned int val);
unsigned long long int atomicOr(unsigned long long int* address,
                                unsigned long long int val);


reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old | val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.
The 64-bit version of atomicOr() is only supported by devices of compute capability 5.0 and higher.



7.14.2.3. atomicXor()ï


int atomicXor(int* address, int val);
unsigned int atomicXor(unsigned int* address,
                       unsigned int val);
unsigned long long int atomicXor(unsigned long long int* address,
                                 unsigned long long int val);


reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old ^ val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.
The 64-bit version of atomicXor() is only supported by devices of compute capability 5.0 and higher.





7.15. Address Space Predicate Functionsï

The functions described in this section have unspecified behavior if the argument is a null pointer.


7.15.1. __isGlobal()ï


__device__ unsigned int __isGlobal(const void *ptr);


Returns 1 if ptr contains the generic address of an object in global memory space, otherwise returns 0.



7.15.2. __isShared()ï


__device__ unsigned int __isShared(const void *ptr);


Returns 1 if ptr contains the generic address of an object in shared memory space, otherwise returns 0.



7.15.3. __isConstant()ï


__device__ unsigned int __isConstant(const void *ptr);


Returns 1 if ptr contains the generic address of an object in constant memory space, otherwise returns 0.



7.15.4. __isGridConstant()ï


__device__ unsigned int __isGridConstant(const void *ptr);


Returns 1 if ptr contains the generic address of a kernel parameter annotated with __grid_constant__, otherwise returns 0. Only supported for compute architectures greater than or equal to 7.x or later.



7.15.5. __isLocal()ï


__device__ unsigned int __isLocal(const void *ptr);


Returns 1 if ptr contains the generic address of an object in local memory space, otherwise returns 0.




7.16. Address Space Conversion Functionsï



7.16.1. __cvta_generic_to_global()ï


__device__ size_t __cvta_generic_to_global(const void *ptr);


Returns the result of executing the PTXcvta.to.global instruction on the generic address denoted by ptr.



7.16.2. __cvta_generic_to_shared()ï


__device__ size_t __cvta_generic_to_shared(const void *ptr);


Returns the result of executing the PTXcvta.to.shared instruction on the generic address denoted by ptr.



7.16.3. __cvta_generic_to_constant()ï


__device__ size_t __cvta_generic_to_constant(const void *ptr);


Returns the result of executing the PTXcvta.to.const instruction on the generic address denoted by ptr.



7.16.4. __cvta_generic_to_local()ï


__device__ size_t __cvta_generic_to_local(const void *ptr);


Returns the result of executing the PTXcvta.to.local instruction on the generic address denoted by ptr.



7.16.5. __cvta_global_to_generic()ï


__device__ void * __cvta_global_to_generic(size_t rawbits);


Returns the generic pointer obtained by executing the PTXcvta.global instruction on the value provided by rawbits.



7.16.6. __cvta_shared_to_generic()ï


__device__ void * __cvta_shared_to_generic(size_t rawbits);


Returns the generic pointer obtained by executing the PTXcvta.shared instruction on the value provided by rawbits.



7.16.7. __cvta_constant_to_generic()ï


__device__ void * __cvta_constant_to_generic(size_t rawbits);


Returns the generic pointer obtained by executing the PTXcvta.const instruction on the value provided by rawbits.



7.16.8. __cvta_local_to_generic()ï


__device__ void * __cvta_local_to_generic(size_t rawbits);


Returns the generic pointer obtained by executing the PTXcvta.local instruction on the value provided by rawbits.




7.17. Alloca Functionï



7.17.1. Synopsisï


__host__ __device__ void * alloca(size_t size);





7.17.2. Descriptionï

The alloca() function allocates size bytes of memory in the stack frame of the caller. The returned value is a pointer to allocated memory, the beginning of the memory is 16 bytes aligned when the function is invoked from device code. The allocated memory is automatically freed when the caller to alloca() is returned.

Note
On Windows platform, <malloc.h> must be included before using alloca(). Using alloca() may cause the stack to overflow, user needs to adjust stack size accordingly.

It is supported with compute capability 5.2 or higher.



7.17.3. Exampleï


__device__ void foo(unsigned int num) {
    int4 *ptr = (int4 *)alloca(num * sizeof(int4));
    // use of ptr
    ...
}






7.18. Compiler Optimization Hint Functionsï

The functions described in this section can be used to provide additional information to the compiler optimizer.


7.18.1. __builtin_assume_aligned()ï


void * __builtin_assume_aligned (const void *exp, size_t align)


Allows the compiler to assume that the argument pointer is aligned to at least align bytes, and returns the argument pointer.
Example:

void *res = __builtin_assume_aligned(ptr, 32); // compiler can assume 'res' is
                                               // at least 32-byte aligned


Three parameter version:

void * __builtin_assume_aligned (const void *exp, size_t align,
                                 <integral type> offset)


Allows the compiler to assume that (char *)exp - offset is aligned to at least align bytes, and returns the argument pointer.
Example:

void *res = __builtin_assume_aligned(ptr, 32, 8); // compiler can assume
                                                  // '(char *)res - 8' is
                                                  // at least 32-byte aligned.





7.18.2. __builtin_assume()ï


void __builtin_assume(bool exp)


Allows the compiler to assume that the Boolean argument is true. If the argument is not true at run time, then the behavior is undefined. Note that if the argument has side effects, the behavior is unspecified.
Example:

 __device__ int get(int *ptr, int idx) {
   __builtin_assume(idx <= 2);
   return ptr[idx];
}





7.18.3. __assume()ï


void __assume(bool exp)


Allows the compiler to assume that the Boolean argument is true. If the argument is not true at run time, then the behavior is undefined. Note that if the argument has side effects, the behavior is unspecified.
Example:

 __device__ int get(int *ptr, int idx) {
   __assume(idx <= 2);
   return ptr[idx];
}





7.18.4. __builtin_expect()ï


long __builtin_expect (long exp, long c)


Indicates to the compiler that it is expected that exp == c, and returns the value of exp. Typically used to indicate branch prediction information to the compiler.
Example:

// indicate to the compiler that likely "var == 0",
// so the body of the if-block is unlikely to be
// executed at run time.
if (__builtin_expect (var, 0))
  doit ();





7.18.5. __builtin_unreachable()ï


void __builtin_unreachable(void)


Indicates to the compiler that control flow never reaches the point where this function is being called from. The program has undefined behavior if the control flow does actually reach this point at run time.
Example:

// indicates to the compiler that the default case label is never reached.
switch (in) {
case 1: return 4;
case 2: return 10;
default: __builtin_unreachable();
}





7.18.6. Restrictionsï

__assume() is only supported when using cl.exe host compiler. The other functions are supported on all platforms, subject to the following restrictions:

If the host compiler supports the function, the function can be invoked from anywhere in translation unit.
Otherwise, the function must be invoked from within the body of a __device__/ __global__function, or only when the __CUDA_ARCH__ macro is defined12.





7.19. Warp Vote Functionsï


int __all_sync(unsigned mask, int predicate);
int __any_sync(unsigned mask, int predicate);
unsigned __ballot_sync(unsigned mask, int predicate);
unsigned __activemask();


Deprecation notice: __any, __all, and __ballot have been deprecated in CUDA 9.0 for all devices.
Removal notice: When targeting devices with compute capability 7.x or higher, __any, __all, and __ballot are no longer available and their sync variants should be used instead.
The warp vote functions allow the threads of a given warp to perform a reduction-and-broadcast operation. These functions take as input an integer predicate from each thread in the warp and compare those values with zero. The results of the comparisons are combined (reduced) across the active threads of the warp in one of the following ways, broadcasting a single return value to each participating thread:


__all_sync(unsigned mask, predicate):

Evaluate predicate for all non-exited threads in mask and return non-zero if and only if predicate evaluates to non-zero for all of them.


__any_sync(unsigned mask, predicate):

Evaluate predicate for all non-exited threads in mask and return non-zero if and only if predicate evaluates to non-zero for any of them.


__ballot_sync(unsigned mask, predicate):

Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp and the Nth thread is active.


__activemask():

Returns a 32-bit integer mask of all currently active threads in the calling warp. The Nth bit is set if the Nth lane in the warp is active when __activemask() is called. Inactive threads are represented by 0 bits in the returned mask. Threads which have exited the program are always marked as inactive. Note that threads that are convergent at an __activemask() call are not guaranteed to be convergent at subsequent instructions unless those instructions are synchronizing warp-builtin functions.


For __all_sync, __any_sync, and __ballot_sync, a mask must be passed that specifies the threads participating in the call. A bit, representing the threadâs lane ID, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.
These intrinsics do not imply a memory barrier. They do not guarantee any memory ordering.



7.20. Warp Match Functionsï

__match_any_sync and __match_all_sync perform a broadcast-and-compare operation of a variable between threads within a warp.
Supported by devices of compute capability 7.x or higher.


7.20.1. Synopsisï


unsigned int __match_any_sync(unsigned mask, T value);
unsigned int __match_all_sync(unsigned mask, T value, int *pred);


T can be int, unsigned int, long, unsigned long, long long, unsigned long long, float or double.



7.20.2. Descriptionï

The __match_sync() intrinsics permit a broadcast-and-compare of a value value across threads in a warp after synchronizing threads named in mask.

__match_any_sync

Returns mask of threads that have same value of value in mask

__match_all_sync

Returns mask if all threads in mask have the same value for value; otherwise 0 is returned. Predicate pred is set to true if all threads in mask have the same value of value; otherwise the predicate is set to false.


The new *_sync match intrinsics take in a mask indicating the threads participating in the call. A bit, representing the threadâs lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.
These intrinsics do not imply a memory barrier. They do not guarantee any memory ordering.




7.21. Warp Reduce Functionsï

The __reduce_sync(unsigned mask, T value) intrinsics perform a reduction operation on the data provided in value after synchronizing threads named in mask. T can be unsigned or signed for {add, min, max} and unsigned only for {and, or, xor} operations.
Supported by devices of compute capability 8.x or higher.


7.21.1. Synopsisï


// add/min/max
unsigned __reduce_add_sync(unsigned mask, unsigned value);
unsigned __reduce_min_sync(unsigned mask, unsigned value);
unsigned __reduce_max_sync(unsigned mask, unsigned value);
int __reduce_add_sync(unsigned mask, int value);
int __reduce_min_sync(unsigned mask, int value);
int __reduce_max_sync(unsigned mask, int value);

// and/or/xor
unsigned __reduce_and_sync(unsigned mask, unsigned value);
unsigned __reduce_or_sync(unsigned mask, unsigned value);
unsigned __reduce_xor_sync(unsigned mask, unsigned value);





7.21.2. Descriptionï



__reduce_add_sync, __reduce_min_sync, __reduce_max_sync


Returns the result of applying an arithmetic add, min, or max reduction operation on the values provided in value by each thread named in mask.


__reduce_and_sync, __reduce_or_sync, __reduce_xor_sync


Returns the result of applying a logical AND, OR, or XOR reduction operation on the values provided in value by each thread named in mask.


The mask indicates the threads participating in the call. A bit, representing the threadâs lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.
These intrinsics do not imply a memory barrier. They do not guarantee any memory ordering.




7.22. Warp Shuffle Functionsï

__shfl_sync, __shfl_up_sync, __shfl_down_sync, and __shfl_xor_sync exchange a variable between threads within a warp.
Supported by devices of compute capability 5.0 or higher.
Deprecation Notice: __shfl, __shfl_up, __shfl_down, and __shfl_xor have been deprecated in CUDA 9.0 for all devices.
Removal Notice: When targeting devices with compute capability 7.x or higher, __shfl, __shfl_up, __shfl_down, and __shfl_xor are no longer available and their sync variants should be used instead.


7.22.1. Synopsisï


T __shfl_sync(unsigned mask, T var, int srcLane, int width=warpSize);
T __shfl_up_sync(unsigned mask, T var, unsigned int delta, int width=warpSize);
T __shfl_down_sync(unsigned mask, T var, unsigned int delta, int width=warpSize);
T __shfl_xor_sync(unsigned mask, T var, int laneMask, int width=warpSize);


T can be int, unsigned int, long, unsigned long, long long, unsigned long long, float or double. With the cuda_fp16.h header included, T can also be __half or __half2. Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162.



7.22.2. Descriptionï

The __shfl_sync() intrinsics permit exchanging of a variable between threads within a warp without use of shared memory. The exchange occurs simultaneously for all active threads within the warp (and named in mask), moving 4 or 8 bytes of data per thread depending on the type.
Threads within a warp are referred to as lanes, and may have an index between 0 and warpSize-1 (inclusive). Four source-lane addressing modes are supported:

__shfl_sync()

Direct copy from indexed lane

__shfl_up_sync()

Copy from a lane with lower ID relative to caller

__shfl_down_sync()

Copy from a lane with higher ID relative to caller

__shfl_xor_sync()

Copy from a lane based on bitwise XOR of own lane ID


Threads may only read data from another thread which is actively participating in the __shfl_sync() command. If the target thread is inactive, the retrieved value is undefined.
All of the __shfl_sync() intrinsics take an optional width parameter which alters the behavior of the intrinsic. width must have a value which is a power of two in the range [1, warpSize] (i.e., 1, 2, 4, 8, 16 or 32). Results are undefined for other values.
__shfl_sync() returns the value of var held by the thread whose ID is given by srcLane. If width is less than warpSize then each subsection of the warp behaves as a separate entity with a starting logical lane ID of 0. If srcLane is outside the range [0:width-1], the value returned corresponds to the value of var held by the srcLane modulo width (i.e. within the same subsection).
__shfl_up_sync() calculates a source lane ID by subtracting delta from the callerâs lane ID. The value of var held by the resulting lane ID is returned: in effect, var is shifted up the warp by delta lanes. If width is less than warpSize then each subsection of the warp behaves as a separate entity with a starting logical lane ID of 0. The source lane index will not wrap around the value of width, so effectively the lower delta lanes will be unchanged.
__shfl_down_sync() calculates a source lane ID by adding delta to the callerâs lane ID. The value of var held by the resulting lane ID is returned: this has the effect of shifting var down the warp by delta lanes. If width is less than warpSize then each subsection of the warp behaves as a separate entity with a starting logical lane ID of 0. As for __shfl_up_sync(), the ID number of the source lane will not wrap around the value of width and so the upper delta lanes will remain unchanged.
__shfl_xor_sync() calculates a source line ID by performing a bitwise XOR of the callerâs lane ID with laneMask: the value of var held by the resulting lane ID is returned. If width is less than warpSize then each group of width consecutive threads are able to access elements from earlier groups of threads, however if they attempt to access elements from later groups of threads their own value of var will be returned. This mode implements a butterfly addressing pattern such as is used in tree reduction and broadcast.
The new *_sync shfl intrinsics take in a mask indicating the threads participating in the call. A bit, representing the threadâs lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.
Threads may only read data from another thread which is actively participating in the __shfl_sync() command. If the target thread is inactive, the retrieved value is undefined.
These intrinsics do not imply a memory barrier. They do not guarantee any memory ordering.



7.22.3. Examplesï



7.22.3.1. Broadcast of a single value across a warpï


#include <stdio.h>

__global__ void bcast(int arg) {
    int laneId = threadIdx.x & 0x1f;
    int value;
    if (laneId == 0)        // Note unused variable for
        value = arg;        // all threads except lane 0
    value = __shfl_sync(0xffffffff, value, 0);   // Synchronize all threads in warp, and get "value" from lane 0
    if (value != arg)
        printf("Thread %d failed.\n", threadIdx.x);
}

int main() {
    bcast<<< 1, 32 >>>(1234);
    cudaDeviceSynchronize();

    return 0;
}





7.22.3.2. Inclusive plus-scan across sub-partitions of 8 threadsï


#include <stdio.h>

__global__ void scan4() {
    int laneId = threadIdx.x & 0x1f;
    // Seed sample starting value (inverse of lane ID)
    int value = 31 - laneId;

    // Loop to accumulate scan within my partition.
    // Scan requires log2(n) == 3 steps for 8 threads
    // It works by an accumulated sum up the warp
    // by 1, 2, 4, 8 etc. steps.
    for (int i=1; i<=4; i*=2) {
        // We do the __shfl_sync unconditionally so that we
        // can read even from threads which won't do a
        // sum, and then conditionally assign the result.
        int n = __shfl_up_sync(0xffffffff, value, i, 8);
        if ((laneId & 7) >= i)
            value += n;
    }

    printf("Thread %d final value = %d\n", threadIdx.x, value);
}

int main() {
    scan4<<< 1, 32 >>>();
    cudaDeviceSynchronize();

    return 0;
}





7.22.3.3. Reduction across a warpï


#include <stdio.h>

__global__ void warpReduce() {
    int laneId = threadIdx.x & 0x1f;
    // Seed starting value as inverse lane ID
    int value = 31 - laneId;

    // Use XOR mode to perform butterfly reduction
    for (int i=16; i>=1; i/=2)
        value += __shfl_xor_sync(0xffffffff, value, i, 32);

    // "value" now contains the sum across all threads
    printf("Thread %d final value = %d\n", threadIdx.x, value);
}

int main() {
    warpReduce<<< 1, 32 >>>();
    cudaDeviceSynchronize();

    return 0;
}







7.23. Nanosleep Functionï



7.23.1. Synopsisï


void __nanosleep(unsigned ns);





7.23.2. Descriptionï

__nanosleep(ns) suspends the thread for a sleep duration of approximately ns nanoseconds.  The maximum sleep duration is approximately 1 millisecond.
It is supported with compute capability 7.0 or higher.



7.23.3. Exampleï

The following code implements a mutex with exponential back-off.

__device__ void mutex_lock(unsigned int *mutex) {
    unsigned int ns = 8;
    while (atomicCAS(mutex, 0, 1) == 1) {
        __nanosleep(ns);
        if (ns < 256) {
            ns *= 2;
        }
    }
}

__device__ void mutex_unlock(unsigned int *mutex) {
    atomicExch(mutex, 0);
}






7.24. Warp Matrix Functionsï

C++ warp matrix operations leverage Tensor Cores to accelerate matrix problems of the form D=A*B+C. These operations are supported on mixed-precision floating point data for devices of compute capability 7.0 or higher. This requires co-operation from all threads in a warp. In addition, these operations are allowed in conditional code only if the condition evaluates identically across the entire warp, otherwise the code execution is likely to hang.


7.24.1. Descriptionï

All following functions and types are defined in the namespace nvcuda::wmma. Sub-byte operations are considered preview, i.e. the data structures and APIs for them are subject to change and may not be compatible with future releases. This extra functionality is defined in the nvcuda::wmma::experimental namespace.

template<typename Use, int m, int n, int k, typename T, typename Layout=void> class fragment;

void load_matrix_sync(fragment<...> &a, const T* mptr, unsigned ldm);
void load_matrix_sync(fragment<...> &a, const T* mptr, unsigned ldm, layout_t layout);
void store_matrix_sync(T* mptr, const fragment<...> &a, unsigned ldm, layout_t layout);
void fill_fragment(fragment<...> &a, const T& v);
void mma_sync(fragment<...> &d, const fragment<...> &a, const fragment<...> &b, const fragment<...> &c, bool satf=false);



fragment

An overloaded class containing a section of a matrix distributed across all threads in the warp. The mapping of matrix elements into fragment internal storage is unspecified and subject to change in future architectures.


Only certain combinations of template arguments are allowed. The first template parameter specifies how the fragment will participate in the matrix operation. Acceptable values for Use are:

matrix_a when the fragment is used as the first multiplicand, A,
matrix_b when the fragment is used as the second multiplicand, B, or

accumulator when the fragment is used as the source or destination accumulators (C or D, respectively).
The m, n and k sizes describe the shape of the warp-wide matrix tiles participating in the multiply-accumulate operation. The dimension of each tile depends on its role. For matrix_a the tile takes dimension m x k; for matrix_b the dimension is k x n, and accumulator tiles are m x n.
The data type, T, may be double, float, __half, __nv_bfloat16, char, or unsigned char for multiplicands and double, float, int, or __half for accumulators. As documented in Element Types and Matrix Sizes, limited combinations of accumulator and multiplicand types are supported. The Layout parameter must be specified for matrix_a and matrix_b fragments. row_major or col_major indicate that elements within a matrix row or column are contiguous in memory, respectively. The Layout parameter for an accumulator matrix should retain the default value of void. A row or column layout is specified only when the accumulator is loaded or stored as described below.



load_matrix_sync

Waits until all warp lanes have arrived at load_matrix_sync and then loads the matrix fragment a from memory. mptr must be a 256-bit aligned pointer pointing to the first element of the matrix in memory. ldm describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout) and must be a multiple of 8 for __half element type or multiple of 4 for float element type. (i.e., multiple of 16 bytes in both cases). If the fragment is an accumulator, the layout argument must be specified as either mem_row_major or mem_col_major. For matrix_a and matrix_b fragments, the layout is inferred from the fragmentâs layout parameter. The values of mptr, ldm, layout and all template parameters for a must be the same for all threads in the warp. This function must be called by all threads in the warp, or the result is undefined.

store_matrix_sync

Waits until all warp lanes have arrived at store_matrix_sync and then stores the matrix fragment a to memory. mptr must be a 256-bit aligned pointer pointing to the first element of the matrix in memory. ldm describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout) and must be a multiple of 8 for __half element type or multiple of 4 for float element type. (i.e., multiple of 16 bytes in both cases). The layout of the output matrix must be specified as either mem_row_major or mem_col_major. The values of mptr, ldm, layout and all template parameters for a must be the same for all threads in the warp.

fill_fragment

Fill a matrix fragment with a constant value v. Because the mapping of matrix elements to each fragment is unspecified, this function is ordinarily called by all threads in the warp with a common value for v.

mma_sync

Waits until all warp lanes have arrived at mma_sync, and then performs the warp-synchronous matrix multiply-accumulate operation D=A*B+C. The in-place operation, C=A*B+C, is also supported. The value of satf and template parameters for each matrix fragment must be the same for all threads in the warp. Also, the template parameters m, n and k must match between fragments A, B, C and D. This function must be called by all threads in the warp, or the result is undefined.


If satf (saturate to finite value) mode is true, the following additional numerical properties apply for the destination accumulator:

If an element result is +Infinity, the corresponding accumulator will contain +MAX_NORM
If an element result is -Infinity, the corresponding accumulator will contain -MAX_NORM
If an element result is NaN, the corresponding accumulator will contain +0

Because the map of matrix elements into each threadâs fragment is unspecified, individual matrix elements must be accessed from memory (shared or global) after calling store_matrix_sync. In the special case where all threads in the warp will apply an element-wise operation uniformly to all fragment elements, direct element access can be implemented using the following fragment class members.

enum fragment<Use, m, n, k, T, Layout>::num_elements;
T fragment<Use, m, n, k, T, Layout>::x[num_elements];


As an example, the following code scales an accumulator matrix tile by half.

wmma::fragment<wmma::accumulator, 16, 16, 16, float> frag;
float alpha = 0.5f; // Same value for all threads in warp
/*...*/
for(int t=0; t<frag.num_elements; t++)
frag.x[t] *= alpha;





7.24.2. Alternate Floating Pointï

Tensor Cores support alternate types of floating point operations on devices with compute capability 8.0 and higher.

__nv_bfloat16

This data format is an alternate fp16 format that has the same range as f32 but reduced precision (7 bits). You can use this data format directly with the __nv_bfloat16 type available in cuda_bf16.h. Matrix fragments with __nv_bfloat16 data types are required to be composed with accumulators of float type. The shapes and operations supported are the same as with __half.

tf32

This data format is a special floating point format supported by Tensor Cores, with the same range as f32 and reduced precision (>=10 bits). The internal layout of this format is implementation defined. In order to use this floating point format with WMMA operations, the input matrices must be manually converted to tf32 precision.
To facilitate conversion, a new intrinsic __float_to_tf32 is provided. While the input and output arguments to the intrinsic are of float type, the output will be tf32 numerically. This new precision is intended to be used with Tensor Cores only, and if mixed with other floattype operations, the precision and range of the result will be undefined.
Once an input matrix (matrix_a or matrix_b) is converted to tf32 precision, the combination of a fragment with precision::tf32 precision, and a data type of float to load_matrix_sync will take advantage of this new capability. Both the accumulator fragments must have float data types. The only supported matrix size is 16x16x8 (m-n-k).
The elements of the fragment are represented as float, hence the mapping from element_type<T> to storage_element_type<T> is:

precision::tf32 -> float







7.24.3. Double Precisionï

Tensor Cores support double-precision floating point operations on devices with compute capability 8.0 and higher. To use this new functionality, a fragment with the double type must be used. The mma_sync operation will be performed with the .rn (rounds to nearest even) rounding modifier.



7.24.4. Sub-byte Operationsï

Sub-byte WMMA operations provide a way to access the low-precision capabilities of Tensor Cores. They are considered a preview feature i.e. the data structures and APIs for them are subject to change and may not be compatible with future releases. This functionality is available via the nvcuda::wmma::experimental namespace:

namespace experimental {
    namespace precision {
        struct u4; // 4-bit unsigned
        struct s4; // 4-bit signed
        struct b1; // 1-bit
   }
    enum bmmaBitOp {
        bmmaBitOpXOR = 1, // compute_75 minimum
        bmmaBitOpAND = 2  // compute_80 minimum
    };
    enum bmmaAccumulateOp { bmmaAccumulateOpPOPC = 1 };
}


For 4 bit precision, the APIs available remain the same, but you must specify experimental::precision::u4 or experimental::precision::s4 as the fragment data type. Since the elements of the fragment are packed together, num_storage_elements will be smaller than num_elements for that fragment. The num_elements variable for a sub-byte fragment, hence returns the number of elements of sub-byte type element_type<T>. This is true for single bit precision as well, in which case, the mapping from element_type<T> to storage_element_type<T> is as follows:

experimental::precision::u4 -> unsigned (8 elements in 1 storage element)
experimental::precision::s4 -> int (8 elements in 1 storage element)
experimental::precision::b1 -> unsigned (32 elements in 1 storage element)
T -> T  //all other types


The allowed layouts for sub-byte fragments is always row_major for matrix_a and col_major for matrix_b.
For sub-byte operations the value of ldm in load_matrix_sync should be a multiple of 32 for element type experimental::precision::u4 and experimental::precision::s4 or a multiple of 128 for element type experimental::precision::b1 (i.e., multiple of 16 bytes in both cases).

Note
Support for the following variants for MMA instructions is deprecated and will be removed in sm_90:



experimental::precision::u4
experimental::precision::s4
experimental::precision::b1 with bmmaBitOp set to bmmaBitOpXOR





bmma_sync

Waits until all warp lanes have executed bmma_sync, and then performs the warp-synchronous bit matrix multiply-accumulate operation D = (A op B) + C, where op consists of a logical operation bmmaBitOp followed by the accumulation defined by bmmaAccumulateOp. The available operations are:
bmmaBitOpXOR, a 128-bit XOR of a row in matrix_a with the 128-bit column of matrix_b
bmmaBitOpAND, a 128-bit AND of a row in matrix_a with the 128-bit column of matrix_b, available on devices with compute capability 8.0 and higher.
The accumulate op is always bmmaAccumulateOpPOPC which counts the number of set bits.





7.24.5. Restrictionsï

The special format required by tensor cores may be different for each major and minor device architecture. This is further complicated by threads holding only a fragment (opaque architecture-specific ABI data structure) of the overall matrix, with the developer not allowed to make assumptions on how the individual parameters are mapped to the registers participating in the matrix multiply-accumulate.
Since fragments are architecture-specific, it is unsafe to pass them from function A to function B if the functions have been compiled for different link-compatible architectures and linked together into the same device executable. In this case, the size and layout of the fragment will be specific to one architecture and using WMMA APIs in the other will lead to incorrect results or potentially, corruption.
An example of two link-compatible architectures, where the layout of the fragment differs, is sm_70 and sm_75.

fragA.cu: void foo() { wmma::fragment<...> mat_a; bar(&mat_a); }
fragB.cu: void bar(wmma::fragment<...> *mat_a) { // operate on mat_a }



// sm_70 fragment layout
$> nvcc -dc -arch=compute_70 -code=sm_70 fragA.cu -o fragA.o
// sm_75 fragment layout
$> nvcc -dc -arch=compute_75 -code=sm_75 fragB.cu -o fragB.o
// Linking the two together
$> nvcc -dlink -arch=sm_75 fragA.o fragB.o -o frag.o


This undefined behavior might also be undetectable at compilation time and by tools at runtime, so extra care is needed to make sure the layout of the fragments is consistent. This linking hazard is most likely to appear when linking with a legacy library that is both built for a different link-compatible architecture and expecting to be passed a WMMA fragment.
Note that in the case of weak linkages (for example, a CUDA C++ inline function), the linker may choose any available function definition which may result in implicit passes between compilation units.
To avoid these sorts of problems, the matrix should always be stored out to memory for transit through external interfaces (e.g. wmma::store_matrix_sync(dst, â¦);) and then it can be safely passed to bar() as a pointer type [e.g. float *dst].
Note that since sm_70 can run on sm_75, the above example sm_75 code can be changed to sm_70 and correctly work on sm_75. However, it is recommended to have sm_75 native code in your application when linking with other sm_75 separately compiled binaries.



7.24.6. Element Types and Matrix Sizesï

Tensor Cores support a variety of element types and matrix sizes. The following table presents the various combinations of matrix_a, matrix_b and accumulator matrix supported:









Matrix A
Matrix B
Accumulator
Matrix Size (m-n-k)




__half
__half
float
16x16x16


__half
__half
float
32x8x16


__half
__half
float
8x32x16


__half
__half
__half
16x16x16


__half
__half
__half
32x8x16


__half
__half
__half
8x32x16


unsigned char
unsigned char
int
16x16x16


unsigned char
unsigned char
int
32x8x16


unsigned char
unsigned char
int
8x32x16


signed char
signed char
int
16x16x16


signed char
signed char
int
32x8x16


signed char
signed char
int
8x32x16



Alternate Floating Point support:









Matrix A
Matrix B
Accumulator
Matrix Size (m-n-k)




__nv_bfloat16
__nv_bfloat16
float
16x16x16


__nv_bfloat16
__nv_bfloat16
float
32x8x16


__nv_bfloat16
__nv_bfloat16
float
8x32x16


precision::tf32
precision::tf32
float
16x16x8



Double Precision Support:









Matrix A
Matrix B
Accumulator
Matrix Size (m-n-k)




double
double
double
8x8x4



Experimental support for sub-byte operations:









Matrix A
Matrix B
Accumulator
Matrix Size (m-n-k)




precision::u4
precision::u4
int
8x8x32


precision::s4
precision::s4
int
8x8x32


precision::b1
precision::b1
int
8x8x128






7.24.7. Exampleï

The following code implements a 16x16x16 matrix multiplication in a single warp.

#include <mma.h>
using namespace nvcuda;

__global__ void wmma_ker(half *a, half *b, float *c) {
   // Declare the fragments
   wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> a_frag;
   wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag;
   wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;

   // Initialize the output to zero
   wmma::fill_fragment(c_frag, 0.0f);

   // Load the inputs
   wmma::load_matrix_sync(a_frag, a, 16);
   wmma::load_matrix_sync(b_frag, b, 16);

   // Perform the matrix multiplication
   wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

   // Store the output
   wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);
}






7.25. DPXï

DPX is a set of functions that enable finding min and max values, as well as fused addition and min/max, for up to three 16 and 32-bit signed or unsigned integer parameters, with optional ReLU (clamping to zero):

three parameters: __vimax3_s32, __vimax3_s16x2, __vimax3_u32, __vimax3_u16x2, __vimin3_s32, __vimin3_s16x2, __vimin3_u32, __vimin3_u16x2
two parameters, with ReLU: __vimax_s32_relu, __vimax_s16x2_relu, __vimin_s32_relu, __vimin_s16x2_relu
three parameters, with ReLU: __vimax3_s32_relu, __vimax3_s16x2_relu, __vimin3_s32_relu, __vimin3_s16x2_relu
two parameters, also returning which parameter was smaller/larger: __vibmax_s32, __vibmax_u32, __vibmin_s32, __vibmin_u32, __vibmax_s16x2, __vibmax_u16x2, __vibmin_s16x2, __vibmin_u16x2
three parameters, comparing (first + second) with the third: __viaddmax_s32, __viaddmax_s16x2, __viaddmax_u32, __viaddmax_u16x2, __viaddmin_s32, __viaddmin_s16x2, __viaddmin_u32, __viaddmin_u16x2
three parameters, with ReLU, comparing (first + second) with the third and a zero: __viaddmax_s32_relu, __viaddmax_s16x2_relu, __viaddmin_s32_relu, __viaddmin_s16x2_relu

These instructions are hardware-accelerated on devices with compute capability 9 and higher, and software emulation on older devices.
Full API can be found in CUDA Math API documentation.
DPX is exceptionally useful when implementing dynamic programming algorithms, such as Smith-Waterman or NeedlemanâWunsch in genomics and Floyd-Warshall in route optimization.


7.25.1. Examplesï

Max value of three signed 32-bit integers, with ReLU

const int a = -15;
const int b = 8;
const int c = 5;
int max_value_0 = __vimax3_s32_relu(a, b, c); // max(-15, 8, 5, 0) = 8
const int d = -2;
const int e = -4;
int max_value_1 = __vimax3_s32_relu(a, d, e); // max(-15, -2, -4, 0) = 0


Min value of the sum of two 32-bit signed integers, another 32-bit signed integer and a zero (ReLU)

const int a = -5;
const int b = 6;
const int c = -2;
int max_value_0 = __viaddmax_s32_relu(a, b, c); // max(-5 + 6, -2, 0) = max(1, -2, 0) = 1
const int d = 4;
int max_value_1 = __viaddmax_s32_relu(a, d, c); // max(-5 + 4, -2, 0) = max(-1, -2, 0) = 0


Min value of two unsigned 32-bit integers and determining which value is smaller

const unsigned int a = 9;
const unsigned int b = 6;
bool smaller_value;
unsigned int min_value = __vibmin_u32(a, b, &smaller_value); // min_value is 6, smaller_value is true


Max values of three pairs of unsigned 16-bit integers

const unsigned a = 0x00050002;
const unsigned b = 0x00070004;
const unsigned c = 0x00020006;
unsigned int max_value = __vimax3_u16x2(a, b, c); // max(5, 7, 2) and max(2, 4, 6), so max_value is 0x00070006






7.26. Asynchronous Barrierï

The NVIDIA C++ standard library introduces a GPU implementation of std::barrier. Along with the implementation of std::barrier the library provides extensions that allow users to specify the scope of barrier objects. The barrier API scopes are documented under Thread Scopes. Devices of compute capability 8.0 or higher provide hardware acceleration for barrier operations and integration of these barriers with the memcpy_async feature. On devices with compute capability below 8.0 but starting 7.0, these barriers are available without hardware acceleration.
nvcuda::experimental::awbarrier is deprecated in favor of cuda::barrier.


7.26.1. Simple Synchronization Patternï

Without the arrive/wait barrier, synchronization is achieved using __syncthreads() (to synchronize all threads in a block) or group.sync() when using Cooperative Groups.

#include <cooperative_groups.h>

__global__ void simple_sync(int iteration_count) {
    auto block = cooperative_groups::this_thread_block();

    for (int i = 0; i < iteration_count; ++i) {
        /* code before arrive */
        block.sync(); /* wait for all threads to arrive here */
        /* code after wait */
    }
}


Threads are blocked at the synchronization point (block.sync()) until all threads have reached the synchronization point. In addition, memory updates that happened before the synchronization point are guaranteed to be visible to all threads in the block after the synchronization point, i.e., equivalent to atomic_thread_fence(memory_order_seq_cst, thread_scope_block) as well as the sync.
This pattern has three stages:

Code before sync performs memory updates that will be read after the sync.
Synchronization point
Code after sync point with visibility of memory updates that happened before sync point.




7.26.2. Temporal Splitting and Five Stages of Synchronizationï

The temporally-split synchronization pattern with the std::barrier is as follows.

#include <cuda/barrier>
#include <cooperative_groups.h>

__device__ void compute(float* data, int curr_iteration);

__global__ void split_arrive_wait(int iteration_count, float *data) {
    using barrier = cuda::barrier<cuda::thread_scope_block>;
    __shared__  barrier bar;
    auto block = cooperative_groups::this_thread_block();

    if (block.thread_rank() == 0) {
        init(&bar, block.size()); // Initialize the barrier with expected arrival count
    }
    block.sync();

    for (int curr_iter = 0; curr_iter < iteration_count; ++curr_iter) {
        /* code before arrive */
       barrier::arrival_token token = bar.arrive(); /* this thread arrives. Arrival does not block a thread */
       compute(data, curr_iter);
       bar.wait(std::move(token)); /* wait for all threads participating in the barrier to complete bar.arrive()*/
        /* code after wait */
    }
}


In this pattern, the synchronization point (block.sync()) is split into an arrive point (bar.arrive()) and a wait point (bar.wait(std::move(token))). A thread begins participating in a cuda::barrier with its first call to bar.arrive(). When a thread calls bar.wait(std::move(token)) it will be blocked until participating threads have completed bar.arrive() the expected number of times as specified by the expected arrival count argument passed to init(). Memory updates that happen before participating threadsâ call to bar.arrive() are guaranteed to be visible to participating threads after their call to bar.wait(std::move(token)). Note that the call to bar.arrive() does not block a thread, it can proceed with other work that does not depend upon memory updates that happen before other participating threadsâ call to bar.arrive().
The arrive and then wait pattern has five stages which may be iteratively repeated:

Code before arrive performs memory updates that will be read after the wait.
Arrive point with implicit memory fence (i.e., equivalent to atomic_thread_fence(memory_order_seq_cst, thread_scope_block)).
Code between arrive and wait.
Wait point.
Code after the wait, with visibility of updates that were performed before the arrive.




7.26.3. Bootstrap Initialization, Expected Arrival Count, and Participationï

Initialization must happen before any thread begins participating in a cuda::barrier.

#include <cuda/barrier>
#include <cooperative_groups.h>

__global__ void init_barrier() {
    __shared__ cuda::barrier<cuda::thread_scope_block> bar;
    auto block = cooperative_groups::this_thread_block();

    if (block.thread_rank() == 0) {
        init(&bar, block.size()); // Single thread initializes the total expected arrival count.
    }
    block.sync();
}


Before any thread can participate in cuda::barrier, the barrier must be initialized using init() with an expected arrival count, block.size() in this example. Initialization must happen before any thread calls bar.arrive(). This poses a bootstrapping challenge in that threads must synchronize before participating in the cuda::barrier, but threads are creating a cuda::barrier in order to synchronize. In this example, threads that will participate are part of a cooperative group and use block.sync() to bootstrap initialization. In this example a whole thread block is participating in initialization, hence __syncthreads() could also be used.
The second parameter of init() is the expected arrival count, i.e., the number of times bar.arrive() will be called by participating threads before a participating thread is unblocked from its call to bar.wait(std::move(token)). In the prior example the cuda::barrier is initialized with the number of threads in the thread block i.e., cooperative_groups::this_thread_block().size(), and all threads within the thread block participate in the barrier.
A cuda::barrier is flexible in specifying how threads participate (split arrive/wait) and which threads participate. In contrast this_thread_block.sync() from cooperative groups or __syncthreads() is applicable to whole-thread-block and __syncwarp(mask) is a specified subset of a warp. If the intention of the user is to synchronize a full thread block or a full warp we recommend using __syncthreads() and __syncwarp(mask) respectively for performance reasons.



7.26.4. A Barrierâs Phase: Arrival, Countdown, Completion, and Resetï

A cuda::barrier counts down from the expected arrival count to zero as participating threads call bar.arrive(). When the countdown reaches zero, a cuda::barrier is complete for the current phase. When the last call to bar.arrive() causes the countdown to reach zero, the countdown is automatically and atomically reset. The reset assigns the countdown to the expected arrival count, and moves the cuda::barrier to the next phase.
A token object of class cuda::barrier::arrival_token, as returned from token=bar.arrive(), is associated with the current phase of the barrier. A call to bar.wait(std::move(token)) blocks the calling thread while the cuda::barrier is in the current phase, i.e., while the phase associated with the token matches the phase of the cuda::barrier. If the phase is advanced (because the countdown reaches zero) before the call to bar.wait(std::move(token)) then the thread does not block; if the phase is advanced while the thread is blocked in bar.wait(std::move(token)), the thread is unblocked.
It is essential to know when a reset could or could not occur, especially in non-trivial arrive/wait synchronization patterns.

A threadâs calls to token=bar.arrive() and bar.wait(std::move(token)) must be sequenced such that token=bar.arrive() occurs during the cuda::barrierâs current phase, and bar.wait(std::move(token)) occurs during the same or next phase.
A threadâs call to bar.arrive() must occur when the barrierâs counter is non-zero. After barrier initialization, if a threadâs call to bar.arrive() causes the countdown to reach zero then a call to bar.wait(std::move(token)) must happen before the barrier can be reused for a subsequent call to bar.arrive().
bar.wait() must only be called using a token object of the current phase or the immediately preceding phase. For any other values of the token object, the behavior is undefined.

For simple arrive/wait synchronization patterns, compliance with these usage rules is straightforward.



7.26.5. Spatial Partitioning (also known as Warp Specialization)ï

A thread block can be spatially partitioned such that warps are specialized to perform independent computations. Spatial partitioning is used in a producer or consumer pattern, where one subset of threads produces data that is concurrently consumed by the other (disjoint) subset of threads.
A producer/consumer spatial partitioning pattern requires two one sided synchronizations to manage a data buffer between the producer and consumer.







Producer
Consumer




wait for buffer to be ready to be filled
signal buffer is ready to be filled


produce data and fill the buffer



signal buffer is filled
wait for buffer to be filled



consume data in filled buffer



Producer threads wait for consumer threads to signal that the buffer is ready to be filled; however, consumer threads do not wait for this signal. Consumer threads wait for producer threads to signal that the buffer is filled; however, producer threads do not wait for this signal. For full producer/consumer concurrency this pattern has (at least) double buffering where each buffer requires two cuda::barriers.

#include <cuda/barrier>
#include <cooperative_groups.h>

using barrier = cuda::barrier<cuda::thread_scope_block>;

__device__ void producer(barrier ready[], barrier filled[], float* buffer, float* in, int N, int buffer_len)
{
    for (int i = 0; i < (N/buffer_len); ++i) {
        ready[i%2].arrive_and_wait(); /* wait for buffer_(i%2) to be ready to be filled */
        /* produce, i.e., fill in, buffer_(i%2)  */
        barrier::arrival_token token = filled[i%2].arrive(); /* buffer_(i%2) is filled */
    }
}

__device__ void consumer(barrier ready[], barrier filled[], float* buffer, float* out, int N, int buffer_len)
{
    barrier::arrival_token token1 = ready[0].arrive(); /* buffer_0 is ready for initial fill */
    barrier::arrival_token token2 = ready[1].arrive(); /* buffer_1 is ready for initial fill */
    for (int i = 0; i < (N/buffer_len); ++i) {
        filled[i%2].arrive_and_wait(); /* wait for buffer_(i%2) to be filled */
        /* consume buffer_(i%2) */
        barrier::arrival_token token = ready[i%2].arrive(); /* buffer_(i%2) is ready to be re-filled */
    }
}

//N is the total number of float elements in arrays in and out
__global__ void producer_consumer_pattern(int N, int buffer_len, float* in, float* out) {

    // Shared memory buffer declared below is of size 2 * buffer_len
    // so that we can alternatively work between two buffers.
    // buffer_0 = buffer and buffer_1 = buffer + buffer_len
    __shared__ extern float buffer[];

    // bar[0] and bar[1] track if buffers buffer_0 and buffer_1 are ready to be filled,
    // while bar[2] and bar[3] track if buffers buffer_0 and buffer_1 are filled-in respectively
    __shared__ barrier bar[4];


    auto block = cooperative_groups::this_thread_block();
    if (block.thread_rank() < 4)
        init(bar + block.thread_rank(), block.size());
    block.sync();

    if (block.thread_rank() < warpSize)
        producer(bar, bar+2, buffer, in, N, buffer_len);
    else
        consumer(bar, bar+2, buffer, out, N, buffer_len);
}


In this example the first warp is specialized as the producer and the remaining warps are specialized as the consumer. All producer and consumer threads participate (call bar.arrive() or bar.arrive_and_wait()) in each of the four cuda::barriers so the expected arrival counts are equal to block.size().
A producer thread waits for the consumer threads to signal that the shared memory buffer can be filled. In order to wait for a cuda::barrier a producer thread must first arrive on that ready[i%2].arrive() to get a token and then ready[i%2].wait(token) with that token. For simplicity ready[i%2].arrive_and_wait() combines these operations.

bar.arrive_and_wait();
/* is equivalent to */
bar.wait(bar.arrive());


Producer threads compute and fill the ready buffer, they then signal that the buffer is filled by arriving on the filled barrier, filled[i%2].arrive(). A producer thread does not wait at this point, instead it waits until the next iterationâs buffer (double buffering) is ready to be filled.
A consumer thread begins by signaling that both buffers are ready to be filled. A consumer thread does not wait at this point, instead it waits for this iterationâs buffer to be filled, filled[i%2].arrive_and_wait(). After the consumer threads consume the buffer they signal that the buffer is ready to be filled again, ready[i%2].arrive(), and then wait for the next iterationâs buffer to be filled.



7.26.6. Early Exit (Dropping out of Participation)ï

When a thread that is participating in a sequence of synchronizations must exit early from that sequence, that thread must explicitly drop out of participation before exiting. The remaining participating threads can proceed normally with subsequent cuda::barrier arrive and wait operations.

#include <cuda/barrier>
#include <cooperative_groups.h>

__device__ bool condition_check();

__global__ void early_exit_kernel(int N) {
    using barrier = cuda::barrier<cuda::thread_scope_block>;
    __shared__ barrier bar;
    auto block = cooperative_groups::this_thread_block();

    if (block.thread_rank() == 0)
        init(&bar , block.size());
    block.sync();

    for (int i = 0; i < N; ++i) {
        if (condition_check()) {
          bar.arrive_and_drop();
          return;
        }
        /* other threads can proceed normally */
        barrier::arrival_token token = bar.arrive();
        /* code between arrive and wait */
        bar.wait(std::move(token)); /* wait for all threads to arrive */
        /* code after wait */
    }
}


This operation arrives on the cuda::barrier to fulfill the participating threadâs obligation to arrive in the current phase, and then decrements the expected arrival count for the next phase so that this thread is no longer expected to arrive on the barrier.



7.26.7. Completion Functionï

The CompletionFunction of cuda::barrier<Scope, CompletionFunction> is executed once per phase, after the last thread arrives and before any thread is unblocked from the wait. Memory operations performed by the threads that arrived at the barrier during the phase are visible to the thread executing the CompletionFunction, and all memory operations performed within the CompletionFunction are visible to all threads waiting at the barrier once they are unblocked from the wait.

#include <cuda/barrier>
#include <cooperative_groups.h>
#include <functional>
namespace cg = cooperative_groups;

__device__ int divergent_compute(int*, int);
__device__ int independent_computation(int*, int);

__global__ void psum(int* data, int n, int* acc) {
  auto block = cg::this_thread_block();

  constexpr int BlockSize = 128;
  __shared__ int smem[BlockSize];
  assert(BlockSize == block.size());
  assert(n % 128 == 0);

  auto completion_fn = [&] {
    int sum = 0;
    for (int i = 0; i < 128; ++i) sum += smem[i];
    *acc += sum;
  };

  // Barrier storage
  // Note: the barrier is not default-constructible because
  //       completion_fn is not default-constructible due
  //       to the capture.
  using completion_fn_t = decltype(completion_fn);
  using barrier_t = cuda::barrier<cuda::thread_scope_block,
                                  completion_fn_t>;
  __shared__ std::aligned_storage<sizeof(barrier_t),
                                  alignof(barrier_t)> bar_storage;

  // Initialize barrier:
  barrier_t* bar = (barrier_t*)&bar_storage;
  if (block.thread_rank() == 0) {
    assert(*acc == 0);
    assert(blockDim.x == blockDim.y == blockDim.y == 1);
    new (bar) barrier_t{block.size(), completion_fn};
    // equivalent to: init(bar, block.size(), completion_fn);
  }
  block.sync();

  // Main loop
  for (int i = 0; i < n; i += block.size()) {
    smem[block.thread_rank()] = data[i] + *acc;
    auto t = bar->arrive();
    // We can do independent computation here
    bar->wait(std::move(t));
    // shared-memory is safe to re-use in the next iteration
    // since all threads are done with it, including the one
    // that did the reduction
  }
}





7.26.8. Memory Barrier Primitives Interfaceï

Memory barrier primitives are C-like interfaces to cuda::barrier functionality. These primitives are available through including the <cuda_awbarrier_primitives.h> header.


7.26.8.1. Data Typesï


typedef /* implementation defined */ __mbarrier_t;
typedef /* implementation defined */ __mbarrier_token_t;





7.26.8.2. Memory Barrier Primitives APIï


uint32_t __mbarrier_maximum_count();
void __mbarrier_init(__mbarrier_t* bar, uint32_t expected_count);



bar must be a pointer to __shared__ memory.
expected_count <= __mbarrier_maximum_count()
Initialize *bar expected arrival count for the current and next phase to expected_count.


void __mbarrier_inval(__mbarrier_t* bar);



bar must be a pointer to the mbarrier object residing in shared memory.
Invalidation of *bar is required before the corresponding shared memory can be repurposed.


__mbarrier_token_t __mbarrier_arrive(__mbarrier_t* bar);



Initialization of *bar must happen before this call.
Pending count must not be zero.
Atomically decrement the pending count for the current phase of the barrier.
Return an arrival token associated with the barrier state immediately prior to the decrement.


__mbarrier_token_t __mbarrier_arrive_and_drop(__mbarrier_t* bar);



Initialization of *bar must happen before this call.
Pending count must not be zero.
Atomically decrement the pending count for the current phase and expected count for the next phase of the barrier.
Return an arrival token associated with the barrier state immediately prior to the decrement.


bool __mbarrier_test_wait(__mbarrier_t* bar, __mbarrier_token_t token);



token must be associated with the immediately preceding phase or current phase of *this.
Returns true if token is associated with the immediately preceding phase of *bar, otherwise returns false.


//Note: This API has been deprecated in CUDA 11.1
uint32_t __mbarrier_pending_count(__mbarrier_token_t token);







7.27. Asynchronous Data Copiesï

CUDA 11 introduces Asynchronous Data operations with memcpy_async API to allow device code to explicitly manage the asynchronous copying of data. The memcpy_async feature enables CUDA kernels to overlap computation with data movement.


7.27.1. memcpy_async APIï

The memcpy_async APIs are provided in the cuda/barrier, cuda/pipeline, and cooperative_groups/memcpy_async.h header files.
The cuda::memcpy_async APIs work with cuda::barrier and cuda::pipeline synchronization primitives, while the cooperative_groups::memcpy_async synchronizes using coopertive_groups::wait.
These APIs have very similar semantics: copy objects from src to dst as-if performed by another thread which, on completion of the copy, can be synchronized through cuda::pipeline, cuda::barrier, or cooperative_groups::wait.
The complete API documentation of the cuda::memcpy_async overloads for cuda::barrier and cuda::pipeline is provided in the libcudacxx API documentation along with some examples.
The API documentation of cooperative_groups::memcpy_async is provided in the cooperative groups Section of the documentation.
The memcpy_async APIs that use cuda::barrier and cuda::pipeline require compute capability 7.0 or higher. On devices with compute capability 8.0 or higher, memcpy_async operations from global to shared memory can benefit from hardware acceleration.



7.27.2. Copy and Compute Pattern - Staging Data Through Shared Memoryï

CUDA applications often employ a copy and compute pattern that:

fetches data from global memory,
stores data to shared memory, and
performs computations on shared memory data, and potentially writes results back to global memory.

The following sections illustrate how this pattern can be expressed without and with the memcpy_async feature:

The section Without memcpy_async introduces an example that does not overlap computation with data movement and uses an intermediate register to copy data.
The section With memcpy_async improves the previous example by introducing the cooperative_groups::memcpy_async and the cuda::memcpy_async APIs to directly copy data from global to shared memory without using intermediate registers.
Section Asynchronous Data Copies using cuda::barrier shows memcpy with cooperative groups and barrier
Section Single-Stage Asynchronous Data Copies using cuda::pipeline show memcpy with single stage pipeline
Section Multi-Stage Asynchronous Data Copies using cuda::pipeline show memcpy with multi stage pipeline




7.27.3. Without memcpy_asyncï

Without memcpy_async, the copy phase of the copy and compute pattern is expressed as shared[local_idx] = global[global_idx]. This global to shared memory copy is expanded to a read from global memory into a register, followed by a write to shared memory from the register.
When this pattern occurs within an iterative algorithm, each thread block needs to synchronize after the shared[local_idx] = global[global_idx] assignment, to ensure all writes to shared memory have completed before the compute phase can begin. The thread block also needs to synchronize again after the compute phase, to prevent overwriting shared memory before all threads have completed their computations. This pattern is illustrated in the following code snippet.

#include <cooperative_groups.h>
__device__ void compute(int* global_out, int const* shared_in) {
    // Computes using all values of current batch from shared memory.
    // Stores this thread's result back to global memory.
}

__global__ void without_memcpy_async(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
  auto grid = cooperative_groups::this_grid();
  auto block = cooperative_groups::this_thread_block();
  assert(size == batch_sz * grid.size()); // Exposition: input size fits batch_sz * grid_size

  extern __shared__ int shared[]; // block.size() * sizeof(int) bytes

  size_t local_idx = block.thread_rank();

  for (size_t batch = 0; batch < batch_sz; ++batch) {
    // Compute the index of the current batch for this block in global memory:
    size_t block_batch_idx = block.group_index().x * block.size() + grid.size() * batch;
    size_t global_idx = block_batch_idx + threadIdx.x;
    shared[local_idx] = global_in[global_idx];

    block.sync(); // Wait for all copies to complete

    compute(global_out + block_batch_idx, shared); // Compute and write result to global memory

    block.sync(); // Wait for compute using shared memory to finish
  }
}





7.27.4. With memcpy_asyncï

With memcpy_async, the assignment of shared memory from global memory

shared[local_idx] = global_in[global_idx];


is replaced with an asynchronous copy operation from cooperative groups

cooperative_groups::memcpy_async(group, shared, global_in + batch_idx, sizeof(int) * block.size());


The cooperative_groups::memcpy_async API copies sizeof(int) * block.size() bytes from global memory starting at global_in + batch_idx to the shared data. This operation happens as-if performed by another thread, which synchronizes with the current threadâs call to cooperative_groups::wait after the copy has completed. Until the copy operation completes, modifying the global data or reading or writing the shared data introduces a data race.
On devices with compute capability 8.0 or higher, memcpy_async transfers from global to shared memory can benefit from hardware acceleration, which avoids transfering the data through an intermediate register.

#include <cooperative_groups.h>
#include <cooperative_groups/memcpy_async.h>

__device__ void compute(int* global_out, int const* shared_in);

__global__ void with_memcpy_async(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
  auto grid = cooperative_groups::this_grid();
  auto block = cooperative_groups::this_thread_block();
  assert(size == batch_sz * grid.size()); // Exposition: input size fits batch_sz * grid_size

  extern __shared__ int shared[]; // block.size() * sizeof(int) bytes

  for (size_t batch = 0; batch < batch_sz; ++batch) {
    size_t block_batch_idx = block.group_index().x * block.size() + grid.size() * batch;
    // Whole thread-group cooperatively copies whole batch to shared memory:
    cooperative_groups::memcpy_async(block, shared, global_in + block_batch_idx, sizeof(int) * block.size());

    cooperative_groups::wait(block); // Joins all threads, waits for all copies to complete

    compute(global_out + block_batch_idx, shared);

    block.sync();
  }
}}





7.27.5. Asynchronous Data Copies using cuda::barrierï

The cuda::memcpy_async overload for cuda::barrier enables synchronizing asynchronous data transfers using a barrier. This overloads executes the copy operation as-if performed by another thread bound to the barrier by: incrementing the expected count of the current phase on creation, and decrementing it on completion of the copy operation, such that the phase of the barrier will only advance when all threads participating in the barrier have arrived, and all memcpy_async bound to the current phase of the barrier have completed. The following example uses a block-wide barrier, where all block threads participate, and swaps the wait operation with a barrier arrive_and_wait, while providing the same functionality as the previous example:

#include <cooperative_groups.h>
#include <cuda/barrier>
__device__ void compute(int* global_out, int const* shared_in);

__global__ void with_barrier(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
  auto grid = cooperative_groups::this_grid();
  auto block = cooperative_groups::this_thread_block();
  assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size

  extern __shared__ int shared[]; // block.size() * sizeof(int) bytes

  // Create a synchronization object (C++20 barrier)
  __shared__ cuda::barrier<cuda::thread_scope::thread_scope_block> barrier;
  if (block.thread_rank() == 0) {
    init(&barrier, block.size()); // Friend function initializes barrier
  }
  block.sync();

  for (size_t batch = 0; batch < batch_sz; ++batch) {
    size_t block_batch_idx = block.group_index().x * block.size() + grid.size() * batch;
    cuda::memcpy_async(block, shared, global_in + block_batch_idx, sizeof(int) * block.size(), barrier);

    barrier.arrive_and_wait(); // Waits for all copies to complete

    compute(global_out + block_batch_idx, shared);

    block.sync();
  }
}





7.27.6. Performance Guidance for memcpy_asyncï

For compute capability 8.x, the pipeline mechanism is shared among CUDA threads in the same CUDA warp. This sharing causes batches of memcpy_async to be entangled within a warp, which can impact performance under certain circumstances.
This section highlights the warp-entanglement effect on commit, wait, and arrive operations. Please refer to the Pipeline Interface and the Pipeline Primitives Interface for an overview of the individual operations.


7.27.6.1. Alignmentï

On devices with compute capability 8.0, the cp.async family of instructions allows copying data from global to shared memory asynchronously. These instructions support copying 4, 8, and 16 bytes at a time. If the size provided to memcpy_async is a multiple of 4, 8, or 16, and both pointers passed to memcpy_async are aligned to a 4, 8, or 16 alignment boundary, then memcpy_async can be implemented using exclusively asynchronous memory operations.
Additionally for achieving best performance when using memcpy_async API, an alignment of 128 Bytes for both shared memory and global memory is required.
For pointers to values of types with an alignment requirement of 1 or 2, it is often not possible to prove that the pointers are always aligned to a higher alignment boundary. Determining whether the cp.async instructions can or cannot be used must be delayed until run-time. Performing such a runtime alignment check increases code-size and adds runtime overhead.
The cuda::aligned_size_t<size_t Align>(size_t size)Shape can be used to supply a proof that both pointers passed to memcpy_async are aligned to an Align alignment boundary and that size is a multiple of Align, by passing it as an argument where the memcpy_async APIs expect a Shape:

cuda::memcpy_async(group, dst, src, cuda::aligned_size_t<16>(N * block.size()), pipeline);


If the proof is incorrect, the behavior is undefined.



7.27.6.2. Trivially copyableï

On devices with compute capability 8.0, the cp.async family of instructions allows copying data from global to shared memory asynchronously. If the pointer types passed to memcpy_async do not point to TriviallyCopyable types, the copy constructor of each output element needs to be invoked, and these instructions cannot be used to accelerate memcpy_async.



7.27.6.3. Warp Entanglement - Commitï

The sequence of memcpy_async batches is shared across the warp. The commit operation is coalesced such that the sequence is incremented once for all converged threads that invoke the commit operation. If the warp is fully converged, the sequence is incremented by one; if the warp is fully diverged, the sequence is incremented by 32.


Let PB be the warp-shared pipelineâs actual sequence of batches.
PB = {BP0, BP1, BP2, â¦, BPL}


Let TB be a threadâs perceived sequence of batches, as if the sequence were only incremented by this threadâs invocation of the commit operation.
TB = {BT0, BT1, BT2, â¦, BTL}
The pipeline::producer_commit() return value is from the threadâs perceived batch sequence.


An index in a threadâs perceived sequence always aligns to an equal or larger index in the actual warp-shared sequence. The sequences are equal only when all commit operations are invoked from converged threads.
BTn â¡ BPm where n <= m


For example, when a warp is fully diverged:

The warp-shared pipelineâs actual sequence would be: PB = {0, 1, 2, 3, ..., 31} (PL=31).

The perceived sequence for each thread of this warp would be:

Thread 0: TB = {0} (TL=0)
Thread 1: TB = {0} (TL=0)
â¦
Thread 31: TB = {0} (TL=0)






7.27.6.4. Warp Entanglement - Waitï

A CUDA thread invokes either pipeline_consumer_wait_prior<N>() or pipeline::consumer_wait() to wait for batches in the perceived sequence TB to complete. Note that pipeline::consumer_wait() is equivalent to pipeline_consumer_wait_prior<N>(), where N =Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  PL.
The pipeline_consumer_wait_prior<N>() function waits for batches in the actual sequence at least up to and including PL-N. Since TL <= PL, waiting for batch up to and including PL-N includes waiting for batch TL-N. Thus, when TL < PL, the thread will unintentionally wait for additional, more recent batches.
In the extreme fully-diverged warp example above, each thread could wait for all 32 batches.



7.27.6.5. Warp Entanglement - Arrive-Onï

Warp-divergence affects the number of times an arrive_on(bar) operation updates the barrier. If the invoking warp is fully converged, then the barrier is updated once. If the invoking warp is fully diverged, then 32 individual updates are applied to the barrier.



7.27.6.6. Keep Commit and Arrive-On Operations Convergedï

It is recommended that commit and arrive-on invocations are by converged threads:

to not over-wait, by keeping threadsâ perceived sequence of batches aligned with the actual sequence, and
to minimize updates to the barrier object.

When code preceding these operations diverges threads, then the warp should be re-converged, via __syncwarp before invoking commit or arrive-on operations.





7.28. Asynchronous Data Copies using cuda::pipelineï

CUDA provides the cuda::pipeline synchronization object to manage and overlap asynchronous data movement with computation.
The API documentation for cuda::pipeline is provided in the libcudacxx API. A pipeline object is a double-ended N stage queue with a head and a tail, and is used to process work in a first-in first-out (FIFO) order. The pipeline object has following member functions to manage the stages of the pipeline.







Pipeline Class Member Function
Description




producer_acquire
Acquires an available stage in the pipeline internal queue.


producer_commit
Commits the asynchronous operations issued after the producer_acquire call on the currently acquired stage of the pipeline.


consumer_wait
Wait for completion of all asynchronous operations on the oldest stage of the pipeline.


consumer_release
Release the oldest stage of the pipeline to the pipeline object for reuse. The released stage can be then acquired by the producer.





7.28.1. Single-Stage Asynchronous Data Copies using cuda::pipelineï

In previous examples we showed how to use cooperative_groups and cuda::barrier to do asynchronous data transfers. In this section, we will use the cuda::pipeline API with a single stage to schedule asynchronous copies. And later we will expand this example to show multi staged overlapped compute and copy.

#include <cooperative_groups/memcpy_async.h>
#include <cuda/pipeline>

__device__ void compute(int* global_out, int const* shared_in);
__global__ void with_single_stage(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
    auto grid = cooperative_groups::this_grid();
    auto block = cooperative_groups::this_thread_block();
    assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size

    constexpr size_t stages_count = 1; // Pipeline with one stage
    // One batch must fit in shared memory:
    extern __shared__ int shared[];  // block.size() * sizeof(int) bytes

    // Allocate shared storage for a single stage cuda::pipeline:
    __shared__ cuda::pipeline_shared_state<
        cuda::thread_scope::thread_scope_block,
        stages_count
    > shared_state;
    auto pipeline = cuda::make_pipeline(block, &shared_state);

    // Each thread processes `batch_sz` elements.
    // Compute offset of the batch `batch` of this thread block in global memory:
    auto block_batch = [&](size_t batch) -> int {
      return block.group_index().x * block.size() + grid.size() * batch;
    };

    for (size_t batch = 0; batch < batch_sz; ++batch) {
        size_t global_idx = block_batch(batch);

        // Collectively acquire the pipeline head stage from all producer threads:
        pipeline.producer_acquire();

        // Submit async copies to the pipeline's head stage to be
        // computed in the next loop iteration
        cuda::memcpy_async(block, shared, global_in + global_idx, sizeof(int) * block.size(), pipeline);
        // Collectively commit (advance) the pipeline's head stage
        pipeline.producer_commit();

        // Collectively wait for the operations committed to the
        // previous `compute` stage to complete:
        pipeline.consumer_wait();

        // Computation overlapped with the memcpy_async of the "copy" stage:
        compute(global_out + global_idx, shared);

        // Collectively release the stage resources
        pipeline.consumer_release();
    }
}





7.28.2. Multi-Stage Asynchronous Data Copies using cuda::pipelineï

In the previous examples with cooperative_groups::wait and cuda::barrier, the kernel threads immediately wait for the data transfer to shared memory to complete. This avoids data transfers from global memory into registers, but does not hide the latency of the memcpy_async operation by overlapping computation.
For that we use the CUDA pipeline feature in the following example. It provides a mechanism for managing a sequence of memcpy_async batches, enabling CUDA kernels to overlap memory transfers with computation. The following example implements a two-stage pipeline that overlaps data-transfer with computation. It:

Initializes the pipeline shared state (more below)
Kickstarts the pipeline by scheduling a memcpy_async for the first batch.
Loops over all the batches: it schedules memcpy_async for the next batch, blocks all threads on the completion of the memcpy_async for the previous batch, and then overlaps the computation on the previous batch with the asynchronous copy of the memory for the next batch.
Finally, it drains the pipeline by performing the computation on the last batch.

Note that, for interoperability with cuda::pipeline, cuda::memcpy_async from the cuda/pipeline header is used here.

#include <cooperative_groups/memcpy_async.h>
#include <cuda/pipeline>

__device__ void compute(int* global_out, int const* shared_in);
__global__ void with_staging(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
    auto grid = cooperative_groups::this_grid();
    auto block = cooperative_groups::this_thread_block();
    assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size

    constexpr size_t stages_count = 2; // Pipeline with two stages
    // Two batches must fit in shared memory:
    extern __shared__ int shared[];  // stages_count * block.size() * sizeof(int) bytes
    size_t shared_offset[stages_count] = { 0, block.size() }; // Offsets to each batch

    // Allocate shared storage for a two-stage cuda::pipeline:
    __shared__ cuda::pipeline_shared_state<
        cuda::thread_scope::thread_scope_block,
        stages_count
    > shared_state;
    auto pipeline = cuda::make_pipeline(block, &shared_state);

    // Each thread processes `batch_sz` elements.
    // Compute offset of the batch `batch` of this thread block in global memory:
    auto block_batch = [&](size_t batch) -> int {
      return block.group_index().x * block.size() + grid.size() * batch;
    };

    // Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block:
    if (batch_sz == 0) return;
    pipeline.producer_acquire();
    cuda::memcpy_async(block, shared + shared_offset[0], global_in + block_batch(0), sizeof(int) * block.size(), pipeline);
    pipeline.producer_commit();

    // Pipelined copy/compute:
    for (size_t batch = 1; batch < batch_sz; ++batch) {
        // Stage indices for the compute and copy stages:
        size_t compute_stage_idx = (batch - 1) % 2;
        size_t copy_stage_idx = batch % 2;

        size_t global_idx = block_batch(batch);

        // Collectively acquire the pipeline head stage from all producer threads:
        pipeline.producer_acquire();

        // Submit async copies to the pipeline's head stage to be
        // computed in the next loop iteration
        cuda::memcpy_async(block, shared + shared_offset[copy_stage_idx], global_in + global_idx, sizeof(int) * block.size(), pipeline);
        // Collectively commit (advance) the pipeline's head stage
        pipeline.producer_commit();

        // Collectively wait for the operations commited to the
        // previous `compute` stage to complete:
        pipeline.consumer_wait();

        // Computation overlapped with the memcpy_async of the "copy" stage:
        compute(global_out + global_idx, shared + shared_offset[compute_stage_idx]);

        // Collectively release the stage resources
        pipeline.consumer_release();
    }

    // Compute the data fetch by the last iteration
    pipeline.consumer_wait();
    compute(global_out + block_batch(batch_sz-1), shared + shared_offset[(batch_sz - 1) % 2]);
    pipeline.consumer_release();
}


A pipeline object is a double-ended queue with a head and a tail, and is used to process work in a first-in first-out (FIFO) order. Producer threads commit work to the pipelineâs head, while consumer threads pull work from the pipelineâs tail. In the example above, all threads are both producer and consumer threads. The threads first commitmemcpy_async operations to fetch the next batch while they wait on the previous batch of memcpy_async operations to complete.


Committing work to a pipeline stage involves:

Collectively acquiring the pipeline head from a set of producer threads using pipeline.producer_acquire().
Submitting memcpy_async operations to the pipeline head.
Collectively commiting (advancing) the pipeline head using pipeline.producer_commit().



Using a previously commited stage involves:

Collectively waiting for the stage to complete, e.g., using pipeline.consumer_wait() to wait on the tail (oldest) stage.
Collectively releasing the stage using pipeline.consumer_release().



cuda::pipeline_shared_state<scope, count> encapsulates the finite resources that allow a pipeline to process up to count concurrent stages. If all resources are in use, pipeline.producer_acquire() blocks producer threads until the resources of the next pipeline stage are released by consumer threads.
This example can be written in a more concise manner by merging the prolog and epilog of the loop with the loop itself as follows:

template <size_t stages_count = 2 /* Pipeline with stages_count stages */>
__global__ void with_staging_unified(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
    auto grid = cooperative_groups::this_grid();
    auto block = cooperative_groups::this_thread_block();
    assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size

    extern __shared__ int shared[]; // stages_count * block.size() * sizeof(int) bytes
    size_t shared_offset[stages_count];
    for (int s = 0; s < stages_count; ++s) shared_offset[s] = s * block.size();

    __shared__ cuda::pipeline_shared_state<
        cuda::thread_scope::thread_scope_block,
        stages_count
    > shared_state;
    auto pipeline = cuda::make_pipeline(block, &shared_state);

    auto block_batch = [&](size_t batch) -> int {
        return block.group_index().x * block.size() + grid.size() * batch;
    };

    // compute_batch: next batch to process
    // fetch_batch:  next batch to fetch from global memory
    for (size_t compute_batch = 0, fetch_batch = 0; compute_batch < batch_sz; ++compute_batch) {
        // The outer loop iterates over the computation of the batches
        for (; fetch_batch < batch_sz && fetch_batch < (compute_batch + stages_count); ++fetch_batch) {
            // This inner loop iterates over the memory transfers, making sure that the pipeline is always full
            pipeline.producer_acquire();
            size_t shared_idx = fetch_batch % stages_count;
            size_t batch_idx = fetch_batch;
            size_t block_batch_idx = block_batch(batch_idx);
            cuda::memcpy_async(block, shared + shared_offset[shared_idx], global_in + block_batch_idx, sizeof(int) * block.size(), pipeline);
            pipeline.producer_commit();
        }
        pipeline.consumer_wait();
        int shared_idx = compute_batch % stages_count;
        int batch_idx = compute_batch;
        compute(global_out + block_batch(batch_idx), shared + shared_offset[shared_idx]);
        pipeline.consumer_release();
    }
}


The pipeline<thread_scope_block> primitive used above is very flexible, and supports two features that our examples above are not using: any arbitrary subset of threads in the block can participate in the pipeline, and from the threads that participate, any subsets can be producers, consumers, or both. In the following example, threads with an âevenâ thread rank are producers, while other threads are consumers:

__device__ void compute(int* global_out, int shared_in);

template <size_t stages_count = 2>
__global__ void with_specialized_staging_unified(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
    auto grid = cooperative_groups::this_grid();
    auto block = cooperative_groups::this_thread_block();

    // In this example, threads with "even" thread rank are producers, while threads with "odd" thread rank are consumers:
    const cuda::pipeline_role thread_role
      = block.thread_rank() % 2 == 0? cuda::pipeline_role::producer : cuda::pipeline_role::consumer;

    // Each thread block only has half of its threads as producers:
    auto producer_threads = block.size() / 2;

    // Map adjacent even and odd threads to the same id:
    const int thread_idx = block.thread_rank() / 2;

    auto elements_per_batch = size / batch_sz;
    auto elements_per_batch_per_block = elements_per_batch / grid.group_dim().x;

    extern __shared__ int shared[]; // stages_count * elements_per_batch_per_block * sizeof(int) bytes
    size_t shared_offset[stages_count];
    for (int s = 0; s < stages_count; ++s) shared_offset[s] = s * elements_per_batch_per_block;

    __shared__ cuda::pipeline_shared_state<
        cuda::thread_scope::thread_scope_block,
        stages_count
    > shared_state;
    cuda::pipeline pipeline = cuda::make_pipeline(block, &shared_state, thread_role);

    // Each thread block processes `batch_sz` batches.
    // Compute offset of the batch `batch` of this thread block in global memory:
    auto block_batch = [&](size_t batch) -> int {
      return elements_per_batch * batch + elements_per_batch_per_block * blockIdx.x;
    };

    for (size_t compute_batch = 0, fetch_batch = 0; compute_batch < batch_sz; ++compute_batch) {
        // The outer loop iterates over the computation of the batches
        for (; fetch_batch < batch_sz && fetch_batch < (compute_batch + stages_count); ++fetch_batch) {
            // This inner loop iterates over the memory transfers, making sure that the pipeline is always full
            if (thread_role == cuda::pipeline_role::producer) {
                // Only the producer threads schedule asynchronous memcpys:
                pipeline.producer_acquire();
                size_t shared_idx = fetch_batch % stages_count;
                size_t batch_idx = fetch_batch;
                size_t global_batch_idx = block_batch(batch_idx) + thread_idx;
                size_t shared_batch_idx = shared_offset[shared_idx] + thread_idx;
                cuda::memcpy_async(shared + shared_batch_idx, global_in + global_batch_idx, sizeof(int), pipeline);
                pipeline.producer_commit();
            }
        }
        if (thread_role == cuda::pipeline_role::consumer) {
            // Only the consumer threads compute:
            pipeline.consumer_wait();
            size_t shared_idx = compute_batch % stages_count;
            size_t global_batch_idx = block_batch(compute_batch) + thread_idx;
            size_t shared_batch_idx = shared_offset[shared_idx] + thread_idx;
            compute(global_out + global_batch_idx, *(shared + shared_batch_idx));
            pipeline.consumer_release();
        }
    }
}


There are some optimizations that pipeline performs, for example, when all threads are both producers and consumers, but in general, the cost of supporting all these features cannot be fully eliminated. For example, pipeline stores and uses a set of barriers in shared memory for synchronization, which is not really necessary if all threads in the block participate in the pipeline.
For the particular case in which all threads in the block participate in the pipeline, we can do better than pipeline<thread_scope_block> by using a pipeline<thread_scope_thread> combined with __syncthreads():

template<size_t stages_count>
__global__ void with_staging_scope_thread(int* global_out, int const* global_in, size_t size, size_t batch_sz) {
    auto grid = cooperative_groups::this_grid();
    auto block = cooperative_groups::this_thread_block();
    auto thread = cooperative_groups::this_thread();
    assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size

    extern __shared__ int shared[]; // stages_count * block.size() * sizeof(int) bytes
    size_t shared_offset[stages_count];
    for (int s = 0; s < stages_count; ++s) shared_offset[s] = s * block.size();

    // No pipeline::shared_state needed
    cuda::pipeline<cuda::thread_scope_thread> pipeline = cuda::make_pipeline();

    auto block_batch = [&](size_t batch) -> int {
        return block.group_index().x * block.size() + grid.size() * batch;
    };

    for (size_t compute_batch = 0, fetch_batch = 0; compute_batch < batch_sz; ++compute_batch) {
        for (; fetch_batch < batch_sz && fetch_batch < (compute_batch + stages_count); ++fetch_batch) {
            pipeline.producer_acquire();
            size_t shared_idx = fetch_batch % stages_count;
            size_t batch_idx = fetch_batch;
            // Each thread fetches its own data:
            size_t thread_batch_idx = block_batch(batch_idx) + threadIdx.x;
            // The copy is performed by a single `thread` and the size of the batch is now that of a single element:
            cuda::memcpy_async(thread, shared + shared_offset[shared_idx] + threadIdx.x, global_in + thread_batch_idx, sizeof(int), pipeline);
            pipeline.producer_commit();
        }
        pipeline.consumer_wait();
        block.sync(); // __syncthreads: All memcpy_async of all threads in the block for this stage have completed here
        int shared_idx = compute_batch % stages_count;
        int batch_idx = compute_batch;
        compute(global_out + block_batch(batch_idx), shared + shared_offset[shared_idx]);
        pipeline.consumer_release();
    }
}


If the compute operation only reads shared memory written to by other threads in the same warp as the current thread, __syncwarp() suffices.



7.28.3. Pipeline Interfaceï

The complete API documentation for cuda::memcpy_async is provided in the libcudacxx API documentation along with some examples.
The pipeline interface requires

at least CUDA 11.0,
at least ISO C++ 2011 compatibility, e.g., to be compiled with -std=c++11, and
#include <cuda/pipeline>.

For a C-like interface, when compiling without ISO C++ 2011 compatibility, see Pipeline Primitives Interface.



7.28.4. Pipeline Primitives Interfaceï

Pipeline primitives are a C-like interface for memcpy_async functionality. The pipeline primitives interface is available by including the <cuda_pipeline.h> header. When compiling without ISO C++ 2011 compatibility, include the <cuda_pipeline_primitives.h> header.


7.28.4.1. memcpy_async Primitiveï


void __pipeline_memcpy_async(void* __restrict__ dst_shared,
                             const void* __restrict__ src_global,
                             size_t size_and_align,
                             size_t zfill=0);




Request that the following operation be submitted for asynchronous evaluation:

size_t i = 0;
for (; i < size_and_align - zfill; ++i) ((char*)dst_shared)[i] = ((char*)src_global)[i]; /* copy */
for (; i < size_and_align; ++i) ((char*)dst_shared)[i] = 0; /* zero-fill */




Requirements:

dst_shared must be a pointer to the shared memory destination for the memcpy_async.
src_global must be a pointer to the global memory source for the memcpy_async.
size_and_align must be 4, 8, or 16.
zfill <= size_and_align.
size_and_align must be the alignment of dst_shared and src_global.



It is a race condition for any thread to modify the source memory or observe the destination memory prior to waiting for the memcpy_async operation to complete. Between submitting a memcpy_async operation and waiting for its completion, any of the following actions introduces a race condition:

Loading from dst_shared.
Storing to dst_shared or src_global.
Applying an atomic update to dst_shared or src_global.






7.28.4.2. Commit Primitiveï


void __pipeline_commit();



Commit submitted memcpy_async to the pipeline as the current batch.




7.28.4.3. Wait Primitiveï


void __pipeline_wait_prior(size_t N);



Let {0, 1, 2, ..., L} be the sequence of indices associated with invocations of __pipeline_commit() by a given thread.
Wait for completion of batches at least up to and including L-N.




7.28.4.4. Arrive On Barrier Primitiveï


void __pipeline_arrive_on(__mbarrier_t* bar);



bar points to a barrier in shared memory.
Increments the barrier arrival count by one, when all memcpy_async operations sequenced before this call have completed, the arrival count is decremented by one and hence the net effect on the arrival count is zero. It is userâs responsibility to make sure that the increment on the arrival count does not exceed __mbarrier_maximum_count().






7.29. Asynchronous Data Copies using Tensor Memory Access (TMA)ï

Many applications require movement of large amounts of data from and to global
memory. Often, the data is laid out in global memory as a multi-dimensional
array with non-sequential data acess patterns. To reduce global memory usage,
sub-tiles of such arrays are copied to shared memory before use in computations.
The loading and storing involves addreses-calculations that can be error-prone
and repetitive. To offload these computations, Compute Capability 9.0 introduces
Tensor Memory Acces (TMA). The primary goal of TMA is to provide an efficient
data transfer mechanism from global memory to shared memory for
multi-dimensional arrays.
Naming. Tensor memory access (TMA) is a broad term used to market the
features described in this section. For the purpose of forward-compatibility and
to reduce discrepancies with the PTX ISA, the text in this section refers to TMA
operations as either bulk-asynchronous copies or bulk tensor asynchronous
copies, depending on the specific type of copy used. The term âbulkâ is used to
contrast these operations with the asynchronous memory operations described in
the previous sections.
Dimensions. TMA supports copying both one-dimensional and multi-dimensional
arrays (up to 5-dimensional). The programming model for bulk-asynchronous
copies of one-dimensional contiguous arrays is different from the programming
model for bulk tensor asynchronous copies of multi-dimensional arrays. To
perform a bulk tensor asynchronous copy of a multi-dimensional array, the
hardware requires a tensor map.
This object describes the layout of the multi-dimensional array in global and
shared memory. A tensor map is created on the host using the cuTensorMapEncode
API.
The tensor map is transferred from host to device as a const kernel
parameter annotated with __grid_constant__, and can be used on the device to
copy a tile of data between shared and global memory. In contrast, performing a
bulk-asynchronous copy of a contiguous one-dimensional array does not require a
tensor map: it can be performed on-device with a pointer and size parameter.
Source and destination. The source and destination addresses of bulk-asynchronous copy
operations can be in shared or global memory. The operations can read data from global to
shared memory, write data from shared to global memory, and also copy from
shared memory to Distributed Shared Memory of another block in the same cluster.
In addition, when in a cluster, a bulk-asynchronous operation can be specified as being
multicast. In this case, data can be transferred from global memory to the
shared memory of multiple blocks within the cluster. The multicast feature is
optimized for target architecture sm_90a and may have significantly reduced performance on
other targets. Hence, it is advised to be used with compute architecture
sm_90a.
Asynchronous. Data transfers using TMA are asynchronous. This allows the initiating
thread to continue computing while the hardware asynchronously copies the data.
Whether the data transfer occurs asynchronously in practice is up to the hardware implementation and may change in the future.
There are several completion mechanisms
that bulk-asynchronous operations can use to signal that they have completed.
When the operation reads from global to shared memory, any
thread in the block can wait for the data to be readable in shared memory by
waiting on a Shared Memory Barrier. When the bulk-asynchronous
operation writes data from shared memory to global or distributed shared memory,
only the initiating thread can wait for the operation to have completed.
This is accomplished using a bulk async-group based completion mechanism. A
table describing the completion mechanisms can be found below and in the PTX ISA.


Table 6 Asynchronous copies with possible source and destinations memory spaces and completion mechanisms. An empty cell indicates that a source-destination pair is not supported.ï



Direction
Completion mechanism


Destination
Source
Asychronous copy
Bulk-asynchronous copy (TMA)




Global
Global




Global
Shared::cta

Bulk async-group


Shared::cta
Global
Async-group, mbarrier
Mbarrier


Shared::cluster
Global

Mbarrier (multicast)


Shared::cta
Shared::cluster

Mbarrier


Shared::cta
Shared::cta







7.29.1. Using TMA to transfer one-dimensional arraysï

This section demonstrates how to write a simple kernel that read-modify-writes a
one-dimensional array using TMA. This shows how to how to load and store data
using bulk-asynchronous copies, as well as how to synchronize threads of
execution with those copies.
The code of the kernel is included below. Some functionality requires inline PTX
assembly that is currently made available through libcu++.
The availability of these wrappers can be
checked with the following code:

#if defined(__CUDA_MINIMUM_ARCH__) && __CUDA_MINIMUM_ARCH__ < 900
static_assert(false, "Device code is being compiled with older architectures that are incompatible with TMA.");
#endif // __CUDA_MINIMUM_ARCH__


The kernel goes through the following stages:

Initialize shared memory barrier.
Initiate bulk-asynchronous copy of a block of memory from global to shared memory.
Arrive and wait on the shared memory barrier.
Increment the shared memory buffer values.
Wait for shared memory writes to be visible to the subsequent bulk-asynchronous copy, i.e., order the shared memory writes in the async proxy before the next step.
Initiate bulk-asynchronous copy of the buffer in shared memory to global memory.
Wait at end of kernel for bulk-asynchronous copy to have finished reading shared memory.


#include <cuda/barrier>
#include <cuda/ptx>
using barrier = cuda::barrier<cuda::thread_scope_block>;
namespace ptx = cuda::ptx;

static constexpr size_t buf_len = 1024;
__global__ void add_one_kernel(int* data, size_t offset)
{
  // Shared memory buffer. The destination shared memory buffer of
  // a bulk operations should be 16 byte aligned.
  __shared__ alignas(16) int smem_data[buf_len];

  // 1. a) Initialize shared memory barrier with the number of threads participating in the barrier.
  //    b) Make initialized barrier visible in async proxy.
  #pragma nv_diag_suppress static_var_with_dynamic_init
  __shared__ barrier bar;
  if (threadIdx.x == 0) { 
    init(&bar, blockDim.x);                      // a)
    ptx::fence_proxy_async(ptx::space_shared);   // b)
  }
  __syncthreads();

  // 2. Initiate TMA transfer to copy global to shared memory.
  if (threadIdx.x == 0) {
    // 3a. cuda::memcpy_async arrives on the barrier and communicates
    //     how many bytes are expected to come in (the transaction count)
    cuda::memcpy_async(
        smem_data, 
        data + offset, 
        cuda::aligned_size_t<16>(sizeof(smem_data)),
        bar
    );
  }
  // 3b. All threads arrive on the barrier
  barrier::arrival_token token = bar.arrive();
  
  // 3c. Wait for the data to have arrived.
  bar.wait(std::move(token));

  // 4. Compute saxpy and write back to shared memory
  for (int i = threadIdx.x; i < buf_len; i += blockDim.x) {
    smem_data[i] += 1;
  }

  // 5. Wait for shared memory writes to be visible to TMA engine.
  ptx::fence_proxy_async(ptx::space_shared);   // b)
  __syncthreads();
  // After syncthreads, writes by all threads are visible to TMA engine.

  // 6. Initiate TMA transfer to copy shared memory to global memory
  if (threadIdx.x == 0) {
    ptx::cp_async_bulk(
        ptx::space_global,
        ptx::space_shared,
        data + offset, smem_data, sizeof(smem_data));
    // 7. Wait for TMA transfer to have finished reading shared memory.
    // Create a "bulk async-group" out of the previous bulk copy operation.
    ptx::cp_async_bulk_commit_group();
    // Wait for the group to have completed reading from shared memory.
    ptx::cp_async_bulk_wait_group_read(ptx::n32_t<0>());
  }
}


Barrier initialization. The barrier is initialized with the number of
threads participating in the block. As a result, the barrier will flip only if
all threads have arrived on this barrier. Shared memory barriers are described
in more detail in Asynchronous Data Copies using cuda::barrier.
To make the initialized barrier visible to subsequent bulk-asynchronous copies, the
fence.proxy.async.shared::cta instruction is used. This instruction ensures that
subsequent bulk-asynchronous copy operations operate on the initialized barrier.
TMA read. The bulk-asynchronous copy instruction directs the
hardware to copy a large chunk of data into shared memory, and to update the
transaction count
of the shared memory barrier after completing the read. In general, issuing as
few bulk copies with as big a size as possible results in the best performance.
Because the copy can be performed asynchronously by the hardware, it is not
necessary to split the copy into smaller chunks.
The thread that initiates the bulk-asynchronous copy operation arrives at the barrier
using mbarrier.expect_tx. This is automatically performed by cuda::memcpy_async. This tells the barrier that the thread has
arrived and also how many bytes (tx / transactions) are expected to arrive. Only
a single thread has to update the expected transaction count. If multiple
threads update the transaction count, the expected transaction will be the sum
of the updates. The barrier will only flip once all threads have arrived and
all bytes have arrived. Once the barrier has flipped, the bytes are safe to read
from shared memory, both by the threads as well as by subsequent
bulk-asynchronous copies. More information about barrier transaction accounting
can be found in the PTX ISA.
Barrier wait. Waiting for the barrier to flip is done using
mbarrier.try_wait. It can either return true, indicating that the wait is
over, or return false, which may mean that the wait timed out. The while loop
waits for completion, and retries on time-out.
SMEM write and sync. The increment of the buffer values reads and writes to shared
memory. To make the writes visible to subsequent bulk-asynchronous copies, the
fence.proxy.async.shared::cta instruction is used. This orders the writes to
shared memory before subsequent reads from bulk-asynchronous copy operations,
which read through the async proxy. So each thread first orders the writes to
objects in shared memory in the async proxy via the
fence.proxy.async.shared::cta, and these operations by all threads are
ordered before the async operation performed in thread 0 using
__syncthreads().
TMA write and sync. The write from shared to global memory is again
initiated by a single thread. The completion of the write is not tracked by a
shared memory barrier. Instead, a thread-local mechanism is used. Multiple
writes can be batched into a so-called bulk async-group. Afterwards, the
thread can wait for all operations in this group to have completed reading from
shared memory (as in the code above) or to have completed writing to global
memory, making the writes visible to the initiating thread. For more information,
refer to the PTX ISA documentation of cp.async.bulk.wait_group.
Note that the bulk-asynchronous and non-bulk asynchronous copy instructions have
different async-groups: there exist both cp.async.wait_group and
cp.async.bulk.wait_group instructions.
The bulk-asynchronous instructions have specific alignment requirements on their source and
destination addresses. More information can be found in the table below.


Table 7 Alignment requirements for one-dimensional bulk-asynchronous operations in Compute Capability 9.0.ï







Address / Size
Alignment




Global memory address
Must be 16 byte aligned.


Shared memory address
Must be 16 byte aligned.


Shared memory barrier address
Must be 8 byte aligned (this is guaranteed by cuda::barrier).


Size of transfer
Must be a multiple of 16 bytes.






7.29.2. Using TMA to transfer multi-dimensional arraysï

The primary difference between the one-dimensional and multi-dimensional case is
that a tensor map must be created on the host and passed to the CUDA kernel.
This section describes how to create a tensor map using the CUDA driver API, how
to pass it to device, and how to use it on device.
Driver API. A tensor map is created using the cuTensorMapEncodeTiled
driver API. This API can be accessed by linking to the driver directly
(-lcuda) or by using the cudaGetDriverEntryPoint
API. Below, we show how to get a pointer to the cuTensorMapEncodeTiled API.
For more information, refer to Driver Entry Point Access.

#include <cudaTypedefs.h> // PFN_cuTensorMapEncodeTiled, CUtensorMap

PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {
  // Get pointer to cuGetProcAddress
  cudaDriverEntryPointQueryResult driver_status;
  void* cuGetProcAddress_ptr = nullptr;
  CUDA_CHECK(cudaGetDriverEntryPoint("cuGetProcAddress", &cuGetProcAddress_ptr, cudaEnableDefault, &driver_status));
  assert(driver_status == cudaDriverEntryPointSuccess);
  PFN_cuGetProcAddress_v12000 cuGetProcAddress = reinterpret_cast<PFN_cuGetProcAddress_v12000>(cuGetProcAddress_ptr);

  // Use cuGetProcAddress to get a pointer to the CTK 12.0 version of cuTensorMapEncodeTiled
  CUdriverProcAddressQueryResult symbol_status;
  void* cuTensorMapEncodeTiled_ptr = nullptr;
  CUresult res = cuGetProcAddress("cuTensorMapEncodeTiled", &cuTensorMapEncodeTiled_ptr, 12000, CU_GET_PROC_ADDRESS_DEFAULT, &symbol_status);
  assert(res == CUDA_SUCCESS && symbol_status == CU_GET_PROC_ADDRESS_SUCCESS);

  return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(cuTensorMapEncodeTiled_ptr);
}


Creation. Creating a tensor map requires many parameters. Among
them are the base pointer to an array in global memory, the size of the array
(in number of elements), the stride from one row to the next (in bytes), the
size of the shared memory buffer (in number of elements). The code below creates
a tensor map to describe a two-dimensional row-major array of size GMEM_HEIGHT
x GMEM_WIDTH. Note the order of the parameters: the fastest moving dimension
comes first.

  CUtensorMap tensor_map{};
  // rank is the number of dimensions of the array.
  constexpr uint32_t rank = 2;
  uint64_t size[rank] = {GMEM_WIDTH, GMEM_HEIGHT};
  // The stride is the number of bytes to traverse from the first element of one row to the next.
  // It must be a multiple of 16.
  uint64_t stride[rank - 1] = {GMEM_WIDTH * sizeof(int)};
  // The box_size is the size of the shared memory buffer that is used as the
  // destination of a TMA transfer.
  uint32_t box_size[rank] = {SMEM_WIDTH, SMEM_HEIGHT};
  // The distance between elements in units of sizeof(element). A stride of 2
  // can be used to load only the real component of a complex-valued tensor, for instance.
  uint32_t elem_stride[rank] = {1, 1};

  // Get a function pointer to the cuTensorMapEncodeTiled driver API.
  auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();

  // Create the tensor descriptor.
  CUresult res = cuTensorMapEncodeTiled(
    &tensor_map,                // CUtensorMap *tensorMap,
    CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_INT32,
    rank,                       // cuuint32_t tensorRank,
    tensor_ptr,                 // void *globalAddress,
    size,                       // const cuuint64_t *globalDim,
    stride,                     // const cuuint64_t *globalStrides,
    box_size,                   // const cuuint32_t *boxDim,
    elem_stride,                // const cuuint32_t *elementStrides,
    // Interleave patterns can be used to accelerate loading of values that
    // are less than 4 bytes long.
    CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,
    // Swizzling can be used to avoid shared memory bank conflicts.
    CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,
    // L2 Promotion can be used to widen the effect of a cache-policy to a wider
    // set of L2 cache lines.
    CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,
    // Any element that is outside of bounds will be set to zero by the TMA transfer.
    CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE
  );


Host-to-device transfer. A bulk tensor asynchronous operations require the
tensor map to be in immutable memory. This can be achieved by using constant
memory or by passing the tensor map as a const __grid_constant__ parameter
to a kernel. When passing the tensor map as a parameter, some versions of the
GCC C++ compiler issue the warning âthe ABI for passing parameters with 64-byte
alignment has changed in GCC 4.6â. This warning can be ignored.

__global__ void kernel(const __grid_constant__ CUtensorMap tensor_map)
{
  // Use tensor_map here.
}
int main() {
  CUtensorMap map;
  // [ ..Initialize map.. ]
  kernel<<<1, 1>>>(map);
}


As an alternative to the __grid_constant__ kernel parameter, a global
constant variable can be used. An example is included
below.

__constant__ CUtensorMap global_tensor_map;
__global__ void kernel()
{
  // Use global_tensor_map here.
}
int main() {
  CUtensorMap local_tensor_map;
  // [ ..Initialize map.. ]
  cudaMemcpyToSymbol(global_tensor_map, &local_tensor_map, sizeof(CUtensorMap));
  kernel<<<1, 1>>>();
}


The following example copies the tensor map to global device memory. Using a
pointer to a tensor map in global device memory is undefined behavior and will
lead to silent and difficult to track down bugs.

__device__ CUtensorMap global_tensor_map;
__global__ void kernel(CUtensorMap *tensor_map)
{
  // Do *not* use tensor_map here. Using a global memory pointer is
  // undefined behavior and can fail silently and unreliably.
}
int main() {
  CUtensorMap local_tensor_map;
  // [ ..Initialize map.. ]
  cudaMemcpy(global_tensor_map, &local_tensor_map, sizeof(CUtensorMap));
  kernel<<<1, 1>>>(global_tensor_map);
}


Use. The kernel below loads a 2D tile of size SMEM_HEIGHT x SMEM_WIDTH
from a larger 2D array. The top-left corner of the tile is indicated by the
indices x and y. The tile is loaded into shared memory, modified, and
written back to global memory.

#include <cuda.h>         // CUtensormap
#include <cuda/barrier>
using barrier = cuda::barrier<cuda::thread_scope_block>;
namespace cde = cuda::device::experimental;

__global__ void kernel(const __grid_constant__ CUtensorMap tensor_map, int x, int y) {
  // The destination shared memory buffer of a bulk tensor operation should be
  // 128 byte aligned.
  __shared__ alignas(128) int smem_buffer[SMEM_HEIGHT][SMEM_WIDTH];

  // Initialize shared memory barrier with the number of threads participating in the barrier.
  #pragma nv_diag_suppress static_var_with_dynamic_init
  __shared__ barrier bar;

  if (threadIdx.x == 0) {
    // Initialize barrier. All `blockDim.x` threads in block participate.
    init(&bar, blockDim.x);
    // Make initialized barrier visible in async proxy.
    cde::fence_proxy_async_shared_cta();    
  }
  // Syncthreads so initialized barrier is visible to all threads.
  __syncthreads();

  barrier::arrival_token token;
  if (threadIdx.x == 0) {
    // Initiate bulk tensor copy.
    cde::cp_async_bulk_tensor_2d_global_to_shared(&smem_buffer, &tensor_map, x, y, bar);
    // Arrive on the barrier and tell how many bytes are expected to come in.
    token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(smem_buffer));
  } else {
    // Other threads just arrive.
    token = bar.arrive();
  }
  // Wait for the data to have arrived.
  bar.wait(std::move(token));

  // Symbolically modify a value in shared memory.
  smem_buffer[0][threadIdx.x] += threadIdx.x;

  // Wait for shared memory writes to be visible to TMA engine.
  cde::fence_proxy_async_shared_cta();
  __syncthreads();
  // After syncthreads, writes by all threads are visible to TMA engine.

  // Initiate TMA transfer to copy shared memory to global memory
  if (threadIdx.x == 0) {
    cde::cp_async_bulk_tensor_2d_shared_to_global(&tensor_map, x, y, &smem_buffer);
    // Wait for TMA transfer to have finished reading shared memory.
    // Create a "bulk async-group" out of the previous bulk copy operation.
    cde::cp_async_bulk_commit_group();
    // Wait for the group to have completed reading from shared memory.
    cde::cp_async_bulk_wait_group_read<0>();
  }

  // Destroy barrier. This invalidates the memory region of the barrier. If
  // further computations were to take place in the kernel, this allows the
  // memory location of the shared memory barrier to be reused.
  if (threadIdx.x == 0) {
    (&bar)->~barrier();
  }
}


Negative indices and out of bounds. When part of the tile that is being
read from global to shared memory is out of bounds, the shared memory that
corresponds to the out of bounds area is zero-filled. The top-left corner
indices of the tile may also be negative. When writing from shared to global
memory, parts of the tile may be out of bounds, but the top left corner cannot
have any negative indices.
Size and stride. The size of a tensor is the number of elements along one
dimension. All sizes must be greater than one. The stride is the number of bytes
between elements of the same dimension. For instance, a 4 x 4 matrix of
integers has sizes 4 and 4. Since it has 4 bytes per element, the strides are 4
and 16 bytes. Due to alignment requirements, a 4 x 3 row-major matrix of
integers must have strides of 4 and 16 bytes as well. Each row is padded with 4
extra bytes to ensure that the start of the next row is aligned to 16 bytes. For
more information regarding alignment, refer to Table
Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9.0..


Table 8 Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9.0.ï







Address / Size
Alignment




Global memory address
Must be 16 byte aligned.


Global memory sizes
Must be greater than or equal to one. Does not have to be a multiple of 16 bytes.


Global memory strides
Must be multiples of 16 bytes.


Shared memory address
Must be 128 byte aligned.


Shared memory barrier address
Must be 8 byte aligned (this is guaranteed by cuda::barrier).


Size of transfer
Must be a multiple of 16 bytes.





7.29.2.1. Multi-dimensional TMA PTX wrappersï

Below, the PTX instructions are ordered by their use in the example code above.
The cp.async.bulk.tensor
instructions initiate a bulk tensor asynchronous copy between global and shared memory. The
wrappers below read from global to shared memory and write from shared to global
memory.

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_1d_global_to_shared(
    void *dest, const CUtensorMap *tensor_map , int c0, cuda::barrier<cuda::thread_scope_block> &bar
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_2d_global_to_shared(
    void *dest, const CUtensorMap *tensor_map , int c0, int c1, cuda::barrier<cuda::thread_scope_block> &bar
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_3d_global_to_shared(
    void *dest, const CUtensorMap *tensor_map, int c0, int c1, int c2, cuda::barrier<cuda::thread_scope_block> &bar
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_4d_global_to_shared(
    void *dest, const CUtensorMap *tensor_map , int c0, int c1, int c2, int c3, cuda::barrier<cuda::thread_scope_block> &bar
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_5d_global_to_shared(
    void *dest, const CUtensorMap *tensor_map , int c0, int c1, int c2, int c3, int c4, cuda::barrier<cuda::thread_scope_block> &bar
);



// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_1d_shared_to_global(
    const CUtensorMap *tensor_map, int c0, const void *src
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_2d_shared_to_global(
    const CUtensorMap *tensor_map, int c0, int c1, const void *src
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_3d_shared_to_global(
    const CUtensorMap *tensor_map, int c0, int c1, int c2, const void *src
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_4d_shared_to_global(
    const CUtensorMap *tensor_map, int c0, int c1, int c2, int c3, const void *src
);

// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor
inline __device__
void cuda::device::experimental::cp_async_bulk_tensor_5d_shared_to_global(
    const CUtensorMap *tensor_map, int c0, int c1, int c2, int c3, int c4, const void *src
);







7.30. Profiler Counter Functionï

Each multiprocessor has a set of sixteen hardware counters that an application can increment with a single instruction by calling the __prof_trigger() function.

void __prof_trigger(int counter);


increments by one per warp the per-multiprocessor hardware counter of index counter. Counters 8 to 15 are reserved and should not be used by applications.
The value of counters 0, 1, â¦, 7 can be obtained via nvprof by nvprof --events prof_trigger_0x where x is 0, 1, â¦, 7. All counters are reset before each kernel launch (note that when collecting counters, kernel launches are synchronous as mentioned in Concurrent Execution between Host and Device).



7.31. Assertionï

Assertion is only supported by devices of compute capability 2.x and higher.

void assert(int expression);


stops the kernel execution if expression is equal to zero. If the program is run within a debugger, this triggers a breakpoint and the debugger can be used to inspect the current state of the device. Otherwise, each thread for which expression is equal to zero prints a message to stderr after synchronization with the host via cudaDeviceSynchronize(), cudaStreamSynchronize(), or cudaEventSynchronize(). The format of this message is as follows:

<filename>:<line number>:<function>:
block: [blockId.x,blockId.x,blockIdx.z],
thread: [threadIdx.x,threadIdx.y,threadIdx.z]
Assertion `<expression>` failed.


Any subsequent host-side synchronization calls made for the same device will return cudaErrorAssert. No more commands can be sent to this device until cudaDeviceReset() is called to reinitialize the device.
If expression is different from zero, the kernel execution is unaffected.
For example, the following program from source file test.cu

#include <assert.h>

__global__ void testAssert(void)
{
    int is_one = 1;
    int should_be_one = 0;

    // This will have no effect
    assert(is_one);

    // This will halt kernel execution
    assert(should_be_one);
}

int main(int argc, char* argv[])
{
    testAssert<<<1,1>>>();
    cudaDeviceSynchronize();

    return 0;
}


will output:

test.cu:19: void testAssert(): block: [0,0,0], thread: [0,0,0] Assertion `should_be_one` failed.


Assertions are for debugging purposes. They can affect performance and it is therefore recommended to disable them in production code. They can be disabled at compile time by defining the NDEBUG preprocessor macro before including assert.h. Note that expression should not be an expression with side effects (something like(++i > 0), for example), otherwise disabling the assertion will affect the functionality of the code.



7.32. Trap functionï

A trap operation can be initiated by calling the __trap() function from any device thread.

void __trap();


The execution of the kernel is aborted and an interrupt is raised in the host program.



7.33. Breakpoint Functionï

Execution of a kernel function can be suspended by calling the __brkpt() function from any device thread.

void __brkpt();





7.34. Formatted Outputï

Formatted output is only supported by devices of compute capability 2.x and higher.

int printf(const char *format[, arg, ...]);


prints formatted output from a kernel to a host-side output stream.
The in-kernel printf() function behaves in a similar way to the standard C-library printf() function, and the user is referred to the host systemâs manual pages for a complete description of printf() behavior. In essence, the string passed in as format is output to a stream on the host, with substitutions made from the argument list wherever a format specifier is encountered. Supported format specifiers are listed below.
The printf() command is executed as any other device-side function: per-thread, and in the context of the calling thread. From a multi-threaded kernel, this means that a straightforward call to printf() will be executed by every thread, using that threadâs data as specified. Multiple versions of the output string will then appear at the host stream, once for each thread which encountered the printf().
It is up to the programmer to limit the output to a single thread if only a single output string is desired (see Examples for an illustrative example).
Unlike the C-standard printf(), which returns the number of characters printed, CUDAâs printf() returns the number of arguments parsed. If no arguments follow the format string, 0 is returned. If the format string is NULL, -1 is returned. If an internal error occurs, -2 is returned.


7.34.1. Format Specifiersï

As for standard printf(), format specifiers take the form: %[flags][width][.precision][size]type
The following fields are supported (see widely-available documentation for a complete description of all behaviors):

Flags: '#' ' ' '0' '+' '-'
Width: '*' '0-9'
Precision: '0-9'
Size: 'h' 'l' 'll'
Type: "%cdiouxXpeEfgGaAs"

Note that CUDAâs printf()will accept any combination of flag, width, precision, size and type, whether or not overall they form a valid format specifier. In other words, â%hdâ will be accepted and printf will expect a double-precision variable in the corresponding location in the argument list.



7.34.2. Limitationsï

Final formatting of the printf()output takes place on the host system. This means that the format string must be understood by the host-systemâs compiler and C library. Every effort has been made to ensure that the format specifiers supported by CUDAâs printf function form a universal subset from the most common host compilers, but exact behavior will be host-OS-dependent.
As described in Format Specifiers, printf() will accept all combinations of valid flags and types. This is because it cannot determine what will and will not be valid on the host system where the final output is formatted. The effect of this is that output may be undefined if the program emits a format string which contains invalid combinations.
The printf() command can accept at most 32 arguments in addition to the format string. Additional arguments beyond this will be ignored, and the format specifier output as-is.
Owing to the differing size of the long type on 64-bit Windows platforms (four bytes on 64-bit Windows platforms, eight bytes on other 64-bit platforms), a kernel which is compiled on a non-Windows 64-bit machine but then run on a win64 machine will see corrupted output for all format strings which include â%ldâ. It is recommended that the compilation platform matches the execution platform to ensure safety.
The output buffer for printf() is set to a fixed size before kernel launch (see Associated Host-Side API). It is circular and if more output is produced during kernel execution than can fit in the buffer, older output is overwritten. It is flushed only when one of these actions is performed:

Kernel launch via <<<>>> or cuLaunchKernel() (at the start of the launch, and if the CUDA_LAUNCH_BLOCKING environment variable is set to 1, at the end of the launch as well),
Synchronization via cudaDeviceSynchronize(), cuCtxSynchronize(), cudaStreamSynchronize(), cuStreamSynchronize(), cudaEventSynchronize(), or cuEventSynchronize(),
Memory copies via any blocking version of cudaMemcpy*() or cuMemcpy*(),
Module loading/unloading via cuModuleLoad() or cuModuleUnload(),
Context destruction via cudaDeviceReset() or cuCtxDestroy().
Prior to executing a stream callback added by cudaStreamAddCallback or cuStreamAddCallback.

Note that the buffer is not flushed automatically when the program exits. The user must call cudaDeviceReset() or cuCtxDestroy() explicitly, as shown in the examples below.
Internally printf() uses a shared data structure and so it is possible that calling printf() might change the order of execution of threads. In particular, a thread which calls printf() might take a longer execution path than one which does not call printf(), and that path length is dependent upon the parameters of the printf(). Note, however, that CUDA makes no guarantees of thread execution order except at explicit __syncthreads() barriers, so it is impossible to tell whether execution order has been modified by printf() or by other scheduling behavior in the hardware.



7.34.3. Associated Host-Side APIï

The following API functions get and set the size of the buffer used to transfer the printf() arguments and internal metadata to the host (default is 1 megabyte):

cudaDeviceGetLimit(size_t* size,cudaLimitPrintfFifoSize)
cudaDeviceSetLimit(cudaLimitPrintfFifoSize, size_t size)




7.34.4. Examplesï

The following code sample:

#include <stdio.h>

__global__ void helloCUDA(float f)
{
    printf("Hello thread %d, f=%f\n", threadIdx.x, f);
}

int main()
{
    helloCUDA<<<1, 5>>>(1.2345f);
    cudaDeviceSynchronize();
    return 0;
}


will output:

Hello thread 2, f=1.2345
Hello thread 1, f=1.2345
Hello thread 4, f=1.2345
Hello thread 0, f=1.2345
Hello thread 3, f=1.2345


Notice how each thread encounters the printf() command, so there are as many lines of output as there were threads launched in the grid. As expected, global values (i.e., float f) are common between all threads, and local values (i.e., threadIdx.x) are distinct per-thread.
The following code sample:

#include <stdio.h>

__global__ void helloCUDA(float f)
{
    if (threadIdx.x == 0)
        printf("Hello thread %d, f=%f\n", threadIdx.x, f) ;
}

int main()
{
    helloCUDA<<<1, 5>>>(1.2345f);
    cudaDeviceSynchronize();
    return 0;
}


will output:

Hello thread 0, f=1.2345


Self-evidently, the if() statement limits which threads will call printf, so that only a single line of output is seen.




7.35. Dynamic Global Memory Allocation and Operationsï

Dynamic global memory allocation and operations are only supported by devices of compute capability 2.x and higher.

__host__ __device__ void* malloc(size_t size);
__device__ void *__nv_aligned_device_malloc(size_t size, size_t align);
__host__ __device__  void free(void* ptr);


allocate and free memory dynamically from a fixed-size heap in global memory.

__host__ __device__ void* memcpy(void* dest, const void* src, size_t size);


copy size bytes from the memory location pointed by src to the memory location pointed by dest.

__host__ __device__ void* memset(void* ptr, int value, size_t size);


set size bytes of memory block pointed by ptr to value (interpreted as an unsigned char).
The CUDA in-kernel malloc()function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the request. The returned pointer is guaranteed to be aligned to a 16-byte boundary.
The CUDA in-kernel __nv_aligned_device_malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the requested size or alignment. The address of the allocated memory will be a multiple of align. align must be a non-zero power of 2.
The CUDA in-kernel free() function deallocates the memory pointed to by ptr, which must have been returned by a previous call to malloc() or __nv_aligned_device_malloc(). If ptr is NULL, the call to free() is ignored. Repeated calls to free() with the same ptr has undefined behavior.
The memory allocated by a given CUDA thread via malloc() or __nv_aligned_device_malloc() remains allocated for the lifetime of the CUDA context, or until it is explicitly released by a call to free(). It can be used by any other CUDA threads even from subsequent kernel launches. Any CUDA thread may free memory allocated by another thread, but care should be taken to ensure that the same pointer is not freed more than once.


7.35.1. Heap Memory Allocationï

The device memory heap has a fixed size that must be specified before any program using malloc(), __nv_aligned_device_malloc() or free() is loaded into the context. A default heap of eight megabytes is allocated if any program uses malloc() or __nv_aligned_device_malloc() without explicitly specifying the heap size.
The following API functions get and set the heap size:

cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize)
cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size)

The heap size granted will be at least size bytes. cuCtxGetLimit()and cudaDeviceGetLimit() return the currently requested heap size.
The actual memory allocation for the heap occurs when a module is loaded into the context, either explicitly via the CUDA driver API (see Module), or implicitly via the CUDA runtime API (see CUDA Runtime). If the memory allocation fails, the module load will generate a CUDA_ERROR_SHARED_OBJECT_INIT_FAILED error.
Heap size cannot be changed once a module load has occurred and it does not resize dynamically according to need.
Memory reserved for the device heap is in addition to memory allocated through host-side CUDA API calls such as cudaMalloc().



7.35.2. Interoperability with Host Memory APIï

Memory allocated via device malloc() or __nv_aligned_device_malloc() cannot be freed using the runtime (i.e., by calling any of the free memory functions from Device Memory).
Similarly, memory allocated via the runtime (i.e., by calling any of the memory allocation functions from Device Memory) cannot be freed via free().
In addition, memory allocated by a call to malloc() or __nv_aligned_device_malloc() in device code cannot be used in any runtime or driver API calls (i.e. cudaMemcpy, cudaMemset, etc).



7.35.3. Examplesï



7.35.3.1. Per Thread Allocationï

The following code sample:

#include <stdlib.h>
#include <stdio.h>

__global__ void mallocTest()
{
    size_t size = 123;
    char* ptr = (char*)malloc(size);
    memset(ptr, 0, size);
    printf("Thread %d got pointer: %p\n", threadIdx.x, ptr);
    free(ptr);
}

int main()
{
    // Set a heap size of 128 megabytes. Note that this must
    // be done before any kernel is launched.
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);
    mallocTest<<<1, 5>>>();
    cudaDeviceSynchronize();
    return 0;
}


will output:

Thread 0 got pointer: 00057020
Thread 1 got pointer: 0005708c
Thread 2 got pointer: 000570f8
Thread 3 got pointer: 00057164
Thread 4 got pointer: 000571d0


Notice how each thread encounters the malloc() and memset() commands and so receives and initializes its own allocation. (Exact pointer values will vary: these are illustrative.)



7.35.3.2. Per Thread Block Allocationï


#include <stdlib.h>

__global__ void mallocTest()
{
    __shared__ int* data;

    // The first thread in the block does the allocation and then
    // shares the pointer with all other threads through shared memory,
    // so that access can easily be coalesced.
    // 64 bytes per thread are allocated.
    if (threadIdx.x == 0) {
        size_t size = blockDim.x * 64;
        data = (int*)malloc(size);
    }
    __syncthreads();

    // Check for failure
    if (data == NULL)
        return;

    // Threads index into the memory, ensuring coalescence
    int* ptr = data;
    for (int i = 0; i < 64; ++i)
        ptr[i * blockDim.x + threadIdx.x] = threadIdx.x;

    // Ensure all threads complete before freeing
    __syncthreads();

    // Only one thread may free the memory!
    if (threadIdx.x == 0)
        free(data);
}

int main()
{
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);
    mallocTest<<<10, 128>>>();
    cudaDeviceSynchronize();
    return 0;
}





7.35.3.3. Allocation Persisting Between Kernel Launchesï


#include <stdlib.h>
#include <stdio.h>

#define NUM_BLOCKS 20

__device__ int* dataptr[NUM_BLOCKS]; // Per-block pointer

__global__ void allocmem()
{
    // Only the first thread in the block does the allocation
    // since we want only one allocation per block.
    if (threadIdx.x == 0)
        dataptr[blockIdx.x] = (int*)malloc(blockDim.x * 4);
    __syncthreads();

    // Check for failure
    if (dataptr[blockIdx.x] == NULL)
        return;

    // Zero the data with all threads in parallel
    dataptr[blockIdx.x][threadIdx.x] = 0;
}

// Simple example: store thread ID into each element
__global__ void usemem()
{
    int* ptr = dataptr[blockIdx.x];
    if (ptr != NULL)
        ptr[threadIdx.x] += threadIdx.x;
}

// Print the content of the buffer before freeing it
__global__ void freemem()
{
    int* ptr = dataptr[blockIdx.x];
    if (ptr != NULL)
        printf("Block %d, Thread %d: final value = %d\n",
                      blockIdx.x, threadIdx.x, ptr[threadIdx.x]);

    // Only free from one thread!
    if (threadIdx.x == 0)
        free(ptr);
}

int main()
{
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);

    // Allocate memory
    allocmem<<< NUM_BLOCKS, 10 >>>();

    // Use memory
    usemem<<< NUM_BLOCKS, 10 >>>();
    usemem<<< NUM_BLOCKS, 10 >>>();
    usemem<<< NUM_BLOCKS, 10 >>>();

    // Free memory
    freemem<<< NUM_BLOCKS, 10 >>>();

    cudaDeviceSynchronize();

    return 0;
}







7.36. Execution Configurationï

Any call to a __global__ function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams).
The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where:

Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched;
Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block;
Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in __shared__; Ns is an optional argument which defaults to 0;
S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0.

As an example, a function declared as

__global__ void Func(float* parameter);


must be called like this:

Func<<< Dg, Db, Ns >>>(parameter);


The arguments to the execution configuration are evaluated before the actual function arguments.
The function call will fail if Dg or Db are greater than the maximum sizes allowed for the device as specified in Compute Capabilities, or if Ns is greater than the maximum amount of shared memory available on the device, minus the amount of shared memory required for static allocation.
Compute capability 9.0 and above allows users to specify compile time thread block cluster dimensions, so that the kernel can use the cluster hierarchy in CUDA. Compile time cluster dimension can be specified using __cluster_dims__([x, [y, [z]]]). The example below shows compile time cluster size of 2 in X dimension and 1 in Y and Z dimension.

__global__ void __cluster_dims__(2, 1, 1) Func(float* parameter);


Thread block cluster dimensions can also be specified at runtime and kernel with the cluster can be launched using cudaLaunchKernelEx API. The API takes a configuration arugument of type cudaLaunchConfig_t, kernel function pointer and kernel arguments. Runtime kernel configuration is shown in the example below.

__global__ void Func(float* parameter);


// Kernel invocation with runtime cluster size
{
    cudaLaunchConfig_t config = {0};
    // The grid dimension is not affected by cluster launch, and is still enumerated
    // using number of blocks.
    // The grid dimension should be a multiple of cluster size.
    config.gridDim = Dg;
    config.blockDim = Db;
    config.dynamicSmemBytes = Ns;

    cudaLaunchAttribute attribute[1];
    attribute[0].id = cudaLaunchAttributeClusterDimension;
    attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension
    attribute[0].val.clusterDim.y = 1;
    attribute[0].val.clusterDim.z = 1;
    config.attrs = attribute;
    config.numAttrs = 1;

    float* parameter;
    cudaLaunchKernelEx(&config, Func, parameter);
}





7.37. Launch Boundsï

As discussed in detail in Multiprocessor Level, the fewer registers a kernel uses, the more threads and thread blocks are likely to reside on a multiprocessor, which can improve performance.
Therefore, the compiler uses heuristics to minimize register usage while keeping register spilling (see Device Memory Accesses) and instruction count to a minimum. An application can optionally aid these heuristics by providing additional information to the compiler in the form of launch bounds that are specified using the __launch_bounds__() qualifier in the definition of a __global__ function:

__global__ void
__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor, maxBlocksPerCluster)
MyKernel(...)
{
    ...
}



maxThreadsPerBlock specifies the maximum number of threads per block with which the application will ever launch MyKernel(); it compiles to the .maxntidPTX directive.
minBlocksPerMultiprocessor is optional and specifies the desired minimum number of resident blocks per multiprocessor; it compiles to the .minnctapersmPTX directive.
maxBlocksPerCluster is optional and specifies the desired maximum number thread blocks per cluster with which the application will ever launch MyKernel(); it compiles to the .maxclusterrankPTX directive.

If launch bounds are specified, the compiler first derives from them the upper limit L on the number of registers the kernel should use to ensure that minBlocksPerMultiprocessor blocks (or a single block if minBlocksPerMultiprocessor is not specified) of maxThreadsPerBlock threads can reside on the multiprocessor (see Hardware Multithreading for the relationship between the number of registers used by a kernel and the number of registers allocated per block). The compiler then optimizes register usage in the following way:

If the initial register usage is higher than L, the compiler reduces it further until it becomes less or equal to L, usually at the expense of more local memory usage and/or higher number of instructions;

If the initial register usage is lower than L

If maxThreadsPerBlock is specified and minBlocksPerMultiprocessor is not, the compiler uses maxThreadsPerBlock to determine the register usage thresholds for the transitions between n and n+1 resident blocks (i.e., when using one less register makes room for an additional resident block as in the example of Multiprocessor Level) and then applies similar heuristics as when no launch bounds are specified;
If both minBlocksPerMultiprocessor and maxThreadsPerBlock are specified, the compiler may increase register usage as high as L to reduce the number of instructions and better hide single thread instruction latency.



A kernel will fail to launch if it is executed with more threads per block than its launch bound maxThreadsPerBlock.
A kernel will fail to launch if it is executed with more thread blocks per cluster than its launch bound maxBlocksPerCluster.
Per thread resources required by a CUDA kernel might limit the maximum block size in an unwanted way. In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument __launch_bounds__(maxThreadsPerBlock) which specifies the largest block size that the kernel will be launched with. Failure to do so could lead to âtoo many resources requested for launchâ errors. Providing the two argument version of __launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor) can improve performance in some cases. The right value for minBlocksPerMultiprocessor should be determined using a detailed per kernel analysis.
Optimal launch bounds for a given kernel will usually differ across major architecture revisions. The sample code below shows how this is typically handled in device code using the __CUDA_ARCH__ macro introduced in Application Compatibility

#define THREADS_PER_BLOCK          256
#if __CUDA_ARCH__ >= 200
    #define MY_KERNEL_MAX_THREADS  (2 * THREADS_PER_BLOCK)
    #define MY_KERNEL_MIN_BLOCKS   3
#else
    #define MY_KERNEL_MAX_THREADS  THREADS_PER_BLOCK
    #define MY_KERNEL_MIN_BLOCKS   2
#endif

// Device code
__global__ void
__launch_bounds__(MY_KERNEL_MAX_THREADS, MY_KERNEL_MIN_BLOCKS)
MyKernel(...)
{
    ...
}


In the common case where MyKernel is invoked with the maximum number of threads per block (specified as the first parameter of __launch_bounds__()), it is tempting to use MY_KERNEL_MAX_THREADS as the number of threads per block in the execution configuration:

// Host code
MyKernel<<<blocksPerGrid, MY_KERNEL_MAX_THREADS>>>(...);


This will not work however since __CUDA_ARCH__ is undefined in host code as mentioned in Application Compatibility, so MyKernel will launch with 256 threads per block even when __CUDA_ARCH__ is greater or equal to 200. Instead the number of threads per block should be determined:


Either at compile time using a macro that does not depend on __CUDA_ARCH__, for example

// Host code
MyKernel<<<blocksPerGrid, THREADS_PER_BLOCK>>>(...);




Or at runtime based on the compute capability

// Host code
cudaGetDeviceProperties(&deviceProp, device);
int threadsPerBlock =
          (deviceProp.major >= 2 ?
                    2 * THREADS_PER_BLOCK : THREADS_PER_BLOCK);
MyKernel<<<blocksPerGrid, threadsPerBlock>>>(...);




Register usage is reported by the --ptxas-options=-v compiler option. The number of resident blocks can be derived from the occupancy reported by the CUDA profiler (see Device Memory Accessesfor a definition of occupancy).
The __launch_bounds__() and __maxnreg__() qualifiers cannot be applied to the same kernel.
Register usage can also be controlled for all __global__ functions in a file using the maxrregcount compiler option. The value of maxrregcount is ignored for functions with launch bounds.



7.38. Maximum Number of Registers per Threadï

To provide a mechanism for low-level performance tuning, CUDA C++ provides the __maxnreg()__ function qualifier to pass performance tuning information to the backend optimizing compiler. The
__maxnreg__() qualifier specifies the maximum number of registers to be allocated to a single thread in a thread block. In the definition of a __global__ function:

__global__ void
__maxnreg__(maxNumberRegistersPerThread)
MyKernel(...)
{
    ...
}



maxNumberRegistersPerThread specifies the maximum number of registers to be allocated to a single thread in a thread block of the kernel MyKernel(); it compiles to the .maxnregPTX directive.

The __launch_bounds__() and __maxnreg__() qualifiers cannot be applied to the same kernel.
Register usage can also be controlled for all __global__ functions in a file using the maxrregcount compiler option. The value of maxrregcount is ignored for functions with the __maxnreg__ qualifier.



7.39. #pragma unrollï

By default, the compiler unrolls small loops with a known trip count. The #pragma unroll directive however can be used to control unrolling of any given loop. It must be placed immediately before the loop and only applies to that loop. It is optionally followed by an integral constant expression (ICE)13. If the ICE is absent, the loop will be completely unrolled if its trip count is constant. If the ICE evaluates to 1, the compiler will not unroll the loop. The pragma will be ignored if the ICE evaluates to a non-positive integer or to an integer greater than the maximum value representable by the int data type.
Examples:

struct S1_t { static const int value = 4; };
template <int X, typename T2>
__device__ void foo(int *p1, int *p2) {

// no argument specified, loop will be completely unrolled
#pragma unroll
for (int i = 0; i < 12; ++i)
  p1[i] += p2[i]*2;

// unroll value = 8
#pragma unroll (X+1)
for (int i = 0; i < 12; ++i)
  p1[i] += p2[i]*4;

// unroll value = 1, loop unrolling disabled
#pragma unroll 1
for (int i = 0; i < 12; ++i)
  p1[i] += p2[i]*8;

// unroll value = 4
#pragma unroll (T2::value)
for (int i = 0; i < 12; ++i)
  p1[i] += p2[i]*16;
}

__global__ void bar(int *p1, int *p2) {
foo<7, S1_t>(p1, p2);
}





7.40. SIMD Video Instructionsï

PTX ISA version 3.0 includes SIMD (Single Instruction, Multiple Data) video instructions which operate on pairs of 16-bit values and quads of 8-bit values. These are available on devices of compute capability 3.0.
The SIMD video instructions are:

vadd2, vadd4
vsub2, vsub4
vavrg2, vavrg4
vabsdiff2, vabsdiff4
vmin2, vmin4
vmax2, vmax4
vset2, vset4

PTX instructions, such as the SIMD video instructions, can be included in CUDA programs by way of the assembler, asm(), statement.
The basic syntax of an asm() statement is:

asm("template-string" : "constraint"(output) : "constraint"(input)"));


An example of using the vabsdiff4 PTX instruction is:

asm("vabsdiff4.u32.u32.u32.add" " %0, %1, %2, %3;": "=r" (result):"r" (A), "r" (B), "r" (C));


This uses the vabsdiff4 instruction to compute an integer quad byte SIMD sum of absolute differences. The absolute difference value is computed for each byte of the unsigned integers A and B in SIMD fashion. The optional accumulate operation (.add) is specified to sum these differences.
Refer to the document âUsing Inline PTX Assembly in CUDAâ for details on using the assembly statement in your code. Refer to the PTX ISA documentation (âParallel Thread Execution ISA Version 3.0â for example) for details on the PTX instructions for the version of PTX that you are using.



7.41. Diagnostic Pragmasï

The following pragmas may be used to control the error severity used when a given diagnostic message is issued.

#pragma nv_diag_suppress
#pragma nv_diag_warning
#pragma nv_diag_error
#pragma nv_diag_default
#pragma nv_diag_once


Uses of these pragmas have the following form:

#pragma nv_diag_xxx error_number, error_number ...


The diagnostic affected is specified using an error number showed in a warning message. Any diagnostic may be overridden to be an error, but only warnings may have their severity suppressed or be restored to a warning after being promoted to an error. The nv_diag_default pragma is used to return the severity of a diagnostic to the one that was in effect before any pragmas were issued (i.e., the normal severity of the message as modified by any command-line options). The following example suppresses the "declared but never referenced" warning on the declaration of foo:

#pragma nv_diag_suppress 177
void foo()
{
  int i=0;
}
#pragma nv_diag_default 177
void bar()
{
  int i=0;
}


The following pragmas may be used to save and restore the current diagnostic pragma state:

#pragma nv_diagnostic push
#pragma nv_diagnostic pop


Examples:

#pragma nv_diagnostic push
#pragma nv_diag_suppress 177
void foo()
{
  int i=0;
}
#pragma nv_diagnostic pop
void bar()
{
  int i=0;
}


Note that the pragmas only affect the nvcc CUDA frontend compiler; they have no effect on the host compiler.
Removal Notice: The support of diagnostic pragmas without nv_ prefix are removed from CUDA 12.0, if the pragmas are inside the device code, warning unrecognized #pragma in device code will be emitted, otherwise they will be passed to the host compiler. If they are intended for CUDA code, use the pragmas with nv_ prefix instead.

11

When the enclosing __host__ function is a template, nvcc may currently fail to issue a diagnostic message in some cases; this behavior may change in the future.

12

The intent is to prevent the host compiler from encountering the call to the function if the host compiler does not support it.


13(1,2)


See the C++ Standard for definition of integral constant expression.






8. Cooperative Groupsï



8.1. Introductionï

Cooperative Groups is an extension to the CUDA programming model, introduced in CUDA 9, for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions.
Historically, the CUDA programming model has provided a single, simple construct for synchronizing cooperating threads: a barrier across all threads of a thread block, as implemented with the __syncthreads() intrinsic function. However, programmers would like to define and synchronize groups of threads at other granularities to enable greater performance, design flexibility, and software reuse in the form of âcollectiveâ group-wide function interfaces. In an effort to express broader patterns of parallel interaction, many performance-oriented programmers have resorted to writing their own ad hoc and unsafe primitives for synchronizing threads within a single warp, or across sets of thread blocks running on a single GPU. Whilst the performance improvements achieved have often been valuable, this has resulted in an ever-growing collection of brittle code that is expensive to write, tune, and maintain over time and across GPU generations. Cooperative Groups addresses this by providing a safe and future-proof mechanism to enable performant code.



8.2. Whatâs New in Cooperative Groupsï



8.2.1. CUDA 12.2ï


barrier_arrive and barrier_wait member functions were added for grid_group and thread_block. Description of the API is available here.




8.2.2. CUDA 12.1ï


invoke_one and invoke_one_broadcast  APIs were added.




8.2.3. CUDA 12.0ï



The following experimental APIs are now moved to the main namespace:

asynchronous reduce and scan update added in CUDA 11.7
thread_block_tile larger than 32 added in CUDA 11.1


It is no longer required to provide memory using the block_tile_memory object in order to create these large tiles on Compute Capability 8.0 or higher.





8.3. Programming Model Conceptï

The Cooperative Groups programming model describes synchronization patterns both within and across CUDA thread blocks. It provides both the means for applications to define their own groups of threads, and the interfaces to synchronize them. It also provides new launch APIs that enforce certain restrictions and therefore can guarantee the synchronization will work. These primitives enable new patterns of cooperative parallelism within CUDA, including producer-consumer parallelism, opportunistic parallelism, and global synchronization across the entire Grid.
The Cooperative Groups programming model consists of the following elements:

Data types for representing groups of cooperating threads;
Operations to obtain implicit groups defined by the CUDA launch API (e.g., thread blocks);
Collectives for partitioning existing groups into new groups;
Collective Algorithms for data movement and manipulation (e.g. memcpy_async, reduce, scan);
An operation to synchronize all threads within the group;
Operations to inspect the group properties;
Collectives that expose low-level, group-specific and often HW accelerated, operations.

The main concept in Cooperative Groups is that of objects naming the set of threads that are part of it. This expression of groups as first-class program objects improves software composition, since collective functions can receive an explicit object representing the group of participating threads. This object also makes programmer intent explicit, which eliminates unsound architectural assumptions that result in brittle code, undesirable restrictions upon compiler optimizations, and better compatibility with new GPU generations.
To write efficient code, its best to use specialized groups (going generic loses a lot of compile time optimizations), and pass these group objects by reference to functions that intend to use these threads in some cooperative fashion.
Cooperative Groups requires CUDA 9.0 or later. To use Cooperative Groups, include the header file:

// Primary header is compatible with pre-C++11, collective algorithm headers require C++11
#include <cooperative_groups.h>
// Optionally include for memcpy_async() collective
#include <cooperative_groups/memcpy_async.h>
// Optionally include for reduce() collective
#include <cooperative_groups/reduce.h>
// Optionally include for inclusive_scan() and exclusive_scan() collectives
#include <cooperative_groups/scan.h>


and use the Cooperative Groups namespace:

using namespace cooperative_groups;
// Alternatively use an alias to avoid polluting the namespace with collective algorithms
namespace cg = cooperative_groups;


The code can be compiled in a normal way using nvcc, however if you wish to use memcpy_async, reduce or scan functionality and your host compilerâs default dialect is not C++11 or higher, then you must add --std=c++11 to the command line.


8.3.1. Composition Exampleï

To illustrate the concept of groups, this example attempts to perform a block-wide sum reduction. Previously, there were hidden constraints on the implementation when writing this code:

__device__ int sum(int *x, int n) {
    // ...
    __syncthreads();
    return total;
}

__global__ void parallel_kernel(float *x) {
    // ...
    // Entire thread block must call sum
    sum(x, n);
}


All threads in the thread block must arrive at the __syncthreads() barrier, however, this constraint is hidden from the developer who might want to use sum(â¦). With Cooperative Groups, a better way of writing this would be:

__device__ int sum(const thread_block& g, int *x, int n) {
    // ...
    g.sync()
    return total;
}

__global__ void parallel_kernel(...) {
    // ...
    // Entire thread block must call sum
    thread_block tb = this_thread_block();
    sum(tb, x, n);
    // ...
}






8.4. Group Typesï



8.4.1. Implicit Groupsï

Implicit groups represent the launch configuration of the kernel. Regardless of how your kernel is written, it always has a set number of threads, blocks and block dimensions, a single grid and grid dimensions. In addition, if the multi-device cooperative launch API is used, it can have multiple grids (single grid per device). These groups provide a starting point for decomposition into finer grained groups which are typically HW accelerated and are more specialized for the problem the developer is solving.
Although you can create an implicit group anywhere in the code, it is dangerous to do so. Creating a handle for an implicit group is a collective operationâall threads in the group must participate. If the group was created in a conditional branch that not all threads reach, this can lead to deadlocks or data corruption. For this reason, it is recommended that you create a handle for the implicit group upfront (as early as possible, before any branching has occurred) and use that handle throughout the kernel. Group handles must be initialized at declaration time (there is no default constructor) for the same reason and copy-constructing them is discouraged.


8.4.1.1. Thread Block Groupï

Any CUDA programmer is already familiar with a certain group of threads: the thread block. The Cooperative Groups extension introduces a new datatype, thread_block, to explicitly represent this concept within the kernel.
class thread_block;
Constructed via:

thread_block g = this_thread_block();


Public Member Functions:
static void sync(): Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive())
thread_block::arrival_token barrier_arrive(): Arrive on the thread_block barrier, returns a token that needs to be passed into barrier_wait(). More details here
void barrier_wait(thread_block::arrival_token&& t): Wait on the thread_block barrier, takes arrival token returned from barrier_arrive() as a rvalue reference. More details here
static unsigned int thread_rank(): Rank of the calling thread within [0, num_threads)
static dim3 group_index(): 3-Dimensional index of the block within the launched grid
static dim3 thread_index(): 3-Dimensional index of the thread within the launched block
static dim3 dim_threads(): Dimensions of the launched block in units of threads
static unsigned int num_threads(): Total number of threads in the group
Legacy member functions (aliases):
static unsigned int size(): Total number of threads in the group (alias of num_threads())
static dim3 group_dim(): Dimensions of the launched block (alias of dim_threads())
Example:

/// Loading an integer from global into shared memory
__global__ void kernel(int *globalInput) {
    __shared__ int x;
    thread_block g = this_thread_block();
    // Choose a leader in the thread block
    if (g.thread_rank() == 0) {
        // load from global into shared for all threads to work with
        x = (*globalInput);
    }
    // After loading data into shared memory, you want to synchronize
    // if all threads in your thread block need to see it
    g.sync(); // equivalent to __syncthreads();
}


Note: that all threads in the group must participate in collective operations, or the behavior is undefined.
Related: The thread_block datatype is derived from the more generic thread_group datatype, which can be used to represent a wider class of groups.



8.4.1.2. Cluster Groupï

This group object represents all the threads launched in a single cluster. Refer to Thread Block Clusters. The APIs are available on all hardware with Compute Capability 9.0+. In such cases, when a non-cluster grid is launched, the APIs assume a 1x1x1 cluster.
class cluster_group;
Constructed via:

cluster_group g = this_cluster();


Public Member Functions:
static void sync(): Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive())
static cluster_group::arrival_token barrier_arrive(): Arrive on the cluster barrier, returns a token that needs to be passed into barrier_wait(). More details here
static void barrier_wait(cluster_group::arrival_token&& t): Wait on the cluster barrier, takes arrival token returned from barrier_arrive() as a rvalue reference. More details here
static unsigned int thread_rank(): Rank of the calling thread within [0, num_threads)
static unsigned int block_rank(): Rank of the calling block within [0, num_blocks)
static unsigned int num_threads(): Total number of threads in the group
static unsigned int num_blocks(): Total number of blocks in the group
static dim3 dim_threads(): Dimensions of the launched cluster in units of threads
static dim3 dim_blocks(): Dimensions of the launched cluster in units of blocks
static dim3 block_index(): 3-Dimensional index of the calling block within the launched cluster
static unsigned int query_shared_rank(const void *addr): Obtain the block rank to which a shared memory address belongs
static T* map_shared_rank(T *addr, int rank): Obtain the address of a shared memory variable of another block in the cluster
Legacy member functions (aliases):
static unsigned int size(): Total number of threads in the group (alias of num_threads())



8.4.1.3. Grid Groupï

This group object represents all the threads launched in a single grid. APIs other than sync() are available at all times, but to be able to synchronize across the grid, you need to use the cooperative launch API.
class grid_group;
Constructed via:

grid_group g = this_grid();


Public Member Functions:
bool is_valid() const: Returns whether the grid_group can synchronize
void sync() const: Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive())
grid_group::arrival_token barrier_arrive(): Arrive on the grid barrier, returns a token that needs to be passed into barrier_wait(). More details here
void barrier_wait(grid_group::arrival_token&& t): Wait on the grid barrier, takes arrival token returned from barrier_arrive() as a rvalue reference. More details here
static unsigned long long thread_rank(): Rank of the calling thread within [0, num_threads)
static unsigned long long block_rank(): Rank of the calling block within [0, num_blocks)
static unsigned long long cluster_rank(): Rank of the calling cluster within [0, num_clusters)
static unsigned long long num_threads(): Total number of threads in the group
static unsigned long long num_blocks(): Total number of blocks in the group
static unsigned long long num_clusters(): Total number of clusters in the group
static dim3 dim_blocks(): Dimensions of the launched grid in units of blocks
static dim3 dim_clusters(): Dimensions of the launched grid in units of clusters
static dim3 block_index(): 3-Dimensional index of the block within the launched grid
static dim3 cluster_index(): 3-Dimensional index of the cluster within the launched grid
Legacy member functions (aliases):
static unsigned long long size(): Total number of threads in the group (alias of num_threads())
static dim3 group_dim(): Dimensions of the launched grid (alias of dim_blocks())



8.4.1.4. Multi Grid Groupï

This group object represents all the threads launched across all devices of a multi-device cooperative launch. Unlike the grid.group, all the APIs require that you have used the appropriate launch API.
class multi_grid_group;
Constructed via:

// Kernel must be launched with the cooperative multi-device API
multi_grid_group g = this_multi_grid();


Public Member Functions:
bool is_valid() const: Returns whether the multi_grid_group can be used
void sync() const: Synchronize the threads named in the group
unsigned long long num_threads() const: Total number of threads in the group
unsigned long long thread_rank() const: Rank of the calling thread within [0, num_threads)
unsigned int grid_rank() const: Rank of the grid within [0,num_grids]
unsigned int num_grids() const: Total number of grids launched
Legacy member functions (aliases):
unsigned long long size() const: Total number of threads in the group (alias of num_threads())
Deprecation Notice: multi_grid_group has been deprecated in CUDA 11.3 for all devices.




8.4.2. Explicit Groupsï



8.4.2.1. Thread Block Tileï

A templated version of a tiled group, where a template parameter is used to specify the size of the tile - with this known at compile time there is the potential for more optimal execution.

template <unsigned int Size, typename ParentT = void>
class thread_block_tile;


Constructed via:

template <unsigned int Size, typename ParentT>
_CG_QUALIFIER thread_block_tile<Size, ParentT> tiled_partition(const ParentT& g)


Size must be a power of 2 and less than or equal to 1024. Notes section describes extra steps needed to create tiles of size larger than 32 on hardware with Compute Capability 7.5 or lower.
ParentT is the parent-type from which this group was partitioned. It is automatically inferred, but a value of void will store this information in the group handle rather than in the type.
Public Member Functions:
void sync() const: Synchronize the threads named in the group
unsigned long long num_threads() const: Total number of threads in the group
unsigned long long thread_rank() const: Rank of the calling thread within [0, num_threads)
unsigned long long meta_group_size() const: Returns the number of groups created when the parent group was partitioned.
unsigned long long meta_group_rank() const: Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size)
T shfl(T var, unsigned int src_rank) const: Refer to Warp Shuffle Functions, Note: For sizes larger than 32 all threads in the group have to specify the same src_rank, otherwise the behavior is undefined.
T shfl_up(T var, int delta) const: Refer to Warp Shuffle Functions, available only for sizes lower or equal to 32.
T shfl_down(T var, int delta) const: Refer to Warp Shuffle Functions, available only for sizes lower or equal to 32.
T shfl_xor(T var, int delta) const: Refer to Warp Shuffle Functions, available only for sizes lower or equal to 32.
T any(int predicate) const: Refer to Warp Vote Functions
T all(int predicate) const: Refer to Warp Vote Functions
T ballot(int predicate) const: Refer to Warp Vote Functions, available only for sizes lower or equal to 32.
unsigned int match_any(T val) const: Refer to Warp Match Functions, available only for sizes lower or equal to 32.
unsigned int match_all(T val, int &pred) const: Refer to Warp Match Functions, available only for sizes lower or equal to 32.
Legacy member functions (aliases):
unsigned long long size() const: Total number of threads in the group (alias of num_threads())
Notes:

thread_block_tile templated data structure is being used here, the size of the group is passed to the tiled_partition call as a template parameter rather than an argument.

shfl, shfl_up, shfl_down, and shfl_xor functions accept objects of any type when compiled with C++11 or later. This means itâs possible to shuffle non-integral types as long as they satisfy the below constraints:

Qualifies as trivially copyable i.e., is_trivially_copyable<T>::value == true
sizeof(T) <= 32 for tile sizes lower or equal 32, sizeof(T) <= 8 for larger tiles



On hardware with Compute Capability 7.5 or lower tiles of size larger than 32 need small amount of memory reserved for them. This can be done using cooperative_groups::block_tile_memory struct template that has to reside in either shared or global memory.

template <unsigned int MaxBlockSize = 1024>
struct block_tile_memory;


MaxBlockSize Specifies the maximal number of threads in the current thread block. This parameter can be used to minimize the shared memory usage of block_tile_memory in kernels launched only with smaller thread counts.
This block_tile_memory needs be then passed into cooperative_groups::this_thread_block, allowing the resulting thread_block to be partitioned into tiles of sizes larger than 32. Overload of this_thread_block accepting block_tile_memory argument is a collective operation and has to be called with all threads in the thread_block.
block_tile_memory can be used on hardware with Compute Capability 8.0 or higher in order to be able to write one source targeting multiple different Compute Capabilities. It should consume no memory when instantiated in shared memory in cases where its not required.


Examples:

/// The following code will create two sets of tiled groups, of size 32 and 4 respectively:
/// The latter has the provenance encoded in the type, while the first stores it in the handle
thread_block block = this_thread_block();
thread_block_tile<32> tile32 = tiled_partition<32>(block);
thread_block_tile<4, thread_block> tile4 = tiled_partition<4>(block);



/// The following code will create tiles of size 128 on all Compute Capabilities.
/// block_tile_memory can be omitted on Compute Capability 8.0 or higher.
__global__ void kernel(...) {
    // reserve shared memory for thread_block_tile usage,
    //   specify that block size will be at most 256 threads.
    __shared__ block_tile_memory<256> shared;
    thread_block thb = this_thread_block(shared);

    // Create tiles with 128 threads.
    auto tile = tiled_partition<128>(thb);

    // ...
}




8.4.2.1.1. Warp-Synchronous Code Patternï

Developers might have had warp-synchronous codes that they previously made implicit assumptions about the warp size and would code around that number. Now this needs to be specified explicitly.

__global__ void cooperative_kernel(...) {
    // obtain default "current thread block" group
    thread_block my_block = this_thread_block();

    // subdivide into 32-thread, tiled subgroups
    // Tiled subgroups evenly partition a parent group into
    // adjacent sets of threads - in this case each one warp in size
    auto my_tile = tiled_partition<32>(my_block);

    // This operation will be performed by only the
    // first 32-thread tile of each block
    if (my_tile.meta_group_rank() == 0) {
        // ...
        my_tile.sync();
    }
}





8.4.2.1.2. Single thread groupï

Group representing the current thread can be obtained from this_thread function:

thread_block_tile<1> this_thread();


The following memcpy_async API uses a thread_group, to copy an int element from source to destination:

#include <cooperative_groups.h>
#include <cooperative_groups/memcpy_async.h>

cooperative_groups::memcpy_async(cooperative_groups::this_thread(), dest, src, sizeof(int));


More detailed examples of using this_thread to perform asynchronous copies can be found in the Single-Stage Asynchronous Data Copies using cuda::pipeline and Multi-Stage Asynchronous Data Copies using cuda::pipeline sections.




8.4.2.2. Coalesced Groupsï

In CUDAâs SIMT architecture, at the hardware level the multiprocessor executes threads in groups of 32 called warps. If there exists a data-dependent conditional branch in the application code such that threads within a warp diverge, then the warp serially executes each branch disabling threads not on that path. The threads that remain active on the path are referred to as coalesced. Cooperative Groups has functionality to discover, and create, a group containing all coalesced threads.
Constructing the group handle via coalesced_threads() is opportunistic. It returns the set of active threads at that point in time, and makes no guarantee about which threads are returned (as long as they are active) or that they will stay coalesced throughout execution (they will be brought back together for the execution of a collective but can diverge again afterwards).
class coalesced_group;
Constructed via:

coalesced_group active = coalesced_threads();


Public Member Functions:
void sync() const: Synchronize the threads named in the group
unsigned long long num_threads() const: Total number of threads in the group
unsigned long long thread_rank() const: Rank of the calling thread within [0, num_threads)
unsigned long long meta_group_size() const: Returns the number of groups created when the parent group was partitioned. If this group was created by querying the set of active threads, e.g. coalesced_threads() the value of meta_group_size() will be 1.
unsigned long long meta_group_rank() const: Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size). If this group was created by querying the set of active threads, e.g. coalesced_threads() the value of meta_group_rank() will always be 0.
T shfl(T var, unsigned int src_rank) const: Refer to Warp Shuffle Functions
T shfl_up(T var, int delta) const: Refer to Warp Shuffle Functions
T shfl_down(T var, int delta) const: Refer to Warp Shuffle Functions
T any(int predicate) const: Refer to Warp Vote Functions
T all(int predicate) const: Refer to Warp Vote Functions
T ballot(int predicate) const: Refer to Warp Vote Functions
unsigned int match_any(T val) const: Refer to Warp Match Functions
unsigned int match_all(T val, int &pred) const: Refer to Warp Match Functions
Legacy member functions (aliases):
unsigned long long size() const: Total number of threads in the group (alias of num_threads())
Notes:
shfl, shfl_up, and shfl_down functions accept objects of any type when compiled with C++11 or later. This means itâs possible to shuffle non-integral types as long as they satisfy the below constraints:

Qualifies as trivially copyable i.e. is_trivially_copyable<T>::value == true
sizeof(T) <= 32

Example:

/// Consider a situation whereby there is a branch in the
/// code in which only the 2nd, 4th and 8th threads in each warp are
/// active. The coalesced_threads() call, placed in that branch, will create (for each
/// warp) a group, active, that has three threads (with
/// ranks 0-2 inclusive).
__global__ void kernel(int *globalInput) {
    // Lets say globalInput says that threads 2, 4, 8 should handle the data
    if (threadIdx.x == *globalInput) {
        coalesced_group active = coalesced_threads();
        // active contains 0-2 inclusive
        active.sync();
    }
}




8.4.2.2.1. Discovery Patternï

Commonly developers need to work with the current active set of threads. No assumption is made about the threads that are present, and instead developers work with the threads that happen to be there. This is seen in the following âaggregating atomic increment across threads in a warpâ example (written using the correct CUDA 9.0 set of intrinsics):

{
    unsigned int writemask = __activemask();
    unsigned int total = __popc(writemask);
    unsigned int prefix = __popc(writemask & __lanemask_lt());
    // Find the lowest-numbered active lane
    int elected_lane = __ffs(writemask) - 1;
    int base_offset = 0;
    if (prefix == 0) {
        base_offset = atomicAdd(p, total);
    }
    base_offset = __shfl_sync(writemask, base_offset, elected_lane);
    int thread_offset = prefix + base_offset;
    return thread_offset;
}


This can be re-written with Cooperative Groups as follows:

{
    cg::coalesced_group g = cg::coalesced_threads();
    int prev;
    if (g.thread_rank() == 0) {
        prev = atomicAdd(p, g.num_threads());
    }
    prev = g.thread_rank() + g.shfl(prev, 0);
    return prev;
}








8.5. Group Partitioningï



8.5.1. tiled_partitionï


template <unsigned int Size, typename ParentT>
thread_block_tile<Size, ParentT> tiled_partition(const ParentT& g);



thread_group tiled_partition(const thread_group& parent, unsigned int tilesz);


The tiled_partition method is a collective operation that partitions the parent group into a one-dimensional, row-major, tiling of subgroups. A total of ((size(parent)/tilesz) subgroups will be created, therefore the parent group size must be evenly divisible by the Size. The allowed parent groups are thread_block or thread_block_tile.
The implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution. Functionality is limited to native hardware sizes, 1/2/4/8/16/32 and the cg::size(parent) must be greater than the Size parameter. The templated version of tiled_partition supports 64/128/256/512 sizes as well, but some additional steps are required on Compute Capability 7.5 or lower, refer to Thread Block Tile for details.
Codegen Requirements: Compute Capability 5.0 minimum, C++11 for sizes larger than 32
Example:

/// The following code will create a 32-thread tile
thread_block block = this_thread_block();
thread_block_tile<32> tile32 = tiled_partition<32>(block);


We can partition each of these groups into even smaller groups, each of size 4 threads:

auto tile4 = tiled_partition<4>(tile32);
// or using a general group
// thread_group tile4 = tiled_partition(tile32, 4);


If, for instance, if we were to then include the following line of code:

if (tile4.thread_rank()==0) printf("Hello from tile4 rank 0\n");


then the statement would be printed by every fourth thread in the block: the threads of rank 0 in each tile4 group, which correspond to those threads with ranks 0,4,8,12,etc. in the block group.



8.5.2. labeled_partitionï


template <typename Label>
coalesced_group labeled_partition(const coalesced_group& g, Label label);



template <unsigned int Size, typename Label>
coalesced_group labeled_partition(const thread_block_tile<Size>& g, Label label);


The labeled_partition method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced. The implementation will evaluate a condition label and assign threads that have the same value for label into the same group.
Label can be any integral type.
The implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution.
Note: This functionality is still being evaluated and may slightly change in the future.
Codegen Requirements: Compute Capability 7.0 minimum, C++11



8.5.3. binary_partitionï


coalesced_group binary_partition(const coalesced_group& g, bool pred);



template <unsigned int Size>
coalesced_group binary_partition(const thread_block_tile<Size>& g, bool pred);


The binary_partition() method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced. The implementation will evaluate a predicate and assign threads that have the same value into the same group. This is a specialized form of labeled_partition(), where the label can only be 0 or 1.
The implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution.
Note: This functionality is still being evaluated and may slightly change in the future.
Codegen Requirements: Compute Capability 7.0 minimum, C++11
Example:

/// This example divides a 32-sized tile into a group with odd
/// numbers and a group with even numbers
_global__ void oddEven(int *inputArr) {
    auto block = cg::this_thread_block();
    auto tile32 = cg::tiled_partition<32>(block);

    // inputArr contains random integers
    int elem = inputArr[block.thread_rank()];
    // after this, tile32 is split into 2 groups,
    // a subtile where elem&1 is true and one where its false
    auto subtile = cg::binary_partition(tile32, (elem & 1));
}






8.6. Group Collectivesï

Cooperative Groups library provides a set of collective operations that can be performed by a group of threads.
These operations require participation of all threads in the specified group in order to complete the operation.
All threads in the group need to pass the same values for corresponding arguments to each collective call, unless
different values are explicitly allowed in the argument description. Otherwise the behavior of the call is undefined.


8.6.1. Synchronizationï



8.6.1.1. barrier_arrive and barrier_waitï


T::arrival_token T::barrier_arrive();
void T::barrier_wait(T::arrival_token&&);


barrier_arrive and barrier_wait member functions provide a synchronization API similar to cuda::barrier (read more). Cooperative Groups automatically initializes the group barrier, but arrive and wait operations have an additional restriction resulting from collective nature of those operations: All threads in the group must arrive and wait at the barrier once per phase.
When barrier_arrive is called with a group, result of calling any collective operation or another barrier arrival with that group is undefined until completion of the barrier phase is observed with barrier_wait call. Threads blocked on barrier_wait might be released from the synchronization before other threads call barrier_wait, but only after all threads in the group called barrier_arrive.
Group type T can be any of the implicit groups .This allows threads to do independent work after they arrive and before they wait for the synchronization to resolve, allowing to hide some of the synchronization latency.
barrier_arrive returns an arrival_token object that must be passed into the corresponding barrier_wait. Token is consumed this way and can not be used for another barrier_wait call.
Example of barrier_arrive and barrier_wait used to synchronize initization of shared memory across the cluster:

#include <cooperative_groups.h>

using namespace cooperative_groups;

void __device__ init_shared_data(const thread_block& block, int *data);
void __device__ local_processing(const thread_block& block);
void __device__ process_shared_data(const thread_block& block, int *data);

__global__ void cluster_kernel() {
    extern __shared__ int array[];
    auto cluster = this_cluster();
    auto block   = this_thread_block();

    // Use this thread block to initialize some shared state
    init_shared_data(block, &array[0]);

    auto token = cluster.barrier_arrive(); // Let other blocks know this block is running and data was initialized

    // Do some local processing to hide the synchronization latency
    local_processing(block);

    // Map data in shared memory from the next block in the cluster
    int *dsmem = cluster.map_shared_rank(&array[0], (cluster.block_rank() + 1) % cluster.num_blocks());

    // Make sure all other blocks in the cluster are running and initialized shared data before accessing dsmem
    cluster.barrier_wait(std::move(token));

    // Consume data in distributed shared memory
    process_shared_data(block, dsmem);
    cluster.sync();
}





8.6.1.2. syncï


static void T::sync();

template <typename T>
void sync(T& group);


sync synchronizes the threads named in the group. Group type T can be any of the existing group types, as all of them support synchronization. Its available as a member function in every group type or as a free function taking a group as parameter.
If the group is a grid_group or a multi_grid_group the kernel must have been launched using the appropriate cooperative launch APIs. Equivalent to T.barrier_wait(T.barrier_arrive()).




8.6.2. Data Transferï



8.6.2.1. memcpy_asyncï

memcpy_async is a group-wide collective memcpy that utilizes hardware accelerated support for non-blocking memory transactions from global to shared memory. Given a set of threads named in the group, memcpy_async will move specified amount of bytes or elements of the input type through a single pipeline stage. Additionally for achieving best performance when using the memcpy_async API, an alignment of 16 bytes for both shared memory and global memory is required. It is important to note that while this is a memcpy in the general case, it is only asynchronous if the source is global memory and the destination is shared memory and both can be addressed with 16, 8, or 4 byte alignments. Asynchronously copied data should only be read following a call to wait or wait_prior which signals that the corresponding stage has completed moving data to shared memory.
Having to wait on all outstanding requests can lose some flexibility (but gain simplicity). In order to efficiently overlap data transfer and execution, its important to be able to kick off an N+1memcpy_async request while waiting on and operating on request N. To do so, use memcpy_async and wait on it using the collective stage-based wait_prior API. See wait and wait_prior for more details.
Usage 1

template <typename TyGroup, typename TyElem, typename TyShape>
void memcpy_async(
  const TyGroup &group,
  TyElem *__restrict__ _dst,
  const TyElem *__restrict__ _src,
  const TyShape &shape
);


Performs a copy of ``shape`` bytes.
Usage 2

template <typename TyGroup, typename TyElem, typename TyDstLayout, typename TySrcLayout>
void memcpy_async(
  const TyGroup &group,
  TyElem *__restrict__ dst,
  const TyDstLayout &dstLayout,
  const TyElem *__restrict__ src,
  const TySrcLayout &srcLayout
);


Performs a copy of ``min(dstLayout, srcLayout)`` elements. If layouts are of type cuda::aligned_size_t<N>, both must specify the same alignment.
Errata
The memcpy_async API introduced in CUDA 11.1 with both src and dst input layouts, expects the layout to be provided in elements rather than bytes. The element type is inferred from TyElem and has the size sizeof(TyElem). If cuda::aligned_size_t<N> type is used as the layout, the number of elements specified times sizeof(TyElem) must be a multiple of N and it is recommended to use std::byte or char as the element type.
If specified shape or layout of the copy is of type cuda::aligned_size_t<N>, alignment will be guaranteed to be at least min(16, N). In that case both dst and src pointers need to be aligned to N bytes and the number of bytes copied needs to be a multiple of N.
Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for asynchronicity, C++11
cooperative_groups/memcpy_async.h header needs to be included.
Example:

/// This example streams elementsPerThreadBlock worth of data from global memory
/// into a limited sized shared memory (elementsInShared) block to operate on.
#include <cooperative_groups.h>
#include <cooperative_groups/memcpy_async.h>

namespace cg = cooperative_groups;

__global__ void kernel(int* global_data) {
    cg::thread_block tb = cg::this_thread_block();
    const size_t elementsPerThreadBlock = 16 * 1024;
    const size_t elementsInShared = 128;
    __shared__ int local_smem[elementsInShared];

    size_t copy_count;
    size_t index = 0;
    while (index < elementsPerThreadBlock) {
        cg::memcpy_async(tb, local_smem, elementsInShared, global_data + index, elementsPerThreadBlock - index);
        copy_count = min(elementsInShared, elementsPerThreadBlock - index);
        cg::wait(tb);
        // Work with local_smem
        index += copy_count;
    }
}





8.6.2.2. wait and wait_priorï


template <typename TyGroup>
void wait(TyGroup & group);

template <unsigned int NumStages, typename TyGroup>
void wait_prior(TyGroup & group);


wait and wait_prior collectives allow to wait for memcpy_async copies to complete. wait blocks calling threads until all previous copies are done. wait_prior allows that the latest NumStages are still not done and waits for all the previous requests. So with N total copies requested, it waits until the first N-NumStages are done and the last NumStages might still be in progress. Both wait and wait_prior will synchronize the named group.
Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for asynchronicity, C++11
cooperative_groups/memcpy_async.h header needs to be included.
Example:

/// This example streams elementsPerThreadBlock worth of data from global memory
/// into a limited sized shared memory (elementsInShared) block to operate on in
/// multiple (two) stages. As stage N is kicked off, we can wait on and operate on stage N-1.
#include <cooperative_groups.h>
#include <cooperative_groups/memcpy_async.h>

namespace cg = cooperative_groups;

__global__ void kernel(int* global_data) {
    cg::thread_block tb = cg::this_thread_block();
    const size_t elementsPerThreadBlock = 16 * 1024 + 64;
    const size_t elementsInShared = 128;
    __align__(16) __shared__ int local_smem[2][elementsInShared];
    int stage = 0;
    // First kick off an extra request
    size_t copy_count = elementsInShared;
    size_t index = copy_count;
    cg::memcpy_async(tb, local_smem[stage], elementsInShared, global_data, elementsPerThreadBlock - index);
    while (index < elementsPerThreadBlock) {
        // Now we kick off the next request...
        cg::memcpy_async(tb, local_smem[stage ^ 1], elementsInShared, global_data + index, elementsPerThreadBlock - index);
        // ... but we wait on the one before it
        cg::wait_prior<1>(tb);

        // Its now available and we can work with local_smem[stage] here
        // (...)
        //

        // Calculate the amount fo data that was actually copied, for the next iteration.
        copy_count = min(elementsInShared, elementsPerThreadBlock - index);
        index += copy_count;

        // A cg::sync(tb) might be needed here depending on whether
        // the work done with local_smem[stage] can release threads to race ahead or not
        // Wrap to the next stage
        stage ^= 1;
    }
    cg::wait(tb);
    // The last local_smem[stage] can be handled here
}






8.6.3. Data Manipulationï



8.6.3.1. reduceï


template <typename TyGroup, typename TyArg, typename TyOp>
auto reduce(const TyGroup& group, TyArg&& val, TyOp&& op) -> decltype(op(val, val));


reduce performs a reduction operation on the data provided by each thread named in the group passed in. This takes advantage of hardware acceleration (on compute 80 and higher devices) for the arithmetic add, min, or max operations and the logical AND, OR, or XOR, as well as providing a software fallback on older generation hardware. Only 4B types are accelerated by hardware.
group: Valid group types are coalesced_group and thread_block_tile.
val: Any type that satisfies the below requirements:

Qualifies as trivially copyable i.e. is_trivially_copyable<TyArg>::value == true
sizeof(T) <= 32 for coalesced_group and tiles of size lower or equal 32, sizeof(T) <= 8 for larger tiles
Has suitable arithmetic or comparative operators for the given function object.

Note: Different threads in the group can pass different values for this argument.
op: Valid function objects that will provide hardware acceleration with integral types are plus(), less(), greater(), bit_and(), bit_xor(), bit_or(). These must be constructed, hence the TyVal template argument is required, i.e. plus<int>(). Reduce also supports lambdas and other function objects that can be invoked using operator()
Asynchronous reduce

template <typename TyGroup, typename TyArg, typename TyAtomic, typename TyOp>
void reduce_update_async(const TyGroup& group, TyAtomic& atomic, TyArg&& val, TyOp&& op);

template <typename TyGroup, typename TyArg, typename TyAtomic, typename TyOp>
void reduce_store_async(const TyGroup& group, TyAtomic& atomic, TyArg&& val, TyOp&& op);

template <typename TyGroup, typename TyArg, typename TyOp>
void reduce_store_async(const TyGroup& group, TyArg* ptr, TyArg&& val, TyOp&& op);


*_async variants of the API are asynchronously calculating the result to either store to or update a specified destination by one of the participating threads, instead of returning it by each thread. To observe the effect of these asynchronous calls, calling group of threads or a larger group containing them need to be synchronized.

In case of the atomic store or update variant, atomic argument can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library. This variant of the API is available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library. Result of the reduction is used to atomically update the atomic according to the specified op, eg. the result is atomically added to the atomic in case of cg::plus(). Type held by the atomic must match the type of TyArg. Scope of the atomic must include all the threads in the group and if multiple groups are using the same atomic concurrently, scope must include all threads in all groups using it. Atomic update is performed with relaxed memory ordering.
In case of the pointer store variant, result of the reduction will be weakly stored into the dst pointer.

Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for HW acceleration, C++11.
cooperative_groups/reduce.h header needs to be included.
Example of approximate standard deviation for integer vector:

#include <cooperative_groups.h>
#include <cooperative_groups/reduce.h>
namespace cg = cooperative_groups;

/// Calculate approximate standard deviation of integers in vec
__device__ int std_dev(const cg::thread_block_tile<32>& tile, int *vec, int length) {
    int thread_sum = 0;

    // calculate average first
    for (int i = tile.thread_rank(); i < length; i += tile.num_threads()) {
        thread_sum += vec[i];
    }
    // cg::plus<int> allows cg::reduce() to know it can use hardware acceleration for addition
    int avg = cg::reduce(tile, thread_sum, cg::plus<int>()) / length;

    int thread_diffs_sum = 0;
    for (int i = tile.thread_rank(); i < length; i += tile.num_threads()) {
        int diff = vec[i] - avg;
        thread_diffs_sum += diff * diff;
    }

    // temporarily use floats to calculate the square root
    float diff_sum = static_cast<float>(cg::reduce(tile, thread_diffs_sum, cg::plus<int>())) / length;

    return static_cast<int>(sqrtf(diff_sum));
}


Example of block wide reduction:

#include <cooperative_groups.h>
#include <cooperative_groups/reduce.h>
namespace cg=cooperative_groups;

/// The following example accepts input in *A and outputs a result into *sum
/// It spreads the data equally within the block
__device__ void block_reduce(const int* A, int count, cuda::atomic<int, cuda::thread_scope_block>& total_sum) {
    auto block = cg::this_thread_block();
    auto tile = cg::tiled_partition<32>(block);
    int thread_sum = 0;

    // Stride loop over all values, each thread accumulates its part of the array.
    for (int i = block.thread_rank(); i < count; i += block.size()) {
        thread_sum += A[i];
    }

    // reduce thread sums across the tile, add the result to the atomic
    // cg::plus<int> allows cg::reduce() to know it can use hardware acceleration for addition
 cg::reduce_update_async(tile, total_sum, thread_sum, cg::plus<int>());

 // synchronize the block, to ensure all async reductions are ready
    block.sync();
}





8.6.3.2. Reduce Operatorsï

Below are the prototypes of function objects for some of the basic operations that can be done with reduce

namespace cooperative_groups {
  template <typename Ty>
  struct cg::plus;

  template <typename Ty>
  struct cg::less;

  template <typename Ty>
  struct cg::greater;

  template <typename Ty>
  struct cg::bit_and;

  template <typename Ty>
  struct cg::bit_xor;

  template <typename Ty>
  struct cg::bit_or;
}


Reduce is limited to the information available to the implementation at compile time. Thus in order to make use of intrinsics introduced in CC 8.0, the cg:: namespace exposes several functional objects that mirror the hardware. These objects appear similar to those presented in the C++ STL, with the exception of less/greater. The reason for any difference from the STL is that these function objects are designed to actually mirror the operation of the hardware intrinsics.
Functional description:

cg::plus: Accepts two values and returns the sum of both using operator+.
cg::less: Accepts two values and returns the lesser using operator<. This differs in that the lower value is returned rather than a Boolean.
cg::greater: Accepts two values and returns the greater using operator<. This differs in that the greater value is returned rather than a Boolean.
cg::bit_and: Accepts two values and returns the result of operator&.
cg::bit_xor: Accepts two values and returns the result of operator^.
cg::bit_or: Accepts two values and returns the result of operator|.

Example:

{
    // cg::plus<int> is specialized within cg::reduce and calls __reduce_add_sync(...) on CC 8.0+
    cg::reduce(tile, (int)val, cg::plus<int>());

    // cg::plus<float> fails to match with an accelerator and instead performs a standard shuffle based reduction
    cg::reduce(tile, (float)val, cg::plus<float>());

    // While individual components of a vector are supported, reduce will not use hardware intrinsics for the following
    // It will also be necessary to define a corresponding operator for vector and any custom types that may be used
    int4 vec = {...};
    cg::reduce(tile, vec, cg::plus<int4>())

    // Finally lambdas and other function objects cannot be inspected for dispatch
    // and will instead perform shuffle based reductions using the provided function object.
    cg::reduce(tile, (int)val, [](int l, int r) -> int {return l + r;});
}





8.6.3.3. inclusive_scan and exclusive_scanï


template <typename TyGroup, typename TyVal, typename TyFn>
auto inclusive_scan(const TyGroup& group, TyVal&& val, TyFn&& op) -> decltype(op(val, val));

template <typename TyGroup, typename TyVal>
TyVal inclusive_scan(const TyGroup& group, TyVal&& val);

template <typename TyGroup, typename TyVal, typename TyFn>
auto exclusive_scan(const TyGroup& group, TyVal&& val, TyFn&& op) -> decltype(op(val, val));

template <typename TyGroup, typename TyVal>
TyVal exclusive_scan(const TyGroup& group, TyVal&& val);


inclusive_scan and exclusive_scan performs a scan operation on the data provided by each thread named in the group passed in. Result for each thread is a reduction of data from threads with lower thread_rank than that thread in case of exclusive_scan. inclusive_scan result also includes the calling thread data in the reduction.
group: Valid group types are coalesced_group and thread_block_tile.
val: Any type that satisfies the below requirements:

Qualifies as trivially copyable i.e. is_trivially_copyable<TyArg>::value == true
sizeof(T) <= 32 for coalesced_group and tiles of size lower or equal 32, sizeof(T) <= 8 for larger tiles
Has suitable arithmetic or comparative operators for the given function object.

Note: Different threads in the group can pass different values for this argument.
op: Function objects defined for convenience are plus(), less(), greater(), bit_and(), bit_xor(), bit_or() described in Reduce Operators. These must be constructed, hence the TyVal template argument is required, i.e. plus<int>(). inclusive_scan and exclusive_scan also supports lambdas and other function objects that can be invoked using operator(). Overloads without this argument use cg::plus<TyVal>().
Scan update

template <typename TyGroup, typename TyAtomic, typename TyVal, typename TyFn>
auto inclusive_scan_update(const TyGroup& group, TyAtomic& atomic, TyVal&& val, TyFn&& op) -> decltype(op(val, val));

template <typename TyGroup, typename TyAtomic, typename TyVal>
TyVal inclusive_scan_update(const TyGroup& group, TyAtomic& atomic, TyVal&& val);

template <typename TyGroup, typename TyAtomic, typename TyVal, typename TyFn>
auto exclusive_scan_update(const TyGroup& group, TyAtomic& atomic, TyVal&& val, TyFn&& op) -> decltype(op(val, val));

template <typename TyGroup, typename TyAtomic, typename TyVal>
TyVal exclusive_scan_update(const TyGroup& group, TyAtomic& atomic, TyVal&& val);


*_scan_update collectives take an additional argument atomic that can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library. These variants of the API are available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library. These variants will perform an update to the atomic according to op with value of the sum of input values of all threads in the group. Previous value of the atomic will be combined with the result of scan by each thread and returned. Type held by the atomic must match the type of TyVal. Scope of the atomic must include all the threads in the group and if multiple groups are using the same atomic concurrently, scope must include all threads in all groups using it. Atomic update is performed with relaxed memory ordering.
Following pseudocode illustrates how the update variant of scan works:

/*
 inclusive_scan_update behaves as the following block,
 except both reduce and inclusive_scan is calculated simultaneously.
auto total = reduce(group, val, op);
TyVal old;
if (group.thread_rank() == selected_thread) {
    atomicaly {
        old = atomic.load();
        atomic.store(op(old, total));
    }
}
old = group.shfl(old, selected_thread);
return op(inclusive_scan(group, val, op), old);
*/


Codegen Requirements: Compute Capability 5.0 minimum, C++11.
cooperative_groups/scan.h header needs to be included.
Example:

#include <stdio.h>
#include <cooperative_groups.h>
#include <cooperative_groups/scan.h>
namespace cg = cooperative_groups;

__global__ void kernel() {
    auto thread_block = cg::this_thread_block();
    auto tile = cg::tiled_partition<8>(thread_block);
    unsigned int val = cg::inclusive_scan(tile, tile.thread_rank());
    printf("%u: %u\n", tile.thread_rank(), val);
}

/*  prints for each group:
    0: 0
    1: 1
    2: 3
    3: 6
    4: 10
    5: 15
    6: 21
    7: 28
*/


Example of stream compaction using exclusive_scan:

#include <cooperative_groups.h>
#include <cooperative_groups/scan.h>
namespace cg = cooperative_groups;

// put data from input into output only if it passes test_fn predicate
template<typename Group, typename Data, typename TyFn>
__device__ int stream_compaction(Group &g, Data *input, int count, TyFn&& test_fn, Data *output) {
    int per_thread = count / g.num_threads();
    int thread_start = min(g.thread_rank() * per_thread, count);
    int my_count = min(per_thread, count - thread_start);

    // get all passing items from my part of the input
    //  into a contagious part of the array and count them.
    int i = thread_start;
    while (i < my_count + thread_start) {
        if (test_fn(input[i])) {
            i++;
        }
        else {
            my_count--;
            input[i] = input[my_count + thread_start];
        }
    }

    // scan over counts from each thread to calculate my starting
    //  index in the output
    int my_idx = cg::exclusive_scan(g, my_count);

    for (i = 0; i < my_count; ++i) {
        output[my_idx + i] = input[thread_start + i];
    }
    // return the total number of items in the output
    return g.shfl(my_idx + my_count, g.num_threads() - 1);
}


Example of dynamic buffer space allocation using exclusive_scan_update:

#include <cooperative_groups.h>
#include <cooperative_groups/scan.h>
namespace cg = cooperative_groups;

// Buffer partitioning is static to make the example easier to follow,
// but any arbitrary dynamic allocation scheme can be implemented by replacing this function.
__device__ int calculate_buffer_space_needed(cg::thread_block_tile<32>& tile) {
    return tile.thread_rank() % 2 + 1;
}

__device__ int my_thread_data(int i) {
    return i;
}

__global__ void kernel() {
    __shared__ extern int buffer[];
    __shared__ cuda::atomic<int, cuda::thread_scope_block> buffer_used;

    auto block = cg::this_thread_block();
    auto tile = cg::tiled_partition<32>(block);
    buffer_used = 0;
    block.sync();

    // each thread calculates buffer size it needs
    int buf_needed = calculate_buffer_space_needed(tile);

    // scan over the needs of each thread, result for each thread is an offset
    // of that threadâs part of the buffer. buffer_used is atomically updated with
    // the sum of all thread's inputs, to correctly offset other tileâs allocations
    int buf_offset =
        cg::exclusive_scan_update(tile, buffer_used, buf_needed);

    // each thread fills its own part of the buffer with thread specific data
    for (int i = 0 ; i < buf_needed ; ++i) {
        buffer[buf_offset + i] = my_thread_data(i);
    }

    block.sync();
    // buffer_used now holds total amount of memory allocated
    // buffer is {0, 0, 1, 0, 0, 1 ...};
}






8.6.4. Execution controlï



8.6.4.1. invoke_one and invoke_one_broadcastï


template<typename Group, typename Fn, typename... Args>
void invoke_one(const Group& group, Fn&& fn, Args&&... args);

template<typename Group, typename Fn, typename... Args>
auto invoke_one_broadcast(const Group& group, Fn&& fn, Args&&... args) -> decltype(fn(args...));


invoke_one selects a single arbitrary thread from the calling group and uses that thread to call the supplied invocable fn with the supplied arguments args.
In case of invoke_one_broadcast the result of the call is also distributed to all threads in the group and returned from this collective.
Calling group can be synchronized with the selected thread before and/or after it calls the supplied invocable. It means that communication within the calling group
is not allowed inside the supplied invocable body, otherwise forward progress is not guaranteed. Communication with threads outside of the calling group is allowed in the
body of the supplied invocable. Thread selection mechanism is not guranteed to be deterministic.
On devices with Compute Capability 9.0 or higher hardware acceleration might be used to select the thread when called with explicit group types.
group: All group types are valid for invoke_one, coalesced_group and thread_block_tile are valid for invoke_one_broadcast.
fn: Function or object that can be invoked using operator().
args: Parameter pack of types matching types of parameters of the supplied invocable fn.
In case of invoke_one_broadcast the return type of the supplied invocable fn must satisfy the below requirements:

Qualifies as trivially copyable i.e. is_trivially_copyable<T>::value == true
sizeof(T) <= 32 for coalesced_group and tiles of size lower or equal 32, sizeof(T) <= 8 for larger tiles

Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 9.0 for hardware acceleration, C++11.
Aggregated atomic example from Discovery pattern section re-written to use invoke_one_broadcast:

#include <cooperative_groups.h>
#include <cuda/atomic>
namespace cg = cooperative_groups;

template<cuda::thread_scope Scope>
__device__ unsigned int atomicAddOneRelaxed(cuda::atomic<unsigned int, Scope>& atomic) {
    auto g = cg::coalesced_threads();
    auto prev = cg::invoke_one_broadcast(g, [&] () {
        return atomic.fetch_add(g.num_threads(), cuda::memory_order_relaxed);
    });
    return prev + g.thread_rank();
}







8.7. Grid Synchronizationï

Prior to the introduction of Cooperative Groups, the CUDA programming model only allowed synchronization between thread blocks at a kernel completion boundary. The kernel boundary carries with it an implicit invalidation of state, and with it, potential performance implications.
For example, in certain use cases, applications have a large number of small kernels, with each kernel representing a stage in a processing pipeline. The presence of these kernels is required by the current CUDA programming model to ensure that the thread blocks operating on one pipeline stage have produced data before the thread block operating on the next pipeline stage is ready to consume it. In such cases, the ability to provide global inter thread block synchronization would allow the application to be restructured to have persistent thread blocks, which are able to synchronize on the device when a given stage is complete.
To synchronize across the grid, from within a kernel, you would simply use the grid.sync() function:

grid_group grid = this_grid();
grid.sync();


And when launching the kernel it is necessary to use, instead of the <<<...>>> execution configuration syntax, the cudaLaunchCooperativeKernel CUDA runtime launch API or the CUDA driver equivalent.
Example:
To guarantee co-residency of the thread blocks on the GPU, the number of blocks launched needs to be carefully considered. For example, as many blocks as there are SMs can be launched as follows:

int dev = 0;
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&deviceProp, dev);
// initialize, then launch
cudaLaunchCooperativeKernel((void*)my_kernel, deviceProp.multiProcessorCount, numThreads, args);


Alternatively, you can maximize the exposed parallelism by calculating how many blocks can fit simultaneously per-SM using the occupancy calculator as follows:

/// This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments
int numBlocksPerSm = 0;
 // Number of threads my_kernel will be launched with
int numThreads = 128;
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&deviceProp, dev);
cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocksPerSm, my_kernel, numThreads, 0);
// launch
void *kernelArgs[] = { /* add kernel args */ };
dim3 dimBlock(numThreads, 1, 1);
dim3 dimGrid(deviceProp.multiProcessorCount*numBlocksPerSm, 1, 1);
cudaLaunchCooperativeKernel((void*)my_kernel, dimGrid, dimBlock, kernelArgs);


It is good practice to first ensure the device supports cooperative launches by querying the device attribute cudaDevAttrCooperativeLaunch:

int dev = 0;
int supportsCoopLaunch = 0;
cudaDeviceGetAttribute(&supportsCoopLaunch, cudaDevAttrCooperativeLaunch, dev);


which will set supportsCoopLaunch to 1 if the property is supported on device 0. Only devices with compute capability of 6.0 and higher are supported. In addition, you need to be running on either of these:

The Linux platform without MPS
The Linux platform with MPS and on a device with compute capability 7.0 or higher
The latest Windows platform




8.8. Multi-Device Synchronizationï

In order to enable synchronization across multiple devices with Cooperative Groups, use of the cudaLaunchCooperativeKernelMultiDevice CUDA API is required. This, a significant departure from existing CUDA APIs, will allow a single host thread to launch a kernel across multiple devices. In addition to the constraints and guarantees made by cudaLaunchCooperativeKernel, this API has additional semantics:

This API will ensure that a launch is atomic, i.e. if the API call succeeds, then the provided number of thread blocks will launch on all specified devices.
The functions launched via this API must be identical. No explicit checks are done by the driver in this regard because it is largely not feasible. It is up to the application to ensure this.
No two entries in the provided cudaLaunchParams may map to the same device.
All devices being targeted by this launch must be of the same compute capability - major and minor versions.
The block size, grid size and amount of shared memory per grid must be the same across all devices. Note that this means the maximum number of blocks that can be launched per device will be limited by the device with the least number of SMs.
Any user defined __device__, __constant__ or __managed__ device global variables present in the module that owns the CUfunction being launched are independently instantiated on every device. The user is responsible for initializing such device global variables appropriately.

Deprecation Notice: cudaLaunchCooperativeKernelMultiDevice has been deprecated in CUDA 11.3 for all devices. Example of an alternative approach can be found in the multi device conjugate gradient sample.
Optimal performance in multi-device synchronization is achieved by enabling peer access via cuCtxEnablePeerAccess or cudaDeviceEnablePeerAccess for all participating devices.
The launch parameters should be defined using an array of structs (one per device), and launched with cudaLaunchCooperativeKernelMultiDevice
Example:

cudaDeviceProp deviceProp;
cudaGetDeviceCount(&numGpus);

// Per device launch parameters
cudaLaunchParams *launchParams = (cudaLaunchParams*)malloc(sizeof(cudaLaunchParams) * numGpus);
cudaStream_t *streams = (cudaStream_t*)malloc(sizeof(cudaStream_t) * numGpus);

// The kernel arguments are copied over during launch
// Its also possible to have individual copies of kernel arguments per device, but
// the signature and name of the function/kernel must be the same.
void *kernelArgs[] = { /* Add kernel arguments */ };

for (int i = 0; i < numGpus; i++) {
    cudaSetDevice(i);
    // Per device stream, but its also possible to use the default NULL stream of each device
    cudaStreamCreate(&streams[i]);
    // Loop over other devices and cudaDeviceEnablePeerAccess to get a faster barrier implementation
}
// Since all devices must be of the same compute capability and have the same launch configuration
// it is sufficient to query device 0 here
cudaGetDeviceProperties(&deviceProp[i], 0);
dim3 dimBlock(numThreads, 1, 1);
dim3 dimGrid(deviceProp.multiProcessorCount, 1, 1);
for (int i = 0; i < numGpus; i++) {
    launchParamsList[i].func = (void*)my_kernel;
    launchParamsList[i].gridDim = dimGrid;
    launchParamsList[i].blockDim = dimBlock;
    launchParamsList[i].sharedMem = 0;
    launchParamsList[i].stream = streams[i];
    launchParamsList[i].args = kernelArgs;
}
cudaLaunchCooperativeKernelMultiDevice(launchParams, numGpus);


Also, as with grid-wide synchronization, the resulting device code looks very similar:

multi_grid_group multi_grid = this_multi_grid();
multi_grid.sync();


However, the code needs to be compiled in separate compilation by passing -rdc=true to nvcc.
It is good practice to first ensure the device supports multi-device cooperative launches by querying the device attribute cudaDevAttrCooperativeMultiDeviceLaunch:

int dev = 0;
int supportsMdCoopLaunch = 0;
cudaDeviceGetAttribute(&supportsMdCoopLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, dev);


which will set supportsMdCoopLaunch to 1 if the property is supported on device 0. Only devices with compute capability of 6.0 and higher are supported. In addition, you need to be running on the Linux platform (without MPS) or on current versions of Windows with the device in TCC mode.
See the cudaLaunchCooperativeKernelMultiDevice API documentation for more information.




9. CUDA Dynamic Parallelismï



9.1. Introductionï



9.1.1. Overviewï

Dynamic Parallelism is an extension to the CUDA programming model enabling a CUDA kernel to create and synchronize with new work directly on the GPU. The creation of parallelism dynamically at whichever point in a program that it is needed offers exciting capabilities.
The ability to create work directly from the GPU can reduce the need to transfer execution control and data between host and device, as launch configuration decisions can now be made at runtime by threads executing on the device. Additionally, data-dependent parallel work can be generated inline within a kernel at run-time, taking advantage of the GPUâs hardware schedulers and load balancers dynamically and adapting in response to data-driven decisions or workloads. Algorithms and programming patterns that had previously required modifications to eliminate recursion, irregular loop structure, or other constructs that do not fit a flat, single-level of parallelism may more transparently be expressed.
This document describes the extended capabilities of CUDA which enable Dynamic Parallelism, including the modifications and additions to the CUDA programming model necessary to take advantage of these, as well as guidelines and best practices for exploiting this added capacity.
Dynamic Parallelism is only supported by devices of compute capability 3.5 and higher.



9.1.2. Glossaryï

Definitions for terms used in this guide.

Grid

A Grid is a collection of Threads. Threads in a Grid execute a Kernel Function and are divided into Thread Blocks.

Thread Block

A Thread Block is a group of threads which execute on the same multiprocessor (SM). Threads within a Thread Block have access to shared memory and can be explicitly synchronized.

Kernel Function

A Kernel Function is an implicitly parallel subroutine that executes under the CUDA execution and memory model for every Thread in a Grid.

Host

The Host refers to the execution environment that initially invoked CUDA. Typically the thread running on a systemâs CPU processor.

Parent

A Parent Thread, Thread Block, or Grid is one that has launched new grid(s), the Child Grid(s). The Parent is not considered completed until all of its launched Child Grids have also completed.

Child

A Child thread, block, or grid is one that has been launched by a Parent grid. A Child grid must complete before the Parent Thread, Thread Block, or Grid are considered complete.

Thread Block Scope

Objects with Thread Block Scope have the lifetime of a single Thread Block. They only have defined behavior when operated on by Threads in the Thread Block that created the object and are destroyed when the Thread Block that created them is complete.

Device Runtime

The Device Runtime refers to the runtime system and APIs available to enable Kernel Functions to use Dynamic Parallelism.






9.2. Execution Environment and Memory Modelï



9.2.1. Execution Environmentï

The CUDA execution model is based on primitives of threads, thread blocks, and grids, with kernel functions defining the program executed by individual threads within a thread block and grid. When a kernel function is invoked the gridâs properties are described by an execution configuration, which has a special syntax in CUDA. Support for dynamic parallelism in CUDA extends the ability to configure, launch, and implicitly synchronize upon new grids to threads that are running on the device.


9.2.1.1. Parent and Child Gridsï

A device thread that configures and launches a new grid belongs to the parent grid, and the grid created by the invocation is a child grid.
The invocation and completion of child grids is properly nested, meaning that the parent grid is not considered complete until all child grids created by its threads have completed, and the runtime guarantees an implicit synchronization between the parent and child.



Figure 26 Parent-Child Launch Nestingï





9.2.1.2. Scope of CUDA Primitivesï

On both host and device, the CUDA runtime offers an API for launching kernels and for tracking dependencies between launches via streams and events. On the host system, the state of launches and the CUDA primitives referencing streams and events are shared by all threads within a process; however processes execute independently and may not share CUDA objects.
On the device, launched kernels and CUDA objects are visible to all threads in a grid. This means, for example, that a stream may be created by one thread and used by any other thread in the grid.



9.2.1.3. Synchronizationï


Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6 and removed for compute_90+ compilation. For compute capability < 9.0, compile-time opt-in by specifying -DCUDA_FORCE_CDP1_IF_SUPPORTED is required to continue using cudaDeviceSynchronize() in device code. Note that this is slated for full removal in a future CUDA release.

CUDA runtime operations from any thread, including kernel launches, are visible across all the threads in a grid. This means that an invoking thread in the parent grid may perform synchronization to control the launch order of grids launched by any thread in the grid on streams created by any thread in the grid. Execution of a grid is not considered complete until all launches by all threads in the grid have completed. If all threads in a grid exit before all child launches have completed, an implicit synchronization operation will automatically be triggered.



9.2.1.4. Streams and Eventsï

CUDA Streams and Events allow control over dependencies between grid launches: grids launched into the same stream execute in-order, and events may be used to create dependencies between streams. Streams and events created on the device serve this exact same purpose.
Streams and events created within a grid exist within grid scope, but have undefined behavior when used outside of the grid where they were created. As described above, all work launched by a grid is implicitly synchronized when the grid exits; work launched into streams is included in this, with all dependencies resolved appropriately. The behavior of operations on a stream that has been modified outside of grid scope is undefined.
Streams and events created on the host have undefined behavior when used within any kernel, just as streams and events created by a parent grid have undefined behavior if used within a child grid.



9.2.1.5. Ordering and Concurrencyï

The ordering of kernel launches from the device runtime follows CUDA Stream ordering semantics. Within a grid, all kernel launches into the same stream (with the exception of the fire-and-forget stream discussed later) are executed in-order. With multiple threads in the same grid launching into the same stream, the ordering within the stream is dependent on the thread scheduling within the grid, which may be controlled with synchronization primitives such as __syncthreads().
Note that while named streams are shared by all threads within a grid, the implicit NULL stream is only shared by all threads within a thread block. If multiple threads in a thread block launch into the implicit stream, then these launches will be executed in-order. If multiple threads in different thread blocks launch into the implicit stream, then these launches may be executed concurrently. If concurrency is desired for launches by multiple threads within a thread block, explicit named streams should be used.
Dynamic Parallelism enables concurrency to be expressed more easily within a program; however, the device runtime introduces no new concurrency guarantees within the CUDA execution model. There is no guarantee of concurrent execution between any number of different thread blocks on a device.
The lack of concurrency guarantee extends to a parent grid and their child grids. When a parent grid launches a child grid, the child may start to execute once stream dependencies are satisfied and hardware resources are available to host the child, but is not guaranteed to begin execution until the parent grid reaches an implicit synchronization point.
While concurrency will often easily be achieved, it may vary as a function of device configuration, application workload, and runtime scheduling. It is therefore unsafe to depend upon any concurrency between different thread blocks.



9.2.1.6. Device Managementï

There is no multi-GPU support from the device runtime; the device runtime is only capable of operating on the device upon which it is currently executing. It is permitted, however, to query properties for any CUDA capable device in the system.




9.2.2. Memory Modelï

Parent and child grids share the same global and constant memory storage, but have distinct local and shared memory.


9.2.2.1. Coherence and Consistencyï



9.2.2.1.1. Global Memoryï

Parent and child grids have coherent access to global memory, with weak consistency guarantees between child and parent. There is only one point of time in the execution of a child grid when its view of memory is fully consistent with the parent thread: at the point when the child grid is invoked by the parent.
All global memory operations in the parent thread prior to the child gridâs invocation are visible to the child grid. With the removal of cudaDeviceSynchronize(), it is no longer possible to access the modifications made by the threads in the child grid from the parent grid. The only way to access the modifications made by the threads in the child grid before the parent grid exits is via a kernel launched into the cudaStreamTailLaunch stream.
In the following example, the child grid executing child_launch is only guaranteed to see the modifications to data made before the child grid was launched. Since thread 0 of the parent is performing the launch, the child will be consistent with the memory seen by thread 0 of the parent. Due to the first __syncthreads() call, the child will see data[0]=0, data[1]=1, â¦, data[255]=255 (without the __syncthreads() call, only data[0]=0 would be guaranteed to be seen by the child). The child grid is only guaranteed to return at an implicit synchronization. This means that the modifications made by the threads in the child grid are never guaranteed to become available to the parent grid. To access modifications made by child_launch, a tail_launch kernel is launched into the cudaStreamTailLaunch stream.

__global__ void tail_launch(int *data) {
   data[threadIdx.x] = data[threadIdx.x]+1;
}

__global__ void child_launch(int *data) {
   data[threadIdx.x] = data[threadIdx.x]+1;
}

__global__ void parent_launch(int *data) {
   data[threadIdx.x] = threadIdx.x;

   __syncthreads();

   if (threadIdx.x == 0) {
       child_launch<<< 1, 256 >>>(data);
       tail_launch<<< 1, 256, 0, cudaStreamTailLaunch >>>(data);
   }
}

void host_launch(int *data) {
    parent_launch<<< 1, 256 >>>(data);
}





9.2.2.1.2. Zero Copy Memoryï

Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above. A kernel may not allocate or free zero-copy memory, but may use pointers to zero-copy passed in from the host program.



9.2.2.1.3. Constant Memoryï

Constants may not be modified from the device. They may only be modified from the host, but the behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point during its lifetime is undefined.



9.2.2.1.4. Shared and Local Memoryï

Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child. Behavior is undefined when an object in one of these locations is referenced outside of the scope within which it belongs, and may cause an error.
The NVIDIA compiler will attempt to warn if it can detect that a pointer to local or shared memory is being passed as an argument to a kernel launch. At runtime, the programmer may use the __isGlobal() intrinsic to determine whether a pointer references global memory and so may safely be passed to a child launch.
Note that calls to cudaMemcpy*Async() or cudaMemset*Async() may invoke new child kernels on the device in order to preserve stream semantics. As such, passing shared or local memory pointers to these APIs is illegal and will return an error.



9.2.2.1.5. Local Memoryï

Local memory is private storage for an executing thread, and is not visible outside of that thread. It is illegal to pass a pointer to local memory as a launch argument when launching a child kernel. The result of dereferencing such a local memory pointer from a child will be undefined.
For example the following is illegal, with undefined behavior if x_array is accessed by child_launch:

int x_array[10];       // Creates x_array in parent's local memory
child_launch<<< 1, 1 >>>(x_array);


It is sometimes difficult for a programmer to be aware of when a variable is placed into local memory by the compiler. As a general rule, all storage passed to a child kernel should be allocated explicitly from the global-memory heap, either with cudaMalloc(), new() or by declaring __device__ storage at global scope. For example:

// Correct - "value" is global storage
__device__ int value;
__device__ void x() {
    value = 5;
    child<<< 1, 1 >>>(&value);
}



// Invalid - "value" is local storage
__device__ void y() {
    int value = 5;
    child<<< 1, 1 >>>(&value);
}





9.2.2.1.6. Texture Memoryï

Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses. Coherence for texture memory is enforced at the invocation of a child grid and when a child grid completes. This means that writes to memory prior to a child kernel launch are reflected in texture memory accesses of the child. Similarly to Global Memory above, writes to memory by a child are never guaranteed to be reflected in the texture memory accesses by a parent. The only way to access the modifications made by the threads in the child grid before the parent grid exits is via a kernel launched into the cudaStreamTailLaunch stream. Concurrent accesses by parent and child may result in inconsistent data.






9.3. Programming Interfaceï



9.3.1. CUDA C++ Referenceï

This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism.
The language interface and API available to CUDA kernels using CUDA C++ for Dynamic Parallelism, referred to as the Device Runtime, is substantially like that of the CUDA Runtime API available on the host. Where possible the syntax and semantics of the CUDA Runtime API have been retained in order to facilitate ease of code reuse for routines that may run in either the host or device environments.
As with all code in CUDA C++, the APIs and code outlined here is per-thread code. This enables each thread to make unique, dynamic decisions regarding what kernel or operation to execute next. There are no synchronization requirements between threads within a block to execute any of the provided device runtime APIs, which enables the device runtime API functions to be called in arbitrarily divergent kernel code without deadlock.


9.3.1.1. Device-Side Kernel Launchï

Kernels may be launched from the device using the standard CUDA <<< >>> syntax:

kernel_name<<< Dg, Db, Ns, S >>>([kernel arguments]);



Dg is of type dim3 and specifies the dimensions and size of the grid
Db is of type dim3 and specifies the dimensions and size of each thread block
Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call in addition to statically allocated memory. Ns is an optional argument that defaults to 0.
S is of type cudaStream_t and specifies the stream associated with this call. The stream must have been allocated in the same grid where the call is being made. S is an optional argument that defaults to the NULL stream.



9.3.1.1.1. Launches are Asynchronousï

Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread. That is to say, the <<<>>> launch command will return immediately and the launching thread will continue to execute until it hits an implicit launch-synchronization point (such as at a kernel launched into the cudaStreamTailLaunch stream).
The child grid launch is posted to the device and will execute independently of the parent thread. The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until the launching thread reaches an implicit launch-synchronization point.



9.3.1.1.2. Launch Environment Configurationï

All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig(), and device limits returned from cudaDeviceGetLimit()) will be inherited from the parent. Likewise, device limits such as stack size will remain as-configured.
For host-launched kernels, per-kernel configurations set from the host will take precedence over the global setting. These configurations will be used when the kernel is launched from the device as well. It is not possible to reconfigure a kernelâs environment from the device.




9.3.1.2. Streamsï

Both named and unnamed (NULL) streams are available from the device runtime. Named streams may be used by any thread within a grid, but stream handles may not be passed to other child/parent kernels. In other words, a stream should be treated as private to the grid in which it is created.
Similar to host-side launch, work launched into separate streams may run concurrently, but actual concurrency is not guaranteed. Programs that depend upon concurrency between child kernels are not supported by the CUDA programming model and will have undefined behavior.
The host-side NULL streamâs cross-stream barrier semantic is not supported on the device (see below for details). In order to retain semantic compatibility with the host runtime, all device streams must be created using the cudaStreamCreateWithFlags() API, passing the cudaStreamNonBlocking flag. The cudaStreamCreate() call is a host-runtime- only API and will fail to compile for the device.
As cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, a kernel launched into the cudaStreamTailLaunch stream should be used instead when the application needs to know that stream-launched child kernels have completed.


9.3.1.2.1. The Implicit (NULL) Streamï

Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details). The device runtime offers a single implicit, unnamed stream shared between all threads in a thread block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks).



9.3.1.2.2. The Fire-and-Forget Streamï

The fire-and-forget named stream (cudaStreamFireAndForget) allows the user to launch fire-and-forget work with less boilerplate and without stream tracking overhead. It is functionally identical to, but faster than, creating a new stream per launch, and launching into that stream.
Fire-and-forget launches are immediately scheduled for launch without any dependency on the completion of previously launched grids. No other grid launches can depend on the completion of a fire-and-forget launch, except through the implicit synchronization at the end of the parent grid. So a tail launch or the next grid in parent gridâs stream wonât launch before a parent gridâs fire-and-forget work has completed.

// In this example, C2's launch will not wait for C1's completion
__global__ void P( ... ) {
   C1<<< ... , cudaStreamFireAndForget >>>( ... );
   C2<<< ... , cudaStreamFireAndForget >>>( ... );
}


The fire-and-forget stream cannot be used to record or wait on events. Attempting to do so results in cudaErrorInvalidValue. The fire-and-forget stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined. Fire-and-forget stream usage requires compilation to be in 64-bit mode.



9.3.1.2.3. The Tail Launch Streamï

The tail launch named stream (cudaStreamTailLaunch) allows a grid to schedule a new grid for launch after its completion. It should be possible to to use a tail launch to achieve the same functionality as a cudaDeviceSynchronize() in most cases.
Each grid has its own tail launch stream. All non-tail launch work launched by a grid is implicitly synchronized before the tail stream is kicked off. I.e. A parent gridâs tail launch does not launch until the parent grid and all work launched by the parent grid to ordinary streams or per-thread or fire-and-forget streams have completed. If two grids are launched to the same gridâs tail launch stream, the later grid does not launch until the earlier grid and all its descendent work has completed.

// In this example, C2 will only launch after C1 completes.
__global__ void P( ... ) {
   C1<<< ... , cudaStreamTailLaunch >>>( ... );
   C2<<< ... , cudaStreamTailLaunch >>>( ... );
}


Grids launched into the tail launch stream will not launch until the completion of all work by the parent grid, including all other grids (and their descendants) launched by the parent in all non-tail launched streams, including work executed or launched after the tail launch.

// In this example, C will only launch after all X, F and P complete.
__global__ void P( ... ) {
   C<<< ... , cudaStreamTailLaunch >>>( ... );
   X<<< ... , cudaStreamPerThread >>>( ... );
   F<<< ... , cudaStreamFireAndForget >>>( ... )
}


The next grid in the parent gridâs stream will not be launched before a parent gridâs tail launch work has completed. In other words, the tail launch stream behaves as if it were inserted between its parent grid and the next grid in its parent gridâs stream.

// In this example, P2 will only launch after C completes.
__global__ void P1( ... ) {
   C<<< ... , cudaStreamTailLaunch >>>( ... );
}

__global__ void P2( ... ) {
}

int main ( ... ) {
   ...
   P1<<< ... >>>( ... );
   P2<<< ... >>>( ... );
   ...
}


Each grid only gets one tail launch stream. To tail launch concurrent grids, it can be done like the example below.

// In this example,  C1 and C2 will launch concurrently after P's completion
__global__ void T( ... ) {
   C1<<< ... , cudaStreamFireAndForget >>>( ... );
   C2<<< ... , cudaStreamFireAndForget >>>( ... );
}

__global__ void P( ... ) {
   ...
   T<<< ... , cudaStreamTailLaunch >>>( ... );
}


The tail launch stream cannot be used to record or wait on events. Attempting to do so results in cudaErrorInvalidValue. The tail launch stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined. Tail launch stream usage requires compilation to be in 64-bit mode.




9.3.1.3. Eventsï

Only the inter-stream synchronization capabilities of CUDA events are supported. This means that cudaStreamWaitEvent() is supported, but cudaEventSynchronize(), cudaEventElapsedTime(), and cudaEventQuery() are not. As cudaEventElapsedTime() is not supported, cudaEvents must be created via cudaEventCreateWithFlags(), passing the cudaEventDisableTiming flag.
As with named streams, event objects may be shared between all threads within the grid which created them but are local to that grid and may not be passed to other kernels. Event handles are not guaranteed to be unique between grids, so using an event handle within a grid that did not create it will result in undefined behavior.



9.3.1.4. Synchronizationï

It is up to the program to perform sufficient inter-thread synchronization, for example via a CUDA Event, if the calling thread is intended to synchronize with child grids invoked from other threads.
As it is not possible to explicitly synchronize child work from a parent thread, there is no way to guarantee that changes occuring in child grids are visible to threads within the parent grid.



9.3.1.5. Device Managementï

Only the device on which a kernel is running will be controllable from that kernel. This means that device APIs such as cudaSetDevice() are not supported by the device runtime. The active device as seen from the GPU (returned from cudaGetDevice()) will have the same device number as seen from the host system. The cudaDeviceGetAttribute() call may request information about another device as this API allows specification of a device ID as a parameter of the call. Note that the catch-all cudaGetDeviceProperties() API is not offered by the device runtime - properties must be queried individually.



9.3.1.6. Memory Declarationsï



9.3.1.6.1. Device and Constant Memoryï

Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime. All kernels may read or write device variables, whether the kernel was initially launched by the host or device runtime. Equivalently, all kernels will have the same view of __constant__s as declared at the module scope.



9.3.1.6.2. Textures and Surfacesï

CUDA supports dynamically created texture and surface objects14, where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host. The device runtime does not allow creation or destruction of texture or surface objects from within device code, but texture and surface objects created from the host may be used and passed around freely on the device. Regardless of where they are created, dynamically created texture objects are always valid and may be passed to child kernels from a parent.

Note
The device runtime does not support legacy module-scope (i.e., Fermi-style) textures and surfaces within a kernel launched from the device. Module-scope (legacy) textures may be created from the host and used in device code as for any kernel, but may only be used by a top-level kernel (i.e., the one which is launched from the host).




9.3.1.6.3. Shared Memory Variable Declarationsï

In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernelâs caller via a launch configuration argument. Both types of declarations are valid under the device runtime.

__global__ void permute(int n, int *data) {
   extern __shared__ int smem[];
   if (n <= 1)
       return;

   smem[threadIdx.x] = data[threadIdx.x];
   __syncthreads();

   permute_data(smem, n);
   __syncthreads();

   // Write back to GMEM since we can't pass SMEM to children.
   data[threadIdx.x] = smem[threadIdx.x];
   __syncthreads();

   if (threadIdx.x == 0) {
       permute<<< 1, 256, n/2*sizeof(int) >>>(n/2, data);
       permute<<< 1, 256, n/2*sizeof(int) >>>(n/2, data+n/2);
   }
}

void host_launch(int *data) {
    permute<<< 1, 256, 256*sizeof(int) >>>(256, data);
}





9.3.1.6.4. Symbol Addressesï

Device-side symbols (i.e., those marked __device__) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernelâs visible address space. This also applies to __constant__ symbols, although in this case the pointer will reference read-only data.
Given that device-side symbols can be referenced directly, those CUDA runtime APIs which reference symbols (e.g., cudaMemcpyToSymbol() or cudaGetSymbolAddress()) are redundant and hence not supported by the device runtime. Note this implies that constant data cannot be altered from within a running kernel, even ahead of a child kernel launch, as references to __constant__ space are read-only.




9.3.1.7. API Errors and Launch Failuresï

As usual for the CUDA runtime, any function may return an error code. The last error code returned is recorded and may be retrieved via the cudaGetLastError() call. Errors are recorded per-thread, so that each thread can identify the most recent error that it has generated. The error code is of type cudaError_t.
Similar to a host-side launch, device-side launches may fail for many reasons (invalid arguments, etc). The user must call cudaGetLastError() to determine if a launch generated an error, however lack of an error after launch does not imply the child kernel completed successfully.
For device-side exceptions, e.g., access to an invalid address, an error in a child grid will be returned to the host.


9.3.1.7.1. Launch Setup APIsï

Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs. It is permitted for a CUDA application to call these APIs itself, with the same requirements as for PTX. In both cases, the user is then responsible for correctly populating all necessary data structures in the correct format according to specification. Backwards compatibility is guaranteed in these data structures.
As with host-side launch, the device-side operator <<<>>> maps to underlying kernel launch APIs. This is so that users targeting PTX will be able to enact a launch, and so that the compiler front-end can translate <<<>>> into these calls.


Table 9 New Device-only Launch Implementation Functionsï







Runtime API Launch Functions
Description of Difference From Host Runtime Behaviour (behavior is identical if no description)




cudaGetParameterBuffer
Generated automatically from <<<>>>. Note different API to host equivalent.


cudaLaunchDevice
Generated automatically from <<<>>>. Note different API to host equivalent.



The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows:

extern   device   cudaError_t cudaGetParameterBuffer(void **params);
extern __device__ cudaError_t cudaLaunchDevice(void *kernel,
                                        void *params, dim3 gridDim,
                                        dim3 blockDim,
                                        unsigned int sharedMemSize = 0,
                                        cudaStream_t stream = 0);






9.3.1.8. API Referenceï

The portions of the CUDA Runtime API supported in the device runtime are detailed here. Host and device runtime APIs have identical syntax; semantics are the same except where indicated. The following table provides an overview of the API relative to the version available from the host.


Table 10 Supported API Functionsï







Runtime API Functions
Details




cudaDeviceGetCacheConfig



cudaDeviceGetLimit



cudaGetLastError
Last error is per-thread state, not per-block state


cudaPeekAtLastError



cudaGetErrorString



cudaGetDeviceCount



cudaDeviceGetAttribute
Will return attributes for any device


cudaGetDevice
Always returns current device ID as would be seen from host


cudaStreamCreateWithFlags
Must pass cudaStreamNonBlocking flag


cudaStreamDestroy



cudaStreamWaitEvent



cudaEventCreateWithFlags
Must pass cudaEventDisableTiming flag


cudaEventRecord



cudaEventDestroy



cudaFuncGetAttributes



cudaMemcpyAsync

Notes about all memcpy/memset functions:

Only async memcpy/set functions are supported
Only device-to-device memcpy is permitted
May not pass in local or shared memory pointers




cudaMemcpy2DAsync


cudaMemcpy3DAsync


cudaMemsetAsync


cudaMemset2DAsync



cudaMemset3DAsync



cudaRuntimeGetVersion



cudaMalloc
May not call cudaFree on the device on a pointer created on the host, and vice-versa


cudaFree


cudaOccupancyMaxActiveBlocksPerMultiprocessor



cudaOccupancyMaxPotentialBlockSize



cudaOccupancyMaxPotentialBlockSizeVariableSMem








9.3.2. Device-side Launch from PTXï

This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language. It provides the low-level details related to supporting kernel launches at the PTX level.


9.3.2.1. Kernel Launch APIsï

Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer(). cudaLaunchDevice() launches the specified kernel with the parameter buffer that is obtained by calling cudaGetParameterBuffer() and filled with the parameters to the launched kernel. The parameter buffer can be NULL, i.e., no need to invoke cudaGetParameterBuffer(), if the launched kernel does not take any parameters.


9.3.2.1.1. cudaLaunchDeviceï

At the PTX level, cudaLaunchDevice()needs to be declared in one of the two forms shown below before it is used.

// PTX-level Declaration of cudaLaunchDevice() when .address_size is 64
.extern .func(.param .b32 func_retval0) cudaLaunchDevice
(
  .param .b64 func,
  .param .b64 parameterBuffer,
  .param .align 4 .b8 gridDimension[12],
  .param .align 4 .b8 blockDimension[12],
  .param .b32 sharedMemSize,
  .param .b64 stream
)
;


The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api.h. The function is defined in the cudadevrt system library, which must be linked with a program in order to use device-side kernel launch functionality.

// CUDA-level declaration of cudaLaunchDevice()
extern "C" __device__
cudaError_t cudaLaunchDevice(void *func, void *parameterBuffer,
                             dim3 gridDimension, dim3 blockDimension,
                             unsigned int sharedMemSize,
                             cudaStream_t stream);


The first parameter is a pointer to the kernel to be is launched, and the second parameter is the parameter buffer that holds the actual parameters to the launched kernel. The layout of the parameter buffer is explained in Parameter Buffer Layout, below. Other parameters specify the launch configuration, i.e., as grid dimension, block dimension, shared memory size, and the stream associated with the launch (please refer to Execution Configuration for the detailed description of launch configuration.



9.3.2.1.2. cudaGetParameterBufferï

cudaGetParameterBuffer() needs to be declared at the PTX level before itâs used. The PTX-level declaration must be in one of the two forms given below, depending on address size:

// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64
.extern .func(.param .b64 func_retval0) cudaGetParameterBuffer
(
  .param .b64 alignment,
  .param .b64 size
)
;


The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration:

// CUDA-level Declaration of cudaGetParameterBuffer()
extern "C" __device__
void *cudaGetParameterBuffer(size_t alignment, size_t size);


The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement in bytes. In the current implementation, the parameter buffer returned by cudaGetParameterBuffer() is always guaranteed to be 64- byte aligned, and the alignment requirement parameter is ignored. However, it is recommended to pass the correct alignment requirement value - which is the largest alignment of any parameter to be placed in the parameter buffer - to cudaGetParameterBuffer() to ensure portability in the future.




9.3.2.2. Parameter Buffer Layoutï

Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned. That is, each parameter must be placed at the nth byte in the parameter buffer, where n is the smallest multiple of the parameter size that is greater than the offset of the last byte taken by the preceding parameter. The maximum size of the parameter buffer is 4KB.
For a more detailed description of PTX code generated by the CUDA compiler, please refer to the PTX-3.5 specification.




9.3.3. Toolkit Support for Dynamic Parallelismï



9.3.3.1. Including Device Runtime API in CUDA Codeï

Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation. There is no need to includecuda_device_runtime_api.h explicitly.



9.3.3.2. Compiling and Linkingï

When compiling and linking CUDA programs using dynamic parallelism with nvcc, the program will automatically link against the static device runtime library libcudadevrt.
The device runtime is offered as a static library (cudadevrt.lib on Windows, libcudadevrt.a under Linux), against which a GPU application that uses the device runtime must be linked. Linking of device libraries can be accomplished through nvcc and/or nvlink. Two simple examples are shown below.
A device runtime program may be compiled and linked in a single step, if all required source files can be specified from the command line:

$ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt


It is also possible to compile CUDA .cu source files first to object files, and then link these together in a two-stage process:

$ nvcc -arch=sm_75 -dc hello_world.cu -o hello_world.o
$ nvcc -arch=sm_75 -rdc=true hello_world.o -o hello -lcudadevrt


Please see the Using Separate Compilation section of The CUDA Driver Compiler NVCC guide for more details.





9.4. Programming Guidelinesï



9.4.1. Basicsï

The device runtime is a functional subset of the host runtime. API level device management, kernel launching, device memcpy, stream management, and event management are exposed from the device runtime.
Programming for the device runtime should be familiar to someone who already has experience with CUDA. Device runtime syntax and semantics are largely the same as that of the host API, with any exceptions detailed earlier in this document.
The following example shows a simple Hello World program incorporating dynamic parallelism:

#include <stdio.h>

__global__ void childKernel()
{
    printf("Hello ");
}

__global__ void tailKernel()
{
    printf("World!\n");
}

__global__ void parentKernel()
{
    // launch child
    childKernel<<<1,1>>>();
    if (cudaSuccess != cudaGetLastError()) {
        return;
    }

    // launch tail into cudaStreamTailLaunch stream
    // implicitly synchronizes: waits for child to complete
    tailKernel<<<1,1,0,cudaStreamTailLaunch>>>();

}

int main(int argc, char *argv[])
{
    // launch parent
    parentKernel<<<1,1>>>();
    if (cudaSuccess != cudaGetLastError()) {
        return 1;
    }

    // wait for parent to complete
    if (cudaSuccess != cudaDeviceSynchronize()) {
        return 2;
    }

    return 0;
}


This program may be built in a single step from the command line as follows:

$ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt





9.4.2. Performanceï



9.4.2.1. Dynamic-parallelism-enabled Kernel Overheadï

System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own. This overhead arises from the device runtimeâs execution tracking and management software and may result in decreased performance. This overhead is, in general, incurred for applications that link against the device runtime library.




9.4.3. Implementation Restrictions and Limitationsï

Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime.


9.4.3.1. Runtimeï



9.4.3.1.1. Memory Footprintï

The device runtime system software reserves memory for various management purposes, in particular a reservation for tracking pending grid launches. Configuration controls are available to reduce the size of this reservation in exchange for certain launch limitations. See Configuration Options, below, for details.



9.4.3.1.2. Pending Kernel Launchesï

When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes. This data is stored within a system-managed launch pool.
The size of the fixed-size launch pool is configurable by calling cudaDeviceSetLimit() from the host and specifying cudaLimitDevRuntimePendingLaunchCount.



9.4.3.1.3. Configuration Optionsï

Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program. Limits must be set before any kernel is launched, and may not be changed while the GPU is actively running programs.
The following named limits may be set:







Limit
Behavior




cudaLimitDevRuntimePendingLaunchCount
Controls the amount of memory set aside for buffering kernel launches and events which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources. When the buffer is full, an attempt to allocate a launch slot during a device side kernel launch will fail and return cudaErrorLaunchOutOfResources, while an attempt to allocate an event slot will fail and return cudaErrorMemoryAllocation. The default number of launch slots is 2048. Applications may increase the number of launch and/or event slots by setting cudaLimitDevRuntimePendingLaunchCount. The number of event slots allocated is twice the value of that limit.


cudaLimitStackSize
Controls the stack size in bytes of each GPU thread. The CUDA driver automatically increases the per-thread stack size for each kernel launch as needed. This size isnât reset back to the original value after each launch. To set the per-thread stack size to a different value, cudaDeviceSetLimit() can be called to set this limit. The stack will be immediately resized, and if necessary, the device will block until all preceding requested tasks are complete. cudaDeviceGetLimit() can be called to get the current per-thread stack size.






9.4.3.1.4. Memory Allocation and Lifetimeï

cudaMalloc() and cudaFree() have distinct semantics between the host and device environments. When invoked from the host, cudaMalloc() allocates a new region from unused device memory. When invoked from the device runtime these functions map to device-side malloc() and free(). This implies that within the device environment the total allocatable memory is limited to the device malloc() heap size, which may be smaller than the available unused device memory. Also, it is an error to invoke cudaFree() from the host program on a pointer which was allocated by cudaMalloc() on the device or vice-versa.









cudaMalloc() on Host
cudaMalloc() on Device




cudaFree() on Host
Supported
Not Supported


cudaFree() on Device
Not Supported
Supported


Allocation limit
Free device memory
cudaLimitMallocHeapSize






9.4.3.1.5. SM Id and Warp Idï

Note that in PTX %smid and %warpid are defined as volatile values. The device runtime may reschedule thread blocks onto different SMs in order to more efficiently manage resources. As such, it is unsafe to rely upon %smid or %warpid remaining unchanged across the lifetime of a thread or thread block.



9.4.3.1.6. ECC Errorsï

No notification of ECC errors is available to code within a CUDA kernel. ECC errors are reported at the host side once the entire launch tree has completed. Any ECC errors which arise during execution of a nested program will either generate an exception or continue execution (depending upon error and configuration).






9.5. CDP2 vs CDP1ï

This section summarises the differences between, and the compatibility and interoperability of, the new (CDP2) and legacy (CDP1) CUDA Dynamic Parallelism interfaces. It also shows how to opt-out of the CDP2 interface on devices of compute capability less than 9.0.


9.5.1. Differences Between CDP1 and CDP2ï

Explicit device-side synchronization is no longer possible with CDP2 or on devices of compute capability 9.0 or higher. Implicit synchronization (such as tail launches) must be used instead.
Attempting to query or set cudaLimitDevRuntimeSyncDepth (or CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH) with CDP2 or on devices of compute capability 9.0 or higher results in cudaErrorUnsupportedLimit.
CDP2 no longer has a virtualized pool for pending launches that donât fit in the fixed-sized pool. cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of launch slots.
For CDP2, there is a limit to the total number of events existing at once (note that events are destroyed only after a launch completes), equal to twice the pending launch count. cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of event slots.
Streams are tracked per grid with CDP2 or on devices of compute capability 9.0 or higher, not per thread block. This allows work to be launched into a stream created by another thread block. Attempting to do so with the CDP1 results in cudaErrorInvalidValue.
CDP2 introduces the tail launch (cudaStreamTailLaunch) and fire-and-forget (cudaStreamFireAndForget) named streams.
CDP2 is supported only under 64-bit compilation mode.



9.5.2. Compatibility and Interoperabilityï

CDP2 is the default. Functions can be compiled with -DCUDA_FORCE_CDP1_IF_SUPPORTED to opt-out of using CDP2 on devices of compute capability less than 9.0.









Function compiler with CUDA 12.0 and newer (default)
Function compiled with pre-CUDA 12.0 or with CUDA 12.0 and newer with -DCUDA_FORCE_CDP1_IF_SUPPORTED specified




Compilation
Compile error if device code references cudaDeviceSynchronize.
Compile error if code references cudaStreamTailLaunch or cudaStreamFireAndForget. Compile error if device code references cudaDeviceSynchronize and code is compiled for sm_90 or newer.


Compute capability < 9.0
New interface is used.
Legacy interface is used.


Compute capability 9.0 and higher
New interface is used.
New interface is used. If function references cudaDeviceSynchronize in device code, function load returns cudaErrorSymbolNotFound (this could happen if the code is compiled for devices of compute capability less than 9.0, but run on devices of compute capability 9.0 or higher using JIT).



Functions using CDP1 and CDP2 may be loaded and run simultaneously in the same context. The CDP1 functions are able to use CDP1-specific features (e.g. cudaDeviceSynchronize) and CDP2 functions are able to use CDP2-specific features (e.g. tail launch and fire-and-forget launch).
A function using CDP1 cannot launch a function using CDP2, and vice versa. If a function that would use CDP1 contains in its call graph a function that would use CDP2, or vice versa, cudaErrorCdpVersionMismatch would result during function load.




9.6. Legacy CUDA Dynamic Parallelism (CDP1)ï

See CUDA Dynamic Parallelism, above, for CDP2 version of document.


9.6.1. Execution Environment and Memory Model (CDP1)ï

See Execution Environment and Memory Model, above, for CDP2 version of document.


9.6.1.1. Execution Environment (CDP1)ï

See Execution Environment, above, for CDP2 version of document.
The CUDA execution model is based on primitives of threads, thread blocks, and grids, with kernel functions defining the program executed by individual threads within a thread block and grid. When a kernel function is invoked the gridâs properties are described by an execution configuration, which has a special syntax in CUDA. Support for dynamic parallelism in CUDA extends the ability to configure, launch, and synchronize upon new grids to threads that are running on the device.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) block is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.



9.6.1.1.1. Parent and Child Grids (CDP1)ï

See Parent and Child Grids, above, for CDP2 version of document.
A device thread that configures and launches a new grid belongs to the parent grid, and the grid created by the invocation is a child grid.
The invocation and completion of child grids is properly nested, meaning that the parent grid is not considered complete until all child grids created by its threads have completed. Even if the invoking threads do not explicitly synchronize on the child grids launched, the runtime guarantees an implicit synchronization between the parent and child.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.




Figure 27 Parent-Child Launch Nestingï





9.6.1.1.2. Scope of CUDA Primitives (CDP1)ï

See Scope of CUDA Primitives, above, for CDP2 version of document.
On both host and device, the CUDA runtime offers an API for launching kernels, for waiting for launched work to complete, and for tracking dependencies between launches via streams and events. On the host system, the state of launches and the CUDA primitives referencing streams and events are shared by all threads within a process; however processes execute independently and may not share CUDA objects.
A similar hierarchy exists on the device: launched kernels and CUDA objects are visible to all threads in a thread block, but are independent between thread blocks. This means for example that a stream may be created by one thread and used by any other thread in the same thread block, but may not be shared with threads in any other thread block.



9.6.1.1.3. Synchronization (CDP1)ï

See Synchronization, above, for CDP2 version of document.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

CUDA runtime operations from any thread, including kernel launches, are visible across a thread block. This means that an invoking thread in the parent grid may perform synchronization on the grids launched by that thread, by other threads in the thread block, or on streams created within the same thread block. Execution of a thread block is not considered complete until all launches by all threads in the block have completed. If all threads in a block exit before all child launches have completed, a synchronization operation will automatically be triggered.



9.6.1.1.4. Streams and Events (CDP1)ï

See Streams and Events, above, for CDP2 version of document.
CUDA Streams and Events allow control over dependencies between grid launches: grids launched into the same stream execute in-order, and events may be used to create dependencies between streams. Streams and events created on the device serve this exact same purpose.
Streams and events created within a grid exist within thread block scope but have undefined behavior when used outside of the thread block where they were created. As described above, all work launched by a thread block is implicitly synchronized when the block exits; work launched into streams is included in this, with all dependencies resolved appropriately. The behavior of operations on a stream that has been modified outside of thread block scope is undefined.
Streams and events created on the host have undefined behavior when used within any kernel, just as streams and events created by a parent grid have undefined behavior if used within a child grid.



9.6.1.1.5. Ordering and Concurrency (CDP1)ï

See Ordering and Concurrency, above, for CDP2 version of document.
The ordering of kernel launches from the device runtime follows CUDA Stream ordering semantics. Within a thread block, all kernel launches into the same stream are executed in-order. With multiple threads in the same thread block launching into the same stream, the ordering within the stream is dependent on the thread scheduling within the block, which may be controlled with synchronization primitives such as __syncthreads().
Note that because streams are shared by all threads within a thread block, the implicit NULL stream is also shared. If multiple threads in a thread block launch into the implicit stream, then these launches will be executed in-order. If concurrency is desired, explicit named streams should be used.
Dynamic Parallelism enables concurrency to be expressed more easily within a program; however, the device runtime introduces no new concurrency guarantees within the CUDA execution model. There is no guarantee of concurrent execution between any number of different thread blocks on a device.
The lack of concurrency guarantee extends to parent thread blocks and their child grids. When a parent thread block launches a child grid, the child is not guaranteed to begin execution until the parent thread block reaches an explicit synchronization point (such as cudaDeviceSynchronize()).

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

While concurrency will often easily be achieved, it may vary as a function of deviceconfiguration, application workload, and runtime scheduling. It is therefore unsafe to depend upon any concurrency between different thread blocks.



9.6.1.1.6. Device Management (CDP1)ï

See Device Management, above, for CDP2 version of document.
There is no multi-GPU support from the device runtime; the device runtime is only capable of operating on the device upon which it is currently executing. It is permitted, however, to query properties for any CUDA capable device in the system.




9.6.1.2. Memory Model (CDP1)ï

See Memory Model, above, for CDP2 version of document.
Parent and child grids share the same global and constant memory storage, but have distinct local and shared memory.


9.6.1.2.1. Coherence and Consistency (CDP1)ï

See Coherence and Consistency, above, for CDP2 version of document.


9.6.1.2.1.1. Global Memory (CDP1)ï

See Global Memory, above, for CDP2 version of document.
Parent and child grids have coherent access to global memory, with weak consistency guarantees between child and parent. There are two points in the execution of a child grid when its view of memory is fully consistent with the parent thread: when the child grid is invoked by the parent, and when the child grid completes as signaled by a synchronization API invocation in the parent thread.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

All global memory operations in the parent thread prior to the child gridâs invocation are visible to the child grid. All memory operations of the child grid are visible to the parent after the parent has synchronized on the child gridâs completion.
In the following example, the child grid executing child_launch is only guaranteed to see the modifications to data made before the child grid was launched. Since thread 0 of the parent is performing the launch, the child will be consistent with the memory seen by thread 0 of the parent. Due to the first __syncthreads() call, the child will see data[0]=0, data[1]=1, â¦, data[255]=255 (without the __syncthreads() call, only data[0] would be guaranteed to be seen by the child). When the child grid returns, thread 0 is guaranteed to see modifications made by the threads in its child grid. Those modifications become available to the other threads of the parent grid only after the second __syncthreads() call:

__global__ void child_launch(int *data) {
   data[threadIdx.x] = data[threadIdx.x]+1;
}

__global__ void parent_launch(int *data) {
   data[threadIdx.x] = threadIdx.x;

   __syncthreads();

   if (threadIdx.x == 0) {
       child_launch<<< 1, 256 >>>(data);
       cudaDeviceSynchronize();
   }

   __syncthreads();
}

void host_launch(int *data) {
    parent_launch<<< 1, 256 >>>(data);
}





9.6.1.2.1.2. Zero Copy Memory (CDP1)ï

See Zero Copy Memory, above, for CDP2 version of document.
Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above. A kernel may not allocate or free zero-copy memory, but may use pointers to zero-copy passed in from the host program.



9.6.1.2.1.3. Constant Memory (CDP1)ï

See Constant Memory, above, for CDP2 version of document.
Constants are immutable and may not be modified from the device, even between parent and child launches. That is to say, the value of all __constant__ variables must be set from the host prior to launch. Constant memory is inherited automatically by all child kernels from their respective parents.
Taking the address of a constant memory object from within a kernel thread has the same semantics as for all CUDA programs, and passing that pointer from parent to child or from a child to parent is naturally supported.



9.6.1.2.1.4. Shared and Local Memory (CDP1)ï

See Shared and Local Memory, above, for CDP2 version of document.
Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child. Behavior is undefined when an object in one of these locations is referenced outside of the scope within which it belongs, and may cause an error.
The NVIDIA compiler will attempt to warn if it can detect that a pointer to local or shared memory is being passed as an argument to a kernel launch. At runtime, the programmer may use the __isGlobal() intrinsic to determine whether a pointer references global memory and so may safely be passed to a child launch.
Note that calls to cudaMemcpy*Async() or cudaMemset*Async() may invoke new child kernels on the device in order to preserve stream semantics. As such, passing shared or local memory pointers to these APIs is illegal and will return an error.



9.6.1.2.1.5. Local Memory (CDP1)ï

See Local Memory, above, for CDP2 version of document.
Local memory is private storage for an executing thread, and is not visible outside of that thread. It is illegal to pass a pointer to local memory as a launch argument when launching a child kernel. The result of dereferencing such a local memory pointer from a child will be undefined.
For example the following is illegal, with undefined behavior if x_array is accessed by child_launch:

int x_array[10];       // Creates x_array in parent's local memory
child_launch<<< 1, 1 >>>(x_array);


It is sometimes difficult for a programmer to be aware of when a variable is placed into local memory by the compiler. As a general rule, all storage passed to a child kernel should be allocated explicitly from the global-memory heap, either with cudaMalloc(), new() or by declaring __device__ storage at global scope. For example:

// Correct - "value" is global storage
__device__ int value;
__device__ void x() {
    value = 5;
    child<<< 1, 1 >>>(&value);
}



// Invalid - "value" is local storage
__device__ void y() {
    int value = 5;
    child<<< 1, 1 >>>(&value);
}





9.6.1.2.1.6. Texture Memory (CDP1)ï

See Texture Memory, above, for CDP2 version of document.
Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses. Coherence for texture memory is enforced at the invocation of a child grid and when a child grid completes. This means that writes to memory prior to a child kernel launch are reflected in texture memory accesses of the child. Similarly, writes to memory by a child will be reflected in the texture memory accesses by a parent, but only after the parent synchronizes on the childâs completion. Concurrent accesses by parent and child may result in inconsistent data.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.







9.6.2. Programming Interface (CDP1)ï

See Programming Interface, above, for CDP2 version of document.


9.6.2.1. CUDA C++ Reference (CDP1)ï

See CUDA C++ Reference, above, for CDP2 version of document.
This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism.
The language interface and API available to CUDA kernels using CUDA C++ for Dynamic Parallelism, referred to as the Device Runtime, is substantially like that of the CUDA Runtime API available on the host. Where possible the syntax and semantics of the CUDA Runtime API have been retained in order to facilitate ease of code reuse for routines that may run in either the host or device environments.
As with all code in CUDA C++, the APIs and code outlined here is per-thread code. This enables each thread to make unique, dynamic decisions regarding what kernel or operation to execute next. There are no synchronization requirements between threads within a block to execute any of the provided device runtime APIs, which enables the device runtime API functions to be called in arbitrarily divergent kernel code without deadlock.


9.6.2.1.1. Device-Side Kernel Launch (CDP1)ï

See Device-Side Kernel Launch, above, for CDP2 version of document.
Kernels may be launched from the device using the standard CUDA <<< >>> syntax:

kernel_name<<< Dg, Db, Ns, S >>>([kernel arguments]);



Dg is of type dim3 and specifies the dimensions and size of the grid
Db is of type dim3 and specifies the dimensions and size of each thread block
Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call and addition to statically allocated memory. Ns is an optional argument that defaults to 0.
S is of type cudaStream_t and specifies the stream associated with this call. The stream must have been allocated in the same thread block where the call is being made. S is an optional argument that defaults to 0.



9.6.2.1.1.1. Launches are Asynchronous (CDP1)ï

See Launches are Asynchronous, above, for CDP2 version of document.
Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread. That is to say, the <<<>>> launch command will return immediately and the launching thread will continue to execute until it hits an explicit launch-synchronization point such as cudaDeviceSynchronize().

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

The grid launch is posted to the device and will execute independently of the parent thread. The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until the launching thread reaches an explicit launch-synchronization point.



9.6.2.1.1.2. Launch Environment Configuration (CDP1)ï

See Launch Environment Configuration, above, for CDP2 version of document.
All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig(), and device limits returned from cudaDeviceGetLimit()) will be inherited from the parent. Likewise, device limits such as stack size will remain as-configured.
For host-launched kernels, per-kernel configurations set from the host will take precedence over the global setting. These configurations will be used when the kernel is launched from the device as well. It is not possible to reconfigure a kernelâs environment from the device.




9.6.2.1.2. Streams (CDP1)ï

See Streams, above, for CDP2 version of document.
Both named and unnamed (NULL) streams are available from the device runtime. Named streams may be used by any thread within a thread-block, but stream handles may not be passed to other blocks or child/parent kernels. In other words, a stream should be treated as private to the block in which it is created. Stream handles are not guaranteed to be unique between blocks, so using a stream handle within a block that did not allocate it will result in undefined behavior.
Similar to host-side launch, work launched into separate streams may run concurrently, but actual concurrency is not guaranteed. Programs that depend upon concurrency between child kernels are not supported by the CUDA programming model and will have undefined behavior.
The host-side NULL streamâs cross-stream barrier semantic is not supported on the device (see below for details). In order to retain semantic compatibility with the host runtime, all device streams must be created using the cudaStreamCreateWithFlags() API, passing the cudaStreamNonBlocking flag. The cudaStreamCreate() call is a host-runtime- only API and will fail to compile for the device.
As cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, cudaDeviceSynchronize() should be used instead when the application needs to know that stream-launched child kernels have completed.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.



9.6.2.1.2.1. The Implicit (NULL) Stream (CDP1)ï

See The Implicit (NULL) Stream, above, for CDP2 version of document.
Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details). The device runtime offers a single implicit, unnamed stream shared between all threads in a block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks).




9.6.2.1.3. Events (CDP1)ï

See Events, above, for CDP2 version of document.
Only the inter-stream synchronization capabilities of CUDA events are supported. This means that cudaStreamWaitEvent() is supported, but cudaEventSynchronize(), cudaEventElapsedTime(), and cudaEventQuery() are not. As cudaEventElapsedTime() is not supported, cudaEvents must be created via cudaEventCreateWithFlags(), passing the cudaEventDisableTiming flag.
As for all device runtime objects, event objects may be shared between all threads within the thread-block which created them but are local to that block and may not be passed to other kernels, or between blocks within the same kernel. Event handles are not guaranteed to be unique between blocks, so using an event handle within a block that did not create it will result in undefined behavior.



9.6.2.1.4. Synchronization (CDP1)ï

See Synchronization, above, for CDP2 version of document.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

The cudaDeviceSynchronize() function will synchronize on all work launched by any thread in the thread-block up to the point where cudaDeviceSynchronize() was called. Note that cudaDeviceSynchronize() may be called from within divergent code (see Block Wide Synchronization (CDP1)).
It is up to the program to perform sufficient additional inter-thread synchronization, for example via a call to __syncthreads(), if the calling thread is intended to synchronize with child grids invoked from other threads.


9.6.2.1.4.1. Block Wide Synchronization (CDP1)ï

See CUDA Dynamic Parallelism, above, for CDP2 version of document.
The cudaDeviceSynchronize() function does not imply intra-block synchronization. In particular, without explicit synchronization via a __syncthreads() directive the calling thread can make no assumptions about what work has been launched by any thread other than itself. For example if multiple threads within a block are each launching work and synchronization is desired for all this work at once (perhaps because of event-based dependencies), it is up to the program to guarantee that this work is submitted by all threads before calling cudaDeviceSynchronize().
Because the implementation is permitted to synchronize on launches from any thread in the block, it is quite possible that simultaneous calls to cudaDeviceSynchronize() by multiple threads will drain all work in the first call and then have no effect for the later calls.




9.6.2.1.5. Device Management (CDP1)ï

See Device Management, above, for CDP2 version of document.
Only the device on which a kernel is running will be controllable from that kernel. This means that device APIs such as cudaSetDevice() are not supported by the device runtime. The active device as seen from the GPU (returned from cudaGetDevice()) will have the same device number as seen from the host system. The cudaDeviceGetAttribute() call may request information about another device as this API allows specification of a device ID as a parameter of the call. Note that the catch-all cudaGetDeviceProperties() API is not offered by the device runtime - properties must be queried individually.



9.6.2.1.6. Memory Declarations (CDP1)ï

See Memory Declarations, above, for CDP2 version of document.


9.6.2.1.6.1. Device and Constant Memory (CDP1)ï

See Device and Constant Memory, above, for CDP2 version of document.
Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime. All kernels may read or write device variables, whether the kernel was initially launched by the host or device runtime. Equivalently, all kernels will have the same view of __constant__s as declared at the module scope.



9.6.2.1.6.2. Textures and Surfaces (CDP1)ï

See Textures and Surfaces, above, for CDP2 version of document.
CUDA supports dynamically created texture and surface objects14, where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host. The device runtime does not allow creation or destruction of texture or surface objects from within device code, but texture and surface objects created from the host may be used and passed around freely on the device. Regardless of where they are created, dynamically created texture objects are always valid and may be passed to child kernels from a parent.

Note
The device runtime does not support legacy module-scope (i.e., Fermi-style) textures and surfaces within a kernel launched from the device. Module-scope (legacy) textures may be created from the host and used in device code as for any kernel, but may only be used by a top-level kernel (i.e., the one which is launched from the host).




9.6.2.1.6.3. Shared Memory Variable Declarations (CDP1)ï

See Shared Memory Variable Declarations, above, for CDP2 version of document.
In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernelâs caller via a launch configuration argument. Both types of declarations are valid under the device runtime.

__global__ void permute(int n, int *data) {
   extern __shared__ int smem[];
   if (n <= 1)
       return;

   smem[threadIdx.x] = data[threadIdx.x];
   __syncthreads();

   permute_data(smem, n);
   __syncthreads();

   // Write back to GMEM since we can't pass SMEM to children.
   data[threadIdx.x] = smem[threadIdx.x];
   __syncthreads();

   if (threadIdx.x == 0) {
       permute<<< 1, 256, n/2*sizeof(int) >>>(n/2, data);
       permute<<< 1, 256, n/2*sizeof(int) >>>(n/2, data+n/2);
   }
}

void host_launch(int *data) {
    permute<<< 1, 256, 256*sizeof(int) >>>(256, data);
}





9.6.2.1.6.4. Symbol Addresses (CDP1)ï

See Symbol Addresses, above, for CDP2 version of document.
Device-side symbols (i.e., those marked __device__) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernelâs visible address space. This also applies to __constant__ symbols, although in this case the pointer will reference read-only data.
Given that device-side symbols can be referenced directly, those CUDA runtime APIs which reference symbols (e.g., cudaMemcpyToSymbol() or cudaGetSymbolAddress()) are redundant and hence not supported by the device runtime. Note this implies that constant data cannot be altered from within a running kernel, even ahead of a child kernel launch, as references to __constant__ space are read-only.




9.6.2.1.7. API Errors and Launch Failures (CDP1)ï

See API Errors and Launch Failures, above, for CDP2 version of document.
As usual for the CUDA runtime, any function may return an error code. The last error code returned is recorded and may be retrieved via the cudaGetLastError() call. Errors are recorded per-thread, so that each thread can identify the most recent error that it has generated. The error code is of type cudaError_t.
Similar to a host-side launch, device-side launches may fail for many reasons (invalid arguments, etc). The user must call cudaGetLastError() to determine if a launch generated an error, however lack of an error after launch does not imply the child kernel completed successfully.
For device-side exceptions, e.g., access to an invalid address, an error in a child grid will be returned to the host instead of being returned by the parentâs call to cudaDeviceSynchronize().


9.6.2.1.7.1. Launch Setup APIs (CDP1)ï

See Launch Setup APIs, above, for CDP2 version of document.
Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs. It is permitted for a CUDA application to call these APIs itself, with the same requirements as for PTX. In both cases, the user is then responsible for correctly populating all necessary data structures in the correct format according to specification. Backwards compatibility is guaranteed in these data structures.
As with host-side launch, the device-side operator <<<>>> maps to underlying kernel launch APIs. This is so that users targeting PTX will be able to enact a launch, and so that the compiler front-end can translate <<<>>> into these calls.


Table 11 New Device-only Launch Implementation Functionsï







Runtime API Launch Functions
Description of Difference From Host Runtime Behaviour (behavior is identical if no description)




cudaGetParameterBuffer
Generated automatically from <<<>>>. Note different API to host equivalent.


cudaLaunchDevice
Generated automatically from <<<>>>. Note different API to host equivalent.



The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows:

extern   device   cudaError_t cudaGetParameterBuffer(void **params);
extern __device__ cudaError_t cudaLaunchDevice(void *kernel,
                                        void *params, dim3 gridDim,
                                        dim3 blockDim,
                                        unsigned int sharedMemSize = 0,
                                        cudaStream_t stream = 0);






9.6.2.1.8. API Reference (CDP1)ï

See API Reference, above, for CDP2 version of document.
The portions of the CUDA Runtime API supported in the device runtime are detailed here. Host and device runtime APIs have identical syntax; semantics are the same except where indicated. The table below provides an overview of the API relative to the version available from the host.


Table 12 Supported API Functionsï







Runtime API Functions
Details




cudaDeviceSynchronize

Synchronizes on work launched from threadâs own block only.
Warning: Note that calling this API from device code is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.



cudaDeviceGetCacheConfig



cudaDeviceGetLimit



cudaGetLastError
Last error is per-thread state, not per-block state


cudaPeekAtLastError



cudaGetErrorString



cudaGetDeviceCount



cudaDeviceGetAttribute
Will return attributes for any device


cudaGetDevice
Always returns current device ID as would be seen from host


cudaStreamCreateWithFlags
Must pass cudaStreamNonBlocking flag


cudaStreamDestroy



cudaStreamWaitEvent



cudaEventCreateWithFlags
Must pass cudaEventDisableTiming flag


cudaEventRecord



cudaEventDestroy



cudaFuncGetAttributes



cudaMemcpyAsync

Notes about all memcpy/memset functions:

Only async memcpy/set functions are supported
Only device-to-device memcpy is permitted
May not pass in local or shared memory pointers




cudaMemcpy2DAsync


cudaMemcpy3DAsync


cudaMemsetAsync


cudaMemset2DAsync



cudaMemset3DAsync



cudaRuntimeGetVersion



cudaMalloc
May not call cudaFree on the device on a pointer created on the host, and vice-versa


cudaFree


cudaOccupancyMaxActiveBlocksPerMultiprocessor



cudaOccupancyMaxPotentialBlockSize



cudaOccupancyMaxPotentialBlockSizeVariableSMem








9.6.2.2. Device-side Launch from PTX (CDP1)ï

See Device-side Launch from PTX, above, for CDP2 version of document.
This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language. It provides the low-level details related to supporting kernel launches at the PTX level.


9.6.2.2.1. Kernel Launch APIs (CDP1)ï

See Kernel Launch APIs, above, for CDP2 version of document.
Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer(). cudaLaunchDevice() launches the specified kernel with the parameter buffer that is obtained by calling cudaGetParameterBuffer() and filled with the parameters to the launched kernel. The parameter buffer can be NULL, i.e., no need to invoke cudaGetParameterBuffer(), if the launched kernel does not take any parameters.


9.6.2.2.1.1. cudaLaunchDevice (CDP1)ï

See cudaLaunchDevice, above, for CDP2 version of document.
At the PTX level, cudaLaunchDevice()needs to be declared in one of the two forms shown below before it is used.

// PTX-level Declaration of cudaLaunchDevice() when .address_size is 64
.extern .func(.param .b32 func_retval0) cudaLaunchDevice
(
  .param .b64 func,
  .param .b64 parameterBuffer,
  .param .align 4 .b8 gridDimension[12],
  .param .align 4 .b8 blockDimension[12],
  .param .b32 sharedMemSize,
  .param .b64 stream
)
;



// PTX-level Declaration of cudaLaunchDevice() when .address_size is 32
.extern .func(.param .b32 func_retval0) cudaLaunchDevice
(
  .param .b32 func,
  .param .b32 parameterBuffer,
  .param .align 4 .b8 gridDimension[12],
  .param .align 4 .b8 blockDimension[12],
  .param .b32 sharedMemSize,
  .param .b32 stream
)
;


The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api.h. The function is defined in the cudadevrt system library, which must be linked with a program in order to use device-side kernel launch functionality.

// CUDA-level declaration of cudaLaunchDevice()
extern "C" __device__
cudaError_t cudaLaunchDevice(void *func, void *parameterBuffer,
                             dim3 gridDimension, dim3 blockDimension,
                             unsigned int sharedMemSize,
                             cudaStream_t stream);


The first parameter is a pointer to the kernel to be is launched, and the second parameter is the parameter buffer that holds the actual parameters to the launched kernel. The layout of the parameter buffer is explained in Parameter Buffer Layout (CDP1), below. Other parameters specify the launch configuration, i.e., as grid dimension, block dimension, shared memory size, and the stream associated with the launch (please refer to Execution Configuration for the detailed description of launch configuration.



9.6.2.2.1.2. cudaGetParameterBuffer (CDP1)ï

See cudaGetParameterBuffer, above, for CDP2 version of document.
cudaGetParameterBuffer() needs to be declared at the PTX level before itâs used. The PTX-level declaration must be in one of the two forms given below, depending on address size:

// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64
// When .address_size is 64
.extern .func(.param .b64 func_retval0) cudaGetParameterBuffer
(
  .param .b64 alignment,
  .param .b64 size
)
;



// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 32
.extern .func(.param .b32 func_retval0) cudaGetParameterBuffer
(
  .param .b32 alignment,
  .param .b32 size
)
;


The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration:

// CUDA-level Declaration of cudaGetParameterBuffer()
extern "C" __device__
void *cudaGetParameterBuffer(size_t alignment, size_t size);


The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement in bytes. In the current implementation, the parameter buffer returned by cudaGetParameterBuffer() is always guaranteed to be 64- byte aligned, and the alignment requirement parameter is ignored. However, it is recommended to pass the correct alignment requirement value - which is the largest alignment of any parameter to be placed in the parameter buffer - to cudaGetParameterBuffer() to ensure portability in the future.




9.6.2.2.2. Parameter Buffer Layout (CDP1)ï

See Parameter Buffer Layout, above, for CDP2 version of document.
Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned. That is, each parameter must be placed at the nth byte in the parameter buffer, where n is the smallest multiple of the parameter size that is greater than the offset of the last byte taken by the preceding parameter. The maximum size of the parameter buffer is 4KB.
For a more detailed description of PTX code generated by the CUDA compiler, please refer to the PTX-3.5 specification.




9.6.2.3. Toolkit Support for Dynamic Parallelism (CDP1)ï

See Toolkit Support for Dynamic Parallelism, above, for CDP2 version of document.


9.6.2.3.1. Including Device Runtime API in CUDA Code (CDP1)ï

See Including Device Runtime API in CUDA Code, above, for CDP2 version of document.
Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation. There is no need to includecuda_device_runtime_api.h explicitly.



9.6.2.3.2. Compiling and Linking (CDP1)ï

See Compiling and Linking, above, for CDP2 version of document.
When compiling and linking CUDA programs using dynamic parallelism with nvcc, the program will automatically link against the static device runtime library libcudadevrt.
The device runtime is offered as a static library (cudadevrt.lib on Windows, libcudadevrt.a under Linux), against which a GPU application that uses the device runtime must be linked. Linking of device libraries can be accomplished through nvcc and/or nvlink. Two simple examples are shown below.
A device runtime program may be compiled and linked in a single step, if all required source files can be specified from the command line:

$ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt


It is also possible to compile CUDA .cu source files first to object files, and then link these together in a two-stage process:

$ nvcc -arch=sm_75 -dc hello_world.cu -o hello_world.o
$ nvcc -arch=sm_75 -rdc=true hello_world.o -o hello -lcudadevrt


Please see the Using Separate Compilation section of The CUDA Driver Compiler NVCC guide for more details.





9.6.3. Programming Guidelines (CDP1)ï

See Programming Guidelines, above, for CDP2 version of document.


9.6.3.1. Basics (CDP1)ï

See Basics, above, for CDP2 version of document.
The device runtime is a functional subset of the host runtime. API level device management, kernel launching, device memcpy, stream management, and event management are exposed from the device runtime.
Programming for the device runtime should be familiar to someone who already has experience with CUDA. Device runtime syntax and semantics are largely the same as that of the host API, with any exceptions detailed earlier in this document.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

The following example shows a simple Hello World program incorporating dynamic parallelism:

#include <stdio.h>

__global__ void childKernel()
{
    printf("Hello ");
}

__global__ void parentKernel()
{
    // launch child
    childKernel<<<1,1>>>();
    if (cudaSuccess != cudaGetLastError()) {
        return;
    }

    // wait for child to complete
    if (cudaSuccess != cudaDeviceSynchronize()) {
        return;
    }

    printf("World!\n");
}

int main(int argc, char *argv[])
{
    // launch parent
    parentKernel<<<1,1>>>();
    if (cudaSuccess != cudaGetLastError()) {
        return 1;
    }

    // wait for parent to complete
    if (cudaSuccess != cudaDeviceSynchronize()) {
        return 2;
    }

    return 0;
}


This program may be built in a single step from the command line as follows:

$ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt





9.6.3.2. Performance (CDP1)ï

See Performance, above, for CDP2 version of document.


9.6.3.2.1. Synchronization (CDP1)ï

See CUDA Dynamic Parallelism, above, for CDP2 version of document.

Warning
Explicit synchronization with child kernels from a parent block (such as using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

Synchronization by one thread may impact the performance of other threads in the same Thread Block, even when those other threads do not call cudaDeviceSynchronize() themselves. This impact will depend upon the underlying implementation. In general the implicit synchronization of child kernels done when a thread block ends is more efficient compared to calling cudaDeviceSynchronize() explicitly. It is therefore recommended to only call cudaDeviceSynchronize() if it is needed to synchronize with a child kernel before a thread block ends.



9.6.3.2.2. Dynamic-parallelism-enabled Kernel Overhead (CDP1)ï

See Dynamic-parallelism-enabled Kernel Overhead, above, for CDP2 version of document.
System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own. This overhead arises from the device runtimeâs execution tracking and management software and may result in decreased performance for example, library calls when made from the device compared to from the host side. This overhead is, in general, incurred for applications that link against the device runtime library.




9.6.3.3. Implementation Restrictions and Limitations (CDP1)ï

See Implementation Restrictions and Limitations, above, for CDP2 version of document.
Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime.


9.6.3.3.1. Runtime (CDP1)ï

See Runtime, above, for CDP2 version of document.


9.6.3.3.1.1. Memory Footprint (CDP1)ï

See Memory Footprint, above, for CDP2 version of document.
The device runtime system software reserves memory for various management purposes, in particular one reservation which is used for saving parent-grid state during synchronization, and a second reservation for tracking pending grid launches. Configuration controls are available to reduce the size of these reservations in exchange for certain launch limitations. See Configuration Options (CDP1), below, for details.
The majority of reserved memory is allocated as backing-store for parent kernel state, for use when synchronizing on a child launch. Conservatively, this memory must support storing of state for the maximum number of live threads possible on the device. This means that each parent generation at which cudaDeviceSynchronize() is callable may require up to 860MB of device memory, depending on the device configuration, which will be unavailable for program use even if it is not all consumed.



9.6.3.3.1.2. Nesting and Synchronization Depth (CDP1)ï

See CUDA Dynamic Parallelism, above, for CDP2 version of document.
Using the device runtime, one kernel may launch another kernel, and that kernel may launch another, and so on. Each subordinate launch is considered a new nesting level, and the total number of levels is the nesting depth of the program. The synchronization depth is defined as the deepest level at which the program will explicitly synchronize on a child launch. Typically this is one less than the nesting depth of the program, but if the program does not need to call cudaDeviceSynchronize() at all levels then the synchronization depth might be substantially different to the nesting depth.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

The overall maximum nesting depth is limited to 24, but practically speaking the real limit will be the amount of memory required by the system for each new level (see Memory Footprint (CDP1) above). Any launch which would result in a kernel at a deeper level than the maximum will fail. Note that this may also apply to cudaMemcpyAsync(), which might itself generate a kernel launch. See Configuration Options (CDP1) for details.
By default, sufficient storage is reserved for two levels of synchronization. This maximum synchronization depth (and hence reserved storage) may be controlled by calling cudaDeviceSetLimit() and specifying cudaLimitDevRuntimeSyncDepth. The number of levels to be supported must be configured before the top-level kernel is launched from the host, in order to guarantee successful execution of a nested program. Calling cudaDeviceSynchronize() at a depth greater than the specified maximum synchronization depth will return an error.
An optimization is permitted where the system detects that it need not reserve space for the parentâs state in cases where the parent kernel never calls cudaDeviceSynchronize(). In this case, because explicit parent/child synchronization never occurs, the memory footprint required for a program will be much less than the conservative maximum. Such a program could specify a shallower maximum synchronization depth to avoid over-allocation of backing store.



9.6.3.3.1.3. Pending Kernel Launches (CDP1)ï

See Pending Kernel Launches, above, for CDP2 version of document.
When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes. This data is stored within a system-managed launch pool.
The launch pool is divided into a fixed-size pool and a virtualized pool with lower performance. The device runtime system software will try to track launch data in the fixed-size pool first. The virtualized pool will be used to track new launches when the fixed-size pool is full.
The size of the fixed-size launch pool is configurable by calling cudaDeviceSetLimit() from the host and specifying cudaLimitDevRuntimePendingLaunchCount.



9.6.3.3.1.4. Configuration Options (CDP1)ï

See Configuration Options, above, for CDP2 version of document.
Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program. Limits must be set before any kernel is launched, and may not be changed while the GPU is actively running programs.

Warning
Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release.

The following named limits may be set:







Limit
Behavior




cudaLimitDevRuntimeSyncDepth
Sets the maximum depth at which cudaDeviceSynchronize() may be called. Launches may be performed deeper than this, but explicit synchronization deeper than this limit will return the cudaErrorLaunchMaxDepthExceeded. The default maximum sync depth is 2.


cudaLimitDevRuntimePendingLaunchCount
Controls the amount of memory set aside for buffering kernel launches which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources. When the buffer is full, the device runtime system software will attempt to track new pending launches in a lower performance virtualized buffer. If the virtualized buffer is also full, i.e. when all available heap space is consumed, launches will not occur, and the threadâs last error will be set to cudaErrorLaunchPendingCountExceeded. The default pending launch count is 2048 launches.


cudaLimitStackSize
Controls the stack size in bytes of each GPU thread. The CUDA driver automatically increases the per-thread stack size for each kernel launch as needed. This size isnât reset back to the original value after each launch. To set the per-thread stack size to a different value, cudaDeviceSetLimit() can be called to set this limit. The stack will be immediately resized, and if necessary, the device will block until all preceding requested tasks are complete. cudaDeviceGetLimit() can be called to get the current per-thread stack size.






9.6.3.3.1.5. Memory Allocation and Lifetime (CDP1)ï

See Memory Allocation and Lifetime, above, for CDP2 version of document.
cudaMalloc() and cudaFree() have distinct semantics between the host and device environments. When invoked from the host, cudaMalloc() allocates a new region from unused device memory. When invoked from the device runtime these functions map to device-side malloc() and free(). This implies that within the device environment the total allocatable memory is limited to the device malloc() heap size, which may be smaller than the available unused device memory. Also, it is an error to invoke cudaFree() from the host program on a pointer which was allocated by cudaMalloc() on the device or vice-versa.









cudaMalloc() on Host
cudaMalloc() on Device




cudaFree() on Host
Supported
Not Supported


cudaFree() on Device
Not Supported
Supported


Allocation limit
Free device memory
cudaLimitMallocHeapSize






9.6.3.3.1.6. SM Id and Warp Id (CDP1)ï

See SM Id and Warp Id, above, for CDP2 version of document.
Note that in PTX %smid and %warpid are defined as volatile values. The device runtime may reschedule thread blocks onto different SMs in order to more efficiently manage resources. As such, it is unsafe to rely upon %smid or %warpid remaining unchanged across the lifetime of a thread or thread block.



9.6.3.3.1.7. ECC Errors (CDP1)ï

See ECC Errors, above, for CDP2 version of document.
No notification of ECC errors is available to code within a CUDA kernel. ECC errors are reported at the host side once the entire launch tree has completed. Any ECC errors which arise during execution of a nested program will either generate an exception or continue execution (depending upon error and configuration).


14(1,2,3)


Dynamically created texture and surface objects are an addition to the CUDA memory model introduced with CUDA 5.0. Please see the CUDA Programming Guide for details.










10. Virtual Memory Managementï



10.1. Introductionï

The Virtual Memory Management APIs provide a way for the application to directly manage the unified virtual address space that CUDA provides to map physical memory to virtual addresses accessible by the GPU. Introduced in CUDA 10.2, these APIs additionally provide a new way to interop with other processes and graphics APIs like OpenGL and Vulkan, as well as provide newer memory attributes that a user can tune to fit their applications.
Historically, memory allocation calls (such as cudaMalloc()) in the CUDA programming model have returned a memory address that points to the GPU memory. The address thus obtained could be used with any CUDA API or inside a device kernel. However, the memory allocated could not be resized depending on the userâs memory needs. In order to increase an allocationâs size, the user had to explicitly allocate a larger buffer, copy data from the initial allocation, free it and then continue to keep track of the newer allocationâs address. This often leads to lower performance and higher peak memory utilization for applications. Essentially, users had a malloc-like interface for allocating GPU memory, but did not have a corresponding realloc to complement it. The Virtual Memory Management APIs decouple the idea of an address and memory and allow the application to handle them separately. The APIs allow applications to map and unmap memory from a virtual address range as they see fit.
In the case of enabling peer device access to memory allocations by using cudaEnablePeerAccess, all past and future user allocations are mapped to the target peer device. This lead to users unwittingly paying runtime cost of mapping all cudaMalloc allocations to peer devices. However, in most situations applications communicate by sharing only a few allocations with another device and not all allocations are required to be mapped to all the devices. With Virtual Memory Management, applications can specifically choose certain allocations to be accessible from target devices.
The CUDA Virtual Memory Management APIs expose fine grained control to the user for managing the GPU memory in applications. It provides APIs that let users:

Place memory allocated on different devices into a contiguous VA range.
Perform interprocess communication for memory sharing using platform-specific mechanisms.
Opt into newer memory types on the devices that support them.

In order to allocate memory, the Virtual Memory Management programming model exposes the following functionality:

Allocating physical memory.
Reserving a VA range.
Mapping allocated memory to the VA range.
Controlling access rights on the mapped range.

Note that the suite of APIs described in this section require a system that supports UVA.



10.2. Query for Supportï

Before attempting to use Virtual Memory Management APIs, applications must ensure that the devices they want to use support CUDA Virtual Memory Management. The following code sample shows querying for Virtual Memory Management support:

int deviceSupportsVmm;
CUresult result = cuDeviceGetAttribute(&deviceSupportsVmm, CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED, device);
if (deviceSupportsVmm != 0) {
    // `device` supports Virtual Memory Management
}





10.3. Allocating Physical Memoryï

The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation. In order to allocate physical memory, applications must use the cuMemCreate API. The allocation created by this function does not have any device or host mappings. The function argument CUmemGenericAllocationHandle describes the properties of the memory to allocate such as the location of the allocation, if the allocation is going to be shared to another process (or other Graphics APIs), or the physical attributes of the memory to be allocated. Users must ensure the requested allocationâs size must be aligned to appropriate granularity. Information regarding an allocationâs granularity requirements can be queried using cuMemGetAllocationGranularity. The following code snippet shows allocating physical memory with cuMemCreate:

CUmemGenericAllocationHandle allocatePhysicalMemory(int device, size_t size) {
    CUmemAllocationProp prop = {};
    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;
    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;
    prop.location.id = device;
    cuMemGetAllocationGranularity(&granularity, &prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);

    // Ensure size matches granularity requirements for the allocation
    size_t padded_size = ROUND_UP(size, granularity);

    // Allocate physical memory
    CUmemGenericAllocationHandle allocHandle;
    cuMemCreate(&allocHandle, padded_size, &prop, 0);

    return allocHandle;
}


The memory allocated by cuMemCreate is referenced by the CUmemGenericAllocationHandle it returns. This is a departure from the cudaMalloc-style of allocation, which returns a pointer to the GPU memory, which was directly accessible by CUDA kernel executing on the device. The memory allocated cannot be used for any operations other than querying properties using cuMemGetAllocationPropertiesFromHandle. In order to make this memory accessible, applications must map this memory into a VA range reserved by cuMemAddressReserve and provide suitable access rights to it. Applications must free the allocated memory using the cuMemRelease API.


10.3.1. Shareable Memory Allocationsï

With cuMemCreate users now have the facility to indicate to CUDA, at allocation time, that they have earmarked a particular allocation for Inter process communication and graphics interop purposes. Applications can do this by setting CUmemAllocationProp::requestedHandleTypes to a platform-specific field. On Windows, when CUmemAllocationProp::requestedHandleTypes is set to CU_MEM_HANDLE_TYPE_WIN32 applications must also specify an LPSECURITYATTRIBUTES attribute in CUmemAllocationProp::win32HandleMetaData. This security attribute defines the scope of which exported allocations may be transferred to other processes.
The CUDA Virtual Memory Management API functions do not support the legacy interprocess communication functions with their memory. Instead, they expose a new mechanism for interprocess communication that uses OS-specific handles. Applications can obtain these OS-specific handles corresponding to the allocations by using cuMemExportToShareableHandle. The handles thus obtained can be transferred by using the usual OS native mechanisms for inter process communication. The recipient process should import the allocation by using cuMemImportFromShareableHandle.
Users must ensure they query for support of the requested handle type before attempting to export memory allocated with cuMemCreate. The following code snippet illustrates query for handle type support in a platform-specific way.

int deviceSupportsIpcHandle;
#if defined(__linux__)
    cuDeviceGetAttribute(&deviceSupportsIpcHandle, CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED, device));
#else
    cuDeviceGetAttribute(&deviceSupportsIpcHandle, CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED, device));
#endif


Users should set the CUmemAllocationProp::requestedHandleTypes appropriately as shown below:

#if defined(__linux__)
    prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR;
#else
    prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_WIN32;
    prop.win32HandleMetaData = // Windows specific LPSECURITYATTRIBUTES attribute.
#endif


The memMapIpcDrv sample can be used as an example for using IPC with Virtual Memory Management allocations.



10.3.2. Memory Typeï

Before CUDA 10.2, applications had no user-controlled way of allocating any special type of memory that certain devices may support. With cuMemCreate, applications can additionally specify memory type requirements using the CUmemAllocationProp::allocFlags to opt into any specific memory features. Applications must also ensure that the requested memory type is supported on the device of allocation.


10.3.2.1. Compressible Memoryï

Compressible memory can be used to accelerate accesses to data with unstructured sparsity and other compressible data patterns. Compression can save DRAM bandwidth, L2 read bandwidth and L2 capacity depending on the data being operated on. Applications that want to allocate compressible memory on devices that support Compute Data Compression can do so by setting CUmemAllocationProp::allocFlags::compressionType to CU_MEM_ALLOCATION_COMP_GENERIC. Users must query if device supports Compute Data Compression by using CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED. The following code snippet illustrates querying compressible memory support cuDeviceGetAttribute.

int compressionSupported = 0;
cuDeviceGetAttribute(&compressionSupported, CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED, device);


On devices that support Compute Data Compression, users must opt in at allocation time as shown below:

prop.allocFlags.compressionType = CU_MEM_ALLOCATION_COMP_GENERIC;


Due to various reasons such as limited HW resources, the allocation may not have compression attributes, the user is expected to query back the properties of the allocated memory using cuMemGetAllocationPropertiesFromHandle and check for compression attribute.

CUmemAllocationPropPrivate allocationProp = {};
cuMemGetAllocationPropertiesFromHandle(&allocationProp, allocationHandle);

if (allocationProp.allocFlags.compressionType == CU_MEM_ALLOCATION_COMP_GENERIC)
{
    // Obtained compressible memory allocation
}







10.4. Reserving a Virtual Address Rangeï

Since with Virtual Memory Management the notions of address and memory are distinct, applications must carve out an address range that can hold the memory allocations made by cuMemCreate. The address range reserved must be at least as large as the sum of the sizes of all the physical memory allocations the user plans to place in them.
Applications can reserve a virtual address range by passing appropriate parameters to cuMemAddressReserve. The address range obtained will not have any device or host physical memory associated with it. The reserved virtual address range can be mapped to memory chunks belonging to any device in the system, thus providing the application a continuous VA range backed and mapped by memory belonging to different devices. Applications are expected to return the virtual address range back to CUDA using cuMemAddressFree. Users must ensure that the entire VA range is unmapped before calling cuMemAddressFree. These functions are conceptually similar to mmap/munmap (on Linux) or VirtualAlloc/VirtualFree (on Windows) functions. The following code snippet illustrates the usage for the function:

CUdeviceptr ptr;
// `ptr` holds the returned start of virtual address range reserved.
CUresult result = cuMemAddressReserve(&ptr, size, 0, 0, 0); // alignment = 0 for default alignment





10.5. Virtual Aliasing Supportï

The Virtual Memory Management APIs provide a way to create multiple virtual memory mappings or âproxiesâ to the same allocation using multiple calls to cuMemMap with different virtual addresses, so-called virtual aliasing. Unless otherwise noted in the PTX ISA, writes to one proxy of the allocation are considered inconsistent and incoherent with any other proxy of the same memory until the writing device operation (grid launch, memcpy, memset, and so on) completes. Grids present on the GPU prior to a writing device operation but reading after the writing device operation completes are also considered to have inconsistent and incoherent proxies.
For example, the following snippet is considered undefined, assuming device pointers A and B are virtual aliases of the same memory allocation:

__global__ void foo(char *A, char *B) {
  *A = 0x1;
  printf("%d\n", *B);    // Undefined behavior!  *B can take on either
// the previous value or some value in-between.
}


The following is defined behavior, assuming these two kernels are ordered monotonically (by streams or events).

__global__ void foo1(char *A) {
  *A = 0x1;
}

__global__ void foo2(char *B) {
  printf("%d\n", *B);    // *B == *A == 0x1 assuming foo2 waits for foo1
// to complete before launching
}

cudaMemcpyAsync(B, input, size, stream1);    // Aliases are allowed at
// operation boundaries
foo1<<<1,1,0,stream1>>>(A);                  // allowing foo1 to access A.
cudaEventRecord(event, stream1);
cudaStreamWaitEvent(stream2, event);
foo2<<<1,1,0,stream2>>>(B);
cudaStreamWaitEvent(stream3, event);
cudaMemcpyAsync(output, B, size, stream3);  // Both launches of foo2 and
                                            // cudaMemcpy (which both
                                            // read) wait for foo1 (which writes)
                                            // to complete before proceeding





10.6. Mapping Memoryï

The allocated physical memory and the carved out virtual address space from the previous two sections represent the memory and address distinction introduced by the Virtual Memory Management APIs. For the allocated memory to be useable, the user must first place the memory in the address space. The address range obtained from cuMemAddressReserve and the physical allocation obtained from cuMemCreate or cuMemImportFromShareableHandle must be associated with each other by using cuMemMap.
Users can associate allocations from multiple devices to reside in contiguous virtual address ranges as long as they have carved out enough address space. In order to decouple the physical allocation and the address range, users must unmap the address of the mapping by using cuMemUnmap. Users can map and unmap memory to the same address range as many times as they want, as long as they ensure that they donât attempt to create mappings on VA range reservations that are already mapped. The following code snippet illustrates the usage for the function:

CUdeviceptr ptr;
// `ptr`: address in the address range previously reserved by cuMemAddressReserve.
// `allocHandle`: CUmemGenericAllocationHandle obtained by a previous call to cuMemCreate.
CUresult result = cuMemMap(ptr, size, 0, allocHandle, 0);





10.7. Controlling Access Rightsï

The Virtual Memory Management APIs enable applications to explicitly protect their VA ranges with access control mechanisms. Mapping the allocation to a region of the address range using cuMemMap does not make the address accessible, and would result in a program crash if accessed by a CUDA kernel. Users must specifically select access control using the cuMemSetAccess function, which allows or restricts access for specific devices to a mapped address range. The following code snippet illustrates the usage for the function:

void setAccessOnDevice(int device, CUdeviceptr ptr, size_t size) {
    CUmemAccessDesc accessDesc = {};
    accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;
    accessDesc.location.id = device;
    accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;

    // Make the address accessible
    cuMemSetAccess(ptr, size, &accessDesc, 1);
}


The access control mechanism exposed with Virtual Memory Management allows users to be explicit about which allocations they want to share with other peer devices on the system. As specified earlier, cudaEnablePeerAccess forces all prior and future cudaMallocâd allocations to be mapped to the target peer device. This can be convenient in many cases as user doesnât have to worry about tracking the mapping state of every allocation to every device in the system. But for users concerned with performance of their applications this approach has performance implications. With access control at allocation granularity Virtual Memory Management exposes a mechanism to have peer mappings with minimal overhead.
The vectorAddMMAP sample can be used as an example for using the Virtual Memory Management APIs.




11. Stream Ordered Memory Allocatorï



11.1. Introductionï

Managing memory allocations using cudaMalloc and cudaFree causes GPU to synchronize across all executing CUDA streams. The Stream Order Memory Allocator enables applications to order memory allocation and deallocation with other work launched into a CUDA stream such as kernel launches and asynchronous copies. This improves application memory use by taking advantage of stream-ordering semantics to reuse memory allocations. The allocator also allows applications to control the allocatorâs memory caching behavior. When set up with an appropriate release threshold, the caching behavior allows the allocator to avoid expensive calls into the OS when the application indicates it is willing to accept a bigger memory footprint. The allocator also supports the easy and secure sharing of allocations between processes.
For many applications, the Stream Ordered Memory Allocator reduces the need for custom memory management abstractions, and makes it easier to create high-performance custom memory management for applications that need it. For applications and libraries that already have custom memory allocators, adopting the Stream Ordered Memory Allocator enables multiple libraries to share a common pool of memory managed by the driver, thus reducing excess memory consumption. Additionally, the driver can perform optimizations based on its awareness of the allocator and other stream management APIs. Finally, Nsight Compute and the Next-Gen CUDA debugger is aware of the allocator as part of their CUDA 11.3 toolkit support.



11.2. Query for Supportï

The user can determine whether or not a device supports the stream ordered memory allocator by calling cudaDeviceGetAttribute() with the device attribute cudaDevAttrMemoryPoolsSupported.
Starting with CUDA 11.3, IPC memory pool support can be queried with the cudaDevAttrMemoryPoolSupportedHandleTypes device attribute. Previous drivers will return cudaErrorInvalidValue as those drivers are unaware of the attribute enum.

int driverVersion = 0;
int deviceSupportsMemoryPools = 0;
int poolSupportedHandleTypes = 0;
cudaDriverGetVersion(&driverVersion);
if (driverVersion >= 11020) {
    cudaDeviceGetAttribute(&deviceSupportsMemoryPools,
                           cudaDevAttrMemoryPoolsSupported, device);
}
if (deviceSupportsMemoryPools != 0) {
    // `device` supports the Stream Ordered Memory Allocator
}

if (driverVersion >= 11030) {
    cudaDeviceGetAttribute(&poolSupportedHandleTypes,
              cudaDevAttrMemoryPoolSupportedHandleTypes, device);
}
if (poolSupportedHandleTypes & cudaMemHandleTypePosixFileDescriptor) {
   // Pools on the specified device can be created with posix file descriptor-based IPC
}


Performing the driver version check before the query avoids hitting a cudaErrorInvalidValue error on drivers where the attribute was not yet defined. One can use cudaGetLastError to clear the error instead of avoiding it.



11.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync)ï

The APIs cudaMallocAsync and cudaFreeAsync form the core of the allocator. cudaMallocAsync returns an allocation and cudaFreeAsync frees an allocation. Both APIs accept stream arguments to define when the allocation will become and stop being available for use. The pointer value returned by cudaMallocAsync is determined synchronously and is available for constructing future work. It is important to note that cudaMallocAsync ignores the current device/context when determining where the allocation will reside. Instead, cudaMallocAsync determines the resident device based on the specified memory pool or the supplied stream. The simplest use pattern is when the memory is allocated, used, and freed back into the same stream.

void *ptr;
size_t size = 512;
cudaMallocAsync(&ptr, size, cudaStreamPerThread);
// do work using the allocation
kernel<<<..., cudaStreamPerThread>>>(ptr, ...);
// An asynchronous free can be specified without synchronizing the cpu and GPU
cudaFreeAsync(ptr, cudaStreamPerThread);


When using an allocation in a stream other than the allocating stream, the user must guarantee that the access will happen after the allocation operation, otherwise the behavior is undefined. The user may make this guarantee either by synchronizing the allocating stream, or by using CUDA events to synchronize the producing and consuming streams.
cudaFreeAsync() inserts a free operation into the stream. The user must guarantee that the free operation happens after the allocation operation and any use of the allocation. Also, any use of the allocation after the free operation starts results in undefined behavior. Events and/or stream synchronizing operations should be used to guarantee any access to the allocation on other streams is complete before the freeing stream begins the free operation.

cudaMallocAsync(&ptr, size, stream1);
cudaEventRecord(event1, stream1);
//stream2 must wait for the allocation to be ready before accessing
cudaStreamWaitEvent(stream2, event1);
kernel<<<..., stream2>>>(ptr, ...);
cudaEventRecord(event2, stream2);
// stream3 must wait for stream2 to finish accessing the allocation before
// freeing the allocation
cudaStreamWaitEvent(stream3, event2);
cudaFreeAsync(ptr, stream3);


The user can free allocations allocated with cudaMalloc() with cudaFreeAsync(). The user must make the same guarantees about accesses being complete before the free operation begins.

cudaMalloc(&ptr, size);
kernel<<<..., stream>>>(ptr, ...);
cudaFreeAsync(ptr, stream);


The user can free memory allocated with cudaMallocAsync with cudaFree(). When freeing such allocations through the cudaFree() API, the driver assumes that all accesses to the allocation are complete and performs no further synchronization. The user can use cudaStreamQuery / cudaStreamSynchronize / cudaEventQuery / cudaEventSynchronize / cudaDeviceSynchronize to guarantee that the appropriate asynchronous work is complete and that the GPU will not try to access the allocation.

cudaMallocAsync(&ptr, size,stream);
kernel<<<..., stream>>>(ptr, ...);
// synchronize is needed to avoid prematurely freeing the memory
cudaStreamSynchronize(stream);
cudaFree(ptr);





11.4. Memory Pools and the cudaMemPool_tï

Memory pools encapsulate virtual address and physical memory resources that are allocated and managed according to the pools attributes and properties. The primary aspect of a memory pool is the kind and location of memory it manages.
All calls to cudaMallocAsync use the resources of a memory pool. In the absence of a specified memory pool, cudaMallocAsync uses the current memory pool of the supplied streamâs device. The current memory pool for a device may be set with cudaDeviceSetMempool and queried with cudaDeviceGetMempool. By default (in the absence of a cudaDeviceSetMempool call), the current memory pool is the default memory pool of a device. The API cudaMallocFromPoolAsync and c++ overloads of cudaMallocAsync allow a user to specify the pool to be used for an allocation without setting it as the current pool. The APIs cudaDeviceGetDefaultMempool and cudaMemPoolCreate give users handles to memory pools.

Note
The mempool current to a device will be local to that device. So allocating without specifying a memory pool will always yield an allocation local to the streamâs device.


Note
cudaMemPoolSetAttribute and cudaMemPoolGetAttribute control the attributes of the memory pools.




11.5. Default/Implicit Poolsï

The default memory pool of a device may be retrieved with the cudaDeviceGetDefaultMempool API. Allocations from the default memory pool of a device are non-migratable device allocation located on that device. These allocations will always be accessible from that device. The accessibility of the default memory pool may be modified with cudaMemPoolSetAccess and queried by cudaMemPoolGetAccess. Since the default pools do not need to be explicitly created, they are sometimes referred to as implicit pools. The default memory pool of a device does not support IPC.



11.6. Explicit Poolsï

The API cudaMemPoolCreate creates an explicit pool. This allows applications to request properties for their allocation beyond what is provided by the default/implict pools. These include properties such as IPC capability, maximum pool size, allocations resident on a specific CPU NUMA node on supported platforms etc.

// create a pool similar to the implicit pool on device 0
int device = 0;
cudaMemPoolProps poolProps = { };
poolProps.allocType = cudaMemAllocationTypePinned;
poolProps.location.id = device;
poolProps.location.type = cudaMemLocationTypeDevice;

cudaMemPoolCreate(&memPool, &poolProps));


The following code snippet illustrates an example of creating an IPC capable memory pool on a valid CPU NUMA node.

// create a pool resident on a CPU NUMA node that is capable of IPC sharing (via a file descriptor).
int cpu_numa_id = 0;
cudaMemPoolProps poolProps = { };
poolProps.allocType = cudaMemAllocationTypePinned;
poolProps.location.id = cpu_numa_id;
poolProps.location.type = cudaMemLocationTypeHostNuma;
poolProps.handleType = cudaMemHandleTypePosixFileDescriptor;

cudaMemPoolCreate(&ipcMemPool, &poolProps));





11.7. Physical Page Caching Behaviorï

By default, the allocator tries to minimize the physical memory owned by a pool. To minimize the OS calls to allocate and free physical memory, applications must configure a memory footprint for each pool. Applications can do this with the release threshold attribute (cudaMemPoolAttrReleaseThreshold).
The release threshold is the amount of memory in bytes a pool should hold onto before trying to release memory back to the OS. When more than the release threshold bytes of memory are held by the memory pool, the allocator will try to release memory back to the OS on the next call to stream, event or device synchronize. Setting the release threshold to UINT64_MAX will prevent the driver from attempting to shrink the pool after every synchronization.

Cuuint64_t setVal = UINT64_MAX;
cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReleaseThreshold, &setVal);


Applications that set cudaMemPoolAttrReleaseThreshold high enough to effectively disable memory pool shrinking may wish to explicitly shrink a memory poolâs memory footprint. cudaMemPoolTrimTo allows such applications to do so. When trimming a memory poolâs footprint, the minBytesToKeep parameter allows an application to hold onto an amount of memory it expects to need in a subsequent phase of execution.

Cuuint64_t setVal = UINT64_MAX;
cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReleaseThreshold, &setVal);

// application phase needing a lot of memory from the stream ordered allocator
for (i=0; i<10; i++) {
    for (j=0; j<10; j++) {
        cudaMallocAsync(&ptrs[j],size[j], stream);
    }
    kernel<<<...,stream>>>(ptrs,...);
    for (j=0; j<10; j++) {
        cudaFreeAsync(ptrs[j], stream);
    }
}

// Process does not need as much memory for the next phase.
// Synchronize so that the trim operation will know that the allocations are no
// longer in use.
cudaStreamSynchronize(stream);
cudaMemPoolTrimTo(mempool, 0);

// Some other process/allocation mechanism can now use the physical memory
// released by the trimming operation.





11.8. Resource Usage Statisticsï

In CUDA 11.3, the pool attributes cudaMemPoolAttrReservedMemCurrent, cudaMemPoolAttrReservedMemHigh, cudaMemPoolAttrUsedMemCurrent, and cudaMemPoolAttrUsedMemHigh were added to query the memory usage of a pool.
Querying the cudaMemPoolAttrReservedMemCurrent attribute of a pool reports the current total physical GPU memory consumed by the pool. Querying the cudaMemPoolAttrUsedMemCurrent of a pool returns the total size of all of the memory allocated from the pool and not available for reuse.
ThecudaMemPoolAttr*MemHigh attributes are watermarks recording the max value achieved by the respective cudaMemPoolAttr*MemCurrent attribute since last reset. They can be reset to the current value by using the cudaMemPoolSetAttribute API.

// sample helper functions for getting the usage statistics in bulk
struct usageStatistics {
    cuuint64_t reserved;
    cuuint64_t reservedHigh;
    cuuint64_t used;
    cuuint64_t usedHigh;
};

void getUsageStatistics(cudaMemoryPool_t memPool, struct usageStatistics *statistics)
{
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrReservedMemCurrent, statistics->reserved);
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrReservedMemHigh, statistics->reservedHigh);
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrUsedMemCurrent, statistics->used);
    cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrUsedMemHigh, statistics->usedHigh);
}


// resetting the watermarks will make them take on the current value.
void resetStatistics(cudaMemoryPool_t memPool)
{
    cuuint64_t value = 0;
    cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReservedMemHigh, &value);
    cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrUsedMemHigh, &value);
}





11.9. Memory Reuse Policiesï

In order to service an allocation request, the driver attempts to reuse memory that was previously freed via cudaFreeAsync() before attempting to allocate more memory from the OS. For example, memory freed in a stream can immediately be reused for a subsequent allocation request in the same stream. Similarly, when a stream is synchronized with the CPU, the memory that was previously freed in that stream becomes available for reuse for an allocation in any stream.
The stream ordered allocator has a few controllable allocation policies. The pool attributes cudaMemPoolReuseFollowEventDependencies, cudaMemPoolReuseAllowOpportunistic, and cudaMemPoolReuseAllowInternalDependencies control these policies. Upgrading to a newer CUDA driver may change, enhance, augment and/or reorder the reuse policies.


11.9.1. cudaMemPoolReuseFollowEventDependenciesï

Before allocating more physical GPU memory, the allocator examines dependency information established by CUDA events and tries to allocate from memory freed in another stream.

cudaMallocAsync(&ptr, size, originalStream);
kernel<<<..., originalStream>>>(ptr, ...);
cudaFreeAsync(ptr, originalStream);
cudaEventRecord(event,originalStream);

// waiting on the event that captures the free in another stream
// allows the allocator to reuse the memory to satisfy
// a new allocation request in the other stream when
// cudaMemPoolReuseFollowEventDependencies is enabled.
cudaStreamWaitEvent(otherStream, event);
cudaMallocAsync(&ptr2, size, otherStream);





11.9.2. cudaMemPoolReuseAllowOpportunisticï

According to the cudaMemPoolReuseAllowOpportunistic policy, the allocator examines freed allocations to see if the freeâs stream order semantic has been met (such as the stream has passed the point of execution indicated by the free). When this is disabled, the allocator will still reuse memory made available when a stream is synchronized with the CPU. Disabling this policy does not stop the cudaMemPoolReuseFollowEventDependencies from applying.

cudaMallocAsync(&ptr, size, originalStream);
kernel<<<..., originalStream>>>(ptr, ...);
cudaFreeAsync(ptr, originalStream);


// after some time, the kernel finishes running
wait(10);

// When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request
// can be fulfilled with the prior allocation based on the progress of originalStream.
cudaMallocAsync(&ptr2, size, otherStream);





11.9.3. cudaMemPoolReuseAllowInternalDependenciesï

Failing to allocate and map more physical memory from the OS, the driver will look for memory whose availability depends on another streamâs pending progress. If such memory is found, the driver will insert the required dependency into the allocating stream and reuse the memory.

cudaMallocAsync(&ptr, size, originalStream);
kernel<<<..., originalStream>>>(ptr, ...);
cudaFreeAsync(ptr, originalStream);

// When cudaMemPoolReuseAllowInternalDependencies is enabled
// and the driver fails to allocate more physical memory, the driver may
// effectively perform a cudaStreamWaitEvent in the allocating stream
// to make sure that future work in âotherStreamâ happens after the work
// in the original stream that would be allowed to access the original allocation.
cudaMallocAsync(&ptr2, size, otherStream);





11.9.4. Disabling Reuse Policiesï

While the controllable reuse policies improve memory reuse, users may want to disable them. Allowing opportunistic reuse (such as cudaMemPoolReuseAllowOpportunistic) introduces run to run variance in allocation patterns based on the interleaving of CPU and GPU execution. Internal dependency insertion (such as cudaMemPoolReuseAllowInternalDependencies) can serialize work in unexpected and potentially non-deterministic ways when the user would rather explicitly synchronize an event or stream on allocation failure.




11.10. Device Accessibility for Multi-GPU Supportï

Just like allocation accessibility controlled through the virtual memory management APIs, memory pool allocation accessibility does not follow cudaDeviceEnablePeerAccess or cuCtxEnablePeerAccess. Instead, the API cudaMemPoolSetAccess modifies what devices can access allocations from a pool. By default, allocations are accessible from the device where the allocations are located. This access cannot be revoked. To enable access from other devices, the accessing device must be peer capable with the memory poolâs device; check with cudaDeviceCanAccessPeer. If the peer capability is not checked, the set access may fail with cudaErrorInvalidDevice. If no allocations had been made from the pool, the cudaMemPoolSetAccess call may succeed even when the devices are not peer capable; in this case, the next allocation from the pool will fail.
It is worth noting that cudaMemPoolSetAccess affects all allocations from the memory pool, not just future ones. Also the accessibility reported by cudaMemPoolGetAccess applies to all allocations from the pool, not just future ones. It is recommended that the accessibility settings of a pool for a given GPU not be changed frequently; once a pool is made accessible from a given GPU, it should remain accessible from that GPU for the lifetime of the pool.

// snippet showing usage of cudaMemPoolSetAccess:
cudaError_t setAccessOnDevice(cudaMemPool_t memPool, int residentDevice,
              int accessingDevice) {
    cudaMemAccessDesc accessDesc = {};
    accessDesc.location.type = cudaMemLocationTypeDevice;
    accessDesc.location.id = accessingDevice;
    accessDesc.flags = cudaMemAccessFlagsProtReadWrite;

    int canAccess = 0;
    cudaError_t error = cudaDeviceCanAccessPeer(&canAccess, accessingDevice,
              residentDevice);
    if (error != cudaSuccess) {
        return error;
    } else if (canAccess == 0) {
        return cudaErrorPeerAccessUnsupported;
    }

    // Make the address accessible
    return cudaMemPoolSetAccess(memPool, &accessDesc, 1);
}





11.11. IPC Memory Poolsï

IPC capable memory pools allow easy, efficient and secure sharing of GPU memory between processes. CUDAâs IPC memory pools provide the same security benefits as CUDAâs virtual memory management APIs.
There are two phases to sharing memory between processes with memory pools. The processes first need to share access to the pool, then share specific allocations from that pool. The first phase establishes and enforces security. The second phase coordinates what virtual addresses are used in each process and when mappings need to be valid in the importing process.


11.11.1. Creating and Sharing IPC Memory Poolsï

Sharing access to a pool involves retrieving an OS native handle to the pool (with the cudaMemPoolExportToShareableHandle() API), transferring the handle to the importing process using the usual OS native IPC mechanisms, and creating an imported memory pool (with the cudaMemPoolImportFromShareableHandle() API). For cudaMemPoolExportToShareableHandle to succeed, the memory pool had to be created with the requested handle type specified in the pool properties structure. Please reference samples for the appropriate IPC mechanisms to transfer the OS native handle between processes. The rest of the procedure can be found in the following code snippets.

// in exporting process
// create an exportable IPC capable pool on device 0
cudaMemPoolProps poolProps = { };
poolProps.allocType = cudaMemAllocationTypePinned;
poolProps.location.id = 0;
poolProps.location.type = cudaMemLocationTypeDevice;

// Setting handleTypes to a non zero value will make the pool exportable (IPC capable)
poolProps.handleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR;

cudaMemPoolCreate(&memPool, &poolProps));

// FD based handles are integer types
int fdHandle = 0;


// Retrieve an OS native handle to the pool.
// Note that a pointer to the handle memory is passed in here.
cudaMemPoolExportToShareableHandle(&fdHandle,
             memPool,
             CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR,
             0);

// The handle must be sent to the importing process with the appropriate
// OS specific APIs.



// in importing process
 int fdHandle;
// The handle needs to be retrieved from the exporting process with the
// appropriate OS specific APIs.
// Create an imported pool from the shareable handle.
// Note that the handle is passed by value here.
cudaMemPoolImportFromShareableHandle(&importedMemPool,
          (void*)fdHandle,
          CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR,
          0);





11.11.2. Set Access in the Importing Processï

Imported memory pools are initially only accessible from their resident device. The imported memory pool does not inherit any accessibility set by the exporting process. The importing process needs to enable access (with cudaMemPoolSetAccess) from any GPU it plans to access the memory from.
If the imported memory pool belongs to a non-visible device in the importing process, the user must use the cudaMemPoolSetAccess API to enable access from the GPUs the allocations will be used on.



11.11.3. Creating and Sharing Allocations from an Exported Poolï

Once the pool has been shared, allocations made with cudaMallocAsync() from the pool in the exporting process can be shared with other processes that have imported the pool. Since the poolâs security policy is established and verified at the pool level, the OS does not need extra bookkeeping to provide security for specific pool allocations; In other words, the opaque cudaMemPoolPtrExportData required to import a pool allocation may be sent to the importing process using any mechanism.
While allocations may be exported and even imported without synchronizing with the allocating stream in any way, the importing process must follow the same rules as the exporting process when accessing the allocation. Namely, access to the allocation must happen after the stream ordering of the allocation operation in the allocating stream. The two following code snippets show cudaMemPoolExportPointer() and cudaMemPoolImportPointer() sharing the allocation with an IPC event used to guarantee that the allocation isnât accessed in the importing process before the allocation is ready.

// preparing an allocation in the exporting process
cudaMemPoolPtrExportData exportData;
cudaEvent_t readyIpcEvent;
cudaIpcEventHandle_t readyIpcEventHandle;

// ipc event for coordinating between processes
// cudaEventInterprocess flag makes the event an ipc event
// cudaEventDisableTiming  is set for performance reasons

cudaEventCreate(
        &readyIpcEvent, cudaEventDisableTiming | cudaEventInterprocess)

// allocate from the exporting mem pool
cudaMallocAsync(&ptr, size,exportMemPool, stream);

// event for sharing when the allocation is ready.
cudaEventRecord(readyIpcEvent, stream);
cudaMemPoolExportPointer(&exportData, ptr);
cudaIpcGetEventHandle(&readyIpcEventHandle, readyIpcEvent);

// Share IPC event and pointer export data with the importing process using
//  any mechanism. Here we copy the data into shared memory
shmem->ptrData = exportData;
shmem->readyIpcEventHandle = readyIpcEventHandle;
// signal consumers data is ready



// Importing an allocation
cudaMemPoolPtrExportData *importData = &shmem->prtData;
cudaEvent_t readyIpcEvent;
cudaIpcEventHandle_t *readyIpcEventHandle = &shmem->readyIpcEventHandle;

// Need to retrieve the ipc event handle and the export data from the
// exporting process using any mechanism.  Here we are using shmem and just
// need synchronization to make sure the shared memory is filled in.

cudaIpcOpenEventHandle(&readyIpcEvent, readyIpcEventHandle);

// import the allocation. The operation does not block on the allocation being ready.
cudaMemPoolImportPointer(&ptr, importedMemPool, importData);

// Wait for the prior stream operations in the allocating stream to complete before
// using the allocation in the importing process.
cudaStreamWaitEvent(stream, readyIpcEvent);
kernel<<<..., stream>>>(ptr, ...);


When freeing the allocation, the allocation needs to be freed in the importing process before it is freed in the exporting process. The following code snippet demonstrates the use of CUDA IPC events to provide the required synchronization between the cudaFreeAsync operations in both processes. Access to the allocation from the importing process is obviously restricted by the free operation in the importing process side. It is worth noting that cudaFree can be used to free the allocation in both processes and that other stream synchronization APIs may be used instead of CUDA IPC events.

// The free must happen in importing process before the exporting process
kernel<<<..., stream>>>(ptr, ...);

// Last access in importing process
cudaFreeAsync(ptr, stream);

// Access not allowed in the importing process after the free
cudaIpcEventRecord(finishedIpcEvent, stream);



// Exporting process
// The exporting process needs to coordinate its free with the stream order
// of the importing processâs free.
cudaStreamWaitEvent(stream, finishedIpcEvent);
kernel<<<..., stream>>>(ptrInExportingProcess, ...);

// The free in the importing process doesnât stop the exporting process
// from using the allocation.
cudFreeAsync(ptrInExportingProcess,stream);





11.11.4. IPC Export Pool Limitationsï

IPC pools currently do not support releasing physical blocks back to the OS. As a result the cudaMemPoolTrimTo API acts as a no-op and the cudaMemPoolAttrReleaseThreshold effectively gets ignored. This behavior is controlled by the driver, not the runtime and may change in a future driver update.



11.11.5. IPC Import Pool Limitationsï

Allocating from an import pool is not allowed; specifically, import pools cannot be set current and cannot be used in the cudaMallocFromPoolAsync API. As such, the allocation reuse policy attributes are meaningless for these pools.
IPC pools currently do not support releasing physical blocks back to the OS. As a result the cudaMemPoolTrimTo API acts as a no-op and the cudaMemPoolAttrReleaseThreshold effectively gets ignored.
The resource usage stat attribute queries only reflect the allocations imported into the process and the associated physical memory.




11.12. Synchronization API Actionsï

One of the optimizations that comes with the allocator being part of the CUDA driver is integration with the synchronize APIs. When the user requests that the CUDA driver synchronize, the driver waits for asynchronous work to complete. Before returning, the driver will determine what frees the synchronization guaranteed to be completed. These allocations are made available for allocation regardless of specified stream or disabled allocation policies. The driver also checks cudaMemPoolAttrReleaseThreshold here and releases any excess physical memory that it can.



11.13. Addendumsï



11.13.1. cudaMemcpyAsync Current Context/Device Sensitivityï

In the current CUDA driver, any async memcpy involving memory from cudaMallocAsync should be done using the specified streamâs context as the calling threadâs current context. This is not necessary for cudaMemcpyPeerAsync, as the device primary contexts specified in the API are referenced instead of the current context.



11.13.2. cuPointerGetAttribute Queryï

Invoking cuPointerGetAttribute on an allocation after invoking cudaFreeAsync on it results in undefined behavior. Specifically, it does not matter if an allocation is still accessible from a given stream: the behavior is still undefined.



11.13.3. cuGraphAddMemsetNodeï

cuGraphAddMemsetNode does not work with memory allocated via the stream ordered allocator. However, memsets of the allocations can be stream captured.



11.13.4. Pointer Attributesï

The cuPointerGetAttributes query works on stream ordered allocations. Since stream ordered allocations are not context associated, querying CU_POINTER_ATTRIBUTE_CONTEXT will succeed but return NULL in *data. The attribute CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL can be used to determine the location of the allocation: this can be useful when selecting a context for making p2h2p copies using cudaMemcpyPeerAsync. The attribute CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE was added in CUDA 11.3 and can be useful for debugging and for confirming which pool an allocation comes from before doing IPC.





12. Graph Memory Nodesï



12.1. Introductionï

Graph memory nodes allow graphs to create and own memory allocations. Graph memory nodes have GPU ordered lifetime semantics, which dictate when memory is allowed to be accessed on the device. These GPU ordered lifetime semantics enable driver-managed memory reuse, and match those of the stream ordered allocation APIs cudaMallocAsync and cudaFreeAsync, which may be captured when creating a graph.
Graph allocations have fixed addresses over the life of a graph including repeated instantiations and launches. This allows the memory to be directly referenced by other operations within the graph without the need of a graph update, even when CUDA changes the backing physical memory. Within a graph, allocations whose graph ordered lifetimes do not overlap may use the same underlying physical memory.
CUDA may reuse the same physical memory for allocations across multiple graphs, aliasing virtual address mappings according to the GPU ordered lifetime semantics. For example when different graphs are launched into the same stream, CUDA may virtually alias the same physical memory to satisfy the needs of allocations which have single-graph lifetimes.



12.2. Support and Compatibilityï

Graph memory nodes require an 11.4 capable CUDA driver and support for the stream ordered allocator on the GPU. The following snippet shows how to check for support on a given device.

int driverVersion = 0;
int deviceSupportsMemoryPools = 0;
int deviceSupportsMemoryNodes = 0;
cudaDriverGetVersion(&driverVersion);
if (driverVersion >= 11020) { // avoid invalid value error in cudaDeviceGetAttribute
    cudaDeviceGetAttribute(&deviceSupportsMemoryPools, cudaDevAttrMemoryPoolsSupported, device);
}
deviceSupportsMemoryNodes = (driverVersion >= 11040) && (deviceSupportsMemoryPools != 0);


Doing the attribute query inside the driver version check avoids an invalid value return code on 11.0 and 11.1 drivers. Be aware that the compute sanitizer emits warnings when it detects CUDA returning error codes, and a version check before reading the attribute will avoid this. Graph memory nodes are only supported on driver versions 11.4 and newer.



12.3. API Fundamentalsï

Graph memory nodes are graph nodes representing either memory allocation or free actions. As a shorthand, nodes that allocate memory are called allocation nodes. Likewise, nodes that free memory are called free nodes. Allocations created by allocation nodes are called graph allocations. CUDA assigns virtual addresses for the graph allocation at node creation time. While these virtual addresses are fixed for the lifetime of the allocation node, the allocation contents are not persistent past the freeing operation and may be overwritten by accesses referring to a different allocation.
Graph allocations are considered recreated every time a graph runs. A graph allocationâs lifetime, which differs from the nodeâs lifetime, begins when GPU execution reaches the allocating graph node and ends when one of the following occurs:

GPU execution reaches the freeing graph node
GPU execution reaches the freeing cudaFreeAsync() stream call
immediately upon the freeing call to cudaFree()


Note
Graph destruction does not automatically free any live graph-allocated memory, even though it ends the lifetime of the allocation node. The allocation must subsequently be freed in another graph, or using cudaFreeAsync()/cudaFree().

Just like other graph nodes, graph memory nodes are ordered within a graph by dependency edges. A program must guarantee that operations accessing graph memory:

are ordered after the allocation node
are ordered before the operation freeing the memory

Graph allocation lifetimes begin and usually end according to GPU execution (as opposed to API invocation). GPU ordering is the order that work runs on the GPU as opposed to the order that the work is enqueued or described. Thus, graph allocations are considered âGPU ordered.â


12.3.1. Graph Node APIsï

Graph memory nodes may be explicitly created with the memory node creation APIs, cudaGraphAddMemAllocNode and cudaGraphAddMemFreeNode. The address allocated by cudaGraphAddMemAllocNode is returned to the user in the dptr field of the passed CUDA_MEM_ALLOC_NODE_PARAMS structure. All operations using graph allocations inside the allocating graph must be ordered after the allocating node. Similarly, any free nodes must be ordered after all uses of the allocation within the graph. cudaGraphAddMemFreeNode creates free nodes.
In the following figure, there is an example graph with an alloc and a free node. Kernel nodes a, b, and c are ordered after the allocation node and before the free node such that the kernels can access the allocation. Kernel node e is not ordered after the alloc node and therefore cannot safely access the memory. Kernel node d is not ordered before the free node, therefore it cannot safely access the memory.



Figure 28 Kernel Nodesï


The following code snippet establishes the graph in this figure:

// Create the graph - it starts out empty
cudaGraphCreate(&graph, 0);

// parameters for a basic allocation
cudaMemAllocNodeParams params = {};
params.poolProps.allocType = cudaMemAllocationTypePinned;
params.poolProps.location.type = cudaMemLocationTypeDevice;
// specify device 0 as the resident device
params.poolProps.location.id = 0;
params.bytesize = size;

cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, &params);
nodeParams->kernelParams[0] = params.dptr;
cudaGraphAddKernelNode(&a, graph, &allocNode, 1, &nodeParams);
cudaGraphAddKernelNode(&b, graph, &a, 1, &nodeParams);
cudaGraphAddKernelNode(&c, graph, &a, 1, &nodeParams);
cudaGraphNode_t dependencies[2];
// kernel nodes b and c are using the graph allocation, so the freeing node must depend on them.  Since the dependency of node b on node a establishes an indirect dependency, the free node does not need to explicitly depend on node a.
dependencies[0] = b;
dependencies[1] = c;
cudaGraphAddMemFreeNode(&freeNode, graph, dependencies, 2, params.dptr);
// free node does not depend on kernel node d, so it must not access the freed graph allocation.
cudaGraphAddKernelNode(&d, graph, &c, 1, &nodeParams);

// node e does not depend on the allocation node, so it must not access the allocation.  This would be true even if the freeNode depended on kernel node e.
cudaGraphAddKernelNode(&e, graph, NULL, 0, &nodeParams);





12.3.2. Stream Captureï

Graph memory nodes can be created by capturing the corresponding stream ordered allocation and free calls cudaMallocAsync and cudaFreeAsync. In this case, the virtual addresses returned by the captured allocation API can be used by other operations inside the graph. Since the stream ordered dependencies will be captured into the graph, the ordering requirements of the stream ordered allocation APIs guarantee that the graph memory nodes will be properly ordered with respect to the captured stream operations (for correctly written stream code).
Ignoring kernel nodes d and e, for clarity, the following code snippet shows how to use stream capture to create the graph from the previous figure:

cudaMallocAsync(&dptr, size, stream1);
kernel_A<<< ..., stream1 >>>(dptr, ...);

// Fork into stream2
cudaEventRecord(event1, stream1);
cudaStreamWaitEvent(stream2, event1);

kernel_B<<< ..., stream1 >>>(dptr, ...);
// event dependencies translated into graph dependencies, so the kernel node created by the capture of kernel C will depend on the allocation node created by capturing the cudaMallocAsync call.
kernel_C<<< ..., stream2 >>>(dptr, ...);

// Join stream2 back to origin stream (stream1)
cudaEventRecord(event2, stream2);
cudaStreamWaitEvent(stream1, event2);

// Free depends on all work accessing the memory.
cudaFreeAsync(dptr, stream1);

// End capture in the origin stream
cudaStreamEndCapture(stream1, &graph);





12.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graphï

Graph allocations do not have to be freed by the allocating graph. When a graph does not free an allocation, that allocation persists beyond the execution of the graph and can be accessed by subsequent CUDA operations. These allocations may be accessed in another graph or directly using a stream operation as long as the accessing operation is ordered after the allocation through CUDA events and other stream ordering mechanisms. An allocation may subsequently be freed by regular calls to cudaFree, cudaFreeAsync, or by the launch of another graph with a corresponding free node, or a subsequent launch of the allocating graph (if it was instantiated with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag). It is illegal to access memory after it has been freed - the free operation must be ordered after all operations accessing the memory using graph dependencies, CUDA events, and other stream ordering mechanisms.

Note
Because graph allocations may share underlying physical memory with each other, the Virtual Aliasing Support rules relating to consistency and coherency must be considered. Simply put, the free operation must be ordered after the full device operation (for example, compute kernel / memcpy) completes. Specifically, out of band synchronization - for example a handshake through memory as part of a compute kernel that accesses the graph-allocated memory - is not sufficient for providing ordering guarantees between the memory writes to graph memory and the free operation of that graph memory.

The following code snippets demonstrate accessing graph allocations outside of the allocating graph with ordering properly established by: using a single stream, using events between streams, and using events baked into the allocating and freeing graph.
Ordering established by using a single stream:

void *dptr;
cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, &params);
dptr = params.dptr;

cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0);

cudaGraphLaunch(allocGraphExec, stream);
kernel<<< â¦, stream >>>(dptr, â¦);
cudaFreeAsync(dptr, stream);


Ordering established by recording and waiting on CUDA events:

void *dptr;

// Contents of allocating graph
cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, &params);
dptr = params.dptr;

// contents of consuming/freeing graph
nodeParams->kernelParams[0] = params.dptr;
cudaGraphAddKernelNode(&a, graph, NULL, 0, &nodeParams);
cudaGraphAddMemFreeNode(&freeNode, freeGraph, &a, 1, dptr);

cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0);
cudaGraphInstantiate(&freeGraphExec, freeGraph, NULL, NULL, 0);

cudaGraphLaunch(allocGraphExec, allocStream);

// establish the dependency of stream2 on the allocation node
// note: the dependency could also have been established with a stream synchronize operation
cudaEventRecord(allocEvent, allocStream)
cudaStreamWaitEvent(stream2, allocEvent);

kernel<<< â¦, stream2 >>> (dptr, â¦);

// establish the dependency between the stream 3 and the allocation use
cudaStreamRecordEvent(streamUseDoneEvent, stream2);
cudaStreamWaitEvent(stream3, streamUseDoneEvent);

// it is now safe to launch the freeing graph, which may also access the memory
cudaGraphLaunch(freeGraphExec, stream3);


Ordering established by using graph external event nodes:

void *dptr;
cudaEvent_t allocEvent; // event indicating when the allocation will be ready for use.
cudaEvent_t streamUseDoneEvent; // event indicating when the stream operations are done with the allocation.

// Contents of allocating graph with event record node
cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, &params);
dptr = params.dptr;
// note: this event record node depends on the alloc node
cudaGraphAddEventRecordNode(&recordNode, allocGraph, &allocNode, 1, allocEvent);
cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0);

// contents of consuming/freeing graph with event wait nodes
cudaGraphAddEventWaitNode(&streamUseDoneEventNode, waitAndFreeGraph, NULL, 0, streamUseDoneEvent);
cudaGraphAddEventWaitNode(&allocReadyEventNode, waitAndFreeGraph, NULL, 0, allocEvent);
nodeParams->kernelParams[0] = params.dptr;

// The allocReadyEventNode provides ordering with the alloc node for use in a consuming graph.
cudaGraphAddKernelNode(&kernelNode, waitAndFreeGraph, &allocReadyEventNode, 1, &nodeParams);

// The free node has to be ordered after both external and internal users.
// Thus the node must depend on both the kernelNode and the
// streamUseDoneEventNode.
dependencies[0] = kernelNode;
dependencies[1] = streamUseDoneEventNode;
cudaGraphAddMemFreeNode(&freeNode, waitAndFreeGraph, &dependencies, 2, dptr);
cudaGraphInstantiate(&waitAndFreeGraphExec, waitAndFreeGraph, NULL, NULL, 0);

cudaGraphLaunch(allocGraphExec, allocStream);

// establish the dependency of stream2 on the event node satisfies the ordering requirement
cudaStreamWaitEvent(stream2, allocEvent);
kernel<<< â¦, stream2 >>> (dptr, â¦);
cudaStreamRecordEvent(streamUseDoneEvent, stream2);

// the event wait node in the waitAndFreeGraphExec establishes the dependency on the âreadyForFreeEventâ that is needed to prevent the kernel running in stream two from accessing the allocation after the free node in execution order.
cudaGraphLaunch(waitAndFreeGraphExec, stream3);





12.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunchï

Under normal circumstances, CUDA will prevent a graph from being relaunched if it has unfreed memory allocations because multiple allocations at the same address will leak memory. Instantiating a graph with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag allows the graph to be relaunched while it still has unfreed allocations. In this case, the launch automatically inserts an asynchronous free of the unfreed allocations.
Auto free on launch is useful for single-producer multiple-consumer algorithms. At each iteration, a producer graph creates several allocations, and, depending on runtime conditions, a varying set of consumers accesses those allocations. This type of variable execution sequence means that consumers cannot free the allocations because a subsequent consumer may require access. Auto free on launch means that the launch loop does not need to track the producerâs allocations - instead, that information remains isolated to the producerâs creation and destruction logic. In general, auto free on launch simplifies an algorithm which would otherwise need to free all the allocations owned by a graph before each relaunch.

Note
The cudaGraphInstantiateFlagAutoFreeOnLaunch flag does not change the behavior of graph destruction. The application must explicitly free the unfreed memory in order to avoid memory leaks, even for graphs instantiated with the flag.
The following code shows the use of cudaGraphInstantiateFlagAutoFreeOnLaunch to simplify a single-producer / multiple-consumer algorithm:


// Create producer graph which allocates memory and populates it with data
cudaStreamBeginCapture(cudaStreamPerThread, cudaStreamCaptureModeGlobal);
cudaMallocAsync(&data1, blocks * threads, cudaStreamPerThread);
cudaMallocAsync(&data2, blocks * threads, cudaStreamPerThread);
produce<<<blocks, threads, 0, cudaStreamPerThread>>>(data1, data2);
...
cudaStreamEndCapture(cudaStreamPerThread, &graph);
cudaGraphInstantiateWithFlags(&producer,
                              graph,
                              cudaGraphInstantiateFlagAutoFreeOnLaunch);
cudaGraphDestroy(graph);

// Create first consumer graph by capturing an asynchronous library call
cudaStreamBeginCapture(cudaStreamPerThread, cudaStreamCaptureModeGlobal);
consumerFromLibrary(data1, cudaStreamPerThread);
cudaStreamEndCapture(cudaStreamPerThread, &graph);
cudaGraphInstantiateWithFlags(&consumer1, graph, 0); //regular instantiation
cudaGraphDestroy(graph);

// Create second consumer graph
cudaStreamBeginCapture(cudaStreamPerThread, cudaStreamCaptureModeGlobal);
consume2<<<blocks, threads, 0, cudaStreamPerThread>>>(data2);
...
cudaStreamEndCapture(cudaStreamPerThread, &graph);
cudaGraphInstantiateWithFlags(&consumer2, graph, 0);
cudaGraphDestroy(graph);

// Launch in a loop
bool launchConsumer2 = false;
do {
    cudaGraphLaunch(producer, myStream);
    cudaGraphLaunch(consumer1, myStream);
    if (launchConsumer2) {
        cudaGraphLaunch(consumer2, myStream);
    }
} while (determineAction(&launchConsumer2));

cudaFreeAsync(data1, myStream);
cudaFreeAsync(data2, myStream);

cudaGraphExecDestroy(producer);
cudaGraphExecDestroy(consumer1);
cudaGraphExecDestroy(consumer2);






12.4. Optimized Memory Reuseï

CUDA reuses memory in two ways:

Virtual and physical memory reuse within a graph is based on virtual address assignment, like in the stream ordered allocator.
Physical memory reuse between graphs is done with virtual aliasing: different graphs can map the same physical memory to their unique virtual addresses.



12.4.1. Address Reuse within a Graphï

CUDA may reuse memory within a graph by assigning the same virtual address ranges to different allocations whose lifetimes do not overlap. Since virtual addresses may be reused, pointers to different allocations with disjoint lifetimes are not guaranteed to be unique.
The following figure shows adding a new allocation node (2) that can reuse the address freed by a dependent node (1).



Figure 29 Adding New Alloc Node 2ï

The following figure shows adding a new alloc node (4). The new alloc node is not dependent on the free node (2) so cannot reuse the address from the associated alloc node (2). If the alloc node (2) used the address freed by free node (1), the new alloc node 3 would need a new address.






Figure 30 Adding New Alloc Node 3ï





12.4.2. Physical Memory Management and Sharingï

CUDA is responsible for mapping physical memory to the virtual address before the allocating node is reached in GPU order. As an optimization for memory footprint and mapping overhead, multiple graphs may use the same physical memory for distinct allocations if they will not run simultaneously; however, physical pages cannot be reused if they are bound to more than one executing graph at the same time, or to a graph allocation which remains unfreed.
CUDA may update physical memory mappings at any time during graph instantiation, launch, or execution. CUDA may also introduce synchronization between future graph launches in order to prevent live graph allocations from referring to the same physical memory. As for any allocate-free-allocate pattern, if a program accesses a pointer outside of an allocationâs lifetime, the erroneous access may silently read or write live data owned by another allocation (even if the virtual address of the allocation is unique). Use of compute sanitizer tools can catch this error.
The following figure shows graphs sequentially launched in the same stream. In this example, each graph frees all the memory it allocates. Since the graphs in the same stream never run concurrently, CUDA can and should use the same physical memory to satisfy all the allocations.



Figure 31 Sequentially Launched Graphsï






12.5. Performance Considerationsï

When multiple graphs are launched into the same stream, CUDA attempts to allocate the same physical memory to them because the execution of these graphs cannot overlap. Physical mappings for a graph are retained between launches as an optimization to avoid the cost of remapping. If, at a later time, one of the graphs is launched such that its execution may overlap with the others (for example if it is launched into a different stream) then CUDA must perform some remapping because concurrent graphs require distinct memory to avoid data corruption.
In general, remapping of graph memory in CUDA is likely caused by these operations:

Changing the stream into which a graph is launched
A trim operation on the graph memory pool, which explicitly frees unused memory (discussed in Physical Memory Footprint)
Relaunching a graph while an unfreed allocation from another graph is mapped to the same memory will cause a remap of memory before relaunch

Remapping must happen in execution order, but after any previous execution of that graph is complete (otherwise memory that is still in use could be unmapped). Due to this ordering dependency, as well as because mapping operations are OS calls, mapping operations can be relatively expensive. Applications can avoid this cost by launching graphs containing allocation memory nodes consistently into the same stream.


12.5.1. First Launch / cudaGraphUploadï

Physical memory cannot be allocated or mapped during graph instantiation because the stream in which the graph will execute is unknown. Mapping is done instead during graph launch. Calling cudaGraphUpload can separate out the cost of allocation from the launch by performing all mappings for that graph immediately and associating the graph with the upload stream. If the graph is then launched into the same stream, it will launch without any additional remapping.
Using different streams for graph upload and graph launch behaves similarly to switching streams, likely resulting in remap operations. In addition, unrelated memory pool management is permitted to pull memory from an idle stream, which could negate the impact of the uploads.




12.6. Physical Memory Footprintï

The pool-management behavior of asynchronous allocation means that destroying a graph which contains memory nodes (even if their allocations are free) will not immediately return physical memory to the OS for use by other processes. To explicitly release memory back to the OS, an application should use the cudaDeviceGraphMemTrim API.
cudaDeviceGraphMemTrim will unmap and release any physical memory reserved by graph memory nodes that is not actively in use. Allocations that have not been freed and graphs that are scheduled or running are considered to be actively using the physical memory and will not be impacted. Use of the trim API will make physical memory available to other allocation APIs and other applications or processes, but will cause CUDA to reallocate and remap memory when the trimmed graphs are next launched. Note that cudaDeviceGraphMemTrim operates on a different pool from cudaMemPoolTrimTo(). The graph memory pool is not exposed to the steam ordered memory allocator. CUDA allows applications to query their graph memory footprint through the cudaDeviceGetGraphMemAttribute API. Querying the attribute cudaGraphMemAttrReservedMemCurrent returns the amount of physical memory reserved by the driver for graph allocations in the current process. Querying cudaGraphMemAttrUsedMemCurrent returns the amount of physical memory currently mapped by at least one graph. Either of these attributes can be used to track when new physical memory is acquired by CUDA for the sake of an allocating graph. Both of these attributes are useful for examining how much memory is saved by the sharing mechanism.



12.7. Peer Accessï

Graph allocations can be configured for access from multiple GPUs, in which case CUDA will map the allocations onto the peer GPUs as required. CUDA allows graph allocations requiring different mappings to reuse the same virtual address. When this occurs, the address range is mapped onto all GPUs required by the different allocations. This means an allocation may sometimes allow more peer access than was requested during its creation; however, relying on these extra mappings is still an error.


12.7.1. Peer Access with Graph Node APIsï

The cudaGraphAddMemAllocNode API accepts mapping requests in the accessDescs array field of the node parameters structures. The poolProps.location embedded structure specifies the resident device for the allocation. Access from the allocating GPU is assumed to be needed, thus the application does not need to specify an entry for the resident device in the accessDescs array.

cudaMemAllocNodeParams params = {};
params.poolProps.allocType = cudaMemAllocationTypePinned;
params.poolProps.location.type = cudaMemLocationTypeDevice;
// specify device 1 as the resident device
params.poolProps.location.id = 1;
params.bytesize = size;

// allocate an allocation resident on device 1 accessible from device 1
cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, &params);

accessDescs[2];
// boilerplate for the access descs (only ReadWrite and Device access supported by the add node api)
accessDescs[0].flags = cudaMemAccessFlagsProtReadWrite;
accessDescs[0].location.type = cudaMemLocationTypeDevice;
accessDescs[1].flags = cudaMemAccessFlagsProtReadWrite;
accessDescs[1].location.type = cudaMemLocationTypeDevice;

// access being requested for device 0 & 2.  Device 1 access requirement left implicit.
accessDescs[0].location.id = 0;
accessDescs[1].location.id = 2;

// access request array has 2 entries.
params.accessDescCount = 2;
params.accessDescs = accessDescs;

// allocate an allocation resident on device 1 accessible from devices 0, 1 and 2. (0 & 2 from the descriptors, 1 from it being the resident device).
cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, &params);





12.7.2. Peer Access with Stream Captureï

For stream capture, the allocation node records the peer accessibility of the allocating pool at the time of the capture. Altering the peer accessibility of the allocating pool after a cudaMallocFromPoolAsync call is captured does not affect the mappings that the graph will make for the allocation.

// boilerplate for the access descs (only ReadWrite and Device access supported by the add node api)
accessDesc.flags = cudaMemAccessFlagsProtReadWrite;
accessDesc.location.type = cudaMemLocationTypeDevice;
accessDesc.location.id = 1;

// let memPool be resident and accessible on device 0

cudaStreamBeginCapture(stream);
cudaMallocAsync(&dptr1, size, memPool, stream);
cudaStreamEndCapture(stream, &graph1);

cudaMemPoolSetAccess(memPool, &accessDesc, 1);

cudaStreamBeginCapture(stream);
cudaMallocAsync(&dptr2, size, memPool, stream);
cudaStreamEndCapture(stream, &graph2);

//The graph node allocating dptr1 would only have the device 0 accessibility even though memPool now has device 1 accessibility.
//The graph node allocating dptr2 will have device 0 and device 1 accessibility, since that was the pool accessibility at the time of the cudaMallocAsync call.







13. Mathematical Functionsï

The reference manual lists, along with their description, all the functions of the C/C++ standard library mathematical functions that are supported in device code, as well as all intrinsic functions (that are only supported in device code).
This section provides accuracy information for some of these functions when applicable. It uses ULP for quantification. For further information on the definition of the Unit in the Last Place (ULP), please see Jean-Michel Mullerâs paper On the definition of ulp(x), RR-5504, LIP RR-2005-09, INRIA, LIP. 2005, pp.16 at https://hal.inria.fr/inria-00070503/document.
Mathematical functions supported in device code do not set the global errno variable, nor report any floating-point exceptions to indicate errors; thus, if error diagnostic mechanisms are required, the user should implement additional screening for inputs and outputs of the functions. The user is responsible for the validity of pointer arguments. The user must not pass uninitialized parameters to the Mathematical functions as this may result in undefined behavior: functions are inlined in the user program and thus are subject to compiler optimizations.


13.1. Standard Functionsï

The functions from this section can be used in both host and device code.
This section specifies the error bounds of each function when executed on the device and also when executed on the host in the case where the host does not supply the function.
The error bounds are generated from extensive but not exhaustive tests, so they are not guaranteed bounds.
Single-Precision Floating-Point Functions
Addition and multiplication are IEEE-compliant, so have a maximum error of 0.5 ulp.
The recommended way to round a single-precision floating-point operand to an integer, with the result being a single-precision floating-point number is rintf(), not roundf(). The reason is that roundf() maps to a 4-instruction sequence on the device, whereas rintf() maps to a single instruction. truncf(), ceilf(), and floorf() each map to a single instruction as well.


Table 13 Single-Precision Mathematical Standard Library Functions with Maximum ULP Error. The maximum error is stated as the absolute value of the difference in ulps between the result returned by the CUDA library function and a correctly rounded single-precision result obtained according to the round-to-nearest ties-to-even rounding mode.ï







Function
Maximum ulp error




x+y
0 (IEEE-754 round-to-nearest-even)


x*y
0 (IEEE-754 round-to-nearest-even)


x/y

0 for compute capability \(\ge 2\) when compiled with -prec-div=true
2 (full range), otherwise



1/x

0 for compute capability \(\ge 2\) when compiled with -prec-div=true
1 (full range), otherwise




rsqrtf(x)
1/sqrtf(x)


2 (full range)
Applies to 1/sqrtf(x) only when it is converted to rsqrtf(x) by the compiler.



sqrtf(x)

0 when compiled with -prec-sqrt=true
Otherwise 1 for compute capability \(\ge 5.2\)
and 3 for older architectures



cbrtf(x)
1 (full range)


rcbrtf(x)
1 (full range)


hypotf(x,y)
3 (full range)


rhypotf(x,y)
2 (full range)


norm3df(x,y,z)
3 (full range)


rnorm3df(x,y,z)
2 (full range)


norm4df(x,y,z,t)
3 (full range)


rnorm4df(x,y,z,t)
2 (full range)


normf(dim,arr)
An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off. .


rnormf(dim,arr)
An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off. .


expf(x)
2 (full range)


exp2f(x)
2 (full range)


exp10f(x)
2 (full range)


expm1f(x)
1 (full range)


logf(x)
1 (full range)


log2f(x)
1 (full range)


log10f(x)
2 (full range)


log1pf(x)
1 (full range)


sinf(x)
2 (full range)


cosf(x)
2 (full range)


tanf(x)
4 (full range)


sincosf(x,sptr,cptr)
2 (full range)


sinpif(x)
1 (full range)


cospif(x)
1 (full range)


sincospif(x,sptr,cptr)
1 (full range)


asinf(x)
2 (full range)


acosf(x)
2 (full range)


atanf(x)
2 (full range)


atan2f(y,x)
3 (full range)


sinhf(x)
3 (full range)


coshf(x)
2 (full range)


tanhf(x)
2 (full range)


asinhf(x)
3 (full range)


acoshf(x)
4 (full range)


atanhf(x)
3 (full range)


powf(x,y)
4 (full range)


erff(x)
2 (full range)


erfcf(x)
4 (full range)


erfinvf(x)
2 (full range)


erfcinvf(x)
4 (full range)


erfcxf(x)
4 (full range)


normcdff(x)
5 (full range)


normcdfinvf(x)
5 (full range)


lgammaf(x)
6 (outside interval -10.001 â¦ -2.264; larger inside)


tgammaf(x)
5 (full range)


fmaf(x,y,z)
0 (full range)


frexpf(x,exp)
0 (full range)


ldexpf(x,exp)
0 (full range)


scalbnf(x,n)
0 (full range)


scalblnf(x,l)
0 (full range)


logbf(x)
0 (full range)


ilogbf(x)
0 (full range)


j0f(x)

9 for |x| < 8
otherwise, the maximum absolute error is 2.2 x 10-6



j1f(x)

9 for |x| < 8
otherwise, the maximum absolute error is 2.2 x 10-6



jnf(n,x)
For n = 128, the maximum absolute error is 2.2 x 10-6


y0f(x)

9 for |x| < 8
otherwise, the maximum absolute error is 2.2 x 10-6



y1f(x)

9 for |x| < 8
otherwise, the maximum absolute error is 2.2 x 10-6



ynf(n,x)

ceil(2 + 2.5n) for |x| < n
otherwise, the maximum absolute error is 2.2 x 10-6



cyl_bessel_i0f(x)
6 (full range)


cyl_bessel_i1f(x)
6 (full range)


fmodf(x,y)
0 (full range)


remainderf(x,y)
0 (full range)


remquof(x,y,iptr)
0 (full range)


modff(x,iptr)
0 (full range)


fdimf(x,y)
0 (full range)


truncf(x)
0 (full range)


roundf(x)
0 (full range)


rintf(x)
0 (full range)


nearbyintf(x)
0 (full range)


ceilf(x)
0 (full range)


floorf(x)
0 (full range)


lrintf(x)
0 (full range)


lroundf(x)
0 (full range)


llrintf(x)
0 (full range)


llroundf(x)
0 (full range)



Double-Precision Floating-Point Functions
The recommended way to round a double-precision floating-point operand to an integer, with the result being a double-precision floating-point number is rint(), not round(). The reason is that round() maps to a 5-instruction sequence on the device, whereas rint() maps to a single instruction. trunc(), ceil(), and floor() each map to a single instruction as well.


Table 14 Double-Precision Mathematical Standard Library Functions with Maximum ULP Error. The maximum error is stated as the absolute value of the difference in ulps between the result returned by the CUDA library function and a correctly rounded double-precision result obtained according to the round-to-nearest ties-to-even rounding mode.ï







Function
Maximum ulp error




x+y
0 (IEEE-754 round-to-nearest-even)


x*y
0 (IEEE-754 round-to-nearest-even)


x/y
0 (IEEE-754 round-to-nearest-even)


1/x
0 (IEEE-754 round-to-nearest-even)


sqrt(x)
0 (IEEE-754 round-to-nearest-even)


rsqrt(x)
1 (full range)


cbrt(x)
1 (full range)


rcbrt(x)
1 (full range)


hypot(x,y)
2 (full range)


rhypot(x,y)
1 (full range)


norm3d(x,y,z)
2 (full range)


rnorm3d(x,y,z)
1 (full range)


norm4d(x,y,z,t)
2 (full range)


rnorm4d(x,y,z,t)
1 (full range)


norm(dim,arr)
An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off.


rnorm(dim,arr)
An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off.


exp(x)
1 (full range)


exp2(x)
1 (full range)


exp10(x)
1 (full range)


expm1(x)
1 (full range)


log(x)
1 (full range)


log2(x)
1 (full range)


log10(x)
1 (full range)


log1p(x)
1 (full range)


sin(x)
2 (full range)


cos(x)
2 (full range)


tan(x)
2 (full range)


sincos(x,sptr,cptr)
2 (full range)


sinpi(x)
2 (full range)


cospi(x)
2 (full range)


sincospi(x,sptr,cptr)
2 (full range)


asin(x)
2 (full range)


acos(x)
2 (full range)


atan(x)
2 (full range)


atan2(y,x)
2 (full range)


sinh(x)
2 (full range)


cosh(x)
1 (full range)


tanh(x)
1 (full range)


asinh(x)
3 (full range)


acosh(x)
3 (full range)


atanh(x)
2 (full range)


pow(x,y)
2 (full range)


erf(x)
2 (full range)


erfc(x)
5 (full range)


erfinv(x)
5 (full range)


erfcinv(x)
6 (full range)


erfcx(x)
4 (full range)


normcdf(x)
5 (full range)


normcdfinv(x)
8 (full range)


lgamma(x)
4 (outside interval -23.0001 â¦ -2.2637; larger inside)


tgamma(x)
10 (full range)


fma(x,y,z)
0 (IEEE-754 round-to-nearest-even)


frexp(x,exp)
0 (full range)


ldexp(x,exp)
0 (full range)


scalbn(x,n)
0 (full range)


scalbln(x,l)
0 (full range)


logb(x)
0 (full range)


ilogb(x)
0 (full range)


j0(x)

7 for |x| < 8
otherwise, the maximum absolute error is 5 x 10-12



j1(x)

7 for |x| < 8
otherwise, the maximum absolute error is 5 x 10-12



jn(n,x)
For n = 128, the maximum absolute error is 5 x 10-12


y0(x)

7 for |x| < 8
otherwise, the maximum absolute error is 5 x 10-12



y1(x)

7 for |x| < 8
otherwise, the maximum absolute error is 5 x 10-12



yn(n,x)
For |x| > 1.5n, the maximum absolute error is 5 x 10-12


cyl_bessel_i0(x)
6 (full range)


cyl_bessel_i1(x)
6 (full range)


fmod(x,y)
0 (full range)


remainder(x,y)
0 (full range)


remquo(x,y,iptr)
0 (full range)


modf(x,iptr)
0 (full range)


fdim(x,y)
0 (full range)


trunc(x)
0 (full range)


round(x)
0 (full range)


rint(x)
0 (full range)


nearbyint(x)
0 (full range)


ceil(x)
0 (full range)


floor(x)
0 (full range)


lrint(x)
0 (full range)


lround(x)
0 (full range)


llrint(x)
0 (full range)


llround(x)
0 (full range)






13.2. Intrinsic Functionsï

The functions from this section can only be used in device code.
Among these functions are the less accurate, but faster versions of some of the functions of Standard Functions.
They have the same name prefixed with __ (such as __sinf(x)).
They are faster as they map to fewer native instructions.
The compiler has an option (-use_fast_math) that forces each function in Table 15
to compile to its intrinsic counterpart. In addition to reducing the accuracy of the affected functions,
it may also cause some differences in special case handling. A more robust approach is to selectively replace
mathematical function calls by calls to intrinsic functions only where it is merited by the performance gains
and where changed properties such as reduced accuracy and different special case handling can be tolerated.


Table 15 Functions Affected by -use_fast_mathï







Operator/Function
Device Function




x/y
__fdividef(x,y)


sinf(x)
__sinf(x)


cosf(x)
__cosf(x)


tanf(x)
__tanf(x)


sincosf(x,sptr,cptr)
__sincosf(x,sptr,cptr)


logf(x)
__logf(x)


log2f(x)
__log2f(x)


log10f(x)
__log10f(x)


expf(x)
__expf(x)


exp10f(x)
__exp10f(x)


powf(x,y)
__powf(x,y)



Single-Precision Floating-Point Functions
__fadd_[rn,rz,ru,rd]() and __fmul_[rn,rz,ru,rd]() map to addition and multiplication operations that the compiler never merges into FMADs. By contrast, additions and multiplications generated from the â*â and â+â operators will frequently be combined into FMADs.
Functions suffixed with _rn operate using the round to nearest even rounding mode.
Functions suffixed with _rz operate using the round towards zero rounding mode.
Functions suffixed with _ru operate using the round up (to positive infinity) rounding mode.
Functions suffixed with _rd operate using the round down (to negative infinity) rounding mode.
The accuracy of floating-point division varies depending on whether the code is compiled with -prec-div=false
or -prec-div=true. When the code is compiled with -prec-div=false, both the regular division /
operator and __fdividef(x,y) have the same accuracy, but for 2126 < |y| < 2128,
__fdividef(x,y) delivers a result of zero, whereas the / operator delivers the correct result to
within the accuracy stated in Table 16.
Also, for 2126 < |y| < 2128, if x is infinity, __fdividef(x,y) delivers
a NaN (as a result of multiplying infinity by zero), while the / operator returns infinity.
On the other hand, the / operator is IEEE-compliant when the code is compiled with -prec-div=true
or without any -prec-div option at all since its default value is true.


Table 16 Single-Precision Floating-Point Intrinsic Functions. (Supported by the CUDA Runtime Library with Respective Error Bounds)ï







Function
Error bounds




__fadd_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__fsub_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__fmul_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__fmaf_[rn,rz,ru,rd](x,y,z)
IEEE-compliant.


__frcp_[rn,rz,ru,rd](x)
IEEE-compliant.


__fsqrt_[rn,rz,ru,rd](x)
IEEE-compliant.


__frsqrt_rn(x)
IEEE-compliant.


__fdiv_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__fdividef(x,y)
For |y| in [\(2^{-126}, 2^{126}\)], the maximum ulp error is 2.


__expf(x)
The maximum ulp error is 2 + floor(abs(1.173 * x)).


__exp10f(x)
The maximum ulp error is 2 + floor(abs(2.97 * x)).


__logf(x)
For x in [0.5, 2], the maximum absolute error is \(2^{-21.41}\), otherwise, the maximum ulp error is 3.


__log2f(x)
For x in [0.5, 2], the maximum absolute error is \(2^{-22}\), otherwise, the maximum ulp error is 2.


__log10f(x)
For x in [0.5, 2], the maximum absolute error is \(2^{-24}\), otherwise, the maximum ulp error is 3.


__sinf(x)
For x in [\(-\pi, \pi\)], the maximum absolute error is \(2^{-21.41}\), and larger otherwise.


__cosf(x)
For x in [\(-\pi, \pi\)], the maximum absolute error is \(2^{-21.19}\), and larger otherwise.


__sincosf(x,sptr,cptr)
Same as __sinf(x) and __cosf(x).


__tanf(x)
Derived from its implementation as __sinf(x) * (1/__cosf(x)).


__powf(x, y)
Derived from its implementation as exp2f(y * __log2f(x)).



Double-Precision Floating-Point Functions
__dadd_rn() and __dmul_rn() map to addition and multiplication operations that the compiler never merges into FMADs. By contrast, additions and multiplications generated from the â*â and â+â operators will frequently be combined into FMADs.


Table 17 Double-Precision Floating-Point Intrinsic Functions. (Supported by the CUDA Runtime Library with Respective Error Bounds)ï







Function
Error bounds




__dadd_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__dsub_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__dmul_[rn,rz,ru,rd](x,y)
IEEE-compliant.


__fma_[rn,rz,ru,rd](x,y,z)
IEEE-compliant.


__ddiv_[rn,rz,ru,rd](x,y)(x,y)

IEEE-compliant.
Requires compute capability > 2.



__drcp_[rn,rz,ru,rd](x)

IEEE-compliant.
Requires compute capability > 2.



__dsqrt_[rn,rz,ru,rd](x)

IEEE-compliant.
Requires compute capability > 2.








14. C++ Language Supportï

As described in Compilation with NVCC, CUDA source files compiled with nvcc can include a mix of host code and device code. The CUDA front-end compiler aims to emulate the host compiler behavior with respect to C++ input code. The input source code is processed according to the C++ ISO/IEC 14882:2003, C++ ISO/IEC 14882:2011, C++ ISO/IEC 14882:2014 or C++ ISO/IEC 14882:2017 specifications, and the CUDA front-end compiler aims to emulate any host compiler divergences from the ISO specification. In addition, the supported language is extended with CUDA-specific constructs described in this document 13, and is subject to the restrictions described below.
C++11 Language Features, C++14 Language Features and C++17 Language Features provide support matrices for the C++11, C++14, C++17 and C++20 features, respectively. Restrictions lists the language restrictions. Polymorphic Function Wrappers and Extended Lambdas describe additional features. Code Samples gives code samples.


14.1. C++11 Language Featuresï

The following table lists new language features that have been accepted into the C++11 standard. The âProposalâ column provides a link to the ISO C++ committee proposal that describes the feature, while the âAvailable in nvcc (device code)â column indicates the first version of nvcc that contains an implementation of this feature (if it has been implemented) for device code.


Table 18 C++11 Language Featuresï








Language Feature
C++11 Proposal
Available in nvcc (device code)




Rvalue references
N2118
7.0


Rvalue references for *this
N2439
7.0


Initialization of class objects by rvalues
N1610
7.0


Non-static data member initializers
N2756
7.0


Variadic templates
N2242
7.0


Extending variadic template template parameters
N2555
7.0


Initializer lists
N2672
7.0


Static assertions
N1720
7.0


auto-typed variables
N1984
7.0


Â  Â  Â  Â  Multi-declarator auto
N1737
7.0


Â  Â  Â  Â  Removal of auto as a storage-class specifier
N2546
7.0


Â  Â  Â  Â  New function declarator syntax
N2541
7.0


Lambda expressions
N2927
7.0


Declared type of an expression
N2343
7.0


Â  Â  Â  Â  Incomplete return types
N3276
7.0


Right angle brackets
N1757
7.0


Default template arguments for function templates
DR226
7.0


Solving the SFINAE problem for expressions
DR339
7.0


Alias templates
N2258
7.0


Extern templates
N1987
7.0


Null pointer constant
N2431
7.0


Strongly-typed enums
N2347
7.0


Forward declarations for enums
N2764
DR1206
7.0


Standardized attribute syntax
N2761
7.0


Generalized constant expressions
N2235
7.0


Alignment support
N2341
7.0


Conditionally-support behavior
N1627
7.0


Changing undefined behavior into diagnosable errors
N1727
7.0


Delegating constructors
N1986
7.0


Inheriting constructors
N2540
7.0


Explicit conversion operators
N2437
7.0


New character types
N2249
7.0


Unicode string literals
N2442
7.0


Raw string literals
N2442
7.0


Universal character names in literals
N2170
7.0


User-defined literals
N2765
7.0


Standard Layout Types
N2342
7.0


Defaulted functions
N2346
7.0


Deleted functions
N2346
7.0


Extended friend declarations
N1791
7.0


Extending sizeof
N2253
DR850
7.0


Inline namespaces
N2535
7.0


Unrestricted unions
N2544
7.0


Local and unnamed types as template arguments
N2657
7.0


Range-based for
N2930
7.0


Explicit virtual overrides
N2928
N3206
N3272
7.0


Minimal support for garbage collection and reachability-based leak detection
N2670
N/A (see Restrictions)


Allowing move constructors to throw [noexcept]
N3050
7.0


Defining move special member functions
N3053
7.0


Concurrency




Sequence points
N2239



Atomic operations
N2427



Strong Compare and Exchange
N2748



Bidirectional Fences
N2752



Memory model
N2429



Data-dependency ordering: atomics and memory model
N2664



Propagating exceptions
N2179



Allow atomics use in signal handlers
N2547



Thread-local storage
N2659



Dynamic initialization and destruction with concurrency
N2660



C99 Features in C++11




__func__ predefined identifier
N2340
7.0


C99 preprocessor
N1653
7.0


long long
N1811
7.0


Extended integral types
N1988







14.2. C++14 Language Featuresï

The following table lists new language features that have been accepted into the C++14 standard.


Table 19 C++14 Language Featuresï








Language Feature
C++14 Proposal
Available in nvcc (device code)




Tweak to certain C++ contextual conversions
N3323
9.0


Binary literals
N3472
9.0


Functions with deduced return type
N3638
9.0


Generalized lambda capture (init-capture)
N3648
9.0


Generic (polymorphic) lambda expressions
N3649
9.0


Variable templates
N3651
9.0


Relaxing requirements on constexpr functions
N3652
9.0


Member initializers and aggregates
N3653
9.0


Clarifying memory allocation
N3664



Sized deallocation
N3778



[[deprecated]] attribute
N3760
9.0


Single-quotation-mark as a digit separator
N3781
9.0






14.3. C++17 Language Featuresï

All C++17 language features are supported in nvcc version 11.0 and later, subject to restrictions described here.



14.4. C++20 Language Featuresï

All C++20 language features are supported in nvcc version 12.0 and later, subject to restrictions described here.



14.5. Restrictionsï



14.5.1. Host Compiler Extensionsï

Host compiler specific language extensions are not supported in device code.
__Complex types are only supported in host code.
__int128 type is supported in device code when compiled in conjunction with a host compiler that supports it.
__float128 type is only supported in host code on 64-bit x86 Linux platforms. A constant expression of __float128 type may be processed by the compiler in a floating point representation with lower precision.



14.5.2. Preprocessor Symbolsï



14.5.2.1. __CUDA_ARCH__ï



The type signature of the following entities shall not depend on whether __CUDA_ARCH__ is defined or not, or on a particular value of __CUDA_ARCH__:

__global__ functions and function templates
__device__ and __constant__ variables
textures and surfaces

Example:

#if !defined(__CUDA_ARCH__)
typedef int mytype;
#else
typedef double mytype;
#endif

__device__ mytype xxx;         // error: xxx's type depends on __CUDA_ARCH__
__global__ void foo(mytype in, // error: foo's type depends on __CUDA_ARCH__
                    mytype *ptr)
{
  *ptr = in;
}




If a __global__ function template is instantiated and launched from the host, then the function template must be instantiated with the same template arguments irrespective of whether __CUDA_ARCH__ is defined and regardless of the value of __CUDA_ARCH__.
Example:

__device__ int result;
template <typename T>
__global__ void kern(T in)
{
  result = in;
}

__host__ __device__ void foo(void)
{
#if !defined(__CUDA_ARCH__)
  kern<<<1,1>>>(1);      // error: "kern<int>" instantiation only
                         // when __CUDA_ARCH__ is undefined!
#endif
}

int main(void)
{
  foo();
  cudaDeviceSynchronize();
  return 0;
}




In separate compilation mode, the presence or absence of a definition of a function or variable with external linkage shall not depend on whether __CUDA_ARCH__ is defined or on a particular value of __CUDA_ARCH__14.
Example:

#if !defined(__CUDA_ARCH__)
void foo(void) { }                  // error: The definition of foo()
                                    // is only present when __CUDA_ARCH__
                                    // is undefined
#endif




In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior. Or, it must be guaranteed that all objects will compile for the same compute_arch. If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__, then the instances of that function in the objects could conflict if the objects are compiled for different compute arch.
For example, if an a.h contains:

template<typename T>
__device__ T* getptr(void)
{
#if __CUDA_ARCH__ == 700
  return NULL; /* no address */
#else
  __shared__ T arr[256];
  return arr;
#endif
}


Then if a.cu and b.cu both include a.h and instantiate getptr for the same type, and b.cu expects a non-NULL address, and compile with:

nvcc âarch=compute_70 âdc a.cu
nvcc âarch=compute_80 âdc b.cu
nvcc âarch=sm_80 a.o b.o


At link time only one version of the getptr is used, so the behavior would depend on which version is chosen. To avoid this, either a.cu and b.cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function.


The compiler does not guarantee that a diagnostic will be generated for the unsupported uses of __CUDA_ARCH__ described above.




14.5.3. Qualifiersï



14.5.3.1. Device Memory Space Specifiersï

The __device__, __shared__, __managed__ and __constant__ memory space specifiers are not allowed on:

class, struct, and union data members,
formal parameters,
non-extern variable declarations within a function that executes on the host.

The __device__, __constant__ and __managed__ memory space specifiers are not allowed on variable declarations that are neither extern nor static within a function that executes on the device.
A __device__, __constant__, __managed__ or __shared__ variable definition cannot have a class type with a non-empty constructor or a non-empty destructor. A constructor for a class type is considered empty at a point in the translation unit, if it is either a trivial constructor or it satisfies all of the following conditions:

The constructor function has been defined.
The constructor function has no parameters, the initializer list is empty and the function body is an empty compound statement.
Its class has no virtual functions, no virtual base classes and no non-static data member initializers.
The default constructors of all base classes of its class can be considered empty.
For all the nonstatic data members of its class that are of class type (or array thereof), the default constructors can be considered empty.

A destructor for a class is considered empty at a point in the translation unit, if it is either a trivial destructor or it satisfies all of the following conditions:

The destructor function has been defined.
The destructor function body is an empty compound statement.
Its class has no virtual functions and no virtual base classes.
The destructors of all base classes of its class can be considered empty.
For all the nonstatic data members of its class that are of class type (or array thereof), the destructor can be considered empty.

When compiling in the whole program compilation mode (see the nvcc user manual for a description of this mode), __device__, __shared__, __managed__ and __constant__ variables cannot be defined as external using the extern keyword. The only exception is for dynamically allocated __shared__ variables as described in index.html#__shared__.
When compiling in the separate compilation mode (see the nvcc user manual for a description of this mode), __device__, __shared__, __managed__ and __constant__ variables can be defined as external using the extern keyword. nvlink will generate an error when it cannot find a definition for an external variable (unless it is a dynamically allocated __shared__ variable).



14.5.3.2. __managed__ Memory Space Specifierï

Variables marked with the __managed__ memory space specifier (âmanagedâ variables) have the following restrictions:

The address of a managed variable is not a constant expression.
A managed variable shall not have a const qualified type.
A managed variable shall not have a reference type.

The address or value of a managed variable shall not be used when the CUDA runtime may not be in a valid state, including the following cases:

In static/dynamic initialization or destruction of an object with static or thread local storage duration.
In code that executes after exit() has been called (for example, a function marked with gccâs â__attribute__((destructor))â).
In code that executes when CUDA runtime may not be initialized (for example, a function marked with gccâs â__attribute__((constructor))â).


A managed variable cannot be used as an unparenthesized id-expression argument to a decltype() expression.
Managed variables have the same coherence and consistency behavior as specified for dynamically allocated managed memory.
When a CUDA program containing managed variables is run on an execution platform with multiple GPUs, the variables are allocated only once, and not per GPU.
A managed variable declaration without the extern linkage is not allowed within a function that executes on the host.
A managed variable declaration without the extern or static linkage is not allowed within a function that executes on the device.

Here are examples of legal and illegal uses of managed variables:

__device__ __managed__ int xxx = 10;         // OK

int *ptr = &xxx;                             // error: use of managed variable
                                             // (xxx) in static initialization
struct S1_t {
  int field;
  S1_t(void) : field(xxx) { };
};
struct S2_t {
  ~S2_t(void) { xxx = 10; }
};

S1_t temp1;                                 // error: use of managed variable
                                            // (xxx) in dynamic initialization

S2_t temp2;                                 // error: use of managed variable
                                            // (xxx) in the destructor of
                                            // object with static storage
                                            // duration

__device__ __managed__ const int yyy = 10;  // error: const qualified type

__device__ __managed__ int &zzz = xxx;      // error: reference type

template <int *addr> struct S3_t { };
S3_t<&xxx> temp;                            // error: address of managed
                                            // variable(xxx) not a
                                            // constant expression

__global__ void kern(int *ptr)
{
  assert(ptr == &xxx);                      // OK
  xxx = 20;                                 // OK
}
int main(void)
{
  int *ptr = &xxx;                          // OK
  kern<<<1,1>>>(ptr);
  cudaDeviceSynchronize();
  xxx++;                                    // OK
  decltype(xxx) qqq;                        // error: managed variable(xxx) used
                                            // as unparenthized argument to
                                            // decltype

  decltype((xxx)) zzz = yyy;                // OK
}





14.5.3.3. Volatile Qualifierï

The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions (Memory Fence Functions) and memory visibility semantics of synchronization functions (Synchronization Functions).
These optimizations can be disabled using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction.




14.5.4. Pointersï

Dereferencing a pointer either to global or shared memory in code that is executed on the host, or to host memory in code that is executed on the device results in an undefined behavior, most often in a segmentation fault and application termination.
The address obtained by taking the address of a __device__, __shared__ or __constant__ variable can only be used in device code. The address of a __device__ or __constant__ variable obtained through cudaGetSymbolAddress() as described in Device Memory can only be used in host code.



14.5.5. Operatorsï



14.5.5.1. Assignment Operatorï

__constant__ variables can only be assigned from the host code through runtime functions (Device Memory); they cannot be assigned from the device code.
__shared__ variables cannot have an initialization as part of their declaration.
It is not allowed to assign values to any of the built-in variables defined in Built-in Variables.



14.5.5.2. Address Operatorï

It is not allowed to take the address of any of the built-in variables defined in Built-in Variables.




14.5.6. Run Time Type Information (RTTI)ï

The following RTTI-related features are supported in host code, but not in device code.

typeid operator
std::type_info
dynamic_cast operator




14.5.7. Exception Handlingï

Exception handling is only supported in host code, but not in device code.
Exception specification is not supported for __global__ functions.



14.5.8. Standard Libraryï

Standard libraries are only supported in host code, but not in device code, unless specified otherwise.



14.5.9. Namespace Reservationsï

Unless an exception is otherwise noted, it is undefined behavior to add any declarations or definitions to cuda::, nv::, cooperative_groups:: or any namespace nested within.
Examples:

namespace cuda{
   // Bad: class declaration added to namespace cuda
   struct foo{};

   // Bad: function definition added to namespace cuda
   cudaStream_t make_stream(){
      cudaStream_t s;
      cudaStreamCreate(&s);
      return s;
   }
} // namespace cuda

namespace cuda{
   namespace utils{
      // Bad: function definition added to namespace nested within cuda
      cudaStream_t make_stream(){
          cudaStream_t s;
          cudaStreamCreate(&s);
          return s;
      }
   } // namespace utils
} // namespace cuda

namespace utils{
   namespace cuda{
     // Okay: namespace cuda may be used nested within a non-reserved namespace
     cudaStream_t make_stream(){
          cudaStream_t s;
          cudaStreamCreate(&s);
          return s;
      }
   } // namespace cuda
} // namespace utils

// Bad: Equivalent to adding symbols to namespace cuda at global scope
using namespace utils;





14.5.10. Functionsï



14.5.10.1. External Linkageï

A call within some device code of a function declared with the extern qualifier is only allowed if the function is defined within the same compilation unit as the device code, i.e., a single file or several files linked together with relocatable device code and nvlink.



14.5.10.2. Implicitly-declared and explicitly-defaulted functionsï

Let F denote a function that is either implicitly-declared or is explicitly-defaulted on its first declaration The execution space specifiers (__host__, __device__) for F are the union of the execution space specifiers of all the functions that invoke it (note that a __global__ caller will be treated as a __device__ caller for this analysis). For example:

class Base {
  int x;
public:
  __host__ __device__ Base(void) : x(10) {}
};

class Derived : public Base {
  int y;
};

class Other: public Base {
  int z;
};

__device__ void foo(void)
{
  Derived D1;
  Other D2;
}

__host__ void bar(void)
{
  Other D3;
}


Here, the implicitly-declared constructor function âDerived::Derivedâ will be treated as a __device__ function, since it is invoked only from the __device__ function âfooâ. The implicitly-declared constructor function âOther::Otherâ will be treated as a __host__ __device__ function, since it is invoked both from a __device__ function âfooâ and a __host__ function âbarâ.
In addition, if F is a virtual destructor, then the execution spaces of each virtual destructor D overridden by F are added to the set of execution spaces for F, if D is either not implicitly defined or is explicitly defaulted on a declaration other than its first declaration.
For example:

struct Base1 { virtual __host__ __device__ ~Base1() { } };
struct Derived1 : Base1 { }; // implicitly-declared virtual destructor
                             // ~Derived1 has __host__ __device__
                             // execution space specifiers

struct Base2 { virtual __device__ ~Base2(); };
__device__ Base2::~Base2() = default;
struct Derived2 : Base2 { }; // implicitly-declared virtual destructor
                             // ~Derived2 has __device__ execution
                             // space specifiers





14.5.10.3. Function Parametersï

__global__ function parameters are passed to the device via constant memory and are limited to 32,764 bytes starting with Volta, and 4 KB on older architectures.
__global__ functions cannot have a variable number of arguments.
__global__ function parameters cannot be pass-by-reference.
In separate compilation mode, if a __device__ or __global__ function is ODR-used in a particular translation unit, then the parameter and return types of the function must be complete in that translation unit.
Example:

//first.cu:
struct S;
__device__ void foo(S); // error: type 'S' is incomplete
__device__ auto *ptr = foo;

int main() { }

//second.cu:
struct S { int x; };
__device__ void foo(S) { }



//compiler invocation
$nvcc -std=c++14 -rdc=true first.cu second.cu -o first
nvlink error   : Prototype doesn't match for '_Z3foo1S' in '/tmp/tmpxft_00005c8c_00000000-18_second.o', first defined in '/tmp/tmpxft_00005c8c_00000000-18_second.o'
nvlink fatal   : merge_elf failed




14.5.10.3.1. __global__ Function Argument Processingï

When a __global__ function is launched from device code, each argument must be trivially copyable and trivially destructible.
When a __global__ function is launched from host code, each argument type is allowed to be non-trivially copyable or non-trivially-destructible, but the processing for such types does not follow the standard C++ model, as described below. User code must ensure that this workflow does not affect program correctness. The workflow diverges from standard C++ in two areas:


Memcpy instead of copy constructor invocation
When lowering a __global__ function launch from host code, the compiler generates stub functions that copy the parameters one or more times by value, before eventually using memcpy to copy the arguments to the __global__ functionâs parameter memory on the device. This occurs even if an argument was non-trivially-copyable, and therefore may break programs where the copy constructor has side effects.
Example:

#include <cassert>
struct S {
 int x;
 int *ptr;
 __host__ __device__ S() { }
 __host__ __device__ S(const S &) { ptr = &x; }
};

__global__ void foo(S in) {
 // this assert may fail, because the compiler
 // generated code will memcpy the contents of "in"
 // from host to kernel parameter memory, so the
 // "in.ptr" is not initialized to "&in.x" because
 // the copy constructor is skipped.
 assert(in.ptr == &in.x);
}

int main() {
  S tmp;
  foo<<<1,1>>>(tmp);
  cudaDeviceSynchronize();
}


Example:

#include <cassert>

__managed__ int counter;
struct S1 {
S1() { }
S1(const S1 &) { ++counter; }
};

__global__ void foo(S1) {

/* this assertion may fail, because
   the compiler generates stub
   functions on the host for a kernel
   launch, and they may copy the
   argument by value more than once.
*/
assert(counter == 1);
}

int main() {
S1 V;
foo<<<1,1>>>(V);
cudaDeviceSynchronize();
}




Destructor may be invoked before the ``__global__`` function has finished
Kernel launches are asynchronous with host execution. As a result, if a __global__ function argument has a non-trivial destructor, the destructor may execute in host code even before the __global__ function has finished execution. This may break programs where the destructor has side effects.
Example:

struct S {
 int *ptr;
 S() : ptr(nullptr) { }
 S(const S &) { cudaMallocManaged(&ptr, sizeof(int)); }
 ~S() { cudaFree(ptr); }
};

__global__ void foo(S in) {

  //error: This store may write to memory that has already been
  //       freed (see below).
  *(in.ptr) = 4;

}

int main() {
 S V;

 /* The object 'V' is first copied by value to a compiler-generated
  * stub function that does the kernel launch, and the stub function
  * bitwise copies the contents of the argument to kernel parameter
  * memory.
  * However, GPU kernel execution is asynchronous with host
  * execution.
  * As a result, S::~S() will execute when the stub function   returns, releasing allocated memory, even though the kernel may not have finished execution.
  */
 foo<<<1,1>>>(V);
 cudaDeviceSynchronize();
}







14.5.10.3.2. Toolkit and Driver Compatibilityï

Developers must use the 12.1 Toolkit and r530 driver or higher to compile, launch, and debug kernels that accept parameters larger than 4KB. If such kernels are launched on older drivers, CUDA will issue the error CUDA_ERROR_NOT_SUPPORTED.



14.5.10.3.3. Link Compatibility across Toolkit Revisionsï

When linking device objects, if at least one device object contains a kernel with a parameter larger than 4KB, the developer must recompile all objects from their respective device sources with the 12.1 toolkit or higher before linking them together. Failure to do so will result in a linker error.




14.5.10.4. Static Variables within Functionï

Variable memory space specifiers are allowed in the declaration of a static variable V within the immediate or nested block scope of a function F where:

F is a __global__ or __device__-only function.
F is a __host__ __device__ function and __CUDA_ARCH__ is defined 17.

If no explicit memory space specifier is present in the declaration of V, an implicit __device__ specifier is assumed during device compilation.
V has the same initialization restrictions as a variable with the same memory space specifiers declared in namespace scope for example a __device__ variable cannot have a ânon-emptyâ constructor (see Device Memory Space Specifiers).
Examples of legal and illegal uses of function-scope static variables are shown below.

struct S1_t {
  int x;
};

struct S2_t {
  int x;
  __device__ S2_t(void) { x = 10; }
};

struct S3_t {
  int x;
  __device__ S3_t(int p) : x(p) { }
};

__device__ void f1() {
  static int i1;              // OK, implicit __device__ memory space specifier
  static int i2 = 11;         // OK, implicit __device__ memory space specifier
  static __managed__ int m1;  // OK
  static __device__ int d1;   // OK
  static __constant__ int c1; // OK

  static S1_t i3;             // OK, implicit __device__ memory space specifier
  static S1_t i4 = {22};      // OK, implicit __device__ memory space specifier

  static __shared__ int i5;   // OK

  int x = 33;
  static int i6 = x;          // error: dynamic initialization is not allowed
  static S1_t i7 = {x};       // error: dynamic initialization is not allowed

  static S2_t i8;             // error: dynamic initialization is not allowed
  static S3_t i9(44);         // error: dynamic initialization is not allowed
}

__host__ __device__ void f2() {
  static int i1;              // OK, implicit __device__ memory space specifier
                              // during device compilation.
#ifdef __CUDA_ARCH__
  static __device__ int d1;   // OK, declaration is only visible during device
                              // compilation  (__CUDA_ARCH__ is defined)
#else
  static int d0;              // OK, declaration is only visible during host
                              // compilation (__CUDA_ARCH__ is not defined)
#endif

  static __device__ int d2;   // error: __device__ variable inside
                              // a host function during host compilation
                              // i.e. when __CUDA_ARCH__ is not defined

  static __shared__ int i2;  // error: __shared__ variable inside
                             // a host function during host compilation
                             // i.e. when __CUDA_ARCH__ is not defined
}





14.5.10.5. Function Pointersï

The address of a __global__ function taken in host code cannot be used in device code (e.g. to launch the kernel). Similarly, the address of a __global__ function taken in device code cannot be used in host code.
It is not allowed to take the address of a __device__ function in host code.



14.5.10.6. Function Recursionï

__global__ functions do not support recursion.



14.5.10.7. Friend Functionsï

A __global__ function or function template cannot be defined in a friend declaration.
Example:

struct S1_t {
  friend __global__
  void foo1(void);  // OK: not a definition
  template<typename T>
  friend __global__
  void foo2(void); // OK: not a definition

  friend __global__
  void foo3(void) { } // error: definition in friend declaration

  template<typename T>
  friend __global__
  void foo4(void) { } // error: definition in friend declaration
};





14.5.10.8. Operator Functionï

An operator function cannot be a __global__ function.



14.5.10.9. Allocation and Deallocation Functionsï

A user-defined operator new, operator new[], operator delete, or operator delete[] cannot be used to replace the corresponding __host__ or __device__ builtins provided by the compiler.




14.5.11. Classesï



14.5.11.1. Data Membersï

Static data members are not supported except for those that are also const-qualified (see Const-qualified variables).



14.5.11.2. Function Membersï

Static member functions cannot be __global__ functions.



14.5.11.3. Virtual Functionsï

When a function in a derived class overrides a virtual function in a base class, the execution space specifiers (i.e., __host__, __device__) on the overridden and overriding functions must match.
It is not allowed to pass as an argument to a __global__ function an object of a class with virtual functions.
If an object is created in host code, invoking a virtual function for that object in device code has undefined behavior.
If an object is created in device code, invoking a virtual function for that object in host code has undefined behavior.
See Windows-Specific for additional constraints when using the Microsoft host compiler.
Example:

struct S1 { virtual __host__ __device__ void foo() { } };

__managed__ S1 *ptr1, *ptr2;

__managed__ __align__(16) char buf1[128];
__global__ void kern() {
  ptr1->foo();     // error: virtual function call on a object
                   //        created in host code.
  ptr2 = new(buf1) S1();
}

int main(void) {
  void *buf;
  cudaMallocManaged(&buf, sizeof(S1), cudaMemAttachGlobal);
  ptr1 = new (buf) S1();
  kern<<<1,1>>>();
  cudaDeviceSynchronize();
  ptr2->foo();  // error: virtual function call on an object
                //        created in device code.
}





14.5.11.4. Virtual Base Classesï

It is not allowed to pass as an argument to a __global__ function an object of a class derived from virtual base classes.
See Windows-Specific for additional constraints when using the Microsoft host compiler.



14.5.11.5. Anonymous Unionsï

Member variables of a namespace scope anonymous union cannot be referenced in a __global__ or __device__ function.



14.5.11.6. Windows-Specificï

The CUDA compiler follows the IA64 ABI for class layout, while the Microsoft host compiler does not. Let T denote a pointer to member type, or a class type that satisfies any of the following conditions:

T has virtual functions.
T has a virtual base class.
T has multiple inheritance with more than one direct or indirect empty base class.
All direct and indirect base classes B of T are empty and the type of the first field F of T uses B in its definition, such that B is laid out at offset 0 in the definition of F.

Let C denote T or a class type that has T as a field type or as a base class type. The CUDA compiler may compute the class layout and size differently than the Microsoft host compiler for the type C.
As long as the type C is used exclusively in host or device code, the program should work correctly.
Passing an object of type C between host and device code has undefined behavior, for example, as an argument to a __global__ function or through cudaMemcpy*() calls.
Accessing an object of type C or any subobject in device code, or invoking a member function in device code, has undefined behavior if the object is created in host code.
Accessing an object of type C or any subobject in host code, or invoking a member function in host code, has undefined behavior if the object is created in device code 18.




14.5.12. Templatesï

A type or template cannot be used in the type, non-type or template template argument of a __global__ function template instantiation or a __device__/__constant__ variable instantiation if either:

The type or template is defined within a __host__ or __host__ __device__.
The type or template is a class member with private or protected access and its parent class is not defined within a __device__ or __global__ function.
The type is unnamed.
The type is compounded from any of the types above.

Example:

template <typename T>
__global__ void myKernel(void) { }

class myClass {
private:
    struct inner_t { };
public:
    static void launch(void)
    {
       // error: inner_t is used in template argument
       // but it is private
       myKernel<inner_t><<<1,1>>>();
    }
};

// C++14 only
template <typename T> __device__ T d1;

template <typename T1, typename T2> __device__ T1 d2;

void fn() {
  struct S1_t { };
  // error (C++14 only): S1_t is local to the function fn
  d1<S1_t> = {};

  auto lam1 = [] { };
  // error (C++14 only): a closure type cannot be used for
  // instantiating a variable template
  d2<int, decltype(lam1)> = 10;
}





14.5.13. Trigraphs and Digraphsï

Trigraphs are not supported on any platform. Digraphs are not supported on Windows.



14.5.14. Const-qualified variablesï

Let âVâ denote a namespace scope variable or a class static member variable that has const qualified type and does not have execution space annotations (for example, __device__, __constant__, __shared__). V is considered to be a host code variable.
The value of V may be directly used in device code, if

V has been initialized with a constant expression before the point of use,
the type of V is not volatile-qualified, and

it has one of the following types:

built-in floating point type except when the Microsoft compiler is used as the host compiler,
built-in integral type.



Device source code cannot contain a reference to V or take the address of V.
Example:

const int xxx = 10;
struct S1_t {  static const int yyy = 20; };

extern const int zzz;
const float www = 5.0;
__device__ void foo(void) {
  int local1[xxx];          // OK
  int local2[S1_t::yyy];    // OK

  int val1 = xxx;           // OK

  int val2 = S1_t::yyy;     // OK

  int val3 = zzz;           // error: zzz not initialized with constant
                            // expression at the point of use.

  const int &val3 = xxx;    // error: reference to host variable
  const int *val4 = &xxx;   // error: address of host variable
  const float val5 = www;   // OK except when the Microsoft compiler is used as
                            // the host compiler.
}
const int zzz = 20;





14.5.15. Long Doubleï

The use of long double type is not supported in device code.



14.5.16. Deprecation Annotationï

nvcc supports the use of deprecated attribute when using gcc, clang, xlC, icc or pgcc host compilers, and the use of deprecated declspec when using the cl.exe host compiler. It also supports the [[deprecated]] standard attribute when the C++14 dialect has been enabled. The CUDA frontend compiler will generate a deprecation diagnostic for a reference to a deprecated entity from within the body of a __device__, __global__ or __host__ __device__ function when __CUDA_ARCH__ is defined (i.e., during device compilation phase). Other references to deprecated entities will be handled by the host compiler, e.g., a reference from within a __host__ function.
The CUDA frontend compiler does not support the #pragma gcc diagnostic or #pragma warning mechanisms supported by various host compilers. Therefore, deprecation diagnostics generated by the CUDA frontend compiler are not affected by these pragmas, but diagnostics generated by the host compiler will be affected. To suppress the warning for device-code, user can use NVIDIA specific pragma #pragma nv_diag_suppress. The nvcc flag -Wno-deprecated-declarations can be used to suppress all deprecation warnings, and the flag -Werror=deprecated-declarations can be used to turn deprecation warnings into errors.



14.5.17. Noreturn Annotationï

nvcc supports the use of noreturn attribute when using gcc, clang, xlC, icc or pgcc host compilers, and the use of noreturn declspec when using the cl.exe host compiler. It also supports the [[noreturn]] standard attribute when the C++11 dialect has been enabled.
The attribute/declspec can be used in both host and device code.



14.5.18. [[likely]] / [[unlikely]] Standard Attributesï

These attributes are accepted in all configurations that support the C++ standard attribute syntax. The attributes can be used to hint to the device compiler optimizer whether a statement is more or less likely to be executed compared to any alternative path that does not include the statement.
Example:

__device__ int foo(int x) {

 if (i < 10) [[likely]] { // the 'if' block will likely be entered
  return 4;
 }
 if (i < 20) [[unlikely]] { // the 'if' block will not likely be entered
  return 1;
 }
 return 0;
}


If these attributes are used in host code when __CUDA_ARCH__ is undefined, then they will be present in the code parsed by the host compiler, which may generate a warning if the attributes are not supported. For example, clang11 host compiler will generate an âunknown attributeâ warning.



14.5.19. const and pure GNU Attributesï

These attributes are supported for both host and device functions, when using a language dialect and host compiler that also supports these attributes e.g. with g++ host compiler.
For a device function annotated with the pure attribute, the device code optimizer assumes that the function does not change any mutable state visible to caller functions (e.g. memory).
For a device function annotated with the const attribute, the device code optimizer assumes that the function does not access or change any mutable state visible to caller functions (e.g. memory).
Example:

__attribute__((const)) __device__ int get(int in);

__device__ int doit(int in) {
int sum = 0;

//because 'get' is marked with 'const' attribute
//device code optimizer can recognize that the
//second call to get() can be commoned out.
sum = get(in);
sum += get(in);

return sum;
}





14.5.20. __nv_pure__ Attributeï

The __nv_pure__ attributed is supported for both host and device functions. For host functions, when using a language dialect that supports the pure GNU attribute, the __nv_pure__ attribute is translated to the pure GNU attribute. Similarly when using MSVC as the host compiler, the attribute is translated to the MSVC noalias attribute.
When a device function is annotated with the __nv_pure__ attribute, the device code optimizer assumes that the function does not change any mutable state visible to caller functions (e.g. memory).



14.5.21. Intel Host Compiler Specificï

The CUDA frontend compiler parser does not recognize some of the intrinsic functions supported by the Intel compiler (e.g. icc). When using the Intel compiler as a host compiler, nvcc will therefore enable the macro __INTEL_COMPILER_USE_INTRINSIC_PROTOTYPES during preprocessing. This macro enables explicit declarations of the Intel compiler intrinsic functions in the associated header files, allowing nvcc to support use of such functions in host code19.



14.5.22. C++11 Featuresï

C++11 features that are enabled by default by the host compiler are also supported by nvcc, subject to the restrictions described in this document. In addition, invoking nvcc with -std=c++11 flag turns on all C++11 features and also invokes the host preprocessor, compiler and linker with the corresponding C++11 dialect option 20.


14.5.22.1. Lambda Expressionsï

The execution space specifiers for all member functions21 of the closure class associated with a lambda expression are derived by the compiler as follows. As described in the C++11 standard, the compiler creates a closure type in the smallest block scope, class scope or namespace scope that contains the lambda expression. The innermost function scope enclosing the closure type is computed, and the corresponding functionâs execution space specifiers are assigned to the closure class member functions. If there is no enclosing function scope, the execution space specifier is __host__.
Examples of lambda expressions and computed execution space specifiers are shown below (in comments).

auto globalVar = [] { return 0; }; // __host__

void f1(void) {
  auto l1 = [] { return 1; };      // __host__
}

__device__ void f2(void) {
  auto l2 = [] { return 2; };      // __device__
}

__host__ __device__ void f3(void) {
  auto l3 = [] { return 3; };      // __host__ __device__
}

__device__ void f4(int (*fp)() = [] { return 4; } /* __host__ */) {
}

__global__ void f5(void) {
  auto l5 = [] { return 5; };      // __device__
}

__device__ void f6(void) {
  struct S1_t {
    static void helper(int (*fp)() = [] {return 6; } /* __device__ */) {
    }
  };
}


The closure type of a lambda expression cannot be used in the type or non-type argument of a __global__ function template instantiation, unless the lambda is defined within a __device__ or __global__ function.
Example:

template <typename T>
__global__ void foo(T in) { };

template <typename T>
struct S1_t { };

void bar(void) {
  auto temp1 = [] { };

  foo<<<1,1>>>(temp1);                    // error: lambda closure type used in
                                          // template type argument
  foo<<<1,1>>>( S1_t<decltype(temp1)>()); // error: lambda closure type used in
                                          // template type argument
}





14.5.22.2. std::initializer_listï

By default, the CUDA compiler will implicitly consider the member functions of std::initializer_list to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code. The nvcc flag --no-host-device-initializer-list will disable this behavior; member functions of std::initializer_list will then be considered as __host__ functions and will not be directly invokable from device code.
Example:

#include <initializer_list>

__device__ int foo(std::initializer_list<int> in);

__device__ void bar(void)
  {
    foo({4,5,6});   // (a) initializer list containing only
                    // constant expressions.

    int i = 4;
    foo({i,5,6});   // (b) initializer list with at least one
                    // non-constant element.
                    // This form may have better performance than (a).
  }





14.5.22.3. Rvalue referencesï

By default, the CUDA compiler will implicitly consider std::move and std::forward function templates to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code. The nvcc flag --no-host-device-move-forward will disable this behavior; std::move and std::forward will then be considered as __host__ functions and will not be directly invokable from device code.



14.5.22.4. Constexpr functions and function templatesï

By default, a constexpr function cannot be called from a function with incompatible execution space 22. The experimental nvcc flag --expt-relaxed-constexpr removes this restriction 23. When this flag is specified, host code can invoke a __device__ constexpr function and device code can invoke a __host__ constexpr function. nvcc will define the macro __CUDACC_RELAXED_CONSTEXPR__ when --expt-relaxed-constexpr has been specified. Note that a function template instantiation may not be a constexpr function even if the corresponding template is marked with the keyword constexpr (C++11 Standard Section [dcl.constexpr.p6]).



14.5.22.5. Constexpr variablesï

Let âVâ denote a namespace scope variable or a class static member variable that has been marked constexpr and that does not have execution space annotations (e.g., __device__, __constant__, __shared__). V is considered to be a host code variable.
If V is of scalar type 24 other than long double and the type is not volatile-qualified, the value of V can be directly used in device code. In addition, if V is of a non-scalar type then scalar elements of V can be used inside a constexpr __device__ or __host__ __device__ function, if the call to the function is a constant expression 25. Device source code cannot contain a reference to V or take the address of V.
Example:

constexpr int xxx = 10;
constexpr int yyy = xxx + 4;
struct S1_t { static constexpr int qqq = 100; };

constexpr int host_arr[] = { 1, 2, 3};
constexpr __device__ int get(int idx) { return host_arr[idx]; }

__device__ int foo(int idx) {
  int v1 = xxx + yyy + S1_t::qqq;  // OK
  const int &v2 = xxx;             // error: reference to host constexpr
                                   // variable
  const int *v3 = &xxx;            // error: address of host constexpr
                                   // variable
  const int &v4 = S1_t::qqq;       // error: reference to host constexpr
                                   // variable
  const int *v5 = &S1_t::qqq;      // error: address of host constexpr
                                   // variable

  v1 += get(2);                    // OK: 'get(2)' is a constant
                                   // expression.
  v1 += get(idx);                  // error: 'get(idx)' is not a constant
                                   // expression
  v1 += host_arr[2];               // error: 'host_arr' does not have
                                   // scalar type.
  return v1;
}





14.5.22.6. Inline namespacesï

For an input CUDA translation unit, the CUDA compiler may invoke the host compiler for compiling the host code within the translation unit. In the code passed to the host compiler, the CUDA compiler will inject additional compiler generated code, if the input CUDA translation unit contained a definition of any of the following entities:

__global__ function or function template instantiation
__device__, __constant__
variables with surface or texture type

The compiler generated code contains a reference to the defined entity. If the entity is defined within an inline namespace and another entity of the same name and type signature is defined in an enclosing namespace, this reference may be considered ambiguous by the host compiler and host compilation will fail.
This limitation can be avoided by using unique names for such entities defined within an inline namespace.
Example:

__device__ int Gvar;
inline namespace N1 {
  __device__ int Gvar;
}

// <-- CUDA compiler inserts a reference to "Gvar" at this point in the
// translation unit. This reference will be considered ambiguous by the
// host compiler and compilation will fail.


Example:

inline namespace N1 {
  namespace N2 {
    __device__ int Gvar;
  }
}

namespace N2 {
  __device__ int Gvar;
}

// <-- CUDA compiler inserts reference to "::N2::Gvar" at this point in
// the translation unit. This reference will be considered ambiguous by
// the host compiler and compilation will fail.




14.5.22.6.1. Inline unnamed namespacesï

The following entities cannot be declared in namespace scope within an inline unnamed namespace:

__managed__, __device__, __shared__ and __constant__ variables
__global__ function and function templates
variables with surface or texture type

Example:

inline namespace {
  namespace N2 {
    template <typename T>
    __global__ void foo(void);            // error

    __global__ void bar(void) { }         // error

    template <>
    __global__ void foo<int>(void) { }    // error

    __device__ int x1b;                   // error
    __constant__ int x2b;                 // error
    __shared__ int x3b;                   // error

    texture<int> q2;                      // error
    surface<int> s2;                      // error
  }
};






14.5.22.7. thread_localï

The thread_local storage specifier is not allowed in device code.



14.5.22.8. __global__ functions and function templatesï

If the closure type associated with a lambda expression is used in a template argument of a __global__ function template instantiation, the lambda expression must either be defined in the immediate or nested block scope of a __device__ or __global__ function, or must be an extended lambda.
Example:

template <typename T>
__global__ void kernel(T in) { }

__device__ void foo_device(void)
{
  // All kernel instantiations in this function
  // are valid, since the lambdas are defined inside
  // a __device__ function.

  kernel<<<1,1>>>( [] __device__ { } );
  kernel<<<1,1>>>( [] __host__ __device__ { } );
  kernel<<<1,1>>>( []  { } );
}

auto lam1 = [] { };

auto lam2 = [] __host__ __device__ { };

void foo_host(void)
{
   // OK: instantiated with closure type of an extended __device__ lambda
   kernel<<<1,1>>>( [] __device__ { } );

   // OK: instantiated with closure type of an extended __host__ __device__
   // lambda
   kernel<<<1,1>>>( [] __host__ __device__ { } );

   // error: unsupported: instantiated with closure type of a lambda
   // that is not an extended lambda
   kernel<<<1,1>>>( []  { } );

   // error: unsupported: instantiated with closure type of a lambda
   // that is not an extended lambda
   kernel<<<1,1>>>( lam1);

   // error: unsupported: instantiated with closure type of a lambda
   // that is not an extended lambda
   kernel<<<1,1>>>( lam2);
}


A __global__ function or function template cannot be declared as constexpr.
A __global__ function or function template cannot have a parameter of type std::initializer_list or va_list.
A __global__ function cannot have a parameter of rvalue reference type.
A variadic __global__ function template has the following restrictions:

Only a single pack parameter is allowed.
The pack parameter must be listed last in the template parameter list.

Example:

// ok
template <template <typename...> class Wrapper, typename... Pack>
__global__ void foo1(Wrapper<Pack...>);

// error: pack parameter is not last in parameter list
template <typename... Pack, template <typename...> class Wrapper>
__global__ void foo2(Wrapper<Pack...>);

// error: multiple parameter packs
template <typename... Pack1, int...Pack2, template<typename...> class Wrapper1,
          template<int...> class Wrapper2>
__global__ void foo3(Wrapper1<Pack1...>, Wrapper2<Pack2...>);





14.5.22.9. __managed__ and __shared__ variablesï

`__managed__ and __shared__ variables cannot be marked with the keyword constexpr.



14.5.22.10. Defaulted functionsï

Execution space specifiers on a function that is explicitly-defaulted on its first declaration are ignored by the CUDA compiler. Instead, the CUDA compiler will infer the execution space specifiers as described in Implicitly-declared and explicitly-defaulted functions.
Execution space specifiers are not ignored if the function is explicitly-defaulted, but not on its first declaration.
Example:

struct S1 {
  // warning: __host__ annotation is ignored on a function that
  //          is explicitly-defaulted on its first declaration
  __host__ S1() = default;
};

__device__ void foo1() {
  //note: __device__ execution space is derived for S1::S1
  //       based on implicit call from within __device__ function
  //       foo1
  S1 s1;
}

struct S2 {
  __host__ S2();
};

//note: S2::S2 is not defaulted on its first declaration, and
//      its execution space is fixed to __host__  based on its
//      first declaration.
S2::S2() = default;

__device__ void foo2() {
   // error: call from __device__ function 'foo2' to
   //        __host__ function 'S2::S2'
   S2 s2;
}






14.5.23. C++14 Featuresï

C++14 features enabled by default by the host compiler are also supported by nvcc. Passing nvcc -std=c++14 flag turns on all C++14 features and also invokes the host preprocessor, compiler and linker with the corresponding C++14 dialect option 26. This section describes the restrictions on the supported C++14 features.


14.5.23.1. Functions with deduced return typeï

A __global__ function cannot have a deduced return type.
If a __device__ function has deduced return type, the CUDA frontend compiler will change the function declaration to have a void return type, before invoking the host compiler. This may cause issues for introspecting the deduced return type of the __device__ function in host code. Thus, the CUDA compiler will issue compile-time errors for referencing such deduced return type outside device function bodies, except if the reference is absent when __CUDA_ARCH__ is undefined.
Examples:

__device__ auto fn1(int x) {
  return x;
}

__device__ decltype(auto) fn2(int x) {
  return x;
}

__device__ void device_fn1() {
  // OK
  int (*p1)(int) = fn1;
}

// error: referenced outside device function bodies
decltype(fn1(10)) g1;

void host_fn1() {
  // error: referenced outside device function bodies
  int (*p1)(int) = fn1;

  struct S_local_t {
    // error: referenced outside device function bodies
    decltype(fn2(10)) m1;

    S_local_t() : m1(10) { }
  };
}

// error: referenced outside device function bodies
template <typename T = decltype(fn2)>
void host_fn2() { }

template<typename T> struct S1_t { };

// error: referenced outside device function bodies
struct S1_derived_t : S1_t<decltype(fn1)> { };





14.5.23.2. Variable templatesï

A __device__/__constant__ variable template cannot have a const qualified type when using the Microsoft host compiler.
Examples:

// error: a __device__ variable template cannot
// have a const qualified type on Windows
template <typename T>
__device__ const T d1(2);

int *const x = nullptr;
// error: a __device__ variable template cannot
// have a const qualified type on Windows
template <typename T>
__device__ T *const d2(x);

// OK
template <typename T>
__device__ const T *d3;

__device__ void fn() {
  int t1 = d1<int>;

  int *const t2 = d2<int>;

  const int *t3 = d3<int>;
}






14.5.24. C++17 Featuresï

C++17 features enabled by default by the host compiler are also supported by nvcc. Passing nvcc -std=c++17 flag turns on all C++17 features and also invokes the host preprocessor, compiler and linker with the corresponding C++17 dialect option 27. This section describes the restrictions on the supported C++17 features.


14.5.24.1. Inline Variableï



A namespace scope inline variable declared with __device__ or __constant__ or __managed__ memory space specifier must have internal linkage, if the code is compiled with nvcc in whole program compilation mode.
Examples:

inline __device__ int xxx; //error when compiled with nvcc in
                           //whole program compilation mode.
                           //ok when compiled with nvcc in
                           //separate compilation mode.

inline __shared__ int yyy0; // ok.

static inline __device__ int yyy; // ok: internal linkage
namespace {
inline __device__ int zzz; // ok: internal linkage
}



When using g++ host compiler, an inline variable declared with __managed__ memory space specifier may not be visible to the debugger.




14.5.24.2. Structured Bindingï

A structured binding cannot be declared with a variable memory space specifier.
Example:

struct S { int x; int y; };
__device__ auto [a1, b1] = S{4,5}; // error






14.5.25. C++20 Featuresï

C++20 features enabled by default by the host compiler are also supported by nvcc. Passing nvcc -std=c++20 flag turns on all C++20 features and also invokes the host preprocessor, compiler and linker with the corresponding C++20 dialect option 28. This section describes the restrictions on the supported C++20 features.


14.5.25.1. Module supportï

Modules are not supported in CUDA C++, in either host or device code. Uses of the module, export and import keywords are diagnosed as errors.



14.5.25.2. Coroutine supportï

Coroutines are not supported in device code. Uses of the co_await, co_yield and co_return keywords in the scope of a device function are diagnosed as error during device compilation.



14.5.25.3. Three-way comparison operatorï

The three-way comparison operator is supported in both host and device code, but some uses implicitly rely on functionality from the Standard Template Library provided by the host implementation. Uses of those operators may require specifying the flag --expt-relaxed-constexpr to silence warnings and the functionality requires that the host implementation satisfies the requirements of device code.
Example:

#include<compare>
struct S {
  int x, y, z;
  auto operator<=>(const S& rhs) const = default;
  __host__ __device__ bool operator<=>(int rhs) const { return false; }
};
__host__ __device__ bool f(S a, S b) {
  if (a <=> 1) // ok, calls a user-defined host-device overload
    return true;
  return a < b; // call to an implicitly-declared function and requires
                // a device-compatible std::strong_ordering implementation
}





14.5.25.4. Consteval functionsï

Ordinarily, cross execution space calls are not allowed, and cause a compiler diagnostic (warning or error). This restriction does not apply when the called function is declared with the consteval specifier. Thus, a __device__ or __global__ function can call a __host__consteval function, and a __host__ function can call a __device__ consteval function.
Example:

namespace N1 {
//consteval host function
consteval int hcallee() { return 10; }

__device__ int dfunc() { return hcallee(); /* OK */ }
__global__ void gfunc() { (void)hcallee(); /* OK */ }
__host__ __device__ int hdfunc() { return hcallee();  /* OK */ }
int hfunc() { return hcallee(); /* OK */ }
} // namespace N1


namespace N2 {
//consteval device function
consteval __device__ int dcallee() { return 10; }

__device__ int dfunc() { return dcallee(); /* OK */ }
__global__ void gfunc() { (void)dcallee(); /* OK */ }
__host__ __device__ int hdfunc() { return dcallee();  /* OK */ }
int hfunc() { return dcallee(); /* OK */ }
}







14.6. Polymorphic Function Wrappersï

A polymorphic function wrapper class template nvstd::function is provided in the nvfunctional header. Instances of this class template can be used to store, copy and invoke any callable target, e.g., lambda expressions. nvstd::function can be used in both host and device code.
Example:

#include <nvfunctional>

__device__ int foo_d() { return 1; }
__host__ __device__ int foo_hd () { return 2; }
__host__ int foo_h() { return 3; }

__global__ void kernel(int *result) {
  nvstd::function<int()> fn1 = foo_d;
  nvstd::function<int()> fn2 = foo_hd;
  nvstd::function<int()> fn3 =  []() { return 10; };

  *result = fn1() + fn2() + fn3();
}

__host__ __device__ void hostdevice_func(int *result) {
  nvstd::function<int()> fn1 = foo_hd;
  nvstd::function<int()> fn2 =  []() { return 10; };

  *result = fn1() + fn2();
}

__host__ void host_func(int *result) {
  nvstd::function<int()> fn1 = foo_h;
  nvstd::function<int()> fn2 = foo_hd;
  nvstd::function<int()> fn3 =  []() { return 10; };

  *result = fn1() + fn2() + fn3();
}


Instances of nvstd::function in host code cannot be initialized with the address of a __device__ function or with a functor whose operator() is a __device__ function. Instances of nvstd::function in device code cannot be initialized with the address of a __host__ function or with a functor whose operator() is a __host__ function.
nvstd::function instances cannot be passed from host code to device code (and vice versa) at run time. nvstd::function cannot be used in the parameter type of a __global__ function, if the __global__ function is launched from host code.
Example:

#include <nvfunctional>

__device__ int foo_d() { return 1; }
__host__ int foo_h() { return 3; }
auto lam_h = [] { return 0; };

__global__ void k(void) {
  // error: initialized with address of __host__ function
  nvstd::function<int()> fn1 = foo_h;

  // error: initialized with address of functor with
  // __host__ operator() function
  nvstd::function<int()> fn2 = lam_h;
}

__global__ void kern(nvstd::function<int()> f1) { }

void foo(void) {
  // error: initialized with address of __device__ function
  nvstd::function<int()> fn1 = foo_d;

  auto lam_d = [=] __device__ { return 1; };

  // error: initialized with address of functor with
  // __device__ operator() function
  nvstd::function<int()> fn2 = lam_d;

  // error: passing nvstd::function from host to device
  kern<<<1,1>>>(fn2);
}


nvstd::function is defined in the nvfunctional header as follows:

namespace nvstd {
  template <class _RetType, class ..._ArgTypes>
  class function<_RetType(_ArgTypes...)>
  {
    public:
      // constructors
      __device__ __host__  function() noexcept;
      __device__ __host__  function(nullptr_t) noexcept;
      __device__ __host__  function(const function &);
      __device__ __host__  function(function &&);

      template<class _F>
      __device__ __host__  function(_F);

      // destructor
      __device__ __host__  ~function();

      // assignment operators
      __device__ __host__  function& operator=(const function&);
      __device__ __host__  function& operator=(function&&);
      __device__ __host__  function& operator=(nullptr_t);
      __device__ __host__  function& operator=(_F&&);

      // swap
      __device__ __host__  void swap(function&) noexcept;

      // function capacity
      __device__ __host__  explicit operator bool() const noexcept;

      // function invocation
      __device__ _RetType operator()(_ArgTypes...) const;
  };

  // null pointer comparisons
  template <class _R, class... _ArgTypes>
  __device__ __host__
  bool operator==(const function<_R(_ArgTypes...)>&, nullptr_t) noexcept;

  template <class _R, class... _ArgTypes>
  __device__ __host__
  bool operator==(nullptr_t, const function<_R(_ArgTypes...)>&) noexcept;

  template <class _R, class... _ArgTypes>
  __device__ __host__
  bool operator!=(const function<_R(_ArgTypes...)>&, nullptr_t) noexcept;

  template <class _R, class... _ArgTypes>
  __device__ __host__
  bool operator!=(nullptr_t, const function<_R(_ArgTypes...)>&) noexcept;

  // specialized algorithms
  template <class _R, class... _ArgTypes>
  __device__ __host__
  void swap(function<_R(_ArgTypes...)>&, function<_R(_ArgTypes...)>&);
}





14.7. Extended Lambdasï

The nvcc flag '--extended-lambda' allows explicit execution space annotations in a lambda expression 29. The execution space annotations should be present after the âlambda-introducerâ and before the optional âlambda-declaratorâ. nvcc will define the macro __CUDACC_EXTENDED_LAMBDA__ when the '--extended-lambda' flag has been specified.
An âextended __device__ lambdaâ is a lambda expression that is annotated explicitly with â__device__â, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function.
An âextended __host__ __device__ lambdaâ is a lambda expression that is annotated explicitly with both â__host__â and â__device__â, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function.
An âextended lambdaâ denotes either an extended __device__ lambda or an extended __host__ __device__ lambda. Extended lambdas can be used in the type arguments of __global__ function template instantiation.
If the execution space annotations are not explicitly specified, they are computed based on the scopes enclosing the closure class associated with the lambda, as described in the section on C++11 support. The execution space annotations are applied to all methods of the closure class associated with the lambda.
Example:

void foo_host(void) {
  // not an extended lambda: no explicit execution space annotations
  auto lam1 = [] { };

  // extended __device__ lambda
  auto lam2 = [] __device__ { };

  // extended __host__ __device__ lambda
  auto lam3 = [] __host__ __device__ { };

  // not an extended lambda: explicitly annotated with only '__host__'
  auto lam4 = [] __host__ { };
}

__host__ __device__ void foo_host_device(void) {
  // not an extended lambda: no explicit execution space annotations
  auto lam1 = [] { };

  // extended __device__ lambda
  auto lam2 = [] __device__ { };

  // extended __host__ __device__ lambda
  auto lam3 = [] __host__ __device__ { };

  // not an extended lambda: explicitly annotated with only '__host__'
  auto lam4 = [] __host__ { };
}

__device__ void foo_device(void) {
  // none of the lambdas within this function are extended lambdas,
  // because the enclosing function is not a __host__ or __host__ __device__
  // function.
  auto lam1 = [] { };
  auto lam2 = [] __device__ { };
  auto lam3 = [] __host__ __device__ { };
  auto lam4 = [] __host__ { };
}

// lam1 and lam2 are not extended lambdas because they are not defined
// within a __host__ or __host__ __device__ function.
auto lam1 = [] { };
auto lam2 = [] __host__ __device__ { };




14.7.1. Extended Lambda Type Traitsï

The compiler provides type traits to detect closure types for extended lambdas at compile time:
__nv_is_extended_device_lambda_closure_type(type): If âtypeâ is the closure class created for an extended __device__ lambda, then the trait is true, otherwise it is false.
__nv_is_extended_device_lambda_with_preserved_return_type(type): If âtypeâ is the closure class created for an extended __device__ lambda and the lambda is defined with trailing return type (with restriction), then the trait is true, otherwise it is false. If the trailing return type definition refers to any lambda parameter name, the return type is not preserved.
__nv_is_extended_host_device_lambda_closure_type(type): If âtypeâ is the closure class created for an extended __host__ __device__ lambda, then the trait is true, otherwise it is false.
These traits can be used in all compilation modes, irrespective of whether lambdas or extended lambdas are enabled30.
Example:

#define IS_D_LAMBDA(X) __nv_is_extended_device_lambda_closure_type(X)
#define IS_DPRT_LAMBDA(X) __nv_is_extended_device_lambda_with_preserved_return_type(X)
#define IS_HD_LAMBDA(X) __nv_is_extended_host_device_lambda_closure_type(X)

auto lam0 = [] __host__ __device__ { };

void foo(void) {
  auto lam1 = [] { };
  auto lam2 = [] __device__ { };
  auto lam3 = [] __host__ __device__ { };
  auto lam4 = [] __device__ () --> double { return 3.14; }
  auto lam5 = [] __device__ (int x) --> decltype(&x) { return 0; }

  // lam0 is not an extended lambda (since defined outside function scope)
  static_assert(!IS_D_LAMBDA(decltype(lam0)), "");
  static_assert(!IS_DPRT_LAMBDA(decltype(lam0)), "");
  static_assert(!IS_HD_LAMBDA(decltype(lam0)), "");

  // lam1 is not an extended lambda (since no execution space annotations)
  static_assert(!IS_D_LAMBDA(decltype(lam1)), "");
  static_assert(!IS_DPRT_LAMBDA(decltype(lam1)), "");
  static_assert(!IS_HD_LAMBDA(decltype(lam1)), "");

  // lam2 is an extended __device__ lambda
  static_assert(IS_D_LAMBDA(decltype(lam2)), "");
  static_assert(!IS_DPRT_LAMBDA(decltype(lam2)), "");
  static_assert(!IS_HD_LAMBDA(decltype(lam2)), "");

  // lam3 is an extended __host__ __device__ lambda
  static_assert(!IS_D_LAMBDA(decltype(lam3)), "");
  static_assert(!IS_DPRT_LAMBDA(decltype(lam3)), "");
  static_assert(IS_HD_LAMBDA(decltype(lam3)), "");

  // lam4 is an extended __device__ lambda with preserved return type
  static_assert(IS_D_LAMBDA(decltype(lam4)), "");
  static_assert(IS_DPRT_LAMBDA(decltype(lam4)), "");
  static_assert(!IS_HD_LAMBDA(decltype(lam4)), "");

  // lam5 is not an extended __device__ lambda with preserved return type
  // because it references the operator()'s parameter types in the trailing return type.
  static_assert(IS_D_LAMBDA(decltype(lam5)), "");
  static_assert(!IS_DPRT_LAMBDA(decltype(lam5)), "");
  static_assert(!IS_HD_LAMBDA(decltype(lam5)), "");
}





14.7.2. Extended Lambda Restrictionsï

The CUDA compiler will replace an extended lambda expression with an instance of a placeholder type defined in namespace scope, before invoking the host compiler. The template argument of the placeholder type requires taking the address of a function enclosing the original extended lambda expression. This is required for the correct execution of any __global__ function template whose template argument involves the closure type of an extended lambda. The enclosing function is computed as follows.
By definition, the extended lambda is present within the immediate or nested block scope of a __host__ or __host__ __device__ function. If this function is not the operator() of a lambda expression, then it is considered the enclosing function for the extended lambda. Otherwise, the extended lambda is defined within the immediate or nested block scope of the operator() of one or more enclosing lambda expressions. If the outermost such lambda expression is defined in the immediate or nested block scope of a function F, then F is the computed enclosing function, else the enclosing function does not exist.
Example:

void foo(void) {
  // enclosing function for lam1 is "foo"
  auto lam1 = [] __device__ { };

  auto lam2 = [] {
     auto lam3 = [] {
        // enclosing function for lam4 is "foo"
        auto lam4 = [] __host__ __device__ { };
     };
  };
}

auto lam6 = [] {
  // enclosing function for lam7 does not exist
  auto lam7 = [] __host__ __device__ { };
};


Here are the restrictions on extended lambdas:


An extended lambda cannot be defined inside another extended lambda expression.
Example:

void foo(void) {
  auto lam1 = [] __host__ __device__  {
    // error: extended lambda defined within another extended lambda
    auto lam2 = [] __host__ __device__ { };
  };
}




An extended lambda cannot be defined inside a generic lambda expression.
Example:

void foo(void) {
  auto lam1 = [] (auto) {
    // error: extended lambda defined within a generic lambda
    auto lam2 = [] __host__ __device__ { };
  };
}




If an extended lambda is defined within the immediate or nested block scope of one or more nested lambda expression, the outermost such lambda expression must be defined inside the immediate or nested block scope of a function.
Example:

auto lam1 = []  {
  // error: outer enclosing lambda is not defined within a
  // non-lambda-operator() function.
  auto lam2 = [] __host__ __device__ { };
};




The enclosing function for the extended lambda must be named and its address can be taken. If the enclosing function is a class member, then the following conditions must be satisfied:

All classes enclosing the member function must have a name.
The member function must not have private or protected access within its parent class.
All enclosing classes must not have private or protected access within their respective parent classes.

Example:

void foo(void) {
  // OK
  auto lam1 = [] __device__ { return 0; };
  {
    // OK
    auto lam2 = [] __device__ { return 0; };
    // OK
    auto lam3 = [] __device__ __host__ { return 0; };
  }
}

struct S1_t {
  S1_t(void) {
    // Error: cannot take address of enclosing function
    auto lam4 = [] __device__ { return 0; };
  }
};

class C0_t {
  void foo(void) {
    // Error: enclosing function has private access in parent class
    auto temp1 = [] __device__ { return 10; };
  }
  struct S2_t {
    void foo(void) {
      // Error: enclosing class S2_t has private access in its
      // parent class
      auto temp1 = [] __device__ { return 10; };
    }
  };
};




It must be possible to take the address of the enclosing routine unambiguously, at the point where the extended lambda has been defined. This may not be feasible in some cases e.g. when a class typedef shadows a template type argument of the same name.
Example:

template <typename> struct A {
  typedef void Bar;
  void test();
};

template<> struct A<void> { };

template <typename Bar>
void A<Bar>::test() {
  /* In code sent to host compiler, nvcc will inject an
     address expression here, of the form:
     (void (A< Bar> ::*)(void))(&A::test))

     However, the class typedef 'Bar' (to void) shadows the
     template argument 'Bar', causing the address
     expression in A<int>::test to actually refer to:
     (void (A< void> ::*)(void))(&A::test))

     ..which doesn't take the address of the enclosing
     routine 'A<int>::test' correctly.
  */
  auto lam1 = [] __host__ __device__ { return 4; };
}

int main() {
  A<int> xxx;
  xxx.test();
}




An extended lambda cannot be defined in a class that is local to a function.
Example:

void foo(void) {
  struct S1_t {
    void bar(void) {
      // Error: bar is member of a class that is local to a function.
      auto lam4 = [] __host__ __device__ { return 0; };
    }
  };
}




The enclosing function for an extended lambda cannot have deduced return type.
Example:

auto foo(void) {
  // Error: the return type of foo is deduced.
  auto lam1 = [] __host__ __device__ { return 0; };
}




__host__ __device__ extended lambdas cannot be generic lambdas.
Example:

void foo(void) {
  // Error: __host__ __device__ extended lambdas cannot be
  // generic lambdas.
  auto lam1 = [] __host__ __device__ (auto i) { return i; };

  // Error: __host__ __device__ extended lambdas cannot be
  // generic lambdas.
  auto lam2 = [] __host__ __device__ (auto ...i) {
               return sizeof...(i);
              };
}




If the enclosing function is an instantiation of a function template or a member function template, and/or the function is a member of a class template, the template(s) must satisfy the following constraints:

The template must have at most one variadic parameter, and it must be listed last in the template parameter list.
The template parameters must be named.
The template instantiation argument types cannot involve types that are either local to a function (except for closure types for extended lambdas), or are private or protected class members.

Example:

template <typename T>
__global__ void kern(T in) { in(); }

template <typename... T>
struct foo {};

template < template <typename...> class T, typename... P1,
          typename... P2>
void bar1(const T<P1...>, const T<P2...>) {
  // Error: enclosing function has multiple parameter packs
  auto lam1 =  [] __device__ { return 10; };
}

template < template <typename...> class T, typename... P1,
          typename T2>
void bar2(const T<P1...>, T2) {
  // Error: for enclosing function, the
  // parameter pack is not last in the template parameter list.
  auto lam1 =  [] __device__ { return 10; };
}

template <typename T, T>
void bar3(void) {
  // Error: for enclosing function, the second template
  // parameter is not named.
  auto lam1 =  [] __device__ { return 10; };
}

int main() {
  foo<char, int, float> f1;
  foo<char, int> f2;
  bar1(f1, f2);
  bar2(f1, 10);
  bar3<int, 10>();
}


Example:

template <typename T>
__global__ void kern(T in) { in(); }

template <typename T>
void bar4(void) {
  auto lam1 =  [] __device__ { return 10; };
  kern<<<1,1>>>(lam1);
}

struct C1_t { struct S1_t { }; friend int main(void); };
int main() {
  struct S1_t { };
  // Error: enclosing function for device lambda in bar4
  // is instantiated with a type local to main.
  bar4<S1_t>();

  // Error: enclosing function for device lambda in bar4
  // is instantiated with a type that is a private member
  // of a class.
  bar4<C1_t::S1_t>();
}



With Visual Studio host compilers, the enclosing function must have external linkage. The restriction is present because this host compiler does not support using the address of non-extern linkage functions as template arguments, which is needed by the CUDA compiler transformations to support extended lambdas.
With Visual Studio host compilers, an extended lambda shall not be defined within the body of an âif-constexprâ block.

An extended lambda has the following restrictions on captured variables:

In the code sent to the host compiler, the variable may be passed by value to a sequence of helper functions before being used to direct-initialize the field of the class type used to represent the closure type for the extended lambda31.
A variable can only be captured by value.
A variable of array type cannot be captured if the number of array dimensions is greater than 7.
For a variable of array type, in the code sent to the host compiler, the closure typeâs array field is first default-initialized, and then each element of the array field is copy-assigned from the corresponding element of the captured array variable. Therefore, the array element type must be default-constructible and copy-assignable in host code.
A function parameter that is an element of a variadic argument pack cannot be captured.
The type of the captured variable cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members.
For a __host__ __device__ extended lambda, the types used in the return or parameter types of the lambda expressionâs operator() cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members.
Init-capture is not supported for __host__ __device__ extended lambdas. Init-capture is supported for __device__ extended lambdas, except when the init-capture is of array type or of type std::initializer_list.
The function call operator for an extended lambda is not constexpr. The closure type for an extended lambda is not a literal type. The constexpr and consteval specifier cannot be used in the declaration of an extended lambda.
A variable cannot be implicitly captured inside an if-constexpr block lexically nested inside an extended lambda, unless it has already been implicitly captured earlier outside the if-constexpr block or appears in the explicit capture list for the extended lambda (see example below).

Example

void foo(void) {
  // OK: an init-capture is allowed for an
  // extended __device__ lambda.
  auto lam1 = [x = 1] __device__ () { return x; };

  // Error: an init-capture is not allowed for
  // an extended __host__ __device__ lambda.
  auto lam2 = [x = 1] __host__ __device__ () { return x; };

  int a = 1;
  // Error: an extended __device__ lambda cannot capture
  // variables by reference.
  auto lam3 = [&a] __device__ () { return a; };

  // Error: by-reference capture is not allowed
  // for an extended __device__ lambda.
  auto lam4 = [&x = a] __device__ () { return x; };

  struct S1_t { };
  S1_t s1;
  // Error: a type local to a function cannot be used in the type
  // of a captured variable.
  auto lam6 = [s1] __device__ () { };

  // Error: an init-capture cannot be of type std::initializer_list.
  auto lam7 = [x = {11}] __device__ () { };

  std::initializer_list<int> b = {11,22,33};
  // Error: an init-capture cannot be of type std::initializer_list.
  auto lam8 = [x = b] __device__ () { };

  // Error scenario (lam9) and supported scenarios (lam10, lam11)
  // for capture within 'if-constexpr' block
  int yyy = 4;
  auto lam9 = [=] __device__ {
    int result = 0;
    if constexpr(false) {
      //Error: An extended __device__ lambda cannot first-capture
      //      'yyy' in constexpr-if context
      result += yyy;
    }
    return result;
  };

  auto lam10 = [yyy] __device__ {
    int result = 0;
    if constexpr(false) {
      //OK: 'yyy' already listed in explicit capture list for the extended lambda
      result += yyy;
    }
    return result;
  };

  auto lam11 = [=] __device__ {
    int result = yyy;
    if constexpr(false) {
      //OK: 'yyy' already implicit captured outside the 'if-constexpr' block
      result += yyy;
    }
    return result;
  };
}




When parsing a function, the CUDA compiler assigns a counter value to each extended lambda within that function. This counter value is used in the substituted named type passed to the host compiler. Hence, whether or not an extended lambda is defined within a function should not depend on a particular value of __CUDA_ARCH__, or on __CUDA_ARCH__ being undefined.
Example

template <typename T>
__global__ void kernel(T in) { in(); }

__host__ __device__ void foo(void) {
  // Error: the number and relative declaration
  // order of extended lambdas depends on
  // __CUDA_ARCH__
#if defined(__CUDA_ARCH__)
  auto lam1 = [] __device__ { return 0; };
  auto lam1b = [] __host___ __device__ { return 10; };
#endif
  auto lam2 = [] __device__ { return 4; };
  kernel<<<1,1>>>(lam2);
}




As described above, the CUDA compiler replaces a __device__ extended lambda defined in a host function with a placeholder type defined in namespace scope. Unless the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true for the closure type of the extended lambda, the placeholder type does not define a operator() function equivalent to the original lambda declaration. An attempt to determine the return type or parameter types of the operator() function of such a lambda may therefore work incorrectly in host code, as the code processed by the host compiler will be semantically different than the input code processed by the CUDA compiler. However, it is OK to introspect the return type or parameter types of the operator() function within device code. Note that this restriction does not apply to __host__ __device__ extended lambdas, or to __device__ extended lambdas for which the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true.
Example

#include <type_traits>
const char& getRef(const char* p) { return *p; }

void foo(void) {
  auto lam1 = [] __device__ { return "10"; };

  // Error: attempt to extract the return type
  // of a __device__ lambda in host code
  std::result_of<decltype(lam1)()>::type xx1 = "abc";


  auto lam2 = [] __host__ __device__  { return "10"; };

  // OK : lam2 represents a __host__ __device__ extended lambda
  std::result_of<decltype(lam2)()>::type xx2 = "abc";

  auto lam3 = []  __device__ () -> const char * { return "10"; };

  // OK : lam3 represents a __device__ extended lambda with preserved return type
  std::result_of<decltype(lam3)()>::type xx2 = "abc";
  static_assert( std::is_same_v< std::result_of<decltype(lam3)()>::type, const char *>);

  auto lam4 = [] __device__ (char x) -> decltype(getRef(&x)) { return 0; };
  // lam4's return type is not preserved because it references the operator()'s
  // parameter types in the trailing return type.
  static_assert( ! __nv_is_extended_device_lambda_with_preserved_return_type(decltype(lam4)), "" );
}



For an extended device lambda:
-  Introspecting the parameter type of operator() is only supported in device code.
-  Introspecting the return type of operator() is supported only in device code, unless the trait function __nv_is_extended_device_lambda_with_preserved_return_type() returns true.

If the functor object represented by an extended lambda is passed from host to device code (e.g., as the argument of a __global__ function), then any expression in the body of the lambda expression that captures variables must be remain unchanged irrespective of whether the __CUDA_ARCH__ macro is defined, and whether the macro has a particular value. This restriction arises because the lambdaâs closure class layout depends on the order in which captured variables are encountered when the compiler processes the lambda expression; the program may execute incorrectly if the closure class layout differs in device and host compilation.
Example

__device__ int result;

template <typename T>
__global__ void kernel(T in) { result = in(); }

void foo(void) {
  int x1 = 1;
  auto lam1 = [=] __host__ __device__ {
    // Error: "x1" is only captured when __CUDA_ARCH__ is defined.
#ifdef __CUDA_ARCH__
    return x1 + 1;
#else
    return 10;
#endif
  };
  kernel<<<1,1>>>(lam1);
}




As described previously, the CUDA compiler replaces an extended __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler. This placeholder type does not define a pointer-to-function conversion operator in host code, however the conversion operator is provided in device code. Note that this restriction does not apply to __host__ __device__ extended lambdas.
Example

template <typename T>
__global__ void kern(T in) {
  int (*fp)(double) = in;

  // OK: conversion in device code is supported
  fp(0);
  auto lam1 = [](double) { return 1; };

  // OK: conversion in device code is supported
  fp = lam1;
  fp(0);
}

void foo(void) {
  auto lam_d = [] __device__ (double) { return 1; };
  auto lam_hd = [] __host__ __device__ (double) { return 1; };
  kern<<<1,1>>>(lam_d);
  kern<<<1,1>>>(lam_hd);

  // OK : conversion for __host__ __device__ lambda is supported
  // in host code
  int (*fp)(double) = lam_hd;

  // Error: conversion for __device__ lambda is not supported in
  // host code.
  int (*fp2)(double) = lam_d;
}




As described previously, the CUDA compiler replaces an extended __device__ or __host__ __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler. This placeholder type may define C++ special member functions (e.g. constructor, destructor). As a result, some standard C++ type traits may return different results for the closure type of the extended lambda, in the CUDA frontend compiler versus the host compiler. The following type traits are affected: std::is_trivially_copyable, std::is_trivially_constructible, std::is_trivially_copy_constructible, std::is_trivially_move_constructible, std::is_trivially_destructible.
Care must be taken that the results of these type traits are not used in __global__ function template instantiation or in __device__ / __constant__ / __managed__ variable template instantiation.
Example

template <bool b>
void __global__ foo() { printf("hi"); }

template <typename T>
void dolaunch() {

// ERROR: this kernel launch may fail, because CUDA frontend compiler
// and host compiler may disagree on the result of
// std::is_trivially_copyable() trait on the closure type of the
// extended lambda
foo<std::is_trivially_copyable<T>::value><<<1,1>>>();
cudaDeviceSynchronize();
}

int main() {
int x = 0;
auto lam1 = [=] __host__ __device__ () { return x; };
dolaunch<decltype(lam1)>();
}




The CUDA compiler will generate compiler diagnostics for a subset of cases described in 1-12; no diagnostic will be generated for cases 13-17, but the host compiler may fail to compile the generated code.



14.7.3. Notes on __host__ __device__ lambdasï

Unlike __device__ lambdas, __host__ __device__ lambdas can be called from host code. As described earlier, the CUDA compiler replaces an extended lambda expression defined in host code with an instance of a named placeholder type. The placeholder type for an extended __host__ __device__ lambda invokes the original lambdaâs operator() with an indirect function call 30.
The presence of the indirect function call may cause an extended __host__ __device__ lambda to be less optimized by the host compiler than lambdas that are implicitly or explicitly __host__ only. In the latter case, the host compiler can easily inline the body of the lambda into the calling context. But in case of an extended __host__Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  __device__ lambda, the host compiler encounters the indirect function call and may not be able to easily inline the original __host__ __device__ lambda body.



14.7.4. *this Capture By Valueï

When a lambda is defined within a non-static class member function, and the body of the lambda refers to a class member variable, C++11/C++14 rules require that the this pointer of the class is captured by value, instead of the referenced member variable. If the lambda is an extended __device__ or __host____device__ lambda defined in a host function, and the lambda is executed on the GPU, accessing the referenced member variable on the GPU will cause a run time error if the this pointer points to host memory.
Example:

#include <cstdio>

template <typename T>
__global__ void foo(T in) { printf("\n value = %d", in()); }

struct S1_t {
  int xxx;
  __host__ __device__ S1_t(void) : xxx(10) { };

  void doit(void) {

    auto lam1 = [=] __device__ {
       // reference to "xxx" causes
       // the 'this' pointer (S1_t*) to be captured by value
       return xxx + 1;

    };

    // Kernel launch fails at run time because 'this->xxx'
    // is not accessible from the GPU
    foo<<<1,1>>>(lam1);
    cudaDeviceSynchronize();
  }
};

int main(void) {
  S1_t s1;
  s1.doit();
}


C++17 solves this problem by adding a new â*thisâ capture mode. In this mode, the compiler makes a copy of the object denoted by â*thisâ instead of capturing the pointer this by value. The â*thisâ capture mode is described in more detail here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html .
The CUDA compiler supports the â*thisâ capture mode for lambdas defined within __device__ and __global__ functions and for extended __device__ lambdas defined in host code, when the --extended-lambda nvcc flag is used.
Hereâs the above example modified to use â*thisâ capture mode:

#include <cstdio>

template <typename T>
__global__ void foo(T in) { printf("\n value = %d", in()); }

struct S1_t {
  int xxx;
  __host__ __device__ S1_t(void) : xxx(10) { };

  void doit(void) {

    // note the "*this" capture specification
    auto lam1 = [=, *this] __device__ {

       // reference to "xxx" causes
       // the object denoted by '*this' to be captured by
       // value, and the GPU code will access copy_of_star_this->xxx
       return xxx + 1;

    };

    // Kernel launch succeeds
    foo<<<1,1>>>(lam1);
    cudaDeviceSynchronize();
  }
};

int main(void) {
  S1_t s1;
  s1.doit();
}


â*thisâ capture mode is not allowed for unannotated lambdas defined in host code, or for extended __host____device__ lambdas. Examples of supported and unsupported usage:

struct S1_t {
  int xxx;
  __host__ __device__ S1_t(void) : xxx(10) { };

  void host_func(void) {

    // OK: use in an extended __device__ lambda
    auto lam1 = [=, *this] __device__ { return xxx; };

    // Error: use in an extended __host__ __device__ lambda
    auto lam2 = [=, *this] __host__ __device__ { return xxx; };

    // Error: use in an unannotated lambda in host function
    auto lam3 = [=, *this]  { return xxx; };
  }

  __device__ void device_func(void) {

    // OK: use in a lambda defined in a __device__ function
    auto lam1 = [=, *this] __device__ { return xxx; };

    // OK: use in a lambda defined in a __device__ function
    auto lam2 = [=, *this] __host__ __device__ { return xxx; };

    // OK: use in a lambda defined in a __device__ function
    auto lam3 = [=, *this]  { return xxx; };
  }

   __host__ __device__ void host_device_func(void) {

    // OK: use in an extended __device__ lambda
    auto lam1 = [=, *this] __device__ { return xxx; };

    // Error: use in an extended __host__ __device__ lambda
    auto lam2 = [=, *this] __host__ __device__ { return xxx; };

    // Error: use in an unannotated lambda in a __host__ __device__ function
    auto lam3 = [=, *this]  { return xxx; };
  }
};





14.7.5. Additional Notesï



ADL Lookup: As described earlier, the CUDA compiler will replace an extended lambda expression with an instance of a placeholder type, before invoking the host compiler. One template argument of the placeholder type uses the address of the function enclosing the original lambda expression. This may cause additional namespaces to participate in argument dependent lookup (ADL), for any host function call whose argument types involve the closure type of the extended lambda expression. This may cause an incorrect function to be selected by the host compiler.
Example:

namespace N1 {
  struct S1_t { };
  template <typename T>  void foo(T);
};

namespace N2 {
  template <typename T> int foo(T);

  template <typename T>  void doit(T in) {     foo(in);  }
}

void bar(N1::S1_t in) {
  /* extended __device__ lambda. In the code sent to the host compiler, this
     is replaced with the placeholder type instantiation expression
     ' __nv_dl_wrapper_t< __nv_dl_tag<void (*)(N1::S1_t in),(&bar),1> > { }'

     As a result, the namespace 'N1' participates in ADL lookup of the
     call to "foo" in the body of N2::doit, causing ambiguity.
  */
  auto lam1 = [=] __device__ { };
  N2::doit(lam1);
}


In the example above, the CUDA compiler replaced the extended lambda with a placeholder type that involves the N1 namespace. As a result, the namespace N1 participates in the ADL lookup for foo(in) in the body of N2::doit, and host compilation fails because multiple overload candidates N1::foo and N2::foo are found.






14.8. Code Samplesï



14.8.1. Data Aggregation Classï


class PixelRGBA {
public:
    __device__ PixelRGBA(): r_(0), g_(0), b_(0), a_(0) { }

    __device__ PixelRGBA(unsigned char r, unsigned char g,
                         unsigned char b, unsigned char a = 255):
                         r_(r), g_(g), b_(b), a_(a) { }

private:
    unsigned char r_, g_, b_, a_;

    friend PixelRGBA operator+(const PixelRGBA&, const PixelRGBA&);
};

__device__
PixelRGBA operator+(const PixelRGBA& p1, const PixelRGBA& p2)
{
    return PixelRGBA(p1.r_ + p2.r_, p1.g_ + p2.g_,
                     p1.b_ + p2.b_, p1.a_ + p2.a_);
}

__device__ void func(void)
{
    PixelRGBA p1, p2;
    // ...      // Initialization of p1 and p2 here
    PixelRGBA p3 = p1 + p2;
}





14.8.2. Derived Classï


__device__ void* operator new(size_t bytes, MemoryPool& p);
__device__ void operator delete(void*, MemoryPool& p);
class Shape {
public:
    __device__ Shape(void) { }
    __device__ void putThis(PrintBuffer *p) const;
    __device__ virtual void Draw(PrintBuffer *p) const {
         p->put("Shapeless");
    }
    __device__ virtual ~Shape() {}
};
class Point : public Shape {
public:
    __device__ Point() : x(0), y(0) {}
    __device__ Point(int ix, int iy) : x(ix), y(iy) { }
    __device__ void PutCoord(PrintBuffer *p) const;
    __device__ void Draw(PrintBuffer *p) const;
    __device__ ~Point() {}
private:
    int x, y;
};
__device__ Shape* GetPointObj(MemoryPool& pool)
{
    Shape* shape = new(pool) Point(rand(-20,10), rand(-100,-20));
    return shape;
}





14.8.3. Class Templateï


template <class T>
class myValues {
    T values[MAX_VALUES];
public:
    __device__ myValues(T clear) { ... }
    __device__ void setValue(int Idx, T value) { ... }
    __device__ void putToMemory(T* valueLocation) { ... }
};

template <class T>
void __global__ useValues(T* memoryBuffer) {
    myValues<T> myLocation(0);
    ...
}

__device__ void* buffer;

int main()
{
    ...
    useValues<int><<<blocks, threads>>>(buffer);
    ...
}





14.8.4. Function Templateï


template <typename T>
__device__ bool func(T x)
{
   ...
   return (...);
}

template <>
__device__ bool func<int>(T x) // Specialization
{
   return true;
}

// Explicit argument specification
bool result = func<double>(0.5);

// Implicit argument deduction
int x = 1;
bool result = func(x);





14.8.5. Functor Classï


class Add {
public:
    __device__  float operator() (float a, float b) const
    {
        return a + b;
    }
};

class Sub {
public:
    __device__  float operator() (float a, float b) const
    {
        return a - b;
    }
};

// Device code
template<class O> __global__
void VectorOperation(const float * A, const float * B, float * C,
                     unsigned int N, O op)
{
    unsigned int iElement = blockDim.x * blockIdx.x + threadIdx.x;
    if (iElement < N)
        C[iElement] = op(A[iElement], B[iElement]);
}

// Host code
int main()
{
    ...
    VectorOperation<<<blocks, threads>>>(v1, v2, v3, N, Add());
    ...
}



15

e.g., the <<<...>>> syntax for launching kernels.

16

This does not apply to entities that may be defined in more than one translation unit, such as compiler generated template instantiations.

17

The intent is to allow variable memory space specifiers for static variables in a __host__ __device__ function during device compilation, but disallow it during host compilation

18

One way to debug suspected layout mismatch of a type C is to use printf to output the values of sizeof(C) and offsetof(C, field) in host and device code.

19

Note that this may negatively impact compile time due to presence of extra declarations.

20

At present, the -std=c++11 flag is supported only for the following host compilers : gcc version >= 4.7, clang, icc >= 15, and xlc >= 13.1

21

including operator()

22

The restrictions are the same as with a non-constexpr callee function.

23

Note that the behavior of experimental flags may change in future compiler releases.

24

C++ Standard Section [basic.types]

25

C++ Standard Section [expr.const]

26

At present, the -std=c++14 flag is supported only for the following host compilers : gcc version >= 5.1, clang version >= 3.7 and icc version >= 17

27

At present, the -std=c++17 flag is supported only for the following host compilers : gcc version >= 7.0, clang version >= 8.0, Visual Studio version >= 2017, pgi compiler version >= 19.0, icc compiler version >= 19.0

28

At present, the -std=c++20 flag is supported only for the following host compilers : gcc version >= 10.0,  clang version >= 10.0, Visual Studio Version >= 2022 and nvc++ version >= 20.7.

29

When using the icc host compiler, this flag is only supported for icc >= 1800.


30(1,2)


The traits will always return false if extended lambda mode is not active.

31

In contrast, the C++ standard specifies that the captured variable is used to direct-initialize the field of the closure type.







15. Texture Fetchingï

This section gives the formula used to compute the value returned by the texture functions of Texture Functions depending on the various attributes of the texture object (see Texture and Surface Memory).
The texture bound to the texture object is represented as an array T of

N texels for a one-dimensional texture,
N x M texels for a two-dimensional texture,
N x M x L texels for a three-dimensional texture.

It is fetched using non-normalized texture coordinates x, y, and z, or the normalized texture coordinates x/N, y/M, and z/L as described in Texture Memory. In this section, the coordinates are assumed to be in the valid range. Texture Memory explained how out-of-range coordinates are remapped to the valid range based on the addressing mode.


15.1. Nearest-Point Samplingï

In this filtering mode, the value returned by the texture fetch is

tex(x)=T[i] for a one-dimensional texture,
tex(x,y)=T[i,j] for a two-dimensional texture,
tex(x,y,z)=T[i,j,k] for a three-dimensional texture,

where i=floor(x), j=floor(y), and k=floor(z).
FIgure 32 illustrates nearest-point sampling for a one-dimensional texture with N=4.



Figure 32 Nearest-Point Sampling Filtering Modeï


For integer textures, the value returned by the texture fetch can be optionally remapped to [0.0, 1.0] (see Texture Memory).



15.2. Linear Filteringï

In this filtering mode, which is only available for floating-point textures, the value returned by the texture fetch is

\(tex(x)=(1-\alpha)T[i]+{\alpha}T[i+1]\) for a one-dimensional texture,
\(tex(x)=(1-\alpha)T[i]+{\alpha}T[i+1]\) for a one-dimensional texture,
\(tex(x,y)=(1-\alpha)(1-\beta)T[i,j]+\alpha(1-\beta)T[i+1,j]+(1-\alpha){\beta}T[i,j+1]+\alpha{\beta}T[i+1,j+1]\) for a two-dimensional texture,

\(tex(x,y,z)\) =
\((1-\alpha)(1-\beta)(1-\gamma)T[i,j,k]+\alpha(1-\beta)(1-\gamma)T[i+1,j,k]+\)
\((1-\alpha)\beta(1-\gamma)T[i,j+1,k]+\alpha\beta(1-\gamma)T[i+1,j+1,k]+\)
\((1-\alpha)(1-\beta){\gamma}T[i,j,k+1]+\alpha(1-\beta){\gamma}T[i+1,j,k+1]+\)
\((1-\alpha)\beta{\gamma}T[i,j+1,k+1]+\alpha\beta{\gamma}T[i+1,j+1,k+1]\)
for a three-dimensional texture,


where:

\(i=floor(x\ B)*, \alpha=frac(x\ B)*, *x\ B\ =x-0.5,\)
\(j=floor(y\ B)*, \beta=frac(y\ B)*, *y\ B\ =y-0.5,\)
\(k=floor(z\ B)*, \gamma=frac(z\ B)*, *z\ B\ = z-0.5,\)

\(\alpha\), \(\beta\), and \(\gamma\) are stored in 9-bit fixed point format with 8 bits of fractional value (so 1.0 is exactly represented).
Figure 33 illustrates linear filtering of a one-dimensional texture with N=4.



Figure 33 Linear Filtering Modeï





15.3. Table Lookupï

A table lookup TL(x) where x spans the interval [0,R] can be implemented as TL(x)=tex((N-1)/R)x+0.5) in order to ensure that TL(0)=T[0] and TL(R)=T[N-1].
Figure 34 illustrates the use of texture filtering to implement a table lookup with R=4 or R=1 from a one-dimensional texture with N=4.



Figure 34 One-Dimensional Table Lookup Using Linear Filteringï






16. Compute Capabilitiesï

The general specifications and features of a compute device depend on its compute capability (see Compute Capability).
Table 20 and Table 21 show the features and technical specifications associated with each compute capability that is currently supported.
Floating-Point Standard reviews the compliance with the IEEE floating-point standard.
Sections Compute Capability 5.x, Compute Capability 6.x, Compute Capability 7.x, Compute Capability 8.x and Compute Capability 9.0 give more details on the architecture of devices of compute capabilities 5.x, 6.x, 7.x, 8.x and 9.0 respectively.


16.1. Feature Availabilityï

A compute feature is introduced with a compute architecture with the intention that the feature will be available on all subsequent architectures.  This is shown in Table 20 by the âyesâ for availability of a feature on compute capabilities subsequent to its introduction.
Highly specialized compute features that are introduced with an architecture may not be guaranteed to be available on all subsequent compute capabilities. These features target acceleration of specialized operations which are not intended for all classes of compute capabilities (denoted by the compute capabilityâs minor number) or are likely to significantly change on future generations (denoted by the compute capabilityâs major number).
There are potentially two sets of compute features for a given compute capability:
Compute Capability #.#: The predominant set of compute features that are introduced with the intent to be available for subsequent compute architectures.  These features and their availability are summarized in Table 20.
Compute Capability #.#a: A small and highly specialized set of features that are introduced to accelerate specialized operations, which are not guaranteed to be available or might change significantly on subsequent compute architecture.  These features are summarized in the respective âCompute Capability #.#ââ subsection.
Compilation of device code targets a particular compute capability.  A feature which appears in device code must be available for the targeted compute capability.  For example:

The compute_90 compilation target allows use of Compute Capability 9.0 features but does not allow use of Compute Capability 9.0a features.
The compute_90a compilation target allows use of the complete set of compute device features, both 9.0a features and 9.0 features.




16.2. Features and Technical Specificationsï



Table 20 Feature Support per Compute Capabilityï













Feature Support

Compute Capability




(Unlisted features are supported for all compute capabilities)
5.0, 5.2
5.3
6.x
7.x
8.x
9.0


Atomic functions operating on 32-bit integer values in global memory (Atomic Functions)
Yes


Atomic functions operating on 32-bit integer values in shared memory (Atomic Functions)
Yes


Atomic functions operating on 64-bit integer values in global memory (Atomic Functions)
Yes


Atomic functions operating on 64-bit integer values in shared memory (Atomic Functions)
Yes


Atomic functions operating on 128-bit integer values in global memory (Atomic Functions)
No
Yes


Atomic functions operating on 128-bit integer values in shared memory (Atomic Functions)
No
Yes


Atomic addition operating on 32-bit floating point values in global and shared memory (atomicAdd())
Yes


Atomic addition operating on 64-bit floating point values in global memory and shared memory (atomicAdd())
No
Yes


Atomic addition operating on float2 and float4 floating point vectors in global memory (atomicAdd())
No
Yes


Warp vote functions (Warp Vote Functions)
Yes


Memory fence functions (Memory Fence Functions)
Yes


Synchronization functions (Synchronization Functions)
Yes


Surface functions (Surface Functions)
Yes


Unified Memory Programming (Unified Memory Programming)
Yes


Dynamic Parallelism (CUDA Dynamic Parallelism)
Yes


Half-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion
No
Yes


Bfloat16-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion
No
Yes


Tensor Cores
No
Yes


Mixed Precision Warp-Matrix Functions (Warp matrix functions)
No
Yes


Hardware-accelerated memcpy_async (Asynchronous Data Copies using cuda::pipeline)
No
Yes


Hardware-accelerated Split Arrive/Wait Barrier (Asynchronous Barrier)
No
Yes


L2 Cache Residency Management (Device Memory L2 Access Management)
No
Yes


DPX Instructions for Accelerated Dynamic Programming
No
Yes


Distributed Shared Memory
No
Yes


Thread Block Cluster
No
Yes


Tensor Memory Accelerator (TMA) unit
No
Yes



Note that the KB and K units used in the following table correspond to 1024 bytes (i.e., a KiB) and 1024 respectively.


Table 21 Technical Specifications per Compute Capabilityï






















Compute Capability




Technical Specifications
5.0
5.2
5.3
6.0
6.1
6.2
7.0
7.2
7.5
8.0
8.6
8.7
8.9
9.0


Maximum number of
resident grids per device
(Concurrent Kernel
Execution)
32
16
128
32
16
128
16
128


Maximum dimensionality of
grid of thread blocks
3


Maximum x -dimension of a
grid of thread blocks
[thread blocks]
231-1


Maximum y- or z-dimension
of a grid of thread
blocks
65535


Maximum dimensionality of
thread block
3


Maximum x- or
y-dimensionality of a
block
1024


Maximum z-dimension
of a block
64


Maximum number of
threads per block
1024


Warp size
32


Maximum number of
resident blocks per SM
32
16
32
16
24
32


Maximum number of
resident warps per SM
64
32
64
48
64


Maximum number of
resident threads per SM
2048
1024
2048
1536
2048


Number of 32-bit
registers per SM
64 K


Maximum number of 32-bit
registers per thread
block
64 K
32 K
64 K
32 K
64 K


Maximum number of 32-bit
registers per thread
255


Maximum amount of shared
memory per SM
64 KB
96 KB
64 KB
96 KB
64 KB
96 KB
64 KB
164
KB
100
KB
164
KB
100
KB
228
KB


Maximum amount of shared
memory per thread block
32
48 KB
96 KB
96 KB
64 KB
163
KB
99 KB
163
KB
99 KB
227
KB


Number of shared
memory banks
32


Maximum amount of local
memory per thread
512 KB


Constant memory size
64 KB


Cache working set per SM
for constant memory
8 KB
4 KB
8 KB


Cache  working set per SM
for texture memory
Between 12 KB and 48 KB
Between 24 KB and 48 KB
32 ~ 128 KB
32 or
64 KB
28 KB
~ 192
KB
28 KB
~ 128
KB
28 KB
~ 192
KB
28 KB
~ 128
KB
28 KB
~ 256
KB


Maximum width for a 1D
texture object using
a CUDA array
65536
131072


Maximum width for a 1D
texture object using
linear memory
227
228
227
228
227
228


Maximum width and number
of layers for a 1D
layered texture object
16384
x
2048
32768
x
2048


Maximum width and height
for a 2D texture
object using a CUDA
array
65536
x
65536
131072
x
65536


Maximum width and height
for a 2D texture
object using
linear memory
65536
x
65536
131072
x
65000


Maximum width and height
for a 2D texture
object using a CUDA
array supporting texture
gather
16384
x
16384
32768
x
32768


Maximum width, height,
and number of layers for
a 2D layered texture
object
16384 x 16384 x 2048
32768 x 32768 x 2048


Maximum width, height,
and depth for a 3D
texture object using
to a CUDA array
4096 x 4096 x 4096
16384 x 16384 x 16384


Maximum width (and
height) for a cubemap
texture object
16384
32768


Maximum width (and
height) and number of
layers for a cubemap
layered texture object
16384
x
2046
32768
x
2046


Maximum number of
textures that can be
bound to a kernel
256


Maximum width for a 1D
surface object using
a CUDA array
16384
32768


Maximum width and number
of layers for a 1D
layered surface object
16384
x
2048
32768
x
2048


Maximum width and height
for a 2D surface
object using a
CUDA array
65536
x
65536
1
31072
x
65536


Maximum width, height,
and number of layers for
a 2D layered surface
object
16384
x
16384 x 2048
32768
x
32768 x 1048


Maximum width, height,
and depth for a 3D
surface object using
a CUDA array
4096
x
4096 x 4096
16384
x
16384 x 16384


Maximum width (and
height) for a cubemap
surface object using
a CUDA array
16384
32768


Maximum width (and
height) and number of
layers for a cubemap
layered surface object
16384
x
2046
32768
x
2046


Maximum number of
surfaces that can use
a kernel
16
32






16.3. Floating-Point Standardï

All compute devices follow the IEEE 754-2008 standard for binary floating-point arithmetic with the following deviations:

There is no dynamically configurable rounding mode; however, most of the operations support multiple IEEE rounding modes, exposed via device intrinsics.
There is no mechanism for detecting that a floating-point exception has occurred and all operations behave as if the IEEE-754 exceptions are always masked, and deliver the masked response as defined by IEEE-754 if there is an exceptional event. For the same reason, while SNaN encodings are supported, they are not signaling and are handled as quiet.
The result of a single-precision floating-point operation involving one or more input NaNs is the quiet NaN of bit pattern 0x7fffffff.
Double-precision floating-point absolute value and negation are not compliant with IEEE-754 with respect to NaNs; these are passed through unchanged.

Code must be compiled with -ftz=false, -prec-div=true, and -prec-sqrt=true to ensure IEEE compliance (this is the default setting; see the nvcc user manual for description of these compilation flags).
Regardless of the setting of the compiler flag -ftz,

atomic single-precision floating-point adds on global memory always operate in flush-to-zero mode, i.e., behave equivalent to FADD.F32.FTZ.RN,
atomic single-precision floating-point adds on shared memory always operate with denormal support, i.e., behave equivalent to FADD.F32.RN.

In accordance to the IEEE-754R standard, if one of the input parameters to fminf(), fmin(), fmaxf(), or fmax() is NaN, but not the other, the result is the non-NaN parameter.
The conversion of a floating-point value to an integer value in the case where the floating-point value falls outside the range of the integer format is left undefined by IEEE-754. For compute devices, the behavior is to clamp to the end of the supported range. This is unlike the x86 architecture behavior.
The behavior of integer division by zero and integer overflow is left undefined by IEEE-754. For compute devices, there is no mechanism for detecting that such integer operation exceptions have occurred. Integer division by zero yields an unspecified, machine-specific value.
https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus includes more information on the floating point accuracy and compliance of NVIDIA GPUs.



16.4. Compute Capability 5.xï



16.4.1. Architectureï

An SM consists of:

128 CUDA cores for arithmetic operations (see Arithmetic Instructions for throughputs of arithmetic operations),
32 special function units for single-precision floating-point transcendental functions,
4 warp schedulers.

When an SM is given warps to execute, it first distributes them among the four schedulers. Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
An SM has:

a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory,
a unified L1/texture cache of 24 KB used to cache reads from global memory,
64 KB of shared memory for devices of compute capability 5.0 or 96 KB of shared memory for devices of compute capability 5.2.

The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned in Texture and Surface Memory.
There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary register spills. Applications may query the L2 cache size by checking the l2CacheSize device property (see Device Enumeration).
The cache behavior (e.g., whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially configured on a per-access basis using modifiers to the load instruction.



16.4.2. Global Memoryï

Global memory accesses are always cached in L2.
Data that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture cache described in the previous section by reading it using the __ldg() function (see Read-Only Data Cache Load Function). When the compiler detects that the read-only condition is satisfied for some data, it will use __ldg() to read it. The compiler might not always be able to detect that the read-only condition is satisfied for some data. Marking pointers used for loading such data with both the const and __restrict__ qualifiers increases the likelihood that the compiler will detect the read-only condition.
Data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified L1/texture cache for devices of compute capability 5.0. For devices of compute capability 5.2, it is, by default, not cached in the unified L1/texture cache, but caching may be enabled using the following mechanisms:

Perform the read using inline assembly with the appropriate modifier as described in the PTX reference manual;
Compile with the -Xptxas -dlcm=ca compilation flag, in which case all reads are cached, except reads that are performed using inline assembly with a modifier that disables caching;
Compile with the -Xptxas -fscm=ca compilation flag, in which case all reads are cached, including reads that are performed using inline assembly regardless of the modifier used.

When caching is enabled using one of the three mechanisms listed above, devices of compute capability 5.2 will cache global memory reads in the unified L1/texture cache for all kernel launches except for the kernel launches for which thread blocks consume too much of the SMâs register file. These exceptions are reported by the profiler.



16.4.3. Shared Memoryï

Shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks. Each bank has a bandwidth of 32 bits per clock cycle.
A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32-bit word (even though the two addresses fall in the same bank). In that case, for read accesses, the word is broadcast to the requesting threads and for write accesses, each address is written by only one of the threads (which thread performs the write is undefined).
Figure 22 shows some examples of strided access.
Figure 23 shows some examples of memory read accesses that involve the broadcast mechanism.



Figure 35 Strided Shared Memory Accesses in 32 bit bank size mode.ï


Left

Linear addressing with a stride of one 32-bit word (no bank conflict).

Middle

Linear addressing with a stride of two 32-bit words (two-way bank conflict).

Right

Linear addressing with a stride of three 32-bit words (no bank conflict).








Figure 36 Irregular Shared Memory Accesses.ï


Left

Conflict-free access via random permutation.

Middle

Conflict-free access since threads 3, 4, 6, 7, and 9 access the same word within bank 5.

Right

Conflict-free broadcast access (threads access the same word within a bank).









16.5. Compute Capability 6.xï



16.5.1. Architectureï

An SM consists of:

64 (compute capability 6.0) or 128 (6.1 and 6.2) CUDA cores for arithmetic operations,
16 (6.0) or 32 (6.1 and 6.2) special function units for single-precision floating-point transcendental functions,
2 (6.0) or 4 (6.1 and 6.2) warp schedulers.

When an SM is given warps to execute, it first distributes them among its schedulers. Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
An SM has:

a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory,
a unified L1/texture cache for reads from global memory of size 24 KB (6.0 and 6.2) or 48 KB (6.1),
a shared memory of size 64 KB (6.0 and 6.2) or 96 KB (6.1).

The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned in Texture and Surface Memory.
There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary register spills. Applications may query the L2 cache size by checking the l2CacheSize device property (see Device Enumeration).
The cache behavior (e.g., whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially configured on a per-access basis using modifiers to the load instruction.



16.5.2. Global Memoryï

Global memory behaves the same way as in devices of compute capability 5.x (See Global Memory).



16.5.3. Shared Memoryï

Shared memory behaves the same way as in devices of compute capability 5.x (See Shared Memory).




16.6. Compute Capability 7.xï



16.6.1. Architectureï

An SM consists of:

64 FP32 cores for single-precision arithmetic operations,
32 FP64 cores for double-precision arithmetic operations,34
64 INT32 cores for integer math,
8 mixed-precision Tensor Cores for deep learning matrix arithmetic
16 special function units for single-precision floating-point transcendental functions,
4 warp schedulers.

An SM statically distributes its warps among its schedulers. Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
An SM has:

a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory,
a unified data cache and shared memory with a total size of 128 KB (Volta) or 96 KB (Turing).

Shared memory is partitioned out of unified data cache, and can be configured to various sizes (See Shared Memory.) The remaining data cache serves as an L1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned in Texture and Surface Memory.



16.6.2. Independent Thread Schedulingï

The Volta architecture introduces Independent Thread Scheduling among threads in a warp, enabling intra-warp synchronization patterns previously unavailable and simplifying code changes when porting CPU code. However, this can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures.
Below are code patterns of concern and suggested corrective actions for Volta-safe code.

For applications using warp intrinsics (__shfl*, __any, __all, __ballot), it is necessary that developers port their code to the new, safe, synchronizing counterpart, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. See Warp Vote Functions and Warp Shuffle Functions for details.

Since the intrinsics are available with CUDA 9.0+, (if necessary) code can be executed conditionally with the following preprocessor macro:

#if defined(CUDART_VERSION) && CUDART_VERSION >= 9000
// *_sync intrinsic
#endif


These intrinsics are available on all architectures, not just Volta or Turing, and in most cases a single code-base will suffice for all architectures. Note, however, that for Pascal and earlier architectures, all threads in mask must execute the same warp intrinsic instruction in convergence, and the union of all values in mask must be equal to the warpâs active mask. The following code pattern is valid on Volta, but not on Pascal or earlier architectures.



if (tid % warpSize < 16) {
    ...
    float swapped = __shfl_xor_sync(0xffffffff, val, 16);
    ...
} else {
    ...
    float swapped = __shfl_xor_sync(0xffffffff, val, 16);
    ...
}




The replacement for __ballot(1) is __activemask(). Note that threads within a warp can diverge even within a single code path. As a result, __activemask() and __ballot(1) may return only a subset of the threads on the current code path. The following invalid code example sets bit i of output to 1 when data[i] is greater than threshold. __activemask() is used in an attempt to enable cases where dataLen is not a multiple of 32.



// Sets bit in output[] to 1 if the correspond element in data[i]
// is greater than 'threshold', using 32 threads in a warp.

for (int i = warpLane; i < dataLen; i += warpSize) {
    unsigned active = __activemask();
    unsigned bitPack = __ballot_sync(active, data[i] > threshold);
    if (warpLane == 0) {
        output[i / 32] = bitPack;
    }
}




This code is invalid because CUDA does not guarantee that the warp will diverge ONLY at the loop condition. When divergence happens for other reasons, conflicting results will be computed for the same 32-bit output element by different subsets of threads in the warp. A correct code might use a non-divergent loop condition together with __ballot_sync() to safely enumerate the set of threads in the warp participating in the threshold calculation as follows.



for (int i = warpLane; i - warpLane < dataLen; i += warpSize) {
    unsigned active = __ballot_sync(0xFFFFFFFF, i < dataLen);
    if (i < dataLen) {
        unsigned bitPack = __ballot_sync(active, data[i] > threshold);
        if (warpLane == 0) {
            output[i / 32] = bitPack;
        }
    }
}




Discovery Pattern demonstrates a valid use case for __activemask().


If applications have warp-synchronous codes, they will need to insert the new __syncwarp() warp-wide barrier synchronization instruction between any steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid.

__shared__ float s_buff[BLOCK_SIZE];
s_buff[tid] = val;
__syncthreads();

// Inter-warp reduction
for (int i = BLOCK_SIZE / 2; i >= 32; i /= 2) {
    if (tid < i) {
        s_buff[tid] += s_buff[tid+i];
    }
    __syncthreads();
}

// Intra-warp reduction
// Butterfly reduction simplifies syncwarp mask
if (tid < 32) {
    float temp;
    temp = s_buff[tid ^ 16]; __syncwarp();
    s_buff[tid] += temp;     __syncwarp();
    temp = s_buff[tid ^ 8];  __syncwarp();
    s_buff[tid] += temp;     __syncwarp();
    temp = s_buff[tid ^ 4];  __syncwarp();
    s_buff[tid] += temp;     __syncwarp();
    temp = s_buff[tid ^ 2];  __syncwarp();
    s_buff[tid] += temp;     __syncwarp();
}

if (tid == 0) {
    *output = s_buff[0] + s_buff[1];
}
__syncthreads();



Although __syncthreads() has been consistently documented as synchronizing all threads in the thread block, Pascal and prior architectures could only enforce synchronization at the warp level. In certain cases, this allowed a barrier to succeed without being executed by every thread as long as at least some thread in every warp reached the barrier. Starting with Volta, the CUDA built-in __syncthreads() and PTX instruction bar.sync (and their derivatives) are enforced per thread and thus will not succeed until reached by all non-exited threads in the block. Code exploiting the previous behavior will likely deadlock and must be modified to ensure that all non-exited threads reach the barrier.

The racecheck and synccheck tools provided by compute-saniter can help with locating violations.
To aid migration while implementing the above-mentioned corrective actions, developers can opt-in to the Pascal scheduling model that does not support independent thread scheduling. See Application Compatibility for details.



16.6.3. Global Memoryï

Global memory behaves the same way as in devices of compute capability 5.x (See Global Memory).



16.6.4. Shared Memoryï

The amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. For the Volta architecture (compute capability 7.0), the unified data cache has a size of 128 KB, and the shared memory capacity can be set to 0, 8, 16, 32, 64 or 96 KB. For the Turing architecture (compute capability 7.5), the unified data cache has a size of 96 KB, and the shared memory capacity can be set to either 32 KB or 64 KB. Unlike Kepler, the driver automatically configures the shared memory capacity for each kernel to avoid shared memory occupancy bottlenecks while also allowing concurrent execution with already launched kernels where possible. In most cases, the driverâs default behavior should provide optimal performance.
Because the driver is not always aware of the full workload, it is sometimes useful for applications to provide additional hints regarding the desired shared memory configuration. For example, a kernel with little or no shared memory use may request a larger carveout in order to encourage concurrent execution with later kernels that require more shared memory. The new cudaFuncSetAttribute() API allows applications to set a preferred shared memory capacity, or carveout, as a percentage of the maximum supported shared memory capacity (96 KB for Volta, and 64 KB for Turing).
cudaFuncSetAttribute() relaxes enforcement of the preferred shared capacity compared to the legacy cudaFuncSetCacheConfig() API introduced with Kepler. The legacy API treated shared memory capacities as hard requirements for kernel launch. As a result, interleaving kernels with different shared memory configurations would needlessly serialize launches behind shared memory reconfigurations. With the new API, the carveout is treated as a hint. The driver may choose a different configuration if required to execute the function or to avoid thrashing.

// Device code
__global__ void MyKernel(...)
{
    __shared__ float buffer[BLOCK_DIM];
    ...
}

// Host code
int carveout = 50; // prefer shared memory capacity 50% of maximum
// Named Carveout Values:
// carveout = cudaSharedmemCarveoutDefault;   //  (-1)
// carveout = cudaSharedmemCarveoutMaxL1;     //   (0)
// carveout = cudaSharedmemCarveoutMaxShared; // (100)
cudaFuncSetAttribute(MyKernel, cudaFuncAttributePreferredSharedMemoryCarveout, carveout);
MyKernel <<<gridDim, BLOCK_DIM>>>(...);


In addition to an integer percentage, several convenience enums are provided as listed in the code comments above. Where a chosen integer percentage does not map exactly to a supported capacity (SM 7.0 devices support shared capacities of 0, 8, 16, 32, 64, or 96 KB), the next larger capacity is used. For instance, in the example above, 50% of the 96 KB maximum is 48 KB, which is not a supported shared memory capacity. Thus, the preference is rounded up to 64 KB.
Compute capability 7.x devices allow a single thread block to address the full capacity of shared memory: 96 KB on Volta, 64 KB on Turing. Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, as such they must use dynamic shared memory (rather than statically sized arrays) and require an explicit opt-in using cudaFuncSetAttribute() as follows.

// Device code
__global__ void MyKernel(...)
{
    extern __shared__ float buffer[];
    ...
}

// Host code
int maxbytes = 98304; // 96 KB
cudaFuncSetAttribute(MyKernel, cudaFuncAttributeMaxDynamicSharedMemorySize, maxbytes);
MyKernel <<<gridDim, blockDim, maxbytes>>>(...);


Otherwise, shared memory behaves the same way as for devices of compute capability 5.x (See Shared Memory).




16.7. Compute Capability 8.xï



16.7.1. Architectureï

A Streaming Multiprocessor (SM) consists of:

64 FP32 cores for single-precision arithmetic operations in devices of compute capability 8.0 and 128 FP32 cores in devices of compute capability 8.6, 8.7 and 8.9,
32 FP64 cores for double-precision arithmetic operations in devices of compute capability 8.0 and 2 FP64 cores in devices of compute capability 8.6, 8.7 and 8.9
64 INT32 cores for integer math,
4 mixed-precision Third-Generation Tensor Cores supporting half-precision (fp16), __nv_bfloat16, tf32, sub-byte and double precision (fp64) matrix arithmetic for compute capabilities 8.0, 8.6 and 8.7 (see Warp matrix functions for details),
4 mixed-precision Fourth-Generation Tensor Cores supporting fp8, fp16, __nv_bfloat16, tf32, sub-byte and fp64 for compute capability 8.9 (see Warp matrix functions for details),
16 special function units for single-precision floating-point transcendental functions,
4 warp schedulers.

An SM statically distributes its warps among its schedulers. Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
An SM has:

a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory,
a unified data cache and shared memory with a total size of 192 KB for devices of compute capability 8.0 and 8.7 (1.5x Voltaâs 128 KB capacity) and 128 KB for devices of compute capabilities 8.6 and 8.9.

Shared memory is partitioned out of the unified data cache, and can be configured to various sizes (see Shared Memory section). The remaining data cache serves as an L1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned in Texture and Surface Memory.



16.7.2. Global Memoryï

Global memory behaves the same way as for devices of compute capability 5.x (See Global Memory).



16.7.3. Shared Memoryï

Similar to the Volta architecture, the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. For the NVIDIA Ampere GPU architecture, the unified data cache has a size of 192 KB for devices of compute capability 8.0 and 8.7 and 128 KB for devices of compute capabilities 8.6 and 8.9. The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132 or 164 KB for devices of compute capability 8.0 and 8.7, and to 0, 8, 16, 32, 64 or 100 KB for devices of compute capabilities 8.6 and 8.9.
An application can set the carveout, i.e., the preferred shared memory capacity, with the cudaFuncSetAttribute().

cudaFuncSetAttribute(kernel_name, cudaFuncAttributePreferredSharedMemoryCarveout, carveout);


The API can specify the carveout either as an integer percentage of the maximum supported shared memory capacity of 164 KB for devices of compute capability 8.0 and 8.7 and 100 KB for devices of compute capabilities 8.6 and 8.9 respectively, or as one of the following values: {cudaSharedmemCarveoutDefault, cudaSharedmemCarveoutMaxL1, or cudaSharedmemCarveoutMaxShared. When using a percentage, the carveout is rounded up to the nearest supported shared memory capacity. For example, for devices of compute capability 8.0, 50% will map to a 100 KB carveout instead of an 82 KB one. Setting the cudaFuncAttributePreferredSharedMemoryCarveout is considered a hint by the driver; the driver may choose a different configuration, if needed.
Devices of compute capability 8.0 and 8.7 allow a single thread block to address up to 163 KB of shared memory, while devices of compute capabilities 8.6 and 8.9 allow up to 99 KB of shared memory. Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays. These kernels require an explicit opt-in by using cudaFuncSetAttribute() to set the cudaFuncAttributeMaxDynamicSharedMemorySize; see Shared Memory for the Volta architecture.
Note that the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per SM. The 1 KB of shared memory not made available to a thread block is reserved for system use.




16.8. Compute Capability 9.0ï



16.8.1. Architectureï

A Streaming Multiprocessor (SM) consists of:

128 FP32 cores for single-precision arithmetic operations,
64 FP64 cores for double-precision arithmetic operations,
64 INT32 cores for integer math,
4 mixed-precision fourth-generation Tensor Cores supporting the new FP8 input type in either E4M3 or E5M2 for exponent (E) and mantissa (M), half-precision (fp16), __nv_bfloat16, tf32, INT8 and double precision (fp64) matrix arithmetic (see Warp Matrix Functions for details) with sparsity support,
16 special function units for single-precision floating-point transcendental functions,
4 warp schedulers.

An SM statically distributes its warps among its schedulers. Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any.
An SM has:

a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory,
a unified data cache and shared memory with a total size of 256 KB for devices of compute capability 9.0 (1.33x NVIDIA Ampere GPU Architectureâs 192 KB capacity).

Shared memory is partitioned out of the unified data cache, and can be configured to various sizes (see Shared Memory section). The remaining data cache serves as an L1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned in Texture and Surface Memory.



16.8.2. Global Memoryï

Global memory behaves the same way as for devices of compute capability 5.x (See Global Memory).



16.8.3. Shared Memoryï

Similar to the NVIDIA Ampere GPU architecture, the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. For the NVIDIA H100 Tensor Core GPU architecture, the unified data cache has a size of 256 KB for devices of compute capability 9.0. The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132, 164, 196 or 228 KB.
As with the NVIDIA Ampere GPU architecture, an application can configure its preferred shared memory capacity, i.e., the carveout. Devices of compute capability 9.0 allow a single thread block to address up to 227 KB of shared memory. Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays. These kernels require an explicit opt-in by using cudaFuncSetAttribute() to set the cudaFuncAttributeMaxDynamicSharedMemorySize; see Shared Memory for the Volta architecture.
Note that the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per SM. The 1 KB of shared memory not made available to a thread block is reserved for system use.

32

above 48 KB requires dynamic shared memory

33

2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5

34

2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5





16.8.4. Features Accelerating Specialized Computationsï

The NVIDIA Hopper GPU architecture includes features to accelerate matrix multiply-accumulate (MMA) computations with:

asynchronous execution of MMA instructions
MMA instructions acting on large matrices spanning a warp-group
dynamic reassignment of register capacity among warp-groups to support even larger matrices, and
operand matrices accessed directly from shared memory

This feature set is only available within the CUDA compilation toolchain through inline PTX.
It is strongly recommended that applications utilize this complex feature set through CUDA-X libraries such as cuBLAS, cuDNN, or cuFFT.
It is strongly recommended that device kernels utilize this complex feature set through CUTLASS, a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) and related computations at all levels and scales within CUDA.





17. Driver APIï

This section assumes knowledge of the concepts described in CUDA Runtime.
The driver API is implemented in the cuda dynamic library (cuda.dll or cuda.so) which is copied on the system during the installation of the device driver. All its entry points are prefixed with cu.
It is a handle-based, imperative API: Most objects are referenced by opaque handles that may be specified to functions to manipulate the objects.
The objects available in the driver API are summarized in Table 22.


Table 22 Objects Available in the CUDA Driver APIï








Object
Handle
Description




Device
CUdevice
CUDA-enabled device


Context
CUcontext
Roughly equivalent to a CPU process


Module
CUmodule
Roughly equivalent to a dynamic library


Function
CUfunction
Kernel


Heap memory
CUdeviceptr
Pointer to device memory


CUDA array
CUarray
Opaque container for one-dimensional or two-dimensional data on the device, readable via texture or surface references


Texture object
CUtexref
Object that describes how to interpret texture memory data


Surface reference
CUsurfref
Object that describes how to read or write CUDA arrays


Stream
CUstream
Object that describes a CUDA stream


Event
CUevent
Object that describes a CUDA event



The driver API must be initialized with cuInit() before any function from the driver API is called. A CUDA context must then be created that is attached to a specific device and made current to the calling host thread as detailed in Context.
Within a CUDA context, kernels are explicitly loaded as PTX or binary objects by the host code as described in Module. Kernels written in C++ must therefore be compiled separately into PTX or binary objects. Kernels are launched using API entry points as described in Kernel Execution.
Any application that wants to run on future device architectures must load PTX, not binary code. This is because binary code is architecture-specific and therefore incompatible with future architectures, whereas PTX code is compiled to binary code at load time by the device driver.
Here is the host code of the sample from Kernels written using the driver API:

int main()
{
    int N = ...;
    size_t size = N * sizeof(float);

    // Allocate input vectors h_A and h_B in host memory
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);

    // Initialize input vectors
    ...

    // Initialize
    cuInit(0);

    // Get number of devices supporting CUDA
    int deviceCount = 0;
    cuDeviceGetCount(&deviceCount);
    if (deviceCount == 0) {
        printf("There is no device supporting CUDA.\n");
        exit (0);
    }

    // Get handle for device 0
    CUdevice cuDevice;
    cuDeviceGet(&cuDevice, 0);

    // Create context
    CUcontext cuContext;
    cuCtxCreate(&cuContext, 0, cuDevice);

    // Create module from binary file
    CUmodule cuModule;
    cuModuleLoad(&cuModule, "VecAdd.ptx");

    // Allocate vectors in device memory
    CUdeviceptr d_A;
    cuMemAlloc(&d_A, size);
    CUdeviceptr d_B;
    cuMemAlloc(&d_B, size);
    CUdeviceptr d_C;
    cuMemAlloc(&d_C, size);

    // Copy vectors from host memory to device memory
    cuMemcpyHtoD(d_A, h_A, size);
    cuMemcpyHtoD(d_B, h_B, size);

    // Get function handle from module
    CUfunction vecAdd;
    cuModuleGetFunction(&vecAdd, cuModule, "VecAdd");

    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    void* args[] = { &d_A, &d_B, &d_C, &N };
    cuLaunchKernel(vecAdd,
                   blocksPerGrid, 1, 1, threadsPerBlock, 1, 1,
                   0, 0, args, 0);

    ...
}


Full code can be found in the vectorAddDrv CUDA sample.


17.1. Contextï

A CUDA context is analogous to a CPU process. All resources and actions performed within the driver API are encapsulated inside a CUDA context, and the system automatically cleans up these resources when the context is destroyed. Besides objects such as modules and texture or surface references, each context has its own distinct address space. As a result, CUdeviceptr values from different contexts reference different memory locations.
A host thread may have only one device context current at a time. When a context is created with cuCtxCreate(), it is made current to the calling host thread. CUDA functions that operate in a context (most functions that do not involve device enumeration or context management) will return CUDA_ERROR_INVALID_CONTEXT if a valid context is not current to the thread.
Each host thread has a stack of current contexts. cuCtxCreate() pushes the new context onto the top of the stack. cuCtxPopCurrent() may be called to detach the context from the host thread. The context is then âfloatingâ and may be pushed as the current context for any host thread. cuCtxPopCurrent() also restores the previous current context, if any.
A usage count is also maintained for each context. cuCtxCreate() creates a context with a usage count of 1. cuCtxAttach() increments the usage count and cuCtxDetach() decrements it. A context is destroyed when the usage count goes to 0 when calling cuCtxDetach() or cuCtxDestroy().
The driver API is interoperable with the runtime and it is possible to access the primary context (see Initialization) managed by the runtime from the driver API via cuDevicePrimaryCtxRetain().
Usage count facilitates interoperability between third party authored code operating in the same context. For example, if three libraries are loaded to use the same context, each library would call cuCtxAttach() to increment the usage count and cuCtxDetach() to decrement the usage count when the library is done using the context. For most libraries, it is expected that the application will have created a context before loading or initializing the library; that way, the application can create the context using its own heuristics, and the library simply operates on the context handed to it. Libraries that wish to create their own contexts - unbeknownst to their API clients who may or may not have created contexts of their own - would use cuCtxPushCurrent() and cuCtxPopCurrent() as illustrated in the following figure.



Figure 37 Library Context Managementï





17.2. Moduleï

Modules are dynamically loadable packages of device code and data, akin to DLLs in Windows, that are output by nvcc (see Compilation with NVCC). The names for all symbols, including functions, global variables, and texture or surface references, are maintained at module scope so that modules written by independent third parties may interoperate in the same CUDA context.
This code sample loads a module and retrieves a handle to some kernel:

CUmodule cuModule;
cuModuleLoad(&cuModule, "myModule.ptx");
CUfunction myKernel;
cuModuleGetFunction(&myKernel, cuModule, "MyKernel");


This code sample compiles and loads a new module from PTX code and parses compilation errors:

#define BUFFER_SIZE 8192
CUmodule cuModule;
CUjit_option options[3];
void* values[3];
char* PTXCode = "some PTX code";
char error_log[BUFFER_SIZE];
int err;
options[0] = CU_JIT_ERROR_LOG_BUFFER;
values[0]  = (void*)error_log;
options[1] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
values[1]  = (void*)BUFFER_SIZE;
options[2] = CU_JIT_TARGET_FROM_CUCONTEXT;
values[2]  = 0;
err = cuModuleLoadDataEx(&cuModule, PTXCode, 3, options, values);
if (err != CUDA_SUCCESS)
    printf("Link error:\n%s\n", error_log);


This code sample compiles, links, and loads a new module from multiple PTX codes and parses link and compilation errors:

#define BUFFER_SIZE 8192
CUmodule cuModule;
CUjit_option options[6];
void* values[6];
float walltime;
char error_log[BUFFER_SIZE], info_log[BUFFER_SIZE];
char* PTXCode0 = "some PTX code";
char* PTXCode1 = "some other PTX code";
CUlinkState linkState;
int err;
void* cubin;
size_t cubinSize;
options[0] = CU_JIT_WALL_TIME;
values[0] = (void*)&walltime;
options[1] = CU_JIT_INFO_LOG_BUFFER;
values[1] = (void*)info_log;
options[2] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
values[2] = (void*)BUFFER_SIZE;
options[3] = CU_JIT_ERROR_LOG_BUFFER;
values[3] = (void*)error_log;
options[4] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
values[4] = (void*)BUFFER_SIZE;
options[5] = CU_JIT_LOG_VERBOSE;
values[5] = (void*)1;
cuLinkCreate(6, options, values, &linkState);
err = cuLinkAddData(linkState, CU_JIT_INPUT_PTX,
                    (void*)PTXCode0, strlen(PTXCode0) + 1, 0, 0, 0, 0);
if (err != CUDA_SUCCESS)
    printf("Link error:\n%s\n", error_log);
err = cuLinkAddData(linkState, CU_JIT_INPUT_PTX,
                    (void*)PTXCode1, strlen(PTXCode1) + 1, 0, 0, 0, 0);
if (err != CUDA_SUCCESS)
    printf("Link error:\n%s\n", error_log);
cuLinkComplete(linkState, &cubin, &cubinSize);
printf("Link completed in %fms. Linker Output:\n%s\n", walltime, info_log);
cuModuleLoadData(cuModule, cubin);
cuLinkDestroy(linkState);


Full code can be found in the ptxjit CUDA sample.



17.3. Kernel Executionï

cuLaunchKernel() launches a kernel with a given execution configuration.
Parameters are passed either as an array of pointers (next to last parameter of cuLaunchKernel()) where the nth pointer corresponds to the nth parameter and points to a region of memory from which the parameter is copied, or as one of the extra options (last parameter of cuLaunchKernel()).
When parameters are passed as an extra option (the CU_LAUNCH_PARAM_BUFFER_POINTER option), they are passed as a pointer to a single buffer where parameters are assumed to be properly offset with respect to each other by matching the alignment requirement for each parameter type in device code.
Alignment requirements in device code for the built-in vector types are listed in Table 5. For all other basic types, the alignment requirement in device code matches the alignment requirement in host code and can therefore be obtained using __alignof(). The only exception is when the host compiler aligns double and long long (and long on a 64-bit system) on a one-word boundary instead of a two-word boundary (for example, using gccâs compilation flag -mno-align-double) since in device code these types are always aligned on a two-word boundary.
CUdeviceptr is an integer, but represents a pointer, so its alignment requirement is __alignof(void*).
The following code sample uses a macro (ALIGN_UP()) to adjust the offset of each parameter to meet its alignment requirement and another macro (ADD_TO_PARAM_BUFFER()) to add each parameter to the parameter buffer passed to the CU_LAUNCH_PARAM_BUFFER_POINTER option.

#define ALIGN_UP(offset, alignment) \
      (offset) = ((offset) + (alignment) - 1) & ~((alignment) - 1)

char paramBuffer[1024];
size_t paramBufferSize = 0;

#define ADD_TO_PARAM_BUFFER(value, alignment)                   \
    do {                                                        \
        paramBufferSize = ALIGN_UP(paramBufferSize, alignment); \
        memcpy(paramBuffer + paramBufferSize,                   \
               &(value), sizeof(value));                        \
        paramBufferSize += sizeof(value);                       \
    } while (0)

int i;
ADD_TO_PARAM_BUFFER(i, __alignof(i));
float4 f4;
ADD_TO_PARAM_BUFFER(f4, 16); // float4's alignment is 16
char c;
ADD_TO_PARAM_BUFFER(c, __alignof(c));
float f;
ADD_TO_PARAM_BUFFER(f, __alignof(f));
CUdeviceptr devPtr;
ADD_TO_PARAM_BUFFER(devPtr, __alignof(devPtr));
float2 f2;
ADD_TO_PARAM_BUFFER(f2, 8); // float2's alignment is 8

void* extra[] = {
    CU_LAUNCH_PARAM_BUFFER_POINTER, paramBuffer,
    CU_LAUNCH_PARAM_BUFFER_SIZE,    &paramBufferSize,
    CU_LAUNCH_PARAM_END
};
cuLaunchKernel(cuFunction,
               blockWidth, blockHeight, blockDepth,
               gridWidth, gridHeight, gridDepth,
               0, 0, 0, extra);


The alignment requirement of a structure is equal to the maximum of the alignment requirements of its fields. The alignment requirement of a structure that contains built-in vector types, CUdeviceptr, or non-aligned double and long long, might therefore differ between device code and host code. Such a structure might also be padded differently. The following structure, for example, is not padded at all in host code, but it is padded in device code with 12 bytes after field f since the alignment requirement for field f4 is 16.

typedef struct {
    float  f;
    float4 f4;
} myStruct;





17.4. Interoperability between Runtime and Driver APIsï

An application can mix runtime API code with driver API code.
If a context is created and made current via the driver API, subsequent runtime calls will pick up this context instead of creating a new one.
If the runtime is initialized (implicitly as mentioned in CUDA Runtime), cuCtxGetCurrent() can be used to retrieve the context created during initialization. This context can be used by subsequent driver API calls.
The implicitly created context from the runtime is called the primary context (see Initialization). It can be managed from the driver API with the Primary Context Management functions.
Device memory can be allocated and freed using either API. CUdeviceptr can be cast to regular pointers and vice-versa:

CUdeviceptr devPtr;
float* d_data;

// Allocation using driver API
cuMemAlloc(&devPtr, size);
d_data = (float*)devPtr;

// Allocation using runtime API
cudaMalloc(&d_data, size);
devPtr = (CUdeviceptr)d_data;


In particular, this means that applications written using the driver API can invoke libraries written using the runtime API (such as cuFFT, cuBLAS, â¦).
All functions from the device and version management sections of the reference manual can be used interchangeably.



17.5. Driver Entry Point Accessï



17.5.1. Introductionï

The Driver Entry Point Access APIs provide a way to retrieve the address of a CUDA driver function. Starting from CUDA 11.3, users can call into available CUDA driver APIs using function pointers obtained from these APIs.
These APIs provide functionality similar to their counterparts, dlsym on POSIX platforms and GetProcAddress on Windows. The provided APIs will let users:

Retrieve the address of a driver function using the CUDA Driver API.
Retrieve the address of a driver function using the CUDA Runtime API.
Request per-thread default stream version of a CUDA driver function. For more details, see Retrieve per-thread default stream versions
Access new CUDA features on older toolkits but with a newer driver.




17.5.2. Driver Function Typedefsï

To help retrieve the CUDA Driver API entry points, the CUDA Toolkit provides access to headers containing the function pointer definitions for all CUDA driver APIs. These headers are installed with the CUDA Toolkit and are made available in the toolkitâs include/ directory. The table below summarizes the header files containing the typedefs for each CUDA API header file.


Table 23 Typedefs header files for CUDA driver APIsï







API header file
API Typedef header file




cuda.h
cudaTypedefs.h


cudaGL.h
cudaGLTypedefs.h


cudaProfiler.h
cudaProfilerTypedefs.h


cudaVDPAU.h
cudaVDPAUTypedefs.h


cudaEGL.h
cudaEGLTypedefs.h


cudaD3D9.h
cudaD3D9Typedefs.h


cudaD3D10.h
cudaD3D10Typedefs.h


cudaD3D11.h
cudaD3D11Typedefs.h



The above headers do not define actual function pointers themselves; they define the typedefs for function pointers. For example, cudaTypedefs.h has the below typedefs for the driver API cuMemAlloc:

typedef CUresult (CUDAAPI *PFN_cuMemAlloc_v3020)(CUdeviceptr_v2 *dptr, size_t bytesize);
typedef CUresult (CUDAAPI *PFN_cuMemAlloc_v2000)(CUdeviceptr_v1 *dptr, unsigned int bytesize);


CUDA driver symbols have a version based naming scheme with a _v* extension in its name except for the first version. When the signature or the semantics of a specific CUDA driver API changes, we increment the version number of the corresponding driver symbol. In the case of the cuMemAlloc driver API, the first driver symbol name is cuMemAlloc and the next symbol name is cuMemAlloc_v2. The typedef for the first version which was introduced in CUDA 2.0 (2000) is PFN_cuMemAlloc_v2000. The typedef for the next version which was introduced in CUDA 3.2 (3020) is PFN_cuMemAlloc_v3020.
The typedefs can be used to more easily define a function pointer of the appropriate type in code:

PFN_cuMemAlloc_v3020 pfn_cuMemAlloc_v2;
PFN_cuMemAlloc_v2000 pfn_cuMemAlloc_v1;


The above method is preferable if users are interested in a specific version of the API. Additionally, the headers have predefined macros for the latest version of all driver symbols that were available when the installed CUDA toolkit was released; these typedefs do not have a _v* suffix. For CUDA 11.3 toolkit, cuMemAlloc_v2 was the latest version and so we can also define its function pointer as below:

PFN_cuMemAlloc pfn_cuMemAlloc;





17.5.3. Driver Function Retrievalï

Using the Driver Entry Point Access APIs and the appropriate typedef, we can get the function pointer to any CUDA driver API.


17.5.3.1. Using the driver APIï

The driver API requires CUDA version as an argument to get the ABI compatible version for the requested driver symbol. CUDA Driver APIs have a per-function ABI denoted with a _v* extension. For example, consider the versions of cuStreamBeginCapture and their corresponding typedefs from cudaTypedefs.h:

// cuda.h
CUresult CUDAAPI cuStreamBeginCapture(CUstream hStream);
CUresult CUDAAPI cuStreamBeginCapture_v2(CUstream hStream, CUstreamCaptureMode mode);

// cudaTypedefs.h
typedef CUresult (CUDAAPI *PFN_cuStreamBeginCapture_v10000)(CUstream hStream);
typedef CUresult (CUDAAPI *PFN_cuStreamBeginCapture_v10010)(CUstream hStream, CUstreamCaptureMode mode);


From the above typedefs in the code snippet, version suffixes _v10000 and _v10010 indicate that the above APIs were introduced in CUDA 10.0 and CUDA 10.1 respectively.

#include <cudaTypedefs.h>

// Declare the entry points for cuStreamBeginCapture
PFN_cuStreamBeginCapture_v10000 pfn_cuStreamBeginCapture_v1;
PFN_cuStreamBeginCapture_v10010 pfn_cuStreamBeginCapture_v2;

// Get the function pointer to the cuStreamBeginCapture driver symbol
cuGetProcAddress("cuStreamBeginCapture", &pfn_cuStreamBeginCapture_v1, 10000, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
// Get the function pointer to the cuStreamBeginCapture_v2 driver symbol
cuGetProcAddress("cuStreamBeginCapture", &pfn_cuStreamBeginCapture_v2, 10010, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);


Referring to the code snippet above, to retrieve the address to the _v1 version of the driver API cuStreamBeginCapture, the CUDA version argument should be exactly 10.0 (10000). Similarly, the CUDA version for retrieving the address to the _v2 version of the API should be 10.1 (10010). Specifying a higher CUDA version for retrieving a specific version of a driver API might not always be portable. For example, using 11030 here would still return the _v2 symbol, but if a hypothetical _v3 version is released in CUDA 11.3, the cuGetProcAddress API would start returning the newer _v3 symbol instead when paired with a CUDA 11.3 driver. Since the ABI and function signatures of the _v2 and _v3 symbols might differ, calling the _v3 function using the _v10010 typedef intended for the _v2 symbol would exhibit undefined behavior.
To retrieve the latest version of a driver API for a given CUDA Toolkit, we can also specify CUDA_VERSION as the version argument and use the unversioned typedef to define the function pointer. Since _v2 is the latest version of the driver API cuStreamBeginCapture in CUDA 11.3, the below code snippet shows a different method to retrieve it.

// Assuming we are using CUDA 11.3 Toolkit

#include <cudaTypedefs.h>

// Declare the entry point
PFN_cuStreamBeginCapture pfn_cuStreamBeginCapture_latest;

// Intialize the entry point. Specifying CUDA_VERSION will give the function pointer to the
// cuStreamBeginCapture_v2 symbol since it is latest version on CUDA 11.3.
cuGetProcAddress("cuStreamBeginCapture", &pfn_cuStreamBeginCapture_latest, CUDA_VERSION, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);


Note that requesting a driver API with an invalid CUDA version will return an error CUDA_ERROR_NOT_FOUND. In the above code examples, passing in a version less than 10000 (CUDA 10.0) would be invalid.



17.5.3.2. Using the runtime APIï

The runtime API cudaGetDriverEntryPoint uses the CUDA runtime version to get the ABI compatible version for the requested driver symbol. In the below code snippet, the minimum CUDA runtime version required would be CUDA 11.2 as cuMemAllocAsync was introduced then.

#include <cudaTypedefs.h>

// Declare the entry point
PFN_cuMemAllocAsync pfn_cuMemAllocAsync;

// Intialize the entry point. Assuming CUDA runtime version >= 11.2
cudaGetDriverEntryPoint("cuMemAllocAsync", &pfn_cuMemAllocAsync, cudaEnableDefault, &driverStatus);

// Call the entry point
if(driverStatus == cudaDriverEntryPointSuccess && pfn_cuMemAllocAsync) {
    pfn_cuMemAllocAsync(...);
}


The runtime API cudaGetDriverEntryPointByVersion uses the user provided CUDA version to get the ABI compatible version for the requested driver symbol. This allows more specific control over the requested ABI version.



17.5.3.3. Retrieve per-thread default stream versionsï

Some CUDA driver APIs can be configured to have default stream or per-thread default stream semantics. Driver APIs having per-thread default stream semantics are suffixed with _ptsz or _ptds in their name. For example, cuLaunchKernel has a per-thread default stream variant named cuLaunchKernel_ptsz. With the Driver Entry Point Access APIs, users can request for the per-thread default stream version of the driver API cuLaunchKernel instead of the default stream version. Configuring the CUDA driver APIs for default stream or per-thread default stream semantics affects the synchronization behavior. More details can be found here.
The default stream or per-thread default stream versions of a driver API can be obtained by one of the following ways:

Use the compilation flag --default-stream per-thread or define the macro CUDA_API_PER_THREAD_DEFAULT_STREAM to get per-thread default stream behavior.
Force default stream or per-thread default stream behavior using the flags CU_GET_PROC_ADDRESS_LEGACY_STREAM/cudaEnableLegacyStream or CU_GET_PROC_ADDRESS_PER_THREAD_DEFAULT_STREAM/cudaEnablePerThreadDefaultStream respectively.




17.5.3.4. Access new CUDA featuresï

It is always recommended to install the latest CUDA toolkit to access new CUDA driver features, but if for some reason, a user does not want to update or does not have access to the latest toolkit, the API can be used to access new CUDA features with only an updated CUDA driver. For discussion, let us assume the user is on CUDA 11.3 and wants to use a new driver API cuFoo available in the CUDA 12.0 driver. The below code snippet illustrates this use-case:

int main()
{
    // Assuming we have CUDA 12.0 driver installed.

    // Manually define the prototype as cudaTypedefs.h in CUDA 11.3 does not have the cuFoo typedef
    typedef CUresult (CUDAAPI *PFN_cuFoo)(...);
    PFN_cuFoo pfn_cuFoo = NULL;
    CUdriverProcAddressQueryResult driverStatus;

    // Get the address for cuFoo API using cuGetProcAddress. Specify CUDA version as
    // 12000 since cuFoo was introduced then or get the driver version dynamically
    // using cuDriverGetVersion
    int driverVersion;
    cuDriverGetVersion(&driverVersion);
    CUresult status = cuGetProcAddress("cuFoo", &pfn_cuFoo, driverVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);

    if (status == CUDA_SUCCESS && pfn_cuFoo) {
        pfn_cuFoo(...);
    }
    else {
        printf("Cannot retrieve the address to cuFoo - driverStatus = %d. Check if the latest driver for CUDA 12.0 is installed.\n", driverStatus);
        assert(0);
    }

    // rest of code here

}






17.5.4. Potential Implications with cuGetProcAddressï

Below is a set of concrete and theoretical examples of potential issues with cuGetProcAddress and cudaGetDriverEntryPoint.


17.5.4.1. Implications with cuGetProcAddress vs Implicit Linkingï

cuDeviceGetUuid was introduced in CUDA 9.2. This API has a newer revision (cuDeviceGetUuid_v2) introduced in CUDA 11.4. To preserve minor version compatibility, cuDeviceGetUuid will not be version bumped to cuDeviceGetUuid_v2 in cuda.h until CUDA 12.0. This means that calling it by obtaining a function pointer to it via cuGetProcAddress might have different behavior. Example using the API directly:

#include <cuda.h>

CUuuid uuid;
CUdevice dev;
CUresult status;

status = cuDeviceGet(&dev, 0); // Get device 0
// handle status

status = cuDeviceGetUuid(&uuid, dev) // Get uuid of device 0


In this example, assume the user is compiling with CUDA 11.4. Note that this will perform the behavior of cuDeviceGetUuid, not _v2 version. Now an example of using cuGetProcAddress:

#include <cudaTypedefs.h>

CUuuid uuid;
CUdevice dev;
CUresult status;
CUdriverProcAddressQueryResult driverStatus;

status = cuDeviceGet(&dev, 0); // Get device 0
// handle status

PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid;
status = cuGetProcAddress("cuDeviceGetUuid", &pfn_cuDeviceGetUuid, CUDA_VERSION, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
if(CUDA_SUCCESS == status && pfn_cuDeviceGetUuid) {
    // pfn_cuDeviceGetUuid points to ???
}


In this example, assume the user is compiling with CUDA 11.4. This will get the function pointer of cuDeviceGetUuid_v2. Calling the function pointer will then invoke the new _v2 function, not the same cuDeviceGetUuid as shown in the previous example.



17.5.4.2. Compile Time vs Runtime Version Usage in cuGetProcAddressï

Letâs take the same issue and make one small tweak. The last example used the compile time constant of CUDA_VERSION to determine which function pointer to obtain. More complications arise if the user queries the driver version dynamically using cuDriverGetVersion or cudaDriverGetVersion to pass to cuGetProcAddress. Example:

#include <cudaTypedefs.h>

CUuuid uuid;
CUdevice dev;
CUresult status;
int cudaVersion;
CUdriverProcAddressQueryResult driverStatus;

status = cuDeviceGet(&dev, 0); // Get device 0
// handle status

status = cuDriverGetVersion(&cudaVersion);
// handle status

PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid;
status = cuGetProcAddress("cuDeviceGetUuid", &pfn_cuDeviceGetUuid, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
if(CUDA_SUCCESS == status && pfn_cuDeviceGetUuid) {
    // pfn_cuDeviceGetUuid points to ???
}


In this example, assume the user is compiling with CUDA 11.3. The user would debug, test, and deploy this application with the known behavior of getting cuDeviceGetUuid (not the _v2 version). Since CUDA has guaranteed ABI compatibility between minor versions, this same application is expected to run after the driver is upgraded to CUDA 11.4 (without updating the toolkit and runtime) without requiring recompilation. This will have undefined behavior though, because now the typedef for PFN_cuDeviceGetUuid will still be of the signature for the original version, but since cudaVersion would now be 11040 (CUDA 11.4), cuGetProcAddress would return the function pointer to the _v2 version, meaning calling it might have undefined behavior.
Note in this case the original (not the _v2 version) typedef looks like:

typedef CUresult (CUDAAPI *PFN_cuDeviceGetUuid_v9020)(CUuuid *uuid, CUdevice_v1 dev);


But the _v2 version typedef looks like:

typedef CUresult (CUDAAPI *PFN_cuDeviceGetUuid_v11040)(CUuuid *uuid, CUdevice_v1 dev);


So in this case, the API/ABI is going to be the same and the runtime API call will likely not cause issuesâonly the potential for unknown uuid return. In Implications to API/ABI, we discuss a more problematic case of API/ABI compatibility.



17.5.4.3. API Version Bumps with Explicit Version Checksï

Above, was a specific concrete example. Now for instance letâs use a theoretical example that still has issues with compatibility across driver versions. Example:

CUresult cuFoo(int bar); // Introduced in CUDA 11.4
CUresult cuFoo_v2(int bar); // Introduced in CUDA 11.5
CUresult cuFoo_v3(int bar, void* jazz); // Introduced in CUDA 11.6

typedef CUresult (CUDAAPI *PFN_cuFoo_v11040)(int bar);
typedef CUresult (CUDAAPI *PFN_cuFoo_v11050)(int bar);
typedef CUresult (CUDAAPI *PFN_cuFoo_v11060)(int bar, void* jazz);


Notice that the API has been modified twice since original creation in CUDA 11.4 and the latest in CUDA 11.6 also modified the API/ABI interface to the function. The usage in user code compiled against CUDA 11.5 is:

#include <cuda.h>
#include <cudaTypedefs.h>

CUresult status;
int cudaVersion;
CUdriverProcAddressQueryResult driverStatus;

status = cuDriverGetVersion(&cudaVersion);
// handle status

PFN_cuFoo_v11040 pfn_cuFoo_v11040;
PFN_cuFoo_v11050 pfn_cuFoo_v11050;
if(cudaVersion < 11050 ) {
    // We know to get the CUDA 11.4 version
    status = cuGetProcAddress("cuFoo", &pfn_cuFoo_v11040, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
    // Handle status and validating pfn_cuFoo_v11040
}
else {
    // Assume >= CUDA 11.5 version we can use the second version
    status = cuGetProcAddress("cuFoo", &pfn_cuFoo_v11050, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
    // Handle status and validating pfn_cuFoo_v11050
}


In this example, without updates for the new typedef in CUDA 11.6 and recompiling the application with those new typedefs and case handling, the application will get the cuFoo_v3 function pointer returned and any usage of that function would then cause undefined behavior. The point of this example was to illustrate that even explicit version checks for cuGetProcAddress may not safely cover the minor version bumps within a CUDA major release.



17.5.4.4. Issues with Runtime API Usageï

The above examples were focused on the issues with the Driver API usage for obtaining the function pointers to driver APIs. Now we will discuss the potential issues with the Runtime API usage for cudaApiGetDriverEntryPoint.
We will start by using the Runtime APIs similar to the above.

#include <cuda.h>
#include <cudaTypedefs.h>
#include <cuda_runtime.h>

CUresult status;
cudaError_t error;
int driverVersion, runtimeVersion;
CUdriverProcAddressQueryResult driverStatus;

// Ask the runtime for the function
PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime;
error = cudaGetDriverEntryPoint ("cuDeviceGetUuid", &pfn_cuDeviceGetUuidRuntime, cudaEnableDefault, &driverStatus);
if(cudaSuccess == error && pfn_cuDeviceGetUuidRuntime) {
    // pfn_cuDeviceGetUuid points to ???
}


The function pointer in this example is even more complicated than the driver only examples above because there is no control over which version of the function to obtain; it will always get the API for the current CUDA Runtime version. See the following table for more information:









Static Runtime Version Linkage




Driver Version Installed
V11.3
V11.4


V11.3
v1
v1x


V11.4
v1
v2




V11.3 => 11.3 CUDA Runtime and Toolkit (includes header files cuda.h and cudaTypedefs.h)
V11.4 => 11.4 CUDA Runtime and Toolkit (includes header files cuda.h and cudaTypedefs.h)
v1 => cuDeviceGetUuid
v2 => cuDeviceGetUuid_v2

x => Implies the typedef function pointer won't match the returned
     function pointer.  In these cases, the typedef at compile time
     using a CUDA 11.4 runtime, would match the _v2 version, but the
     returned function pointer would be the original (non _v2) function.


The problem in the table comes in with a newer CUDA 11.4 Runtime and Toolkit and older driver (CUDA 11.3) combination, labeled as v1x in the above. This combination would have the driver returning the pointer to the older function (non _v2), but the typedef used in the application would be for the new function pointer.



17.5.4.5. Issues with Runtime API and Dynamic Versioningï

More complications arise when we consider different combinations of the CUDA version with which an application is compiled, CUDA runtime version, and CUDA driver version that an application dynamically links against.

#include <cuda.h>
#include <cudaTypedefs.h>
#include <cuda_runtime.h>

CUresult status;
cudaError_t error;
int driverVersion, runtimeVersion;
CUdriverProcAddressQueryResult driverStatus;
enum cudaDriverEntryPointQueryResult runtimeStatus;

PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidDriver;
status = cuGetProcAddress("cuDeviceGetUuid", &pfn_cuDeviceGetUuidDriver, CUDA_VERSION, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
if(CUDA_SUCCESS == status && pfn_cuDeviceGetUuidDriver) {
    // pfn_cuDeviceGetUuidDriver points to ???
}

// Ask the runtime for the function
PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime;
error = cudaGetDriverEntryPoint ("cuDeviceGetUuid", &pfn_cuDeviceGetUuidRuntime, cudaEnableDefault, &runtimeStatus);
if(cudaSuccess == error && pfn_cuDeviceGetUuidRuntime) {
    // pfn_cuDeviceGetUuidRuntime points to ???
}

// Ask the driver for the function based on the driver version (obtained via runtime)
error = cudaDriverGetVersion(&driverVersion);
PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidDriverDriverVer;
status = cuGetProcAddress ("cuDeviceGetUuid", &pfn_cuDeviceGetUuidDriverDriverVer, driverVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
if(CUDA_SUCCESS == status && pfn_cuDeviceGetUuidDriverDriverVer) {
    // pfn_cuDeviceGetUuidDriverDriverVer points to ???
}


The following matrix of function pointers is expected:














Function Pointer
Application Compiled/Runtime Dynamic Linked Version/Driver Version


(3 => CUDA 11.3 and 4 => CUDA 11.4)


3/3/3
3/3/4
3/4/3
3/4/4
4/3/3
4/3/4
4/4/3
4/4/4


pfn_cuDeviceGetUuidDriver
t1/v1
t1/v1
t1/v1
t1/v1
N/A
N/A
t2/v1
t2/v2


pfn_cuDeviceGetUuidRuntime
t1/v1
t1/v1
t1/v1
t1/v2
N/A
N/A
t2/v1
t2/v2


pfn_cuDeviceGetUuidDriverDriverVer
t1/v1
t1/v2
t1/v1
t1/v2
N/A
N/A
t2/v1
t2/v2




tX -> Typedef version used at compile time
vX -> Version returned/used at runtime


If the application is compiled against CUDA Version 11.3, it would have the typedef for the original function, but if compiled against CUDA Version 11.4, it would have the typedef for the _v2 function. Because of that, notice the number of cases where the typedef does not match the actual version returned/used.



17.5.4.6. Issues with Runtime API allowing CUDA Versionï

Unless specified otherwise, the CUDA runtime API cudaGetDriverEntryPointByVersion will have similar implications as the driver entry point cuGetProcAddress since it allows for the user to request a specific CUDA driver version.



17.5.4.7. Implications to API/ABIï

In the above examples using cuDeviceGetUuid, the implications of the mismatched API are minimal, and may not be entirely noticeable to many users as the _v2 was added to support Multi-Instance GPU (MIG) mode. So, on a system without MIG, the user might not even realize they are getting a different API.
More problematic is an API which changes its application signature (and hence ABI) such as cuCtxCreate. The _v2 version, introduced in CUDA 3.2 is currently used as the default cuCtxCreate when using cuda.h but now has a newer version introduced in CUDA 11.4 (cuCtxCreate_v3). The API signature has been modified as well, and now takes extra arguments. So, in some of the cases above, where the typedef to the function pointer doesnât match the returned function pointer, there is a chance for non-obvious ABI incompatibility which would lead to undefined behavior.
For example, assume the following code compiled against a CUDA 11.3 toolkit with a CUDA 11.4 driver installed:

PFN_cuCtxCreate cuUnknown;
CUdriverProcAddressQueryResult driverStatus;

status = cuGetProcAddress("cuCtxCreate", (void**)&cuUnknown, cudaVersion, CU_GET_PROC_ADDRESS_DEFAULT, &driverStatus);
if(CUDA_SUCCESS == status && cuUnknown) {
    status = cuUnknown(&ctx, 0, dev);
}


Running this code where cudaVersion is set to anything >=11040 (indicating CUDA 11.4) could have undefined behavior due to not having adequately supplied all the parameters required for the _v3 version of the cuCtxCreate_v3 API.




17.5.5. Determining cuGetProcAddress Failure Reasonsï

There are two types of errors with cuGetProcAddress. Those are (1) API/usage errors and (2) inability to find the driver API requested. The first error type will return error codes from the API via the CUresult return value. Things like passing NULL as the pfn variable or passing invalid flags.
The second error type encodes in the CUdriverProcAddressQueryResult *symbolStatus and can be used to help distinguish potential issues with the driver not being able to find the symbol requested. Take the following example:

// cuDeviceGetExecAffinitySupport was introduced in release CUDA 11.4
#include <cuda.h>
CUdriverProcAddressQueryResult driverStatus;
cudaVersion = ...;
status = cuGetProcAddress("cuDeviceGetExecAffinitySupport", &pfn, cudaVersion, 0, &driverStatus);
if (CUDA_SUCCESS == status) {
    if (CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT == driverStatus) {
        printf("We can use the new feature when you upgrade cudaVersion to 11.4, but CUDA driver is good to go!\n");
        // Indicating cudaVersion was < 11.4 but run against a CUDA driver >= 11.4
    }
    else if (CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND == driverStatus) {
        printf("Please update both CUDA driver and cudaVersion to at least 11.4 to use the new feature!\n");
        // Indicating driver is < 11.4 since string not found, doesn't matter what cudaVersion was
    }
    else if (CU_GET_PROC_ADDRESS_SUCCESS == driverStatus && pfn) {
        printf("You're using cudaVersion and CUDA driver >= 11.4, using new feature!\n");
        pfn();
    }
}


The first case with the return code CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT indicates that the symbol was found when searching in the CUDA driver but it was added later than the cudaVersion supplied. In the example, specifying cudaVersion as anything 11030 or less and when running against a CUDA driver >= CUDA 11.4 would give this result of CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT. This is because cuDeviceGetExecAffinitySupport was added in CUDA 11.4 (11040).
The second case with the return code CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND indicates that the symbol was not found when searching in the CUDA driver. This can be due to a few reasons such as unsupported CUDA function due to older driver as well as just having a typo. In the latter, similar to the last example if the user had put symbol as CUDeviceGetExecAffinitySupport - notice the capital CU to start the string - cuGetProcAddress would not be able to find the API because the string doesnât match. In the former case an example might be the user developing an application against a CUDA driver supporting the new API, and deploying the application against an older CUDA driver. Using the last example, if the developer developed against CUDA 11.4 or later but was deployed against a CUDA 11.3 driver, during their development they may have had a succesful cuGetProcAddress, but when deploying an application running against a CUDA 11.3 driver the call would no longer work with the CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND returned in driverStatus.





18. CUDA Environment Variablesï

The following table lists the CUDA environment variables. Environment variables related to the Multi-Process Service are documented in the Multi-Process Service section of the GPU Deployment and Management guide.


Table 24 CUDA Environment Variablesï








Variable
Values
Description




Device Enumeration and Properties




CUDA_VISIBLE_DEVICES
A comma-separated sequence of GPU identifiers
MIG support: MIG-<GPU-UUID>/<GPU instance ID>/<compute instance ID>
GPU identifiers are given as integer indices or as UUID strings. GPU UUID strings should follow the same format as given by nvidia-smi, such as GPU-8932f937-d72c-4106-c12f-20bd9faed9f6. However, for convenience, abbreviated forms are allowed; simply specify enough digits from the beginning of the GPU UUID to uniquely identify that GPU in the target system. For example, CUDA_VISIBLE_DEVICES=GPU-8932f937 may be a valid way to refer to the above GPU UUID, assuming no other GPU in the system shares this prefix.
Only the devices whose index is present in the sequence are visible to CUDA applications and they are enumerated in the order of the sequence. If one of the indices is invalid, only the devices whose index precedes the invalid index are visible to CUDA applications. For example, setting CUDA_VISIBLE_DEVICES to 2,1 causes device 0 to be invisible and device 2 to be enumerated before device 1. Setting CUDA_VISIBLE_DEVICES to 0,2,-1,1 causes devices 0 and 2 to be visible and device 1 to be invisible.
MIG format starts with MIG keyword and GPU UUID should follow the same format as given by nvidia-smi. For example, MIG-GPU-8932f937-d72c-4106-c12f-20bd9faed9f6/1/2. Only single MIG instance enumeration is supported.


CUDA_MANAGED_FORCE_DEVICE_ALLOC
0 or 1 (default is 0)
Forces the driver to place all managed allocations in device memory.


CUDA_DEVICE_ORDER
FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST)
FASTEST_FIRST causes CUDA to enumerate the available devices in fastest to slowest order using a simple heuristic. PCI_BUS_ID orders devices by PCI bus ID in ascending order.


Compilation




CUDA_CACHE_DISABLE
0 or 1 (default is 0)
Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation. When disabled, no binary code is added to or retrieved from the cache.


CUDA_CACHE_PATH
filepath

Specifies the folder where the just-in-time compiler caches binary codes; the default values are:

on Windows, %APPDATA%\NVIDIA\ComputeCache
on Linux, ~/.nv/ComputeCache




CUDA_CACHE_MAXSIZE
integer (default is 1073741824 (1 GiB) for desktop/server platforms and 268435456 (256 MiB) for embedded platforms and the maximum is 4294967296 (4 GiB))
Specifies the size in bytes of the cache used by the just-in-time compiler. Binary codes whose size exceeds the cache size are not cached. Older binary codes are evicted from the cache to make room for newer binary codes if needed.


CUDA_FORCE_PTX_JIT
0 or 1 (default is 0)
When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility) and to just-in-time compile embedded PTX code instead. If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application forward compatibility with future architectures (see Just-in-Time Compilation).


CUDA_DISABLE_PTX_JIT
0 or 1 (default is 0)
When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility). If a kernel does not have embedded binary code or the embedded binary was compiled for an incompatible architecture, then it will fail to load. This environment variable can be used to validate that an application has the compatible SASS code generated for each kernel.(see Binary Compatibility).


CUDA_FORCE_JIT
0 or 1 (default is 0)
When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility) and to just-in-time compile embedded PTX code instead. If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application forward compatibility with future architectures (see Just-in-Time Compilation). The behavior can be overridden for embedded PTX by setting CUDA_FORCE_PTX_JIT=0.


CUDA_DISABLE_JIT
0 or 1 (default is 0)
When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility). If a kernel does not have embedded binary code or the embedded binary was compiled for an incompatible architecture, then it will fail to load. This environment variable can be used to validate that an application has the compatible SASS code generated for each kernel.(see Binary Compatibility). The behavior can be overridden for embedded PTX by setting CUDA_DISABLE_PTX_JIT=0.


Execution




CUDA_LAUNCH_BLOCKING
0 or 1 (default is 0)
Disables (when set to 1) or enables (when set to 0) asynchronous kernel launches.


CUDA_DEVICE_MAX_CONNECTIONS
1 to 32 (default is 8)
Sets the number of compute and copy engine concurrent connections (work queues) from the host to each device of compute capability 3.5 and above.


CUDA_AUTO_BOOST
0 or 1
Overrides the autoboost behavior set by the âauto-boost-default option of nvidia-smi. If an application requests via this environment variable a behavior that is different from nvidia-smiâs, its request is honored if there is no other application currently running on the same GPU that successfully requested a different behavior, otherwise it is ignored.


CUDA_SCALE_LAUNCH_QUEUES
â0.25xâ, â0.5xâ, â2xâ or â4xâ
Scales the size of the queues available for launching work by a fixed multiplier.


cuda-gdb (on Linux platform)




CUDA_DEVICE_WAITS_ON_EXCEPTION
0 or 1 (default is 0)
When set to 1, a CUDA application will halt when a device exception occurs, allowing a debugger to be attached for further debugging.


MPS service (on Linux platform)




CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT
Percentage value (between 0 - 100, default is 0)
Devices of compute capability 8.x allow, a portion of L2 cache to be set-aside for persisting data accesses to global memory. When using CUDA MPS service, the set-aside size can only be controlled using this environment variable, before starting CUDA MPS control daemon. I.e., the environment variable should be set before running the command nvidia-cuda-mps-control -d.


Module loading




CUDA_MODULE_LOADING
DEFAULT, LAZY, EAGER (default is LAZY)
Specifies the module loading mode for the application. When set to EAGER, all kernels and data from a cubin, fatbin or a PTX file are fully loaded upon corresponding cuModuleLoad* and cuLibraryLoad* API call. When set to LAZY, loading of specific kernels is delayed to the point a CUfunc handle is extracted with cuModuleGetFunction or cuKernelGetFunction API calls and data from the cubin is loaded at load of first kernel in the cubin or at first access of variables in the cubin. Default behavior may change in future CUDA releases.


CUDA_MODULE_DATA_LOADING
DEFAULT, LAZY, EAGER (default is LAZY)
Specifies the data loading mode for the application. When set to EAGER, all data from a cubin, fatbin or a PTX file are fully loaded to memory upon corresponding cuLibraryLoad*. This doesnât affect the LAZY or EAGER loading of kernels. When set to LAZY, loading of data is delayed to the point at which a handle is required. Default behavior may change in future CUDA releases. Data loading behavior is inherited from CUDA_MODULE_LOADING if this environment variable is not set.


Pre-loading dependent libraries




CUDA_FORCE_PRELOAD_LIBRARIES
0 or 1 (default is 0)
When set to 1, forces the driver to preload the libraries required for NVVM and PTX just-in-time compilation during driver initialization. This will increase the memory footprint and the time taken for CUDA driver initialization. This environment variable needs to be set to avoid certain deadlock situations involving multiple CUDA threads.


CUDA Graphs




CUDA_GRAPHS_USE_NODE_PRIORITY
0 or 1
Overrides the cudaGraphInstantiateFlagUseNodePriority flag on graph instantiation. When set to 1, the flag will be set for all graphs and when set to 0, the flag will be cleared for all graphs.






19. Unified Memory Programmingï


Note
This chapter applies to devices with compute capability 5.0 or higher unless stated otherwise.
For devices with compute capability lower than 5.0, refer to the CUDA toolkit documentation for CUDA 11.8.

This documentation on Unified Memory is divided into 3 parts:

General description of unified memory
Unified Memory on devices with full CUDA Unified Memory support
Unified Memory on devices without full CUDA Unified Memory support



19.1. Unified Memory Introductionï

CUDA Unified Memory provides all processors with:

a single unified memory pool, that is,
a single pointer value enables all processors in the system
(all CPUs, all GPUs, etc.)
to access this memory with all of their native memory operations
(pointer dereferenes, atomics, etc.).
concurrent access to the unified memory pool from all processors in the system.

Unified Memory improves GPU programming in several ways:

Producitivity: GPU programs may access Unified Memory from GPU and CPU threads
concurrently without needing to create separate allocations (cudaMalloc()) and
copy memory manually back and forth (cudaMemcpy*()).

Performance:

Data access speed may be maximized by migrating data towards processors that access it most frequently.
Applications can trigger manual migration of data and may use hints to control migration heuristics.
Total system memory usage may be reduced by avoiding duplicating memory on both CPUs and GPUs.


Functionality: it enables GPU programs to work on data that exceeds the GPU memoryâs capacity.

With CUDA Unified Memory, data movement still takes place, and hints may improve performance.
These hints are not required for correctness or functionality, that is, programmers may focus on parallelizing
their applications across GPUs and CPUs first, and worry about data-movement later in the development cycle as a performance optimzation.
Note that the physical location of data is invisible to a program and may be changed at any time,
but accesses to the dataâs virtual address will remain valid and coherent from any processor regardless of locality.
There are two main ways to obtain CUDA Unified Memory:

System-Allocated Memory: memory allocated on the host with system APIs:
stack variables, global-/file-scope variables, malloc() / mmap()
(see System Allocator for in-depth examples), thread locals, etc.
CUDA APIs that explicitly allocate Unified Memory: memory allocated with, for example, cudaMallocManaged(),
are available on more systems and may perform better than System-Allocated Memory.



19.1.1. System Requirements for Unified Memoryï

The following table shows the different levels of support for CUDA Unified Memory,
the device properties required to detect these levels of support
and links to the documentation specific to each level of support:


Table 25 Overview of levels of unified memory supportï








Unified Memory Support Level
System device properties
Further documentation




Full CUDA Unified Memory: all memory has full support. This includes System-Allocated and CUDA Managed Memory.


Set to 1: pageableMemoryAccess


Systems with hardware acceleration also have the following properties set to 1:

hostNativeAtomicSupported, pageableMemoryAccessUsesHostPageTables, directManagedMemAccessFromHost



Unified Memory on devices with full CUDA Unified Memory support


Only CUDA Managed Memory has full support.


Set to 1: concurrentManagedAccess

Set to 0: pageableMemoryAccess



Unified Memory on devices with only CUDA Managed Memory support


CUDA Managed Memory without full support: unified addressing but no concurrent access.


Set to 1: managedMemory

Set to 0: concurrentManagedAccess





Unified Memory on Windows or devices with compute capability 5.x
CUDA for Tegra Memory Management
Unified Memory on Tegra




No Unified Memory support.
Set to 0: managedMemory
CUDA for Tegra Memory Management



The behavior of an application that attempts to use Unified Memory on a system that does not support it is undefined.
The following properties enable CUDA applications to check the level of system support for Unified Memory, and
to be portable between systems with different levels of support:


pageableMemoryAccess: This property is set to 1 on systems with CUDA Unified Memory support where
all threads may access System-Allocated Memory and CUDA Managed Memory.
These systems include NVIDIA Grace Hopper, IBM Power9 + Volta, and modern Linux systems with HMM enabled (see next bullet), among others.

Linux HMM requires Linux kernel version 6.1.24+, 6.2.11+ or 6.3+,
devices with compute capability 7.5 or higher and
a CUDA driver version 535+ installed with
Open Kernel Modules.


concurrentManagedAccess: This property is set to 1 on systems with full CUDA Managed Memory support.
When this property is set to 0, there is only partial support for Unified Memory in CUDA Managed Memory.
For Tegra support of Unified Memory, see
CUDA for Tegra Memory Management.

A program may query the level of GPU support for CUDA Unified Memory, by querying the attributes in
Table Overview of levels of unified memory support above using  cudaGetDeviceProperties().



19.1.2. Programming Modelï

With CUDA Unified Memory, separate allocations between host and device, and explicit memory transfers between them, are no longer required.
Programs may allocate Unified Memory in the following ways:

System-Allocation APIs: on systems with full CUDA Unified Memory support via
any system allocation of the host process (Câs malloc(), C++âs new operator, POSIXâs mmap and so on).
CUDA Managed Memory Allocation APIs: via the cudaMallocManaged() API which is syntactically similar to cudaMalloc().
CUDA Managed Variables: variables declared with __managed__, which are semantically similar to a __device__ variable.

Most examples in this chapter provide at least two versions, one using CUDA Managed Memory and one using System-Allocated Memory.
Tabs allow you to choose between them. The following samples illustrate how Unified Memory simplifies CUDA programs:



System (malloc())









__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  int* d_ptr = nullptr;
  // Does not require any unified memory support
  cudaMalloc(&d_ptr, sizeof(int));
  write_value<<<1, 1>>>(d_ptr, 1);
  int host;
  // Copy memory back to the host and synchronize
  cudaMemcpy(&host, d_ptr, sizeof(int),
             cudaMemcpyDefault);
  printf("value = %d\n", host); 
  cudaFree(d_ptr); 
  return 0;
}





__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  // Requires System-Allocated Memory support
  int* ptr = (int*)malloc(sizeof(int));
  write_value<<<1, 1>>>(ptr, 1);
  // Synchronize required
  // (before, cudaMemcpy was synchronizing)
  cudaDeviceSynchronize();
  printf("value = %d\n", *ptr); 
  free(ptr); 
  return 0;
}









System (Stack)









__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  int* d_ptr = nullptr;
  // Does not require any unified memory support
  cudaMalloc(&d_ptr, sizeof(int));
  write_value<<<1, 1>>>(d_ptr, 1);
  int host;
  // Copy memory back to the host and synchronize
  cudaMemcpy(&host, d_ptr, sizeof(int),
             cudaMemcpyDefault);
  printf("value = %d\n", host); 
  cudaFree(d_ptr); 
  return 0;
}





__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  // Requires System-Allocated Memory support
  int value;
  write_value<<<1, 1>>>(&value, 1);
  // Synchronize required
  // (before, cudaMemcpy was synchronizing)
  cudaDeviceSynchronize();
  printf("value = %d\n", value);
  return 0;
}









Managed (cudaMallocManaged())









__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  int* d_ptr = nullptr;
  // Does not require any unified memory support
  cudaMalloc(&d_ptr, sizeof(int));
  write_value<<<1, 1>>>(d_ptr, 1);
  int host;
  // Copy memory back to the host and synchronize
  cudaMemcpy(&host, d_ptr, sizeof(int),
             cudaMemcpyDefault);
  printf("value = %d\n", host); 
  cudaFree(d_ptr); 
  return 0;
}





__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  int* ptr = nullptr;
  // Requires CUDA Managed Memory support
  cudaMallocManaged(&ptr, sizeof(int));
  write_value<<<1, 1>>>(ptr, 1);
  // Synchronize required
  // (before, cudaMemcpy was synchronizing)
  cudaDeviceSynchronize();
  printf("value = %d\n", *ptr); 
  cudaFree(ptr); 
  return 0;
}









Managed (__managed__)









__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  int* d_ptr = nullptr;
  // Does not require any unified memory support
  cudaMalloc(&d_ptr, sizeof(int));
  write_value<<<1, 1>>>(d_ptr, 1);
  int host;
  // Copy memory back to the host and synchronize
  cudaMemcpy(&host, d_ptr, sizeof(int),
             cudaMemcpyDefault);
  printf("value = %d\n", host); 
  cudaFree(d_ptr); 
  return 0;
}





__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

// Requires CUDA Managed Memory support
__managed__ int value;

int main() {
  write_value<<<1, 1>>>(&value, 1);
  // Synchronize required
  // (before, cudaMemcpy was synchronizing)
  cudaDeviceSynchronize();
  printf("value = %d\n", value);
  return 0;
}








These examples combine two numbers together on the GPU with a per-thread ID returning the values in an array:

Without Unified Memory: both host- and device-side storage for the return values is required (host_ret and ret in the example),
as is an explicit copy between the two using cudaMemcpy().

With Unified Memory: GPU accesses data directly from the host. ret may be used without a separate host_ret allocation and
no copy routine is required, greatly simplifying and reducing the size of the program. With:

System Allocated: no other changes required.
Managed Memory: data allocation changed to use cudaMallocManaged(), which returns a pointer valid from both host and device code.





19.1.2.1. Allocation APIs for System-Allocated Memoryï

On systems with full CUDA Unified Memory support, all memory is unified memory.
This includes memory allocated with system allocation APIs, such as malloc(), mmap(), C++ new() operator,
and also automatic variables on CPU thread stacks, thread locals, global variables, and so on.
System-Allocated Memory may be popullated on first touch, depending on the API and system settings used.
First touch means that:
- The allocation APIs allocate virtual memory and return immediately, and
- physical memory is populated when a thread accesses the memory for the first time.
Usually, the physical memory will be chosen âcloseâ to the processor that thread is running on. For example,
- GPU thread accesses it first: physical GPU memory of GPU that thread runs on is chosen.
- CPU thread accesses it first: physical CPU memory in the memory NUMA node of the CPU core that thread runs on is chosen.
CUDA Unified Memory Hint and Prefetch APIs,  cudaMemAdvise and cudaMemPreftchAsync, may be used on System-Allocated Memory.
These APIs are covered below in the Data Usage Hints section.

__global__ void printme(char *str) {
  printf(str);
}

int main() {
  // Allocate 100 bytes of memory, accessible to both Host and Device code
  char *s = (char*)malloc(100);
  // Physical allocation placed in CPU memory because host accesses "s" first
  strncpy(s, "Hello Unified Memory\n", 99);
  // Here we pass "s" to a kernel without explicitly copying
  printme<<< 1, 1 >>>(s);
  cudaDeviceSynchronize();
  // Free as for normal CUDA allocations
  cudaFree(s); 
  return  0;
}





19.1.2.2. Allocation API for CUDA Managed Memory: cudaMallocManaged()ï

On systems with CUDA Managed Memory support, unified memory may be allocated using:

__host__ cudaError_t cudaMallocManaged(void **devPtr, size_t size);


This API is syntactically identical to cudaMalloc(): it allocates size bytes of managed memory and
sets devPtr to refer to the allocation.
CUDA Managed Memory is also deallocated with cudaFree().
On systems with full CUDA Managed Memory support, managed memory allocations
may be accessed concurrently by all CPUs and GPUs in the system.
Replacing host calls to cudaMalloc() with cudaMallocManaged(), does not impact program semantics on these systems;
device code is not able to call cudaMallocManaged().
The following example shows the use of cudaMallocManaged():

__global__ void printme(char *str) {
  printf(str);
}

int main() {
  // Allocate 100 bytes of memory, accessible to both Host and Device code
  char *s;
  cudaMallocManaged(&s, 100);
  // Note direct Host-code use of "s"
  strncpy(s, "Hello Unified Memory\n", 99);
  // Here we pass "s" to a kernel without explicitly copying
  printme<<< 1, 1 >>>(s);
  cudaDeviceSynchronize();
  // Free as for normal CUDA allocations
  cudaFree(s); 
  return  0;
}



Note
For systems that support CUDA Managed Memory allocations, but do not provide full support,
see Coherency and Concurrency.
Implementation details (may change any time):

Devices of compute capability 5.x allocate CUDA Managed Memory on the GPU.
Devices of compute capability 6.x and greater populate the memory on first touch, just like System-Allocated Memory APIs.





19.1.2.3. Global-Scope Managed Variables Using __managed__ï

CUDA __managed__ variables behave as if they were allocated via cudaMallocManaged()
(see Explicit Allocation Using cudaMallocManaged() ).
They simplify programs with global variables, making it particularly easy to exchange data between host and device
without manual allocations or copying.
On systems with full CUDA Unified Memory support,
file-scope or global-scope variables cannot be directly accessed by device code.
But a pointer to these variables may be passed to the kernel as an argument,
see System Allocator for examples.



System Allocator

__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

int main() {
  // Requires System-Allocated Memory support
  int value;
  write_value<<<1, 1>>>(&value, 1);
  // Synchronize required
  // (before, cudaMemcpy was synchronizing)
  cudaDeviceSynchronize();
  printf("value = %d\n", value);
  return 0;
}





Managed

__global__ void write_value(int* ptr, int v) {
  *ptr = v;
}

// Requires CUDA Managed Memory support
__managed__ int value;

int main() {
  write_value<<<1, 1>>>(&value, 1);
  // Synchronize required
  // (before, cudaMemcpy was synchronizing)
  cudaDeviceSynchronize();
  printf("value = %d\n", value);
  return 0;
}




Note the absence of explicit cudaMemcpy() commands and the fact that the return array ret is visible on both CPU and GPU.
CUDA __managed__ variable implies __device__ and is equivalent to __managed__ __device__, which is also allowed.
Variables marked __constant__ may not be marked as __managed__.
A valid CUDA context is necessary for the correct operation of __managed__ variables.
Accessing __managed__ variables can trigger CUDA context creation if a context for the current device hasnât already been created.
In the example above, accessing x before the kernel launch triggers context creation on device 0.
In the absence of that access, the kernel launch would have triggered context creation.
C++ objects declared as __managed__ are subject to certain specific constraints, particularly where static initializers are concerned.
Please refer to C++ Language Support for a list of these constraints.

Note
For devices with CUDA Managed Memory without full support,
visibility of __managed__ variables for asynchronous operations executing in CUDA streams
is discussed in the section on
Managing Data Visibility and Concurrent CPU + GPU Access with Streams.




19.1.2.4. Difference between Unified Memory and Mapped Memoryï

The main difference between Unified Memory and CUDA Mapped Memory is that
CUDA Mapped Memory does not guarantee that all kinds of memory accesses (for example atomics) are supported on all systems,
while Unified Memory does. The limited set of memory operations that are guaranteed to be portably supported by CUDA Mapped Memory
is available on more systems than Unified Memory.



19.1.2.5. Pointer Attributesï

CUDA Programs may check whether a pointer addresses a CUDA Managed Memory allocation by calling cudaPointerGetAttributes() and
testing whether the pointer attribute value is cudaMemoryTypeManaged.
This API returns cudaMemoryTypeHost for system-allocated memory that has been registered with cudaHostRegister()
and cudaMemoryTypeUnregistered for system-allocated memory that CUDA is unaware of.
Pointer attributes do not state where the memory resides, they state how the memory was allocated or registered.
The following example shows how to detect the type of pointer at runtime:

char const* kind(cudaPointerAttributes a, bool pma, bool cma) {
    switch(a.type) {
    case cudaMemoryTypeHost: return pma?
      "Unified: CUDA Host or Registered Memory" :
      "Not Unified: CUDA Host or Registered Memory";
    case cudaMemoryTypeDevice: return "Not Unified: CUDA Device Memory";
    case cudaMemoryTypeManaged: return cma?
      "Unified: CUDA Managed Memory" : "Not Unified: CUDA Managed Memory";
    case cudaMemoryTypeUnregistered: return pma?
      "Unified: System-Allocated Memory" :
      "Not Unified: System-Allocated Memory";
    default: return "unknown";
    }
}

void check_pointer(int i, void* ptr) {
  cudaPointerAttributes attr;
  cudaPointerGetAttributes(&attr, ptr);
  int pma = 0, cma = 0, device = 0;
  cudaGetDevice(&device);
  cudaDeviceGetAttribute(&pma, cudaDevAttrPageableMemoryAccess, device);
  cudaDeviceGetAttribute(&cma, cudaDevAttrConcurrentManagedAccess, device);
  printf("Pointer %d: memory is %s\n", i, kind(attr, pma, cma));
}

__managed__ int managed_var = 5;

int main() {
  int* ptr[5];
  ptr[0] = (int*)malloc(sizeof(int));
  cudaMallocManaged(&ptr[1], sizeof(int));
  cudaMallocHost(&ptr[2], sizeof(int));
  cudaMalloc(&ptr[3], sizeof(int));
  ptr[4] = &managed_var;

  for (int i = 0; i < 5; ++i) check_pointer(i, ptr[i]);
  
  cudaFree(ptr[3]);
  cudaFreeHost(ptr[2]);
  cudaFree(ptr[1]);
  free(ptr[0]);
  return 0;
}





19.1.2.6. Runtime detection of Unified Memory Support Levelï

The following example shows how to detect the Unified Memory support level at runtime:

int main() {
  int d;
  cudaGetDevice(&d);

  int pma = 0;
  cudaDeviceGetAttribute(&pma, cudaDevAttrPageableMemoryAccess, d);
  printf("Full Unified Memory Support: %s\n", pma == 1? "YES" : "NO");
  
  int cma = 0;
  cudaDeviceGetAttribute(&cma, cudaDevAttrConcurrentManagedAccess, d);
  printf("CUDA Managed Memory with full support: %s\n", cma == 1? "YES" : "NO");

  return 0;
}





19.1.2.7. GPU Memory Oversubscriptionï

Unified Memory enables applications to oversubscribe the memory of any individual processor:
in other words they can allocate and share arrays larger than
the memory capacity of any individual processor in the system,
enabling among others out-of-core processing of datasets that do not fit within
a single GPU, without adding significant complexity to the programming model.



19.1.2.8. Performance Hintsï

The following sections describes the available unified memory performance hints,
which may be used on all Unified Memory, for example, CUDA Managed memory or,
on systems with full CUDA Unified Memory support,
also all System-Allocated Memory.
These APIs are hints, that is, they do not impact the semantics of applications, only their peformance.
That is, they can be added or removed anywhere on any application without impacting its results.
CUDA Unified Memory may not always have all the information necessary to make
the best performance decisions related to unified memory.
These performance hints enable the application to provide CUDA with more information.
Note that applications should only use these hints if they improve their performance.


19.1.2.8.1. Data Prefetchingï

The cudaMemPrefetchAsync API is an asynchronous stream-ordered API that may migrate data to reside closer to the specified processor.
The data may be accessed while it is being prefetched.
The migration does not begin until all prior operations in the stream have completed,
and completes before any subsequent operation in the stream.

cudaError_t cudaMemPrefetchAsync(const void *devPtr,
                                 size_t count,
                                 int dstDevice,
                                 cudaStream_t stream);


A memory region containing [devPtr, devPtr + count) may be migrated to
the destination device dstDevice - or CPU if cudaCpuDeviceId used -
when the prefetch task is executed in the given stream.
Consider a simple code example below:



System Allocator

void test_prefetch_sam(cudaStream_t s) {
  char *data = (char*)malloc(N);
  init_data(data, N);                                     // execute on CPU
  cudaMemPrefetchAsync(data, N, myGpuId, s);              // prefetch to GPU
  mykernel<<<(N + TPB - 1) / TPB, TPB, 0, s>>>(data, N);  // execute on GPU
  cudaMemPrefetchAsync(data, N, cudaCpuDeviceId, s);      // prefetch to CPU
  cudaStreamSynchronize(s);
  use_data(data, N);
  free(data);
}





Managed

void test_prefetch_managed(cudaStream_t s) {
  char *data;
  cudaMallocManaged(&data, N);
  init_data(data, N);                                     // execute on CPU
  cudaMemPrefetchAsync(data, N, myGpuId, s);              // prefetch to GPU
  mykernel<<<(N + TPB - 1) / TPB, TPB, 0, s>>>(data, N);  // execute on GPU
  cudaMemPrefetchAsync(data, N, cudaCpuDeviceId, s);      // prefetch to CPU
  cudaStreamSynchronize(s);
  use_data(data, N);
  cudaFree(data);
}







19.1.2.8.2. Data Usage Hintsï

When multiple processors simultaneously access the same data,
cudaMemAdvise may be used to hint how the data at
[devPtr, devPtr + count) will be accessed:

cudaError_t cudaMemAdvise(const void *devPtr,
                          size_t count,
                          enum cudaMemoryAdvise advice,
                          int device);


Where advice may take the following values:

cudaMemAdviseSetReadMostly:
This implies that the data is mostly going to be read from and only occasionally written to.
In general, it allows trading off read bandwidth for write bandwidth on this region.
Example:


void test_advise_managed(cudaStream_t stream) {
  char *dataPtr;
  size_t dataSize = 64 * TPB;  // 16 KiB
  // Allocate memory using cudaMallocManaged
  // (malloc may be used on systems with full CUDA Unified memory support)
  cudaMallocManaged(&dataPtr, dataSize);
  // Set the advice on the memory region
  cudaMemAdvise(dataPtr, dataSize, cudaMemAdviseSetReadMostly, myGpuId);
  int outerLoopIter = 0;
  while (outerLoopIter < maxOuterLoopIter) {
    // The data is written to in the outer loop on the CPU
    init_data(dataPtr, dataSize);
    // The data is made available to all GPUs by prefetching.
    // Prefetching here causes read duplication of data instead
    // of data migration
    for (int device = 0; device < maxDevices; device++) {
      cudaMemPrefetchAsync(dataPtr, dataSize, device, stream);
    }
    // The kernel only reads this data in the inner loop
    int innerLoopIter = 0;
    while (innerLoopIter < maxInnerLoopIter) {
      mykernel<<<32, TPB, 0, stream>>>((const char *)dataPtr, dataSize);
      innerLoopIter++;
    }
    outerLoopIter++;
  }
  cudaFree(dataPtr);
}



cudaMemAdviseSetPreferredLocation:
In general, any memory may be migrated at any time to any location, for example,
when a given processor is running out of physical memory.
This hint tells the system that migrating this memory region away from
its preferred location is undesired, by setting the preferred location for
the data to be the physical memory belonging to device.
Passing in a value of cudaCpuDeviceId for device sets the preferred location as CPU memory.
Other hints, like cudaMemPrefetchAsync, may override this hint,
leading the memory to be migrated away from its preferred location.


cudaMemAdviseSetAccessedBy:
In some systems, it may be beneficial for performance to establish a
mapping into memory before accessing the data from a given processor.
This hint tells the system that the data will be frequently accessed by device,
enabling the system to assume that creating these mappings pays off.
This hint does not imply where the data should reside,
but it can be combined with cudaMemAdviseSetPreferredLocation to specify that.

Each advice can be also unset by using one of the following values:
cudaMemAdviseUnsetReadMostly, cudaMemAdviseUnsetPreferredLocation and
cudaMemAdviseUnsetAccessedBy.



19.1.2.8.3. Querying Data Usage Attributes on Managed Memoryï

A program can query memory range attributes assigned through cudaMemAdvise
or cudaMemPrefetchAsync on CUDA Managed Memory by using the following API:

cudaMemRangeGetAttribute(void *data,
                         size_t dataSize,
                         enum cudaMemRangeAttribute attribute,
                         const void *devPtr,
                         size_t count);


This function queries an attribute of the memory range starting at devPtr with a size of count bytes.
The memory range must refer to managed memory allocated via cudaMallocManaged or
declared via __managed__ variables. It is possible to query the following attributes:

cudaMemRangeAttributeReadMostly:
the result returned will be 1 if the entire memory range has the cudaMemAdviseSetReadMostly attribute set, or 0 otherwise.
cudaMemRangeAttributePreferredLocation:
the result returned will be a GPU device id or cudaCpuDeviceId if the entire
memory range has the corresponding processor as preferred location,
otherwise cudaInvalidDeviceId will be returned.
An application can use this query API to make decision about staging data through
CPU or GPU depending on the preferred location attribute of the managed pointer.
Note that the actual location of the memory range at the time
of the query may be different from the preferred location.
cudaMemRangeAttributeAccessedBy:
will return the list of devices that have that advise set for that memory range.
cudaMemRangeAttributeLastPrefetchLocation:
will return the last location to which the memory range was prefetched
explicitly using cudaMemPrefetchAsync.
Note that this simply returns the last location that the application requested to prefetch the memory range to.
It gives no indication as to whether the prefetch operation to that location has completed or even begun.

Additionally, multiple attributes can be queried by using corresponding cudaMemRangeGetAttributes function.






19.2. Unified memory on devices with full CUDA Unified Memory supportï



19.2.1. System-Allocated Memory: in-depth examplesï

Systems with full CUDA Unified Memory support
allow the device to access any memory owned by the host process interacting with the device.
This section shows a few advanced use-cases, using a kernel that simply prints
the first 8 characters of an input character array to the standard output stream:

__global__ void kernel(const char* type, const char* data) {
  static const int n_char = 8;
  printf("%s - first %d characters: '", type, n_char);
  for (int i = 0; i < n_char; ++i) printf("%c", data[i]);
  printf("'\n");
}


The following tabs show various ways of how this kernel may be called:



Malloc

void test_malloc() {
  const char test_string[] = "Hello World";
  char* heap_data = (char*)malloc(sizeof(test_string));
  strncpy(heap_data, test_string, sizeof(test_string));
  kernel<<<1, 1>>>("malloc", heap_data);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
  free(heap_data);
}





Managed

void test_managed() {
  const char test_string[] = "Hello World";
  char* data;
  cudaMallocManaged(&data, sizeof(test_string));
  strncpy(data, test_string, sizeof(test_string));
  kernel<<<1, 1>>>("managed", data);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
  cudaFree(data);
}





Stack variable

void test_stack() {
  const char test_string[] = "Hello World";
  kernel<<<1, 1>>>("stack", test_string);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
}





File-scope static variable

void test_static() {
  static const char test_string[] = "Hello World";
  kernel<<<1, 1>>>("static", test_string);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
}





Global-scope variable

const char global_string[] = "Hello World";

void test_global() {
  kernel<<<1, 1>>>("global", global_string);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
}





Global-scope extern variable

// declared in separate file, see below
extern char* ext_data;

void test_extern() {
  kernel<<<1, 1>>>("extern", ext_data);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
}



/** This may be a non-CUDA file */
char* ext_data;
static const char global_string[] = "Hello World";

void __attribute__ ((constructor)) setup(void) {
  ext_data = (char*)malloc(sizeof(global_string));
  strncpy(ext_data, global_string, sizeof(global_string));
}

void __attribute__ ((destructor)) tear_down(void) {
  free(ext_data);
}




The first three tabs above show the example as already detailed in the
Programming Model section.
The next three tabs show various ways a file-scope or global-scope variable can
be accessed from the device.
Note that for the extern variable, it could be declared and its memory
owned and managed by a third-party library, which does not interact with CUDA at all.
Also note that stack variables as well as file-scope and global-scope variables can
only be accessed through a pointer by the GPU. In this specific example, this is
convenient because the character array is already declared as a pointer: const char*.
However, consider the following example with a global-scope integer:

// this variable is declared at global scope
int global_variable;

__global__ void kernel_uncompilable() {
  // this causes a compilation error: global (__host__) variables must not
  // be accessed from __device__ / __global__ code
  printf("%d\n", global_variable);
}

// On systems with pageableMemoryAccess set to 1, we can access the address
// of a global variable. The below kernel takes that address as an argument
__global__ void kernel(int* global_variable_addr) {
  printf("%d\n", *global_variable_addr);
}
int main() {
  kernel<<<1, 1>>>(&global_variable);
  ...
  return 0;
}


In the example above, we need to ensure to pass a pointer to the global variable
to the kernel instead of directly accessing the global variable in the kernel.
This is because global variables without the __managed__ specifier are declared
as __host__-only by default, thus most compilers wonât allow using these
variables directly in device code as of now.


19.2.1.1. File-backed Unified Memoryï

Since systems with full CUDA Unified Memory support
allow the device to access any memory owned by the host process,
they can directly access file-backed memory.
Here, we show a modified version of the initial example shown in the previous section to use
file-backed memory in order to print a string from the GPU, read directly from an input file.
In the following example, the memory is backed by a physical file, but the example
applies to memory-backed files, too, as shown in the section on
Inter-Process Communication with Unified Memory.

__global__ void kernel(const char* type, const char* data) {
  static const int n_char = 8;
  printf("%s - first %d characters: '", type, n_char);
  for (int i = 0; i < n_char; ++i) printf("%c", data[i]);
  printf("'\n");
}



void test_file_backed() {
  int fd = open(INPUT_FILE_NAME, O_RDONLY);
  ASSERT(fd >= 0, "Invalid file handle");
  struct stat file_stat;
  int status = fstat(fd, &file_stat);
  ASSERT(status >= 0, "Invalid file stats");
  char* mapped = (char*)mmap(0, file_stat.st_size, PROT_READ, MAP_PRIVATE, fd, 0);
  ASSERT(mapped != MAP_FAILED, "Cannot map file into memory");
  kernel<<<1, 1>>>("file-backed", mapped);
  ASSERT(cudaDeviceSynchronize() == cudaSuccess,
    "CUDA failed with '%s'", cudaGetErrorString(cudaGetLastError()));
  ASSERT(munmap(mapped, file_stat.st_size) == 0, "Cannot unmap file");
  ASSERT(close(fd) == 0, "Cannot close file");
}


Note that on systems without the hostNativeAtomicSupported property, including
systems with Linux HMM enabled,
atomic accesses to file-backed memory are not supported.



19.2.1.2. Inter-Process Communication (IPC) with Unified Memoryï


Note
As of now, using IPC with Unified Memory can have significant performance implications.

Many applications prefer to manage one GPU per process, but still need to use Unified Memory,
for example for over-subscription, and access it from multiple GPUs.
CUDA IPC (see Interprocess Communication )
does not support Managed Memory: handles to this type of memory may not be shared through
any of the mechanisms discussed in this section.
On systems with full CUDA Unified Memory support,
System-Allocated Memory is Inter-Process Communication (IPC) capable.
Once access to System-Allocated Memory has been shared with other processes,
the same Unified Memory Programming Model applies,
similar to File-backed Unified Memory.
See the following references for more information on various ways of creating
IPC-capable System-Allocated Memory under Linux:

mmap with MAP_SHARED
POSIX IPC APIs
Linux memfd_create

Note that it is not possible to share memory between different hosts and their devices using this technique.




19.2.2. Performance Tuningï

In order to achieve good performance with Unified Memory, it is important to:

Understand how paging works on your system, and how to avoid unnecessary page faults.
Understand the various mechanisms allowing to keep data local to the accessing processor.
Consider tuning your application for the granularity of memory transfers of your system.

As general advice, Unified Memory Performance Hints
might provide improved performance, but using them incorrectly might degrade performance
compared to the default behavior.
Also note that any hint has a performance cost associated with it on the host,
thus useful hints must at the very least improve performance enough to overcome this cost.


19.2.2.1. Memory Paging and Page Sizesï

Many of the sections for unified memory performance tuning assume prior knowledge on virtual addressing,
memory pages and page sizes.
This section attempts to define all necessary terms and explain why paging matters for performance.
All currently supported systems for Unified Memory use a virtual address space:
this means that memory addresses used by an application represent a virtual location
which might be mapped to a physical location where the memory actually resides.
All currently supported processors, including both CPUs and GPUs, additionally use
memory paging. Because all systems use a virtual address space, there are two types
of memory pages:

Virtual pages: this represents a fixed-size contiguous chunk of virtual memory
per process tracked by the operating system, which can be mapped into physical memory.
Note that the virtual page is linked to the mapping: for example, a single
virtual address might be mapped into physical memory using different page sizes.
Physical pages: this represents a fixed-size contiguous chunk of memory
the processorâs main Memory Management Unit (MMU) supports and into which
a virtual page can be mapped.

Currently, all x86_64 CPUs use 4KiB physical pages.
Arm CPUs support multiple physical page sizes - 4KiB, 16KiB, 32KiB and 64KiB - depending on the exact CPU.
Finally, NVIDIA GPUs support multiple physical page sizes, but prefer 2MiB physical pages or larger.
Note that these sizes are subject to change in future hardware.
The default page size of virtual pages usually corresponds to the physical page size,
but an application may use different page sizes as long as they are supported by the
operating system and the hardware. Typically, supported virtual page sizes must be
powers of 2 and multiples of the physical page size.
The logical entity tracking the mapping of virtual pages into physical pages will be referred to as a page table,
and each mapping of a given virtual page with a given virtual size to physical pages is called a page table entry (PTE).
All supported processors provide specific caches for the page table to speed up the translation of
virtual addresses to physical addresses. These caches are called translation lookaside buffers (TLBs).
There are two important aspects for performance tuning of applications:

the choice of virtual page size,
whether the system offers a combined page table used by both CPUs and GPUs,
or separate page tables for each CPU and GPU individually.



19.2.2.1.1. Choosing the right page sizeï

In general, small page sizes lead to less (virtual) memory fragmentation but more TLB misses,
whereas larger page sizes lead to more memory fragmentation but less TLB misses.
Additionally, memory migration is generally more expensive with larger page sizes compared to
smaller page sizes, because we typically migrate full memory pages. This can cause
larger latency spikes in an application using large page sizes. See also the next section
for more details on page faults.
One important aspect for performance tuning is that TLB misses are generally
significantly more expensive on the GPU compared to the CPU. This means that
if a GPU thread frequently accesses random locations of Unified Memory mapped
using a small enough page size, it might be significantly slower compared to
the same accesses to Unified Memory mapped using a large enough page size.
While a similar effect might occur for a CPU thread randomly accessing a large
area of memory mapped using a small page size, the slowdown is less pronounced,
meaning that the application might want to trade-off this slowdown with
having less memory fragmentation.
Note that in general, applications should not tune their performance to the
physical page size of a given processor, since physical page sizes are subject
to change depending on the hardware. The advice above only applies to virtual
page sizes.



19.2.2.1.2. CPU and GPU page tables: hardware coherency vs. software coherencyï


Note
In the remainder of the performance tuning documentation, we will refer
to systems with a combined page table for both CPUs and GPUs as hardware
coherent systems. Systems with separate page tables for CPUs and GPUs are
referred to as software coherent.

Hardware coherent systems such as NVIDIA Grace Hopper offer a logically combined
page table for both CPUs and GPUs.
This is important because in order to access
System-Allocated Memory from the GPU,
the GPU uses whichever page table entry was created by the CPU for the requested memory.
If that page table entry uses the default CPU page size of 4KiB or 64KiB,
accesses to large virtual memory areas will cause significant TLB misses,
thus significant slowdowns.
See the section on configuring huge pages for examples on how to ensure
System-Allocated Memory uses large enough page sizes to avoid this type of issue.
On the other hand, on systems where the CPUs and GPUs each have their own logical
page table, different performance tuning aspects should be considered:
in order to guarantee coherency, these systems
usually use page faults in case a processor accesses a memory address mapped
into the physical memory of a different processor. Such a page fault means that:

it needs to be ensured that the currently owning processor (where the physical page currently resides)
cannot access this page anymore, either by deleting the page table entry or updating it.
it needs to be ensured that the processor requesting access can access this page,
either by creating a new page table entry or updating and existing entry, such that
it becomes valid/active.
the physical page backing this virtual page must be moved/migrated to the processor
requesting access: this can be an expensive operation, and the amount of work
is proportional to the page size.

Overall, hardware coherent systems provide significant performance benefits
compared to software coherent systems in cases where frequent concurrent accesses
to the same memory page are made by both CPU and GPU threads:

less page-faults: these systems do not need to use page-faults for emulating coherency or migrating memory,
less contention: these systems are coherent at cache-line granularity instead of page-size granularity, that is,
when there is contention from multiple processors within a cache line, only the cache line is exchanged which is much smaller than the smallest page-size,
and when the different processors access different cache-lines within a page, then there is no contention.

This impacts the performance of the following scenarios:

Atomic updates to the same address concurrently from both CPUs and GPUs.
Signaling a GPU thread from a CPU thread or vice-versa.





19.2.2.2. Direct Unified Memory Access from hostï

Some devices have hardware support for coherent reads, stores and atomic accesses
from the host on GPU-resident unified memory.
These devices have the attribute cudaDevAttrDirectManagedMemAccessFromHost set to 1.
Note that all hardware coherent systems have
this attribute set for NVLink-connected devices.
On these systems, the host has direct access to GPU-resident memory without page faults and
data migration (see Data Usage Hints
for more details on memory usage hints). Note that with CUDA Managed Memory,
the cudaMemAdviseSetAccessedBy hint with cudaCpuDeviceId is necessary
to enable this direct access without page faults.
Consider an example code below:



System Allocator

__global__ void write(int *ret, int a, int b) {
  ret[threadIdx.x] = a + b + threadIdx.x;
}

__global__ void append(int *ret, int a, int b) {
  ret[threadIdx.x] += a + b + threadIdx.x;
}
void test_malloc() {
  int *ret = (int*)malloc(1000 * sizeof(int));
  // for shared page table systems, the following hint is not necesary
  cudaMemAdvise(ret, 1000 * sizeof(int), cudaMemAdviseSetAccessedBy, cudaCpuDeviceId);

  write<<< 1, 1000 >>>(ret, 10, 100);            // pages populated in GPU memory
  cudaDeviceSynchronize();
  for(int i = 0; i < 1000; i++)
      printf("%d: A+B = %d\n", i, ret[i]);        // directManagedMemAccessFromHost=1: CPU accesses GPU memory directly without migrations
                                                  // directManagedMemAccessFromHost=0: CPU faults and triggers device-to-host migrations
  append<<< 1, 1000 >>>(ret, 10, 100);            // directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations
  cudaDeviceSynchronize();                        // directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations
  free(ret);
}





Managed

__global__ void write(int *ret, int a, int b) {
  ret[threadIdx.x] = a + b + threadIdx.x;
}

__global__ void append(int *ret, int a, int b) {
  ret[threadIdx.x] += a + b + threadIdx.x;
}

void test_managed() {
  int *ret;
  cudaMallocManaged(&ret, 1000 * sizeof(int));
  cudaMemAdvise(ret, 1000 * sizeof(int), cudaMemAdviseSetAccessedBy, cudaCpuDeviceId);  // set direct access hint

  write<<< 1, 1000 >>>(ret, 10, 100);            // pages populated in GPU memory
  cudaDeviceSynchronize();
  for(int i = 0; i < 1000; i++)
      printf("%d: A+B = %d\n", i, ret[i]);        // directManagedMemAccessFromHost=1: CPU accesses GPU memory directly without migrations
                                                  // directManagedMemAccessFromHost=0: CPU faults and triggers device-to-host migrations
  append<<< 1, 1000 >>>(ret, 10, 100);            // directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations
  cudaDeviceSynchronize();                        // directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations
  cudaFree(ret); 
}




After write kernel is completed, ret will be created and initialized in GPU memory.
Next, the CPU will access ret followed by append kernel using the same ret memory again.
This code will show different behavior depending on the system architecture and support of hardware coherency:

On systems with directManagedMemAccessFromHost=1:
CPU accesses to the managed buffer will not trigger any migrations;
the data will remain resident in GPU memory and any subsequent GPU kernels
can continue to access it directly without inflicting faults or migrations.
On systems with directManagedMemAccessFromHost=0:
CPU accesses to the managed buffer will page fault and initiate data migration;
any GPU kernel trying to access the same data first time will page fault and
migrate pages back to GPU memory.




19.2.2.3. Host Native Atomicsï

Some devices, including NVLink-connected devices in
hardware coherent systems, support hardware-accelerated
atomic accesses to CPU-resident memory. This implies that atomic accesses to host memory
do not have to be emulated with a page fault.
For these devices, the attribute cudaDevAttrHostNativeAtomicSupported is set to 1.





19.3. Unified memory on devices without full CUDA Unified Memory supportï



19.3.1. Unified memory on devices with only CUDA Managed Memory supportï

For devices with compute capability 6.x or higher but without pageable memory access,
CUDA Managed Memory is fully supported and coherent.
The programming model and performance tuning of unified memory is largely similar
to the model as described in
Unified memory on devices with full CUDA Unified Memory support,
with the notable exception that system allocators cannot be used to allocate memory.
Thus, the following list of sub-sections do not apply:

System Allocator
Hardware/Software Coherency




19.3.2. Unified memory on Windows or devices with compute capability 5.xï

Devices with compute capability lower than 6.0 or Windows platforms support CUDA Managed Memory v1.0 with limited support for data migration and coherency as well as memory oversubscription. The following sub-sections describe in more detail how to use and optimize Managed Memory on these platforms.


19.3.2.1. Data Migration and Coherencyï

GPU architectures of compute capability lower than 6.0 do not support fine-grained movement of the managed data to GPU on-demand. Whenever a GPU kernel is launched all managed memory generally has to be transferred to GPU memory to avoid faulting on memory access. With compute capability 6.x a new GPU page faulting mechanism is introduced that provides more seamless Unified Memory functionality. Combined with the system-wide virtual address space, page faulting provides several benefits. First, page faulting means that the CUDA system software doesnât need to synchronize all managed memory allocations to the GPU before each kernel launch. If a kernel running on the GPU accesses a page that is not resident in its memory, it faults, allowing the page to be automatically migrated to the GPU memory on-demand. Alternatively, the page may be mapped into the GPU address space for access over the PCIe or NVLink interconnects (mapping on access can sometimes be faster than migration). Note that Unified Memory is system-wide: GPUs (and CPUs) can fault on and migrate memory pages either from CPU memory or from the memory of other GPUs in the system.



19.3.2.2. GPU Memory Oversubscriptionï

Devices of compute capability lower than 6.0 cannot allocate more managed memory than the physical size of GPU memory.



19.3.2.3. Multi-GPUï

On systems with devices of compute capabilities lower than 6.0 managed allocations are automatically visible to all GPUs in a system via the peer-to-peer capabilities of the GPUs. Managed memory allocations behave similar to unmanaged memory allocated using cudaMalloc(): the current active device is the home for the physical allocation but other GPUs in the system will access the memory at reduced bandwidth over the PCIe bus.
On Linux the managed memory is allocated in GPU memory as long as all GPUs that are actively being used by a program have the peer-to-peer support. If at any time the application starts using a GPU that doesnât have peer-to-peer support with any of the other GPUs that have managed allocations on them, then the driver will migrate all managed allocations to system memory. In this case, all GPUs experience PCIe bandwidth restrictions.
On Windows, if peer mappings are not available (for example, between GPUs of different architectures), then the system will automatically fall back to using zero-copy memory, regardless of whether both GPUs are actually used by a program. If only one GPU is actually going to be used, it is necessary to set the CUDA_VISIBLE_DEVICES environment variable before launching the program. This constrains which GPUs are visible and allows managed memory to be allocated in GPU memory.
Alternatively, on Windows users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a non-zero value to force the driver to always use device memory for physical storage. When this environment variable is set to a non-zero value, all devices used in that process that support managed memory have to be peer-to-peer compatible with each other. The error ::cudaErrorInvalidDevice will be returned if a device that supports managed memory is used and it is not peer-to-peer compatible with any of the other managed memory supporting devices that were previously used in that process, even if ::cudaDeviceReset has been called on those devices. These environment variables are described in CUDA Environment Variables. Note that starting from CUDA 8.0 CUDA_MANAGED_FORCE_DEVICE_ALLOC has no effect on Linux operating systems.



19.3.2.4. Coherency and Concurrencyï

Simultaneous access to managed memory on devices of compute capability lower than 6.0 is not possible, because coherence could not be guaranteed if the CPU accessed a Unified Memory allocation while a GPU kernel was active.


19.3.2.4.1. GPU Exclusive Access To Managed Memoryï

To ensure coherency on pre-6.x GPU architectures, the Unified Memory programming model puts constraints on data accesses while both the CPU and GPU are executing concurrently. In effect, the GPU has exclusive access to all managed data while any kernel operation is executing, regardless of whether the specific kernel is actively using the data. When managed data is used with cudaMemcpy*() or cudaMemset*(), the system may choose to access the source or destination from the host or the device, which will put constraints on concurrent CPU access to that data while the cudaMemcpy*() or cudaMemset*() is executing. See Memcpy()/Memset() Behavior With Managed Memory for further details.
It is not permitted for the CPU to access any managed allocations or variables while the GPU is active for devices with concurrentManagedAccess property set to 0. On these systems concurrent CPU/GPU accesses, even to different managed memory allocations, will cause a segmentation fault because the page is considered inaccessible to the CPU.

__device__ __managed__ int x, y=2;
__global__  void  kernel() {
    x = 10;
}
int main() {
    kernel<<< 1, 1 >>>();
    y = 20;            // Error on GPUs not supporting concurrent access

    cudaDeviceSynchronize();
    return  0;
}


In example above, the GPU program kernel is still active when the CPU touches y. (Note how it occurs before cudaDeviceSynchronize().) The code runs successfully on devices of compute capability 6.x due to the GPU page faulting capability which lifts all restrictions on simultaneous access. However, such memory access is invalid on pre-6.x architectures even though the CPU is accessing different data than the GPU. The program must explicitly synchronize with the GPU before accessing y:

__device__ __managed__ int x, y=2;
__global__  void  kernel() {
    x = 10;
}
int main() {
    kernel<<< 1, 1 >>>();
    cudaDeviceSynchronize();
    y = 20;            //  Success on GPUs not supporing concurrent access
    return  0;
}


As this example shows, on systems with pre-6.x GPU architectures, a CPU thread may not access any managed data in between performing a kernel launch and a subsequent synchronization call, regardless of whether the GPU kernel actually touches that same data (or any managed data at all). The mere potential for concurrent CPU and GPU access is sufficient for a process-level exception to be raised.
Note that if memory is dynamically allocated with cudaMallocManaged() or cuMemAllocManaged() while the GPU is active, the behavior of the memory is unspecified until additional work is launched or the GPU is synchronized. Attempting to access the memory on the CPU during this time may or may not cause a segmentation fault. This does not apply to memory allocated using the flag cudaMemAttachHost or CU_MEM_ATTACH_HOST.



19.3.2.4.2. Explicit Synchronization and Logical GPU Activityï

Note that explicit synchronization is required even if kernel runs quickly and finishes before the CPU touches y in the above example. Unified Memory uses logical activity to determine whether the GPU is idle. This aligns with the CUDA programming model, which specifies that a kernel can run at any time following a launch and is not guaranteed to have finished until the host issues a synchronization call.
Any function call that logically guarantees the GPU completes its work is valid. This includes cudaDeviceSynchronize(); cudaStreamSynchronize() and cudaStreamQuery() (provided it returns cudaSuccess and not cudaErrorNotReady) where the specified stream is the only stream still executing on the GPU; cudaEventSynchronize() and cudaEventQuery() in cases where the specified event is not followed by any device work; as well as uses of cudaMemcpy() and cudaMemset() that are documented as being fully synchronous with respect to the host.
Dependencies created between streams will be followed to infer completion of other streams by synchronizing on a stream or event. Dependencies can be created via cudaStreamWaitEvent() or implicitly when using the default (NULL) stream.
It is legal for the CPU to access managed data from within a stream callback, provided no other stream that could potentially be accessing managed data is active on the GPU. In addition, a callback that is not followed by any device work can be used for synchronization: for example, by signaling a condition variable from inside the callback; otherwise, CPU access is valid only for the duration of the callback(s).
There are several important points of note:

It is always permitted for the CPU to access non-managed zero-copy data while the GPU is active.
The GPU is considered active when it is running any kernel, even if that kernel does not make use of managed data. If a kernel might use data, then access is forbidden, unless device property concurrentManagedAccess is 1.
There are no constraints on concurrent inter-GPU access of managed memory, other than those that apply to multi-GPU access of non-managed memory.
There are no constraints on concurrent GPU kernels accessing managed data.

Note how the last point allows for races between GPU kernels, as is currently the case for non-managed GPU memory. As mentioned previously, managed memory functions identically to non-managed memory from the perspective of the GPU. The following code example illustrates these points:

int main() {
    cudaStream_t stream1, stream2;
    cudaStreamCreate(&stream1);
    cudaStreamCreate(&stream2);
    int *non_managed, *managed, *also_managed;
    cudaMallocHost(&non_managed, 4);    // Non-managed, CPU-accessible memory
    cudaMallocManaged(&managed, 4);
    cudaMallocManaged(&also_managed, 4);
    // Point 1: CPU can access non-managed data.
    kernel<<< 1, 1, 0, stream1 >>>(managed);
    *non_managed = 1;
    // Point 2: CPU cannot access any managed data while GPU is busy,
    //          unless concurrentManagedAccess = 1
    // Note we have not yet synchronized, so "kernel" is still active.
    *also_managed = 2;      // Will issue segmentation fault
    // Point 3: Concurrent GPU kernels can access the same data.
    kernel<<< 1, 1, 0, stream2 >>>(managed);
    // Point 4: Multi-GPU concurrent access is also permitted.
    cudaSetDevice(1);
    kernel<<< 1, 1 >>>(managed);
    return  0;
}





19.3.2.4.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streamsï

Until now it was assumed that for SM architectures before 6.x: 1) any active kernel may use any managed memory, and 2) it was invalid to use managed memory from the CPU while a kernel is active. Here we present a system for finer-grained control of managed memory designed to work on all devices supporting managed memory, including older architectures with concurrentManagedAccess equal to 0.
The CUDA programming model provides streams as a mechanism for programs to indicate dependence and independence among kernel launches. Kernels launched into the same stream are guaranteed to execute consecutively, while kernels launched into different streams are permitted to execute concurrently. Streams describe independence between work items and hence allow potentially greater efficiency through concurrency.
Unified Memory builds upon the stream-independence model by allowing a CUDA program to explicitly associate managed allocations with a CUDA stream. In this way, the programmer indicates the use of data by kernels based on whether they are launched into a specified stream or not. This enables opportunities for concurrency based on program-specific data access patterns. The function to control this behavior is:

cudaError_t cudaStreamAttachMemAsync(cudaStream_t stream,
                                     void *ptr,
                                     size_t length=0,
                                     unsigned int flags=0);


The cudaStreamAttachMemAsync() function associates length bytes of memory starting from ptr with the specified stream. (Currently, length must always be 0 to indicate that the entire region should be attached.) Because of this association, the Unified Memory system allows CPU access to this memory region so long as all operations in stream have completed, regardless of whether other streams are active. In effect, this constrains exclusive ownership of the managed memory region by an active GPU to per-stream activity instead of whole-GPU activity.
Most importantly, if an allocation is not associated with a specific stream, it is visible to all running kernels regardless of their stream. This is the default visibility for a cudaMallocManaged() allocation or a __managed__ variable; hence, the simple-case rule that the CPU may not touch the data while any kernel is running.
By associating an allocation with a specific stream, the program makes a guarantee that only kernels launched into that stream will touch that data. No error checking is performed by the Unified Memory system: it is the programmerâs responsibility to ensure that guarantee is honored.
In addition to allowing greater concurrency, the use of cudaStreamAttachMemAsync() can (and typically does) enable data transfer optimizations within the Unified Memory system that may affect latencies and other overhead.



19.3.2.4.4. Stream Association Examplesï

Associating data with a stream allows fine-grained control over CPU + GPU concurrency, but what data is visible to which streams must be kept in mind when using devices of compute capability lower than 6.0. Looking at the earlier synchronization example:

__device__ __managed__ int x, y=2;
__global__  void  kernel() {
    x = 10;
}
int main() {
    cudaStream_t stream1;
    cudaStreamCreate(&stream1);
    cudaStreamAttachMemAsync(stream1, &y, 0, cudaMemAttachHost);
    cudaDeviceSynchronize();          // Wait for Host attachment to occur.
    kernel<<< 1, 1, 0, stream1 >>>(); // Note: Launches into stream1.
    y = 20;                           // Success â a kernel is running but âyâ
                                      // has been associated with no stream.
    return  0;
}


Here we explicitly associate y with host accessibility, thus enabling access at all times from the CPU. (As before, note the absence of cudaDeviceSynchronize() before the access.) Accesses to y by the GPU running kernel will now produce undefined results.
Note that associating a variable with a stream does not change the associating of any other variable. For example, associating x with stream1 does not ensure that only x is accessed by kernels launched in stream1, thus an error is caused by this code:

__device__ __managed__ int x, y=2;
__global__  void  kernel() {
    x = 10;
}
int main() {
    cudaStream_t stream1;
    cudaStreamCreate(&stream1);
    cudaStreamAttachMemAsync(stream1, &x);// Associate âxâ with stream1.
    cudaDeviceSynchronize();              // Wait for âxâ attachment to occur.
    kernel<<< 1, 1, 0, stream1 >>>();     // Note: Launches into stream1.
    y = 20;                               // ERROR: âyâ is still associated globally
                                          // with all streams by default
    return  0;
}


Note how the access to y will cause an error because, even though x has been associated with a stream, we have told the system nothing about who can see y. The system therefore conservatively assumes that kernel might access it and prevents the CPU from doing so.



19.3.2.4.5. Stream Attach With Multithreaded Host Programsï

The primary use for cudaStreamAttachMemAsync() is to enable independent task parallelism using CPU threads. Typically in such a program, a CPU thread creates its own stream for all work that it generates because using CUDAâs NULL stream would cause dependencies between threads.
The default global visibility of managed data to any GPU stream can make it difficult to avoid interactions between CPU threads in a multi-threaded program. Function cudaStreamAttachMemAsync() is therefore used to associate a threadâs managed allocations with that threadâs own stream, and the association is typically not changed for the life of the thread.
Such a program would simply add a single call to cudaStreamAttachMemAsync() to use unified memory for its data accesses:

// This function performs some task, in its own private stream.
void run_task(int *in, int *out, int length) {
    // Create a stream for us to use.
    cudaStream_t stream;
    cudaStreamCreate(&stream);
    // Allocate some managed data and associate with our stream.
    // Note the use of the host-attach flag to cudaMallocManaged();
    // we then associate the allocation with our stream so that
    // our GPU kernel launches can access it.
    int *data;
    cudaMallocManaged((void **)&data, length, cudaMemAttachHost);
    cudaStreamAttachMemAsync(stream, data);
    cudaStreamSynchronize(stream);
    // Iterate on the data in some way, using both Host & Device.
    for(int i=0; i<N; i++) {
        transform<<< 100, 256, 0, stream >>>(in, data, length);
        cudaStreamSynchronize(stream);
        host_process(data, length);    // CPU uses managed data.
        convert<<< 100, 256, 0, stream >>>(out, data, length);
    }
    cudaStreamSynchronize(stream);
    cudaStreamDestroy(stream);
    cudaFree(data);
}


In this example, the allocation-stream association is established just once, and then data is used repeatedly by both the host and device. The result is much simpler code than occurs with explicitly copying data between host and device, although the result is the same.



19.3.2.4.6. Advanced Topic: Modular Programs and Data Access Constraintsï

In the previous example cudaMallocManaged() specifies the cudaMemAttachHost flag, which creates an allocation that is initially invisible to device-side execution. (The default allocation would be visible to all GPU kernels on all streams.) This ensures that there is no accidental interaction with another threadâs execution in the interval between the data allocation and when the data is acquired for a specific stream.
Without this flag, a new allocation would be considered in-use on the GPU if a kernel launched by another thread happens to be running. This might impact the threadâs ability to access the newly allocated data from the CPU (for example, within a base-class constructor) before it is able to explicitly attach it to a private stream. To enable safe independence between threads, therefore, allocations should be made specifying this flag.

Note
An alternative would be to place a process-wide barrier across all threads after the allocation has been attached to the stream. This would ensure that all threads complete their data/stream associations before any kernels are launched, avoiding the hazard. A second barrier would be needed before the stream is destroyed because stream destruction causes allocations to revert to their default visibility. The cudaMemAttachHost flag exists both to simplify this process, and because it is not always possible to insert global barriers where required.




19.3.2.4.7. Memcpy()/Memset() Behavior With Stream-associated Unified Memoryï

See Memcpy()/Memset() Behavior With Unified Memory for a general overview of cudaMemcpy* / cudaMemset* behavior on devices with concurrentManagedAccess set. On devices where concurrentManagedAccess is not set, the following rules apply:
If cudaMemcpyHostTo* is specified and the source data is unified memory, then it will be accessed from the host if it is coherently accessible from the host in the copy stream (1); otherwise it will be accessed from the device. Similar rules apply to the destination when cudaMemcpy*ToHost is specified and the destination is unified memory.
If cudaMemcpyDeviceTo* is specified and the source data is unified memory, then it will be accessed from the device. The source must be coherently accessible from the device in the copy stream (2); otherwise, an error is returned. Similar rules apply to the destination when cudaMemcpy*ToDevice is specified and the destination is unified memory.
If cudaMemcpyDefault is specified, then unified memory will be accessed from the host either if it cannot be coherently accessed from the device in the copy stream (2) or if the preferred location for the data is cudaCpuDeviceId and it can be coherently accessed from the host in the copy stream (1); otherwise, it will be accessed from the device.
When using cudaMemset*() with unified memory, the data must be coherently accessible from the device in the stream being used for the cudaMemset() operation (2); otherwise, an error is returned.
When data is accessed from the device either by cudaMemcpy* or cudaMemset*, the stream of operation is considered to be active on the GPU. During this time, any CPU access of data that is associated with that stream or data that has global visibility, will result in a segmentation fault if the GPU has a zero value for the device attribute concurrentManagedAccess. The program must synchronize appropriately to ensure the operation has completed before accessing any associated data from the CPU.



Coherently accessible from the host in a given stream means that the memory neither has global visibility nor is it associated with the given stream.






Coherently accessible from the device in a given stream means that the memory either has global visibility or is associated with the given stream.










20. Lazy Loadingï



20.1. What is Lazy Loading?ï

Lazy Loading delays loading of CUDA modules and kernels from program initalization closer to kernels execution.
If a program does not use every single kernel it has included, then some kernels will be loaded unneccesarily.
This is very common, especially if you include any libraries.
Most of the time, programs only use a small amount of kernels from libraries they include.
Thanks to Lazy Loading, programs are able to only load kernels they are actually going to use, saving time on initialization.
This reduces memory overhead, both on GPU memory and host memory.
Lazy Loading is enabled by setting the CUDA_MODULE_LOADING environment variable to LAZY.
Firstly, CUDA Runtime will no longer load all modules during program initialization, with the exception of modules containing managed variables.
Each module will be loaded on first usage of a variable or a kernel from that module.
This optimization is only relevant to CUDA Runtime users, CUDA Driver users who use cuModuleLoad are unaffected. This optimization shipped in CUDA 11.8.
The behavior for CUDA Driver users who use cuLibraryLoad to load module data into memory can be changed by
setting the CUDA_MODULE_DATA_LOADING environment variable.
Secondly, loading a module (cuModuleLoad*() family of functions) will not be loading kernels immediately,
instead it will delay loading of a kernel until cuModuleGetFunction() is called.
There are certain exceptions here, some kernels have to be loaded during cuModuleLoad*(),
such as kernels of which pointers are stored in global variables.
This optimization is relevant to both CUDA Runtime and CUDA Driver users.
CUDA Runtime will only call cuModuleGetFunction() when a kernel is used/referenced for the first time.
This optimization shipped in CUDA 11.7.
Both of these optimizations are designed to be invisible to the user, assuming CUDA Programming Model is followed.



20.2. Lazy Loading version supportï

Lazy Loading is a CUDA Runtime and CUDA Driver feature. Upgrades to both might be necessary to utilize the feature.


20.2.1. Driverï

Lazy Loading requires R515+ user-mode library, but it supports Forward Compatibility, meaning it can run on top of older kernel mode drivers.
Without R515+ user-mode library, Lazy Loading is not available in any shape or form, even if toolkit version is 11.7+.



20.2.2. Toolkitï

Lazy Loading was introduced in CUDA 11.7, and received a significant upgrade in CUDA 11.8.
If your application uses CUDA Runtime, then in order to see benefits from Lazy Loading your application must use 11.7+ CUDA Runtime.
As CUDA Runtime is usually linked statically into programs and libraries,
this means that you have to recompile your program with CUDA 11.7+ toolkit and use CUDA 11.7+ libraries.
Otherwise you will not see the benefits of Lazy Loading, even if your driver version supports it.
If only some of your libraries are 11.7+, you will only see benefits of Lazy Loading in those libraries.
Other libraries will still load everything eagerly.



20.2.3. Compilerï

Lazy Loading does not require any compiler support. Both SASS and PTX compiled with pre-11.7 compilers can be loaded with Lazy Loading enabled,
and will see full benefits of the feature. However, 11.7+ CUDA Runtime is still required, as described above.




20.3. Triggering loading of kernels in lazy modeï

Loading kernels and variables happens automatically, without any need for explicit loading.
Simply launching a kernel or referencing a variable or a kernel will automatically load relevant modules and kernels.
However, if for any reason you wish to load a kernel without executing it or modifying it in any way, we recommend the following.


20.3.1. CUDA Driver APIï

Loading of kernels happens during cuModuleGetFunction() call.
This call is necessary even without Lazy Loading, as it is the only way to obtain a kernel handle.
However, you can also use this API to control with finer granularity when kernels are loaded.



20.3.2. CUDA Runtime APIï

CUDA Runtime API manages module management automatically,
so we recommend simply using cudaFuncGetAttributes() to reference the kernel.
This will ensure that the kernel is loaded without changing the state.




20.4. Querying whether Lazy Loading is turned onï

In order to check whether user enabled Lazy Loading, CUresult cuModuleGetLoadingMode ( CUmoduleLoadingMode* mode ) can be used.
Itâs important to note that CUDA must be initialized before running this function. Sample usage can be seen in the snippet below.

#include "cuda.h"
#include "assert.h"
#include "iostream"

int main() {
        CUmoduleLoadingMode mode;

        assert(CUDA_SUCCESS == cuInit(0));
        assert(CUDA_SUCCESS == cuModuleGetLoadingMode(&mode));

        std::cout << "CUDA Module Loading Mode is " << ((mode == CU_MODULE_LAZY_LOADING) ? "lazy" : "eager") << std::endl;

        return 0;
}





20.5. Possible issues when adopting lazy loadingï

Lazy Loading is designed so that it should not require any modifications to applications to use it.
That said, there are some caveats, especially when applications are not fully compliant with CUDA Programming Model.


20.5.1. Concurrent executionï

Loading kernels might require context synchronization.
Some programs incorrectly treat the possibility of concurrent execution of kernels as a guarantee.
In such cases, if program assumes that two kernels will be able to execute concurrently,
and one of the kernels will not return without the other kernel executing, there is a possibility of a deadlock.
If kernel A will be spinning in an infinite loop until kernel B is executing.
In such case launching kernel B will trigger lazy loading of kernel B. If this loading will require context synchronization,
then we have a deadlock: kernel A is waiting for kernel B, but loading kernel B is stuck waiting for kernel A to finish to synchronize the context.
Such program is an anti-pattern, but if for any reason you want to keep it you can do the following:

preload all kernels that you hope to execute concurrently prior to launching them
run application with CUDA_MODULE_DATA_LOADING=EAGER to force loading data eagerly without forcing each function to load eagerly




20.5.2. Allocatorsï

Lazy Loading delays loading code from initialization phase of the program closer to execution phase.
Loading code onto the GPU requires memory allocation.
If your application tries to allocate the entire VRAM on startup, e.g. to use it for its own allocator,
then it might turn out that there will be no more memory left to load the kernels.
This is despite the fact that overall Lazy Loading frees up more memory for the user.
CUDA will need to allocate some memory to load each kernel, which usually happens at first launch time of each kernel.
If your application allocator greedily allocated everything, CUDA will fail to allocate memory.
Possible solutions:

use cudaMallocAsync() instead of an allocator that allocates the entire VRAM on startup
add some buffer to compensate for the delayed loading of kernels
preload all kernels that will be used in the program before trying to initialize your allocator




20.5.3. Autotuningï

Some applications launch several kernels implementing the same functionality to determine which one is the fastest.
While it is overall advisable to run at least one warmup iteration, it becomes especially important with Lazy Loading.
After all, including time taken to load the kernel will skew your results.
Possible solutions:

do at least one warmup interaction prior to measurement
preload the benchmarked kernel prior to launching it






21. Extended GPU Memoryï

The Extended GPU Memory (EGM) feature, utilizing the high-bandwidth
NVLink-C2C, facilitates efficient access to all system memory by GPUs,
in a single-node system.
EGM applies to integrated CPU-GPU NVIDIA systems by allowing physical memory
allocation that can be accessed from any GPU
thread within the setup. EGM ensures that all GPUs can access
its resources at the speed of either GPU-GPU NVLink or NVLink-C2C.



In this setup, memory accesses occur via the local high-bandwidth
NVLink-C2C. For remote memory accesses,
GPU NVLink and, in some cases, NVLink-C2C are used. With EGM, GPU
threads gain the capability to access all available memory resources,
including CPU attached memory and HBM3, over the NVSwitch fabric.


21.1. Preliminariesï

Before diving into API changes for EGM functionalities, we are going to
cover currently supported topologies, identifier assignment,
prerequisites for virtual memory management, and CUDA types for EGM.


21.1.1. EGM Platforms: System topologyï

Currently, EGM can be enabled in three platforms: (1) Single-Node, Single-GPU:
Consists of an Arm-based CPU, CPU attached memory, and a GPU. Between the CPU
and the GPU there is a high bandwidth C2C (Chip-to-Chip) interconnect.
(2) Single-Node, Multi-GPU: Consists of fully connected four
single-node, single-GPU platforms. (3) Multi-Node, Single-GPU:
Two or more single-node multi-socket systems.

Note
Using cgroups to limit available devices will block routing over EGM
and cause performance issues. Use CUDA_VISIBLE_DEVICES instead.




21.1.2. Socket Identifiers: What are they? How to access them?ï

NUMA (Non-Uniform Memory Access) is a memory architecture used in
multi-processor computer systems such that the memory is divided into
multiple nodes. Each node has its own processors and memory. In such a
system, NUMA divides the system into nodes and assigns a unique
identifier (numaID) to every node.
EGM uses the NUMA node identifier which is assigned by the operating
system. Note that, this identifier is different from the ordinal of a
device and it is associated with the closest host node. In addition to
the existing methods, the user can obtain the identifier of the host
node (numaID) by calling cuDeviceGetAttribute
with CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID attribute type as follows:

int numaId;
cuDeviceGetAttribute(&numaId, CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID, deviceOrdinal);





21.1.3. Allocators and EGM supportï

Mapping system memory as EGM does not cause any performance issues. In
fact, accessing a remote socketâs system memory mapped as EGM is going
to be faster. Because, with EGM traffic is guaranteed to be routed over
NVLinks. Currently, cuMemCreate and cudaMemPoolCreate allocators are
supported with appropriate location type and NUMA identifiers.



21.1.4. Memory management extensions to current APIsï

Currently, EGM memory can be mapped with Virtual Memory (cuMemCreate) Â or
Stream Ordered Memory (cudaMemPoolCreate) allocators. The user is
responsible for allocating physical memory and mapping it to a virtual
memory address space on all sockets.

Note
Multi-node, single-GPU platforms require interprocess
communication. Therefore we encourage the reader to see Chapter 3


Note
We encourage readers to read CUDA Programming Guideâs Chapter 10 and Chapter 11 for a better understanding.

New CUDA property types have been added to APIs for allowing those
approaches to understand allocation locations using NUMA-like node
identifiers:







CUDA Type
Used with


CU_MEM_LOCATION_TYPE_HOST_NUMA
CUmemAllocationProp for
cuMemCreate


cudaMemLocationTypeHostNuma
cudaMemPoolProps for
cudaMemPoolCreate




Note
Please see  CUDA Driver API
and CUDA Runtime Data Types
to find more about NUMA specific CUDA types.





21.2. Using the EGM Interfaceï



21.2.1. Single-Node, Single-GPUï

Any of the existing CUDA host allocators as well as system allocated
memory can be used to benefit from high-bandwidth C2C. To the user,
local access is what a host allocation is today.

Note
Refer to the tuning guide for more information about memory allocators and page sizes.




21.2.2. Single-Node, Multi-GPUï

In a multi-GPU system, the user has to provide host information for
the placement. As we mentioned, a natural way to express that
information would be by using NUMA node IDs and EGM follows this
approach. Therefore, using the cuDeviceGetAttribute function the
user should be able to learn the closest NUMA node id. (See Socket Identifiers: What are they? How to access them?).
Then the user can allocate and manage EGM memory using VMM (Virtual
Memory Management) API or CUDA Memory Pool.


21.2.2.1. Using VMM APIsï

The first step in memory allocation using Virtual Memory Management APIs
is to create a physical memory chunk that will provide a backing for the
allocation. See CUDA Programming Guideâs Virtual Memory Management section
for more details. In EGM allocations the user has to explicitly provide
CU_MEM_LOCATION_TYPE_HOST_NUMA Â as the location type and
numaID as the location identifier. Also in EGM, allocationsÂ must
be aligned to appropriate granularity of the platform. The following
code snippet shows allocating physical memory with cuMemCreate:

CUmemAllocationProp prop{};
prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;
prop.location.type = CU_MEM_LOCATION_TYPE_HOST_NUMA;
prop.location.id = numaId;
size_t granularity = 0;
cuMemGetAllocationGranularity(&granularity, &prop, MEM_ALLOC_GRANULARITY_MINIMUM);
size_t padded_size = ROUND_UP(size, granularity);
CUmemGenericAllocationHandle allocHandle;
cuMemCreate(&allocHandle, padded_size, &prop, 0);


After physical memory allocation, we have to reserve an address space
and map it to a pointer. These procedures do not have EGM-specific
changes:

CUdeviceptr dptr;
cuMemAddressReserve(&dptr, padded_size, 0, 0, 0);
cuMemMap(dptr, padded_size, 0, allocHandle, 0);


Finally, the user has to explicitly protect mapped virtual address
ranges. Otherwise access to the mapped space would result in a crash.
Similar to the memory allocation, the user has to provide
CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and
numaId as the location identifier. Following code snippet create
an access descriptors for the host node and the GPU to give read and
write access for the mapped memory to both of them:

CUmemAccessDesc accessDesc[2]{{}};
accessDesc[0].location.type = CU_MEM_LOCATION_TYPE_HOST_NUMA;
accessDesc[0].location.id = numaId;
accessDesc[0].flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;
accessDesc[1].location.type = CU_MEM_LOCATION_TYPE_DEVICE;
accessDesc[1].location.id = currentDev;
accessDesc[1].flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;
cuMemSetAccess(dptr, size, accessDesc, 2);





21.2.2.2. Using CUDA Memory Poolï

To define EGM, the user can create a memory pool on a node and give
access to peers. In this case, the user has to explicitly define
cudaMemLocationTypeHostNuma as the location type and numaId
as the location identifier. The following code snippet shows creating a
memory pool cudaMemPoolCreate:

cudaSetDevice(homeDevice);
cudaMemPoolProps props{};
props.allocType = cudaMemAllocationTypePinned;
props.location.type = cudaMemLocationTypeHostNuma;
props.location.id = numaId;
cudaMemPoolCreate(&memPool, &props);


Additionally, for direct connect peer access, it is also possible to use
the existing peer access API, cudaMemPoolSetAccess. An example
for an accessingDevice is shown in the following code snippet:

cudaMemAccessDesc desc{};
desc.flags = cudaMemAccessFlagsProtReadWrite;
desc.location.type = cudaMemLocationTypeDevice;
desc.location.id = accessingDevice;
cudaMemPoolSetAccess(memPool, &desc, 1);


When the memory pool is created, and accesses are given, the user can
set created memory pool to the residentDevice and start allocating
memory using cudaMallocAsync:

cudaDeviceSetMemPool(residentDevice, memPool);
cudaMallocAsync(&ptr, size, memPool, stream);



Note
EGM is mapped with 2MB pages. Therefore, users may encounter more TLB
misses when accessing very large allocations.





21.2.3. Multi-Node, Single-GPUï

Beyond memory allocation, remote peer access does not have EGM-specific
modification and it follows CUDA inter process (IPC) protocol. See
CUDA Programming Guide
for more details in IPC.
The user should allocate memory using cuMemCreate and again the
user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as
the location type and numaID as the location identifier. In
addition CU_MEM_HANDLE_TYPE_FABRIC should be defined as the
requested handle type. The following code snippet shows allocating
physical memory onÂ Node A:

CUmemAllocationProp prop{};
prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;
prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_FABRIC;
prop.location.type = CU_MEM_LOCATION_TYPE_HOST_NUMA;
prop.location.id = numaId;
size_t granularity = 0;
cuMemGetAllocationGranularity(&granularity, &prop,
                              MEM_ALLOC_GRANULARITY_MINIMUM);
size_t padded_size = ROUND_UP(size, granularity);
size_t page_size = ...;
assert(padded_size % page_size == 0);
CUmemGenericAllocationHandle allocHandle;
cuMemCreate(&allocHandle, padded_size, &prop, 0);


After creating allocation handle using cuMemCreate the user can
export that handle to the other node, Node B, calling
cuMemExportToShareableHandle:

cuMemExportToShareableHandle(&fabricHandle, allocHandle,
                             CU_MEM_HANDLE_TYPE_FABRIC, 0);
// At this point, fabricHandle should be sent to Node B via TCP/IP.


On Node B, the handle can be imported using
cuMemImportFromShareableHandle and treated as any other fabric
handle

// At this point, fabricHandle should be received from Node A via TCP/IP.
CUmemGenericAllocationHandle allocHandle;
cuMemImportFromShareableHandle(&allocHandle, &fabricHandle,
                               CU_MEM_HANDLE_TYPE_FABRIC);


When handle is imported at Node B, then the user can reserve an address
space and map it locally in a regular fashion:

size_t granularity = 0;
cuMemGetAllocationGranularity(&granularity, &prop,
                              MEM_ALLOC_GRANULARITY_MINIMUM);
size_t padded_size = ROUND_UP(size, granularity);
size_t page_size = ...;
assert(padded_size % page_size == 0);
CUdeviceptr dptr;
cuMemAddressReserve(&dptr, padded_size, 0, 0, 0);
cuMemMap(dptr, padded_size, 0, allocHandle, 0);


As the final step, the user should give appropriate accesses to each of
the local GPUs at Node B. An example code snippet that gives read and
write access to eight local GPUs:

// Give all 8 local Â GPUS access to exported EGM memory located on Node A.                                                               |
CUmemAccessDesc accessDesc[8];
for (int i = 0; i < 8; i++) {
   accessDesc[i].location.type = CU_MEM_LOCATION_TYPE_DEVICE;
   accessDesc[i].location.id = i;
   accessDesc[i].flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;
}
cuMemSetAccess(dptr, size, accessDesc, 8);







22. Noticesï



22.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



22.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



22.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      



















CUDA C++ Best Practices Guide











































1. Preface
1.1. What Is This Document?
1.2. Who Should Read This Guide?

1.3. Assess, Parallelize, Optimize, Deploy
1.3.1. Assess
1.3.2. Parallelize
1.3.3. Optimize
1.3.4. Deploy


1.4. Recommendations and Best Practices
1.5. Assessing Your Application



2. Heterogeneous Computing
2.1. Differences between Host and Device
2.2. What Runs on a CUDA-Enabled Device?



3. Application Profiling

3.1. Profile
3.1.1. Creating the Profile
3.1.2. Identifying Hotspots

3.1.3. Understanding Scaling
3.1.3.1. Strong Scaling and Amdahlâs Law
3.1.3.2. Weak Scaling and Gustafsonâs Law
3.1.3.3. Applying Strong and Weak Scaling






4. Parallelizing Your Application

5. Getting Started
5.1. Parallel Libraries
5.2. Parallelizing Compilers
5.3. Coding to Expose Parallelism



6. Getting the Right Answer

6.1. Verification
6.1.1. Reference Comparison
6.1.2. Unit Testing


6.2. Debugging

6.3. Numerical Accuracy and Precision
6.3.1. Single vs. Double Precision
6.3.2. Floating Point Math Is not Associative
6.3.3. IEEE 754 Compliance
6.3.4. x86 80-bit Computations




7. Optimizing CUDA Applications

8. Performance Metrics

8.1. Timing
8.1.1. Using CPU Timers
8.1.2. Using CUDA GPU Timers



8.2. Bandwidth
8.2.1. Theoretical Bandwidth Calculation
8.2.2. Effective Bandwidth Calculation
8.2.3. Throughput Reported by Visual Profiler





9. Memory Optimizations

9.1. Data Transfer Between Host and Device
9.1.1. Pinned Memory
9.1.2. Asynchronous and Overlapping Transfers with Computation
9.1.3. Zero Copy
9.1.4. Unified Virtual Addressing



9.2. Device Memory Spaces

9.2.1. Coalesced Access to Global Memory
9.2.1.1. A Simple Access Pattern
9.2.1.2. A Sequential but Misaligned Access Pattern
9.2.1.3. Effects of Misaligned Accesses
9.2.1.4. Strided Accesses



9.2.2. L2 Cache
9.2.2.1. L2 Cache Access Window
9.2.2.2. Tuning the Access Window Hit-Ratio



9.2.3. Shared Memory
9.2.3.1. Shared Memory and Memory Banks
9.2.3.2. Shared Memory in Matrix Multiplication (C=AB)
9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT)
9.2.3.4. Asynchronous Copy from Global Memory to Shared Memory


9.2.4. Local Memory

9.2.5. Texture Memory
9.2.5.1. Additional Texture Capabilities


9.2.6. Constant Memory

9.2.7. Registers
9.2.7.1. Register Pressure




9.3. Allocation
9.4. NUMA Best Practices



10. Execution Configuration Optimizations

10.1. Occupancy
10.1.1. Calculating Occupancy


10.2. Hiding Register Dependencies
10.3. Thread and Block Heuristics
10.4. Effects of Shared Memory
10.5. Concurrent Kernel Execution
10.6. Multiple contexts



11. Instruction Optimization

11.1. Arithmetic Instructions
11.1.1. Division Modulo Operations
11.1.2. Loop Counters Signed vs. Unsigned
11.1.3. Reciprocal Square Root
11.1.4. Other Arithmetic Instructions
11.1.5. Exponentiation With Small Fractional Arguments
11.1.6. Math Libraries
11.1.7. Precision-related Compiler Flags


11.2. Memory Instructions



12. Control Flow
12.1. Branching and Divergence
12.2. Branch Predication


13. Deploying CUDA Applications

14. Understanding the Programming Environment
14.1. CUDA Compute Capability
14.2. Additional Hardware Data
14.3. Which Compute Capability Target
14.4. CUDA Runtime



15. CUDA Compatibility Developerâs Guide
15.1. CUDA Toolkit Versioning
15.2. Source Compatibility

15.3. Binary Compatibility
15.3.1. CUDA Binary (cubin) Compatibility



15.4. CUDA Compatibility Across Minor Releases

15.4.1. Existing CUDA Applications within Minor Versions of CUDA
15.4.1.1. Handling New CUDA Features and Driver APIs
15.4.1.2. Using PTX
15.4.1.3. Dynamic Code Generation
15.4.1.4. Recommendations for building a minor-version compatible library
15.4.1.5. Recommendations for taking advantage of minor version compatibility in your application







16. Preparing for Deployment
16.1. Testing for CUDA Availability
16.2. Error Handling
16.3. Building for Maximum Compatibility

16.4. Distributing the CUDA Runtime and Libraries

16.4.1. CUDA Toolkit Library Redistribution
16.4.1.1. Which Files to Redistribute
16.4.1.2. Where to Install Redistributed CUDA Libraries







17. Deployment Infrastructure Tools

17.1. Nvidia-SMI
17.1.1. Queryable state
17.1.2. Modifiable state


17.2. NVML
17.3. Cluster Management Tools
17.4. Compiler JIT Cache Management Tools
17.5. CUDA_VISIBLE_DEVICES



18. Recommendations and Best Practices
18.1. Overall Performance Optimization Strategies



19. nvcc Compiler Switches
19.1. nvcc



20. Notices
20.1. Notice
20.2. OpenCL
20.3. Trademarks








CUDA C++ Best Practices Guide






 Â»

1. Preface



v12.5 |
PDF
|
Archive
Â 






CUDA C++ Best Practices Guide
The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs.


1. Prefaceï



1.1. What Is This Document?ï

This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIAÂ® CUDAÂ® GPUs. It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures.
While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored. As a result, it is recommended that first-time readers proceed through the guide sequentially. This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later.



1.2. Who Should Read This Guide?ï

The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code.
This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website https://docs.nvidia.com/cuda/. The following documents are especially important resources:

CUDA Installation Guide
CUDA C++ Programming Guide
CUDA Toolkit Reference Manual

In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide).



1.3. Assess, Parallelize, Optimize, Deployï

This guide introduces the Assess, Parallelize, Optimize, Deploy(APOD) design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible.
APOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production.





1.3.1. Assessï

For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration.
By understanding the end-userâs requirements and constraints and by applying Amdahlâs and Gustafsonâs laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application.



1.3.2. Parallelizeï

Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS, cuFFT, or Thrust, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.
On the other hand, some applicationsâ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput.



1.3.3. Optimizeï

After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned.
Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developerâs optimization efforts and provide references into the relevant portions of the optimization section of this guide.



1.3.4. Deployï

Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.
Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.




1.4. Recommendations and Best Practicesï

Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C++ code. These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority.
Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied. This approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization.
The criteria of benefit and scope for establishing priority will vary depending on the nature of the program. In this guide, they represent a typical case. Your code might reflect different priority factors. Regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items.

Note
Code samples throughout the guide omit error checking for conciseness. Production code should, however, systematically check the error code returned by each API call and check for failures in kernel launches by calling cudaGetLastError().




1.5. Assessing Your Applicationï

From supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance. The core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network. As a result, all modern processors require parallel code in order to achieve good utilization of their computational power.
While processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using MPI). In order to profit from any modern processor architecture, GPUs included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future.




2. Heterogeneous Computingï

CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices.
While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. This capability makes them well suited to computations that can leverage parallel execution.
However, the device is based on a distinctly different design from the host system, and itâs important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively.


2.1. Differences between Host and Deviceï

The primary differences are in threading model and in separate physical memories:

Threading resources

Execution pipelines on host systems can support a limited number of concurrent threads. For example, servers that have two 32 core processors can run only 64 threads concurrently (or small multiple of that if the CPUs support simultaneous multithreading). By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads). Modern NVIDIA GPUs can support up to 2048 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads to more than 160,000 concurrently active threads.

Threads

Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches (when two threads are swapped) are therefore slow and expensive. By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work (in warps of 32 threads each). If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution. In short, CPU cores are designed to minimize latency for a small number of threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput.

RAM

The host system and the device each have their own distinct attached physical memories 1. As the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in What Runs on a CUDA-Enabled Device?.


These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming. Other differences are discussed as they arise elsewhere in this document. Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device.



2.2. What Runs on a CUDA-Enabled Device?ï

The following issues should be considered when determining what parts of an application to run on the device:

The device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel. This typically involves arithmetic on large data sets (such as matrices) where the same operation can be performed across thousands, if not millions, of elements at the same time. This is a requirement for good performance on CUDA: the software must use a large number (generally thousands or tens of thousands) of concurrent threads. The support for running numerous threads in parallel derives from CUDAâs use of a lightweight threading model described above.

To use CUDA, data values must be transferred from the host to the device. These transfers are costly in terms of performance and should be minimized. (See Data Transfer Between Host and Device.) This cost has several ramifications:


The complexity of operations should justify the cost of moving data to and from the device. Code that transfers data for brief use by a small number of threads will see little or no performance benefit. The ideal scenario is one in which many threads perform a substantial amount of work.
For example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit. The issue here is the number of operations performed per data element transferred. For the preceding procedure, assuming matrices of size NxN, there are N2 operations (additions) and 3N2 elements transferred, so the ratio of operations to elements transferred is 1:3 or O(1). Performance benefits can be more readily achieved when this ratio is higher. For example, a matrix multiplication of the same matrices requires N3 operations (multiply-add), so the ratio of operations to elements transferred is O(N), in which case the larger the matrix the greater the performance benefit. The types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions. It is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device.

Data should be kept on the device as long as possible. Because transfers should be minimized, programs that run multiple kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations. So, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in some subsequent calculation, the matrix addition should be performed locally on the device. This approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host. Even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory. Data Transfer Between Host and Device provides further details, including the measurements of bandwidth between the host and the device versus within the device proper.


For best performance, there should be some coherence in memory access by adjacent threads running on the device. Certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation. Data that cannot be laid out so as to enable coalescing, or that doesnât have enough locality to use the L1 or texture caches effectively, will tend to see lesser speedups when used in computations on GPUs. A noteworthy exception to this are completely random memory access patterns. In general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency. However, compared to cache based architectures, like CPUs, latency hiding architectures, like GPUs, tend to cope better with completely random memory access patterns.


1

On Systems on a Chip with integrated GPUs, such as NVIDIAÂ® TegraÂ®, host and device memory are physically the same, but there is still a logical distinction between host and device memory. See the Application Note on CUDA for Tegra for details.






3. Application Profilingï



3.1. Profileï

Many codes accomplish a significant portion of the work with a relatively small amount of code. Using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization.


3.1.1. Creating the Profileï

There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time.

Note
High Priority: To maximize developer productivity, profile the application to determine hotspots and bottlenecks.

The most important consideration with any profiling activity is to ensure that the workload is realistic - i.e., that information gained from the test and decisions based upon that information are relevant to real data. Using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions.
There are a number of tools that can be used to generate the profile. The following example is based on gprof, which is an open-source profiler for Linux platforms from the GNU Binutils collection.

$ gcc -O2 -g -pg myprog.c
$ gprof ./a.out > profile.txt
Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 33.34      0.02     0.02     7208     0.00     0.00  genTimeStep
 16.67      0.03     0.01      240     0.04     0.12  calcStats
 16.67      0.04     0.01        8     1.25     1.25  calcSummaryData
 16.67      0.05     0.01        7     1.43     1.43  write
 16.67      0.06     0.01                             mcount
  0.00      0.06     0.00      236     0.00     0.00  tzset
  0.00      0.06     0.00      192     0.00     0.00  tolower
  0.00      0.06     0.00       47     0.00     0.00  strlen
  0.00      0.06     0.00       45     0.00     0.00  strchr
  0.00      0.06     0.00        1     0.00    50.00  main
  0.00      0.06     0.00        1     0.00     0.00  memcpy
  0.00      0.06     0.00        1     0.00    10.11  print
  0.00      0.06     0.00        1     0.00     0.00  profil
  0.00      0.06     0.00        1     0.00    50.00  report





3.1.2. Identifying Hotspotsï

In the example above, we can clearly see that the function genTimeStep() takes one-third of the total running time of the application. This should be our first candidate function for parallelization. Understanding Scaling discusses the potential benefit we might expect from such parallelization.
It is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as calcStats() and calcSummaryData(). Parallelizing these functions as well should increase our speedup potential. However, since APOD is a cyclical process, we might opt to parallelize these functions in a subsequent APOD pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes.



3.1.3. Understanding Scalingï

The amount of performance benefit an application will realize by running on CUDA depends entirely on the extent to which it can be parallelized. Code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device.

Note
High Priority: To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code.

By understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy. Strong Scaling and Amdahlâs Law describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size. Weak Scaling and Gustafsonâs Law describes weak scaling, where the speedup is attained by growing the problem size. In many applications, a combination of strong and weak scaling is desirable.


3.1.3.1. Strong Scaling and Amdahlâs Lawï

Strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system. An application that exhibits linear strong scaling has a speedup equal to the number of processors used.
Strong scaling is usually equated with Amdahlâs Law, which specifies the maximum speedup that can be expected by parallelizing portions of a serial program. Essentially, it states that the maximum speedup S of a program is:
\(S = \frac{1}{(1 - P) + \frac{P}{N}}\)
Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs.
The larger N is(that is, the greater the number of processors), the smaller the P/N fraction. It can be simpler to view N as a very large number, which essentially transforms the equation into \(S = 1/(1 - P)\). Now, if 3/4 of the running time of a sequential program is parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4.
In reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some degree of strong scaling. For most purposes, the key point is that the larger the parallelizable portion P is, the greater the potential speedup. Conversely, if P is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors N does little to improve performance. Therefore, to get the largest speedup for a fixed problem size, it is worthwhile to spend effort on increasing P, maximizing the amount of code that can be parallelized.



3.1.3.2. Weak Scaling and Gustafsonâs Lawï

Weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size per processor; i.e., where the overall problem size increases as the number of processors is increased.
Weak scaling is often equated with Gustafsonâs Law, which states that in practice, the problem size scales with the number of processors. Because of this, the maximum speedup S of a program is:
\(S = N + (1 - P)(1 - N)\)
Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs.
Another way of looking at Gustafsonâs Law is that it is not the problem size that remains constant as we scale up the system but rather the execution time. Note that Gustafsonâs Law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem.



3.1.3.3. Applying Strong and Weak Scalingï

Understanding which type of scaling is most applicable to an application is an important part of estimating speedup. For some applications the problem size will remain constant and hence only strong scaling is applicable. An example would be modeling how two molecules interact with each other, where the molecule sizes are fixed.
For other applications, the problem size will grow to fill the available processors. Examples include modeling fluids or structures as meshes or grids and some Monte Carlo simulations, where increasing the problem size provides increased accuracy.
Having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either Amdahlâs or Gustafsonâs Law to determine an upper bound for the speedup.






4. Parallelizing Your Applicationï

Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS, cuFFT, or Thrust, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.
On the other hand, some applicationsâ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput.



5. Getting Startedï

There are several key strategies for parallelizing sequential code. While the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore CPUs or for use on CUDA GPUs.


5.1. Parallel Librariesï

The most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf. The CUDA Toolkit includes a number of such libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as cuBLAS, cuFFT, and so on.
The key here is that libraries are most useful when they match well with the needs of the application. Applications already using other BLAS libraries can often quite easily switch to cuBLAS, for example, whereas applications that do little to no linear algebra will have little use for cuBLAS. The same goes for other CUDA Toolkit libraries: cuFFT has an interface similar to that of FFTW, etc.
Also of note is the Thrust library, which is a parallel C++ template library similar to the C++ Standard Template Library. Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically. As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.



5.2. Parallelizing Compilersï

Another common approach to parallelization of sequential codes is to make use of parallelizing compilers. Often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself. By exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture.
The OpenACC standard provides a set of compiler directives to specify loops and regions of code in standard C, C++ and Fortran that should be offloaded from a host CPU to an attached accelerator such as a CUDA GPU. The details of managing the accelerator device are handled implicitly by an OpenACC-enabled compiler and runtime.
See http://www.openacc.org/ for details.



5.3. Coding to Expose Parallelismï

For applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as CUDA C++ that integrate seamlessly with existing sequential code are essential.
Once we have located a hotspot in our applicationâs profile assessment and determined that custom code is the best approach, we can use CUDA C++ to expose the parallelism in that portion of our code as a CUDA kernel. We can then launch this kernel onto the GPU and retrieve the results without requiring major rewrites to the rest of our application.
This approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code. More difficult to parallelize are applications with a very flat profile - i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base. For the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, CPU and GPU alike, so it is well worth the effort should it become necessary.




6. Getting the Right Answerï

Obtaining the right answer is clearly the principal goal of all computation. On parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming. These include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way CPU and GPU processors operate. This chapter examines issues that can affect the correctness of returned data and points to appropriate solutions.


6.1. Verificationï



6.1.1. Reference Comparisonï

A key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results. After each change is made, ensure that the results match using whatever criteria apply to the particular algorithm. Some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; see Numerical Accuracy and Precision regarding numerical accuracy. For other algorithms, implementations may be considered correct if they match the reference within some small epsilon.
Note that the process used for validating numerical results can easily be extended to validate performance results as well. We want to ensure that each change we make is correct and that it improves performance (and by how much). Checking these things frequently as an integral part of our cyclical APOD process will help ensure that we achieve the desired results as rapidly as possible.



6.1.2. Unit Testingï

A useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level. For example, we can write our CUDA kernels as a collection of many short __device__ functions rather than one large monolithic __global__ function; each device function can be tested independently before hooking them all together.
For example, many kernels have complex addressing logic for accessing memory in addition to their actual computation. If we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts. (Note that the CUDA compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least write something out to global memory as a result of our addressing logic in order to successfully apply this strategy.)
Going a step further, if most functions are defined as __host__ __device__ rather than just __device__ functions, then these functions can be tested on both the CPU and the GPU, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results. If there are differences, then those differences will be seen early and can be understood in the context of a simple function.
As a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both CPU and GPU execution paths in our application: if the bulk of the work of our CUDA kernels is done in __host__ __device__ functions, we can easily call those functions from both the host code and the device code without duplication.




6.2. Debuggingï

CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see: https://developer.nvidia.com/cuda-gdb.
The NVIDIA Nsight Visual Studio Edition for Microsoft Windows 7, Windows HPC Server 2008, Windows 8.1, and Windows 10 is available as a free plugin for Microsoft Visual Studio; see: https://developer.nvidia.com/nsight-visual-studio-edition.
Several third-party debuggers support CUDA debugging as well; see: https://developer.nvidia.com/debugging-solutions for more details.



6.3. Numerical Accuracy and Precisionï

Incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored. The following sections explain the principal items of interest. Other peculiarities of floating-point arithmetic are presented in Features and Technical Specifications of the CUDA C++ Programming Guide as well as in a whitepaper and accompanying webinar on floating-point precision and performance available from https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus.


6.3.1. Single vs. Double Precisionï

Devices of compute capability 1.3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide). Results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues. Therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact.



6.3.2. Floating Point Math Is not Associativeï

Each floating-point arithmetic operation involves a certain amount of rounding. Consequently, the order in which arithmetic operations are performed is important. If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math. When you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results. This limitation is not specific to CUDA, but an inherent part of parallel computation on floating-point values.



6.3.3. IEEE 754 Complianceï

All CUDA compute devices follow the IEEE 754 standard for binary floating-point representation, with some small exceptions. These exceptions, which are detailed in Features and Technical Specifications of the CUDA C++ Programming Guide, can lead to results that differ from IEEE 754 values computed on the host system.
One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. Its result will often differ slightly from results obtained by doing the two operations separately.



6.3.4. x86 80-bit Computationsï

x86 processors can use an 80-bit double extended precision math when performing floating-point calculations. The results of these calculations can frequently differ from pure 64-bit operations performed on the CUDA device. To get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively). This is done with the FLDCW x86 assembly instruction or the equivalent operating system API.





7. Optimizing CUDA Applicationsï

After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned.
Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developerâs optimization efforts and provide references into the relevant portions of the optimization section of this guide.



8. Performance Metricsï

When attempting to optimize CUDA code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement. This chapter discusses how to correctly measure performance using CPU timers and CUDA events. It then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses.


8.1. Timingï

CUDA calls and kernel executions can be timed using either CPU or GPU timers. This section examines the functionality, advantages, and pitfalls of both approaches.


8.1.1. Using CPU Timersï

Any CPU timer can be used to measure the elapsed time of a CUDA call or kernel execution. The details of various CPU timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide.
When using CPU timers, it is critical to remember that many CUDA API functions are asynchronous; that is, they return control back to the calling CPU thread prior to completing their work. All kernel launches are asynchronous, as are memory-copy functions with the Async suffix on their names. Therefore, to accurately measure the elapsed time for a particular call or sequence of CUDA calls, it is necessary to synchronize the CPU thread with the GPU by calling cudaDeviceSynchronize() immediately before starting and stopping the CPU timer. cudaDeviceSynchronize()blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed.
Although it is also possible to synchronize the CPU thread with a particular stream or event on the GPU, these synchronization functions are not suitable for timing code in streams other than the default stream. cudaStreamSynchronize() blocks the CPU thread until all CUDA calls previously issued into the given stream have completed. cudaEventSynchronize() blocks until a given event in a particular stream has been recorded by the GPU. Because the driver may interleave execution of CUDA calls from other non-default streams, calls in other streams may be included in the timing.
Because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream.
Be aware that CPU-to-GPU synchronization points such as those mentioned in this section imply a stall in the GPUâs processing pipeline and should thus be used sparingly to minimize their performance impact.



8.1.2. Using CUDA GPU Timersï

The CUDA event API provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds. How to time code using CUDA events illustrates their use.
How to time code using CUDA events

cudaEvent_t start, stop;
float time;

cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord( start, 0 );
kernel<<<grid,threads>>> ( d_odata, d_idata, size_x, size_y,
                           NUM_REPS);
cudaEventRecord( stop, 0 );
cudaEventSynchronize( stop );

cudaEventElapsedTime( &time, start, stop );
cudaEventDestroy( start );
cudaEventDestroy( stop );


Here cudaEventRecord() is used to place the start and stop events into the default stream, stream 0. The device will record a timestamp for the event when it reaches that event in the stream. The cudaEventElapsedTime() function returns the time elapsed between the recording of the start and stop events. This value is expressed in milliseconds and has a resolution of approximately half a microsecond. Like the other calls in this listing, their specific operation, parameters, and return values are described in the CUDA Toolkit Reference Manual. Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent.




8.2. Bandwidthï

Bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance. Almost all changes to code should be made in the context of how they affect bandwidth. As described in Memory Optimizations of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors.
To measure performance accurately, it is useful to calculate theoretical and effective bandwidth. When the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it.

Note
High Priority: Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits.



8.2.1. Theoretical Bandwidth Calculationï

Theoretical bandwidth can be calculated using hardware specifications available in the product literature. For example, the NVIDIA Tesla V100 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz and a 4096-bit-wide memory interface.
Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s:
\(\left. \left( 0.877 \times 10^{9} \right. \times (4096/8) \times 2 \right) \div 10^{9} = 898\text{GB/s}\)
In this calculation, the memory clock rate is converted in to Hz, multiplied by the interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate. Finally, this product is divided by 109 to convert the result to GB/s.

Note
Some calculations use 10243 instead of 109 for the final calculation. In such a case, the bandwidth would be 836.4 GiB/s. It is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid.


Note
On GPUs with GDDR memory with ECC enabled the available DRAM is reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled, though the exact impact of ECC on bandwidth can be higher and depends on the memory access pattern. HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection.2




8.2.2. Effective Bandwidth Calculationï

Effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program. To do so, use this equation:
\(\text{Effective\ bandwidth} = \left( {\left( B_{r} + B_{w} \right) \div 10^{9}} \right) \div \text{time}\)
Here, the effective bandwidth is in units of GB/s, Br is the number of bytes read per kernel, Bw is the number of bytes written per kernel, and time is given in seconds.
For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used:
\(\text{Effective\ bandwidth} = \left( {\left( 2048^{2} \times 4 \times 2 \right) \div 10^{9}} \right) \div \text{time}\)
The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the read and write), divided by 109 (or 1,0243) to obtain GB of memory transferred. This number is divided by the time in seconds to obtain GB/s.



8.2.3. Throughput Reported by Visual Profilerï

For devices with compute capability of 2.0 or greater, the Visual Profiler can be used to collect several different memory throughput measures. The following throughput metrics can be displayed in the Details or Detail Graphs view:

Requested Global Load Throughput
Requested Global Store Throughput
Global Load Throughput
Global Store Throughput
DRAM Read Throughput
DRAM Write Throughput

The Requested Global Load Throughput and Requested Global Store Throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under Effective Bandwidth Calculation.
Because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel. For global memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store Throughput values.
Itâs important to note that both numbers are useful. The actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (see Coalesced Access to Global Memory). For global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global Memory Load Efficiency and Global Memory Store Efficiency metrics.

2

As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory.







9. Memory Optimizationsï

Memory optimizations are the most important area for performance. The goal is to maximize the use of the hardware by maximizing bandwidth. Bandwidth is best served by using as much fast memory and as little slow-access memory as possible. This chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively.


9.1. Data Transfer Between Host and Deviceï

The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory and device memory (16 GB/s on the PCIe x16 Gen3). Hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU.

Note
High Priority: Minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host CPU.

Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.
Also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer.
Finally, higher bandwidth between the host and the device is achieved when using page-locked (or pinned) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section of this document.


9.1.1. Pinned Memoryï

Page-locked or pinned memory transfers attain the highest bandwidth between the host and the device. On PCIe x16 Gen3 cards, for example, pinned memory can attain roughly 12 GB/s transfer rates.
Pinned memory is allocated using the cudaHostAlloc() functions in the Runtime API. The bandwidthTest CUDA Sample shows how to use these functions as well as how to measure memory transfer performance.
For regions of system memory that have already been pre-allocated, cudaHostRegister() can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it.
Pinned memory should not be overused. Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance. Furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters.



9.1.2. Asynchronous and Overlapping Transfers with Computationï

Data transfers between the host and the device using cudaMemcpy() are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete. The cudaMemcpyAsync() function is a non-blocking variant of cudaMemcpy() in which control is returned immediately to the host thread. In contrast with cudaMemcpy(), the asynchronous transfer version requires pinned host memory (see Pinned Memory), and it contains an additional argument, a stream ID. A stream is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device.
Asynchronous transfers enable overlap of data transfers with computation in two different ways. On all CUDA-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations. For example, Overlapping computation and data transfers demonstrates how host computation in the routine cpuFunction() is performed while data is transferred to the device and a kernel using the device is executed.
Overlapping computation and data transfers

cudaMemcpyAsync(a_d, a_h, size, cudaMemcpyHostToDevice, 0);
kernel<<<grid, block>>>(a_d);
cpuFunction();


The last argument to the cudaMemcpyAsync() function is the stream ID, which in this case uses the default stream, stream 0. The kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed. Because the memory copy and the kernel both return control to the host immediately, the host function cpuFunction() overlaps their execution.
In Overlapping computation and data transfers, the memory copy and kernel execution occur sequentially. On devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device. Whether a device has this capability is indicated by the asyncEngineCount field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample). On devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream IDs). Non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished.
Concurrent copy and execute illustrates the basic technique.
Concurrent copy and execute

cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);
cudaMemcpyAsync(a_d, a_h, size, cudaMemcpyHostToDevice, stream1);
kernel<<<grid, block, 0, stream2>>>(otherData_d);


In this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of the cudaMemcpyAsync call and the kernelâs execution configuration.
Concurrent copy and execute demonstrates how to overlap kernel execution with asynchronous data transfer. This technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives. Sequential copy and execute and Staged concurrent copy and execute demonstrate this. They produce equivalent results. The first segment shows the reference sequential implementation, which transfers and operates on an array of N floats (where N is assumed to be evenly divisible by nThreads).
Sequential copy and execute

cudaMemcpy(a_d, a_h, N*sizeof(float), dir);
kernel<<<N/nThreads, nThreads>>>(a_d);


Staged concurrent copy and execute shows how the transfer and kernel execution can be broken up into nStreams stages. This approach permits some overlapping of the data transfer and execution.
Staged concurrent copy and execute

size=N*sizeof(float)/nStreams;
for (i=0; i<nStreams; i++) {
    offset = i*N/nStreams;
    cudaMemcpyAsync(a_d+offset, a_h+offset, size, dir, stream[i]);
    kernel<<<N/(nThreads*nStreams), nThreads, 0,
             stream[i]>>>(a_d+offset);
}


(In Staged concurrent copy and execute, it is assumed that N is evenly divisible by nThreads*nStreams.) Because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete. Current GPUs can simultaneously process asynchronous data transfers and execute kernels. GPUs with a single copy engine can perform one asynchronous data transfer and execute kernels whereas GPUs with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels. The number of copy engines on a GPU is given by the asyncEngineCount field of the cudaDeviceProp structure, which is also listed in the output of the deviceQuery CUDA Sample. (It should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous CUDA calls complete. It will not allow any other CUDA call to begin until it has completed.) A diagram depicting the timeline of execution for the two code segments is shown in Figure 1, and nStreams is equal to 4 for Staged concurrent copy and execute in the bottom half of the figure.



Timeline comparison for copy and kernel executionï


Top

Sequential

Bottom

Concurrent





For this example, it is assumed that the data transfer and kernel execution times are comparable. In such cases, and when the execution time (tE) exceeds the transfer time (tT), a rough estimate for the overall time is tE + tT/nStreams for the staged version versus tE + tT for the sequential version. If the transfer time exceeds the execution time, a rough estimate for the overall time is tT + tE/nStreams.



9.1.3. Zero Copyï

Zero copy is a feature that was added in version 2.2 of the CUDA Toolkit. It enables GPU threads to directly access host memory. For this purpose, it requires mapped pinned (non-pageable) memory. On integrated GPUs (i.e., GPUs with the integrated field of the CUDA device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated GPU and CPU memory are physically the same. On discrete GPUs, mapped pinned memory is advantageous only in certain cases. Because the data is not cached on the GPU, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced. Zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams.

Note
Low Priority: Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later.

The host code in Zero-copy host code shows how zero copy is typically set up.
Zero-copy host code

float *a_h, *a_map;
...
cudaGetDeviceProperties(&prop, 0);
if (!prop.canMapHostMemory)
    exit(0);
cudaSetDeviceFlags(cudaDeviceMapHost);
cudaHostAlloc(&a_h, nBytes, cudaHostAllocMapped);
cudaHostGetDevicePointer(&a_map, a_h, 0);
kernel<<<gridSize, blockSize>>>(a_map);


In this code, the canMapHostMemory field of the structure returned by cudaGetDeviceProperties() is used to check that the device supports mapping host memory to the deviceâs address space. Page-locked memory mapping is enabled by calling cudaSetDeviceFlags() with cudaDeviceMapHost. Note that cudaSetDeviceFlags() must be called prior to setting a device or making a CUDA call that requires state (that is, essentially, before a context is created). Page-locked mapped host memory is allocated using cudaHostAlloc(), and the pointer to the mapped device address space is obtained via the function cudaHostGetDevicePointer(). In the code in Zero-copy host code, kernel() can reference the mapped pinned host memory using the pointer a_map in exactly the same was as it would if a_map referred to a location in device memory.

Note
Mapped pinned host memory allows you to overlap CPU-GPU memory transfers with computation while avoiding the use of CUDA streams. But since any repeated access to such memory areas causes repeated CPU-GPU transfers, consider creating a second area in device memory to manually cache the previously read host memory data.




9.1.4. Unified Virtual Addressingï

Devices of compute capability 2.0 and later support a special addressing mode called Unified Virtual Addressing (UVA) on 64-bit Linux and Windows. With UVA, the host memory and the device memories of all installed supported devices share a single virtual address space.
Prior to UVA, an application had to keep track of which pointers referred to device memory (and for which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer. Using UVA, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using cudaPointerGetAttributes().
Under UVA, pinned host memory allocated with cudaHostAlloc() will have identical host and device pointers, so it is not necessary to call cudaHostGetDevicePointer() for such allocations. Host memory allocations pinned after-the-fact via cudaHostRegister(), however, will continue to have different device pointers than their host pointers, so cudaHostGetDevicePointer() remains necessary in that case.
UVA is also a necessary precondition for enabling peer-to-peer (P2P) transfer of data directly across the PCIe bus or NVLink for supported GPUs in supported configurations, bypassing host memory.
See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P.




9.2. Device Memory Spacesï

CUDA devices use several memory spaces, which have different characteristics that reflect their distinct usages in CUDA applications. These memory spaces include global, local, shared, texture, and registers, as shown in Figure 2.



Memory spaces on a CUDA deviceï


Of these different memory spaces, global memory is the most plentiful; see Features and Technical Specifications of the CUDA C++ Programming Guide for the amounts of memory available in each memory space at each compute capability level. Global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file.
The various principal traits of the memory types are shown in Table 1.


Table 1. Salient Features of Device Memoryï











Memory
Location on/off chip
Cached
Access
Scope
Lifetime




Register
On
n/a
R/W
1 thread
Thread


Local
Off
Yesâ â 
R/W
1 thread
Thread


Shared
On
n/a
R/W
All threads in block
Block


Global
Off
â 
R/W
All threads + host
Host allocation


Constant
Off
Yes
R
All threads + host
Host allocation


Texture
Off
Yes
R
All threads + host
Host allocation


â  Cached in L1 and L2 by default on devices of compute capability 6.0 and 7.x; cached only in L2 by default on devices of lower compute capabilities, though some allow opt-in to caching in L1 as well via compilation flags.







â â  Cached in L1 and L2 by default except on devices of compute capability 5.x; devices of compute capability 5.x cache locals only in L2.








In the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array. Texture references that are bound to CUDA arrays can be written to via surface-write operations by binding a surface to the same underlying CUDA array storage). Reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified.


9.2.1. Coalesced Access to Global Memoryï

A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.

Note
High Priority: Ensure global memory accesses are coalesced whenever possible.

The access requirements for coalescing depend on the compute capability of the device and are documented in the CUDA C++ Programming Guide.
For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.
For certain devices of compute capability 5.2, L1-caching of accesses to global memory can be optionally enabled. If L1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments.

Note
On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.

On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.
Coalescing concepts are illustrated in the following simple examples. These examples assume compute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted.


9.2.1.1. A Simple Access Patternï

The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the k-th thread accesses the k-th word in a 32-byte aligned array. Not all threads need to participate.
For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent float values), four coalesced 32-byte transactions will service that memory access. Such a pattern is shown in Figure 3.



Coalesced accessï


This access pattern results in four 32-byte transactions, indicated by the red rectangles.
If from any of the four 32-byte segments only a subset of the words are requested (e.g. if several threads had accessed the same word or if some threads did not participate in the access), the full segment is fetched anyway. Furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would have been performed by a device with compute capability 6.0 or higher.



9.2.1.2. A Sequential but Misaligned Access Patternï

If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments will be requested, as shown in Figure 4.



Misaligned sequential addresses that fall within five 32-byte segmentsï


Memory allocated through the CUDA Runtime API, such as via cudaMalloc(), is guaranteed to be aligned to at least 256 bytes. Therefore, choosing sensible thread block sizes, such as multiples of the warp size (i.e., 32 on current GPUs), facilitates memory accesses by warps that are properly aligned. (Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example.)



9.2.1.3. Effects of Misaligned Accessesï

It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in A copy kernel that illustrates misaligned accesses.
A copy kernel that illustrates misaligned accesses

__global__ void offsetCopy(float *odata, float* idata, int offset)
{
    int xid = blockIdx.x * blockDim.x + threadIdx.x + offset;
    odata[xid] = idata[xid];
}


In A copy kernel that illustrates misaligned accesses, data is copied from the input array idata to the output array, both of which exist in global memory. The kernel is executed within a loop in host code that varies the parameter offset from 0 to 32. (e.g. Figure 4 corresponds to this misalignments) The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 (compute capability 7.0) is shown in Figure 5.



Performance of offsetCopy kernelï


For the NVIDIA Tesla V100, global memory accesses with no offset or with offsets that are multiples of 8 words result in four 32-byte transactions. The achieved bandwidth is approximately 790 GB/s. Otherwise, five 32-byte segments are loaded per warp, and we would expect approximately 4/5th of the memory throughput achieved with no offsets.
In this particular example, the offset memory throughput achieved is, however, approximately 9/10th, because adjacent warps reuse the cache lines their neighbors fetched. So while the impact is still evident it is not as large as we might have expected. It would have been more so if adjacent warps had not exhibited such a high degree of reuse of the over-fetched cache lines.



9.2.1.4. Strided Accessesï

As seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact. It may be different with non-unit-strided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices. For this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices.
To illustrate the effect of strided access on effective bandwidth, see the kernel strideCopy() in A kernel to illustrate non-unit stride data copy, which copies data with a stride of stride elements between threads from idata to odata.
A kernel to illustrate non-unit stride data copy

__global__ void strideCopy(float *odata, float* idata, int stride)
{
    int xid = (blockIdx.x*blockDim.x + threadIdx.x)*stride;
    odata[xid] = idata[xid];
}


Figure 6 illustrates such a situation; in this case, threads within a warp access words in memory with a stride of 2. This action leads to a load of eight L2 cache segments per warp on the Tesla V100 (compute capability 7.0).



Adjacent threads accessing memory with a stride of 2ï


A stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth. As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp, as indicated in Figure 7.



Performance of strideCopy kernelï


As illustrated in Figure 7, non-unit-stride global memory accesses should be avoided whenever possible. One method for doing so utilizes shared memory, which is discussed in the next section.




9.2.2. L2 Cacheï

Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache. Because L2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory.
For more details refer to the L2 Access Management section in the CUDA C++ Programming Guide.


9.2.2.1. L2 Cache Access Windowï

When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting. On the other hand, if the data is only accessed once, such data accesses can be considered to be streaming. A portion of the L2 cache can be set aside for persistent accesses to a data region in global memory. If this set-aside portion is not used by persistent accesses, then streaming or normal data accesses can use it.
The L2 cache set-aside size for persisting accesses may be adjusted, within limits:

cudaGetDeviceProperties(&prop, device_id);
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, prop.persistingL2CacheMaxSize); /* Set aside max possible size of L2 cache for persisting accesses */


Mapping of user data to L2 set-aside portion can be controlled using an access policy window on a CUDA stream or CUDA graph kernel node. The example below shows how to use the access policy window on a CUDA stream.

cudaStreamAttrValue stream_attribute;                                         // Stream level attributes data structure
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(ptr); // Global Memory data pointer
stream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    // Number of bytes for persisting accesses.
                                                                              // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)
stream_attribute.accessPolicyWindow.hitRatio  = 1.0;                          // Hint for L2 cache hit ratio for persisting accesses in the num_bytes region
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; // Type of access property on cache hit
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  // Type of access property on cache miss.

//Set the attributes to a CUDA stream of type cudaStream_t
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);


The access policy window requires a value for hitRatio and num_bytes. Depending on the value of the num_bytes parameter and the size of L2 cache, one may need to tune the value of hitRatio to avoid thrashing of L2 cache lines.



9.2.2.2. Tuning the Access Window Hit-Ratioï

The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property. For example, if the hitRatio value is 0.6, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property. To understand the effect of hitRatio and num_bytes, we use a sliding window micro benchmark.
This microbenchmark uses a 1024 MB region in GPU global memory. First, we set aside 30 MB of the L2 cache for persisting accesses using cudaDeviceSetLimit(), as discussed above. Then, as shown in the figure below, we specify that the accesses to the first freqSize * sizeof(int) bytes of the memory region are persistent. This data will thus use the L2 set-aside portion. In our experiment, we vary the size of this persistent data region from 10 MB to 60 MB to model various scenarios where data fits in or exceeds the available L2 set-aside portion of 30 MB. Note that the NVIDIA Tesla A100 GPU has 40 MB of total L2 cache capacity. Accesses to the remaining data of the memory region (i.e., streaming data) are considered normal or streaming accesses and will thus use the remaining 10 MB of the non set-aside L2 portion (unless part of the L2 set-aside portion is unused).



Mapping Persistent data accesses to set-aside L2 in sliding window experimentï


Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment.

__global__ void kernel(int *data_persistent, int *data_streaming, int dataSize, int freqSize) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;

    /*Each CUDA thread accesses one element in the persistent data section
      and one element in the streaming data section.
      Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much
      smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data
      in the persistent region is accessed more frequently*/

    data_persistent[tid % freqSize] = 2 * data_persistent[tid % freqSize];
    data_streaming[tid % dataSize] = 2 * data_streaming[tid % dataSize];
}

stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(data_persistent);
stream_attribute.accessPolicyWindow.num_bytes = freqSize * sizeof(int);   //Number of bytes for persisting accesses in range 10-60 MB
stream_attribute.accessPolicyWindow.hitRatio  = 1.0;                      //Hint for cache hit ratio. Fixed value 1.0


The performance of the above kernel is shown in the chart below. When the persistent data region fits well into the 30 MB set-aside portion of the L2 cache, a performance increase of as much as 50% is observed. However, once the size of this persistent data region exceeds the size of the L2 set-aside cache portion, approximately 10% performance drop is observed due to thrashing of L2 cache lines.



The performance of the sliding-window benchmark with fixed hit-ratio of 1.0ï


In order to optimize the performance, when the size of the persistent data is more than the size of the set-aside L2 cache portion, we tune the num_bytes and hitRatio parameters in the access window as below.

stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(data_persistent);
stream_attribute.accessPolicyWindow.num_bytes = 20*1024*1024;                                  //20 MB
stream_attribute.accessPolicyWindow.hitRatio  = (20*1024*1024)/((float)freqSize*sizeof(int));  //Such that up to 20MB of data is resident.


We fix the num_bytes in the access window to 20 MB and tune the hitRatio such that a random 20 MB of the total persistent data is resident in the L2 set-aside cache portion. The remaining portion of this persistent data will be accessed using the streaming property. This helps in reducing cache thrashing. The results are shown in the chart below, where we see good performance regardless of whether the persistent data fits in the L2 set-aside or not.



The performance of the sliding-window benchmark with tuned hit-ratioï






9.2.3. Shared Memoryï

Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section.


9.2.3.1. Shared Memory and Memory Banksï

To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules (banks) that can be accessed simultaneously. Therefore, any memory load or store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single bank.
However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized. The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests. The one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads.
To minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests.
On devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle, and successive 32-bit words are assigned to successive banks. The warp size is 32 threads and the number of banks is also 32, so bank conflicts can occur between any threads in the warp. See Compute Capability 5.x in the CUDA C++ Programming Guide for further details.



9.2.3.2. Shared Memory in Matrix Multiplication (C=AB)ï

Shared memory enables cooperation between threads in a block. When multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once. Shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and then reordering it in shared memory. Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a warp in shared memory.
The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN. To keep the kernels simple, M and N are multiples of 32, since the warp size (w) is 32 for current devices.
A natural decomposition of the problem is to use a block and tile size of wxw threads. Therefore, in terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11. A grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different tile in C from a single tile of A and a single tile of B.



Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C).ï


To do this, the simpleMultiply kernel (Unoptimized matrix multiplication) calculates the output elements of a tile of matrix C.
Unoptimized matrix multiplication

__global__ void simpleMultiply(float *a, float* b, float *c,
                               int N)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;
    for (int i = 0; i < TILE_DIM; i++) {
        sum += a[row*TILE_DIM+i] * b[i*N+col];
    }
    c[row*N+col] = sum;
}


In Unoptimized matrix multiplication, a, b, and c are pointers to global memory for the matrices A, B, and C, respectively; blockDim.x, blockDim.y, and TILE_DIM are all equal to w. Each thread in the wxw-thread block calculates one element in a tile of C. row and col are the row and column of the element in C being calculated by a particular thread. The for loop over i multiplies a row of A by a column of B, which is then written to C.
The effective bandwidth of this kernel is 119.9 GB/s on an NVIDIA Tesla V100. To analyze performance, it is necessary to consider how warps access global memory in the for loop. Each warp of threads calculates one row of a tile of C, which depends on a single row of A and an entire tile of B as illustrated in Figure 12.



Computing a row of a tile. Computing a row of a tile in C using one row of A and an entire tile of B.ï


For each iteration i of the for loop, the threads in a warp read a row of the B tile, which is a sequential and coalesced access for all compute capabilities.
However, for each iteration i, all threads in a warp read the same value from global memory for matrix A, as the index row*TILE_DIM+i is constant within a warp. Even though such an access requires only 1 transaction on devices of compute capability 2.0 or higher, there is wasted bandwidth in the transaction, because only one 4-byte word out of 8 words in a 32-byte cache segment is used. We can reuse this cache line in subsequent iterations of the loop, and we would eventually utilize all 8 words; however, when many warps execute on the same multiprocessor simultaneously, as is generally the case, the cache line may easily be evicted from the cache between iterations i and i+1.
The performance on a device of any compute capability can be improved by reading a tile of A into shared memory as shown in Using shared memory to improve the global memory load efficiency in matrix multiplication.
Using shared memory to improve the global memory load efficiency in matrix multiplication

__global__ void coalescedMultiply(float *a, float* b, float *c,
                                  int N)
{
    __shared__ float aTile[TILE_DIM][TILE_DIM];

    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;
    aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];
    __syncwarp();
    for (int i = 0; i < TILE_DIM; i++) {
        sum += aTile[threadIdx.y][i]* b[i*N+col];
    }
    c[row*N+col] = sum;
}


In Using shared memory to improve the global memory load efficiency in matrix multiplication, each element in a tile of A is read from global memory only once, in a fully coalesced fashion (with no wasted bandwidth), to shared memory. Within each iteration of the for loop, a value in shared memory is broadcast to all threads in a warp. Instead of a __syncthreads()synchronization barrier call, a __syncwarp() is sufficient after reading the tile of A into shared memory because only threads within the warp that write the data into shared memory read this data. This kernel has an effective bandwidth of 144.4 GB/s on an NVIDIA Tesla V100. This illustrates the use of the shared memory as a user-managed cache when the hardware L1 cache eviction policy does not match up well with the needs of the application or when L1 cache is not used for reads from global memory.
A further improvement can be made to how Using shared memory to improve the global memory load efficiency in matrix multiplication deals with matrix B. In calculating each of the rows of a tile of matrix C, the entire tile of B is read. The repeated reading of the B tile can be eliminated by reading it into shared memory once (Improvement by reading additional data into shared memory).
Improvement by reading additional data into shared memory

__global__ void sharedABMultiply(float *a, float* b, float *c,
                                 int N)
{
    __shared__ float aTile[TILE_DIM][TILE_DIM],
                     bTile[TILE_DIM][TILE_DIM];
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;
    aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];
    bTile[threadIdx.y][threadIdx.x] = b[threadIdx.y*N+col];
    __syncthreads();
    for (int i = 0; i < TILE_DIM; i++) {
        sum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];
    }
    c[row*N+col] = sum;
}


Note that in Improvement by reading additional data into shared memory, a __syncthreads() call is required after reading the B tile because a warp reads data from shared memory that were written to shared memory by different warps. The effective bandwidth of this routine is 195.5 GB/s on an NVIDIA Tesla V100. Note that the performance improvement is not due to improved coalescing in either case, but to avoiding redundant transfers from global memory.
The results of the various optimizations are summarized in Table 2.


Table 2. Performance Improvements Optimizing C = AB Matrix Multiply
:class table-no-stripesï







Optimization
NVIDIA Tesla V100




No optimization
119.9 GB/s


Coalesced using shared memory to store a tile of A
144.4 GB/s


Using shared memory to eliminate redundant reads of a tile of B
195.5 GB/s




Note
Medium Priority: Use shared memory to avoid redundant transfers from global memory.




9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT)ï

A variant of the previous matrix multiplication can be used to illustrate how strided accesses to global memory, as well as shared memory bank conflicts, are handled. This variant simply uses the transpose of A in place of B, so C = AAT.
A simple implementation for C = AAT is shown in Unoptimized handling of strided accesses to global memory
Unoptimized handling of strided accesses to global memory

__global__ void simpleMultiply(float *a, float *c, int M)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;
    for (int i = 0; i < TILE_DIM; i++) {
        sum += a[row*TILE_DIM+i] * a[col*TILE_DIM+i];
    }
    c[row*M+col] = sum;
}


In Unoptimized handling of strided accesses to global memory, the row-th, col-th element of C is obtained by taking the dot product of the row-th and col-th rows of A. The effective bandwidth for this kernel is 12.8 GB/s on an NVIDIA Tesla V100. These results are substantially lower than the corresponding measurements for the C = AB kernel. The difference is in how threads in a half warp access elements of A in the second term, a[col*TILE_DIM+i], for each iteration i. For a warp of threads, col represents sequential columns of the transpose of A, and therefore col*TILE_DIM represents a strided access of global memory with a stride of w, resulting in plenty of wasted bandwidth.
The way to avoid strided access is to use shared memory as before, except in this case a warp reads a row of A into a column of a shared memory tile, as shown in An optimized handling of strided accesses using coalesced reads from global memory.
An optimized handling of strided accesses using coalesced reads from global memory

__global__ void coalescedMultiply(float *a, float *c, int M)
{
    __shared__ float aTile[TILE_DIM][TILE_DIM],
                     transposedTile[TILE_DIM][TILE_DIM];
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;
    aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];
    transposedTile[threadIdx.x][threadIdx.y] =
        a[(blockIdx.x*blockDim.x + threadIdx.y)*TILE_DIM +
        threadIdx.x];
    __syncthreads();
    for (int i = 0; i < TILE_DIM; i++) {
        sum += aTile[threadIdx.y][i]* transposedTile[i][threadIdx.x];
    }
    c[row*M+col] = sum;
}


An optimized handling of strided accesses using coalesced reads from global memory uses the shared transposedTile to avoid uncoalesced accesses in the second term in the dot product and the shared aTile technique from the previous example to avoid uncoalesced accesses in the first term. The effective bandwidth of this kernel is 140.2 GB/s on an NVIDIA Tesla V100.These results are lower than those obtained by the final kernel for C = AB. The cause of the difference is shared memory bank conflicts.
The reads of elements in transposedTile within the for loop are free of conflicts, because threads of each half warp read across rows of the tile, resulting in unit stride across the banks. However, bank conflicts occur when copying the tile from global memory into shared memory. To enable the loads from global memory to be coalesced, data are read from global memory sequentially. However, this requires writing to shared memory in columns, and because of the use of wxw tiles in shared memory, this results in a stride between threads of w banks - every thread of the warp hits the same bank (Recall that w is selected as 32). These many-way bank conflicts are very expensive. The simple remedy is to pad the shared memory array so that it has an extra column, as in the following line of code.

__shared__ float transposedTile[TILE_DIM][TILE_DIM+1];


This padding eliminates the conflicts entirely, because now the stride between threads is w+1 banks (i.e., 33 for current devices), which, due to modulo arithmetic used to compute bank indices, is equivalent to a unit stride. After this change, the effective bandwidth is 199.4 GB/s on an NVIDIA Tesla V100, which is comparable to the results from the last C = AB kernel.
The results of these optimizations are summarized in Table 3.


Table 3. Performance Improvements Optimizing C = AAT Matrix Multiplicationï







Optimization
NVIDIA Tesla V100




No optimization
12.8 GB/s


Using shared memory to coalesce global reads
140.2 GB/s


Removing bank conflicts
199.4 GB/s



These results should be compared with those in Table 2. As can be seen from these tables, judicious use of shared memory can dramatically improve performance.
The examples in this section have illustrated three reasons to use shared memory:

To enable coalesced accesses to global memory, especially to avoid large strides (for general matrices, strides are much larger than 32)
To eliminate (or reduce) redundant loads from global memory
To avoid wasted bandwidth




9.2.3.4. Asynchronous Copy from Global Memory to Shared Memoryï

CUDA 11.0 introduces an async-copy feature that can be used within device code to explicitly manage the asynchronous copying of data from global memory to shared memory. This feature enables CUDA kernels to overlap copying data from global to shared memory with computation. It also avoids an intermediary register file access traditionally present between the global memory read and the shared memory write.
For more details refer to the memcpy_async section in the CUDA C++ Programming Guide.
To understand the performance difference between synchronous copy and asynchronous copy of data from global memory to shared memory, consider the following micro benchmark CUDA kernels for demonstrating the synchronous and asynchronous approaches. Asynchronous copies are hardware accelerated for NVIDIA A100 GPU.

template <typename T>
__global__ void pipeline_kernel_sync(T *global, uint64_t *clock, size_t copy_count) {
  extern __shared__ char s[];
  T *shared = reinterpret_cast<T *>(s);

  uint64_t clock_start = clock64();

  for (size_t i = 0; i < copy_count; ++i) {
    shared[blockDim.x * i + threadIdx.x] = global[blockDim.x * i + threadIdx.x];
  }

  uint64_t clock_end = clock64();

  atomicAdd(reinterpret_cast<unsigned long long *>(clock),
            clock_end - clock_start);
}

template <typename T>
__global__ void pipeline_kernel_async(T *global, uint64_t *clock, size_t copy_count) {
  extern __shared__ char s[];
  T *shared = reinterpret_cast<T *>(s);

  uint64_t clock_start = clock64();

  //pipeline pipe;
  for (size_t i = 0; i < copy_count; ++i) {
    __pipeline_memcpy_async(&shared[blockDim.x * i + threadIdx.x],
                            &global[blockDim.x * i + threadIdx.x], sizeof(T));
  }
  __pipeline_commit();
  __pipeline_wait_prior(0);

  uint64_t clock_end = clock64();

  atomicAdd(reinterpret_cast<unsigned long long *>(clock),
            clock_end - clock_start);
}


The synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory. In the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as __pipeline_memcpy_async() function is called. The __pipeline_wait_prior(0) will wait until all the instructions in the pipe object have been executed. Using asynchronous copies does not use any intermediate register. Not using intermediate registers can help reduce register pressure and can increase kernel occupancy. Data copied from global memory to shared memory using asynchronous copy instructions can be cached in the L1 cache or the L1 cache can be optionally bypassed. If individual CUDA threads are copying elements of 16 bytes, the L1 cache can be bypassed. This difference is illustrated in Figure 13.



Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memoryï


We evaluate the performance of both kernels using elements of size 4B, 8B and 16B per thread i.e., using int, int2 and int4 for the template parameter. We adjust the copy_count in the kernels such that each thread block copies from 512 bytes up to 48 MB. The performance of the kernels is shown in Figure 14.



Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memoryï


From the performance chart, the following observations can be made for this experiment.

Best performance with synchronous copy is achieved when the copy_count parameter is a multiple of 4 for all three element sizes. The compiler can optimize groups of 4 load and store instructions. This is evident from the saw tooth curves.
Asynchronous copy achieves better performance in nearly all cases.
The async-copy does not require the copy_count parameter to be a multiple of 4, to maximize performance through compiler optimizations.
Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes.





9.2.4. Local Memoryï

Local memory is so named because its scope is local to the thread, not because of its physical location. In fact, local memory is off-chip. Hence, access to local memory is as expensive as access to global memory. In other words, the term local in the name does not imply faster access.
Local memory is used only to hold automatic variables. This is done by the nvcc compiler when it determines that there is insufficient register space to hold the variable. Automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically.
Inspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line options to nvcc) reveals whether a variable has been placed in local memory during the first compilation phases. If it has, it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics. If it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture. There is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the--ptxas-options=-v option.



9.2.5. Texture Memoryï

The read-only texture memory space is cached. Therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache. The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance. Texture memory is also designed for streaming fetches with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency.
In certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory.


9.2.5.1. Additional Texture Capabilitiesï

If textures are fetched using tex1D(),tex2D(), or tex3D() rather than tex1Dfetch(), the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in Table 4.


Table 4. Useful Features for tex1D(), tex2D(), and tex3D() Fetchesï








Feature
Use
Caveat




Filtering
Fast, low-precision interpolation between texels
Valid only if the texture reference returns floating-point data


Normalized texture coordinates
Resolution-independent coding
None


Addressing modes
Automatic handling of boundary cases1
Can be used only with normalized texture coordinates


1 The automatic handling of boundary cases in the bottom row of Table 4 refers to how a texture coordinate is resolved when it falls outside the valid addressing range. There are two options: clamp and wrap. If x is the coordinate and N is the number of texels for a one-dimensional texture, then with clamp, x is replaced by 0 if x < 0 and by 1-1/N if 1 <x. With wrap, x is replaced by frac(x) where frac(x) = x - floor(x). Floor returns the largest integer less than or equal to x. So, in clamp mode where N = 1, an x of 1.3 is clamped to 1.0; whereas in wrap mode, it is converted to 0.3





Within a kernel call, the texture cache is not kept coherent with respect to global memory writes, so texture fetches from addresses that have been written via global stores in the same kernel call return undefined data. That is, a thread can safely read a memory location via texture if the location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread within the same kernel call.




9.2.6. Constant Memoryï

There is a total of 64 KB constant memory on a device. The constant memory space is cached. As a result, a read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp accesses only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access.



9.2.7. Registersï

Generally, accessing a register consumes zero extra clock cycles per instruction, but delays may occur due to register read-after-write dependencies and register memory bank conflicts.
The compiler and hardware thread scheduler will schedule instructions as optimally as possible to avoid register memory bank conflicts. An application has no direct control over these bank conflicts. In particular, there is no register-related reason to pack data into vector data types such as float4 or int4 types.


9.2.7.1. Register Pressureï

Register pressure occurs when there are not enough registers available for a given task. Even though each multiprocessor contains thousands of 32-bit registers (see Features and Technical Specifications of the CUDA C++ Programming Guide), these are partitioned among concurrent threads. To prevent the compiler from allocating too many registers, use the -maxrregcount=N compiler command-line option (see nvcc) or the launch bounds kernel definition qualifier (see Execution Configuration of the CUDA C++ Programming Guide) to control the maximum number of registers to allocated per thread.





9.3. Allocationï

Device memory allocation and de-allocation via cudaMalloc() and cudaFree() are expensive operations. It is recommended to use cudaMallocAsync() and cudaFreeAsync() which are stream ordered pool allocators to manage device memory.



9.4. NUMA Best Practicesï

Some recent Linux distributions enable automatic NUMA balancing (or âAutoNUMAâ) by default. In some instances, operations performed by automatic NUMA balancing may degrade the performance of applications running on NVIDIA GPUs. For optimal performance, users should manually tune the NUMA characteristics of their application.
The optimal NUMA tuning will depend on the characteristics and desired hardware affinities of each application and node, but in general applications computing on NVIDIA GPUs are advised to choose a policy that disables automatic NUMA balancing. For example, on IBM Newell POWER9 nodes (where the CPUs correspond to NUMA nodes 0 and 8), use:

numactl --membind=0,8


to bind memory allocations to the CPUs.




10. Execution Configuration Optimizationsï

One of the keys to good performance is to keep the multiprocessors on the device as busy as possible. A device in which work is poorly balanced across the multiprocessors will deliver suboptimal performance. Hence, itâs important to design your application to use threads and blocks in a way that maximizes hardware utilization and to limit practices that impede the free distribution of work. A key concept in this effort is occupancy, which is explained in the following sections.
Hardware utilization can also be improved in some cases by designing your application so that multiple, independent kernels can execute at the same time. Multiple kernels executing at the same time is known as concurrent kernel execution. Concurrent kernel execution is described below.
Another important concept is the management of system resources allocated for a particular task. How to manage this resource utilization is discussed in the final sections of this chapter.


10.1. Occupancyï

Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. This metric is occupancy.
Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. (To determine the latter number, see the deviceQuery CUDA Sample or refer to Compute Capabilities in the CUDA C++ Programming Guide.) Another way to view occupancy is the percentage of the hardwareâs ability to process warps that is actively in use.
Higher occupancy does not always equate to higher performance-there is a point above which additional occupancy does not improve performance. However, low occupancy always interferes with the ability to hide memory latency, resulting in performance degradation.
Per thread resources required by a CUDA kernel might limit the maximum block size in an unwanted way. In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument __launch_bounds__(maxThreadsPerBlock) which specifies the largest block size that the kernel will be launched with. Failure to do so could lead to âtoo many resources requested for launchâ errors. Providing the two argument version of __launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor) can improve performance in some cases. The right value for minBlocksPerMultiprocessor should be determined using a detailed per kernel analysis.


10.1.1. Calculating Occupancyï

One of several factors that determine occupancy is register availability. Register storage enables threads to keep local variables nearby for low-latency access. However, the set of registers (known as the register file) is a limited commodity that all threads resident on a multiprocessor must share. Registers are allocated to an entire block all at once. So, if each thread block uses many registers, the number of thread blocks that can be resident on a multiprocessor is reduced, thereby lowering the occupancy of the multiprocessor. The maximum number of registers per thread can be set manually at compilation time per-file using the -maxrregcount option or per-kernel using the __launch_bounds__ qualifier (see Register Pressure).
For purposes of calculating occupancy, the number of registers used by each thread is one of the key factors. For example, on devices of compute capability 7.0 each multiprocessor has 65,536 32-bit registers and can have a maximum of 2048 simultaneous threads resident (64 warps x 32 threads per warp). This means that in one of these devices, for a multiprocessor to have 100% occupancy, each thread can use at most 32 registers. However, this approach of determining how register count affects occupancy does not take into account the register allocation granularity. For example, on a device of compute capability 7.0, a kernel with 128-thread blocks using 37 registers per thread results in an occupancy of 75% with 12 active 128-thread blocks per multi-processor, whereas a kernel with 320-thread blocks using the same 37 registers per thread results in an occupancy of 63% because only four 320-thread blocks can reside on a multiprocessor. Furthermore, register allocations are rounded up to the nearest 256 registers per warp.
The number of registers available, the maximum number of simultaneous threads resident on each multiprocessor, and the register allocation granularity vary over different compute capabilities. Because of these nuances in register allocation and the fact that a multiprocessorâs shared memory is also partitioned between resident thread blocks, the exact relationship between register usage and occupancy can be difficult to determine. The --ptxasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  options=v option of nvcc details the number of registers used per thread for each kernel. See Hardware Multithreading of the CUDA C++ Programming Guide for the register allocation formulas for devices of various compute capabilities and Features and Technical Specifications of the CUDA C++ Programming Guide for the total number of registers available on those devices. Alternatively, NVIDIA provides an occupancy calculator in the form of an Excel spreadsheet that enables developers to hone in on the optimal balance and to test different possible scenarios more easily. This spreadsheet, shown in Figure 15, is called CUDA_Occupancy_Calculator.xls and is located in the tools subdirectory of the CUDA Toolkit installation.



Using the CUDA Occupancy Calculator to project GPU multiprocessor occupancyï


In addition to the calculator spreadsheet, occupancy can be determined using the NVIDIA Nsight Compute Profiler. Details about occupancy are displayed in the Occupancy section.
An application can also use the Occupancy API from the CUDA Runtime, e.g. cudaOccupancyMaxActiveBlocksPerMultiprocessor, to dynamically select launch configurations based on runtime parameters.




10.2. Hiding Register Dependenciesï


Note
Medium Priority: To hide latency arising from register dependencies, maintain sufficient numbers of active threads per multiprocessor (i.e., sufficient occupancy).

Register dependencies arise when an instruction uses a result stored in a register written by an instruction before it. The latency of most arithmetic instructions is typically 4 cycles on devices of compute capability 7.0. So threads must wait approximatly 4 cycles before using an arithmetic result. However, this latency can be completely hidden by the execution of threads in other warps. See Registers for details.



10.3. Thread and Block Heuristicsï


Note
Medium Priority: The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing.

The dimension and size of blocks per grid and the dimension and size of threads per block are both important factors. The multidimensional aspect of these parameters allows easier mapping of multidimensional problems to CUDA and does not play a role in performance. As a result, this section discusses size but not dimension.
Latency hiding and occupancy depend on the number of active warps per multiprocessor, which is implicitly determined by the execution parameters along with resource (register and shared memory) constraints. Choosing execution parameters is a matter of striking a balance between latency hiding (occupancy) and resource utilization.
Choosing the execution configuration parameters should be done in tandem; however, there are certain heuristics that apply to each parameter individually. When choosing the first execution configuration parameter-the number of blocks per grid, or grid size - the primary concern is keeping the entire GPU busy. The number of blocks in a grid should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. Furthermore, there should be multiple active blocks per multiprocessor so that blocks that arenât waiting for a __syncthreads() can keep the hardware busy. This recommendation is subject to resource availability; therefore, it should be determined in the context of the second execution parameter - the number of threads per block, or block size - as well as shared memory usage. To scale to future devices, the number of blocks per kernel launch should be in the thousands.
When choosing the block size, it is important to remember that multiple concurrent blocks can reside on a multiprocessor, so occupancy is not determined by block size alone. In particular, a larger block size does not imply a higher occupancy.
As mentioned in Occupancy, higher occupancy does not always equate to better performance. For example, improving occupancy from 66 percent to 100 percent generally does not translate to a similar increase in performance. A lower occupancy kernel will have more registers available per thread than a higher occupancy kernel, which may result in less register spilling to local memory; in particular, with a high degree of exposed instruction-level parallelism (ILP) it is, in some cases, possible to fully cover latency with a low occupancy.
There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed:

Threads per block should be a multiple of warp size to avoid wasting computation on under-populated warps and to facilitate coalescing.
A minimum of 64 threads per block should be used, and only if there are multiple concurrent blocks per multiprocessor.
Between 128 and 256 threads per block is a good initial range for experimentation with different block sizes.
Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call __syncthreads().

Note that when a thread block allocates more registers than are available on a multiprocessor, the kernel launch fails, as it will when too much shared memory or too many threads are requested.



10.4. Effects of Shared Memoryï

Shared memory can be helpful in several situations, such as helping to coalesce or eliminate redundant access to global memory. However, it also can act as a constraint on occupancy. In many cases, the amount of shared memory required by a kernel is related to the block size that was chosen, but the mapping of threads to shared memory elements does not need to be one-to-one. For example, it may be desirable to use a 64x64 element shared memory array in a kernel, but because the maximum number of threads per block is 1024, it is not possible to launch a kernel with 64x64 threads per block. In such cases, kernels with 32x32 or 64x16 threads can be launched with each thread processing four elements of the shared memory array. The approach of using a single thread to process multiple elements of a shared memory array can be beneficial even if limits such as threads per block are not an issue. This is because some operations common to each element can be performed by the thread once, amortizing the cost over the number of shared memory elements processed by the thread.
A useful technique to determine the sensitivity of performance to occupancy is through experimentation with the amount of dynamically allocated shared memory, as specified in the third parameter of the execution configuration. By simply increasing this parameter (without modifying the kernel), it is possible to effectively reduce the occupancy of the kernel and measure its effect on performance.



10.5. Concurrent Kernel Executionï

As described in Asynchronous and Overlapping Transfers with Computation, CUDA streams can be used to overlap kernel execution with data transfers. On devices that are capable of concurrent kernel execution, streams can also be used to execute multiple kernels simultaneously to more fully take advantage of the deviceâs multiprocessors. Whether a device has this capability is indicated by the concurrentKernels field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample). Non-default streams (streams other than stream 0) are required for concurrent execution because kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished.
The following example illustrates the basic technique. Because kernel1 and kernel2 are executed in different, non-default streams, a capable device can execute the kernels at the same time.

cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);
kernel1<<<grid, block, 0, stream1>>>(data_1);
kernel2<<<grid, block, 0, stream2>>>(data_2);





10.6. Multiple contextsï

CUDA work occurs within a process space for a particular GPU known as a context. The context encapsulates kernel launches and memory allocations for that GPU as well as supporting constructs such as the page tables. The context is explicit in the CUDA Driver API but is entirely implicit in the CUDA Runtime API, which creates and manages contexts automatically.
With the CUDA Driver API, a CUDA application process can potentially create more than one context for a given GPU. If multiple CUDA application processes access the same GPU concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless Multi-Process Service is in use.
While multiple contexts (and their associated resources such as global memory allocations) can be allocated concurrently on a given GPU, only one of these contexts can execute work at any given moment on that GPU; contexts sharing the same GPU are time-sliced. Creating additional contexts incurs memory overhead for per-context data and time overhead for context switching. Furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently (see also Concurrent Kernel Execution).
Therefore, it is best to avoid multiple contexts per GPU within the same CUDA application. To assist with this, the CUDA Driver API provides methods to access and manage a special context on each GPU called the primary context. These are the same contexts used implicitly by the CUDA Runtime when there is not already a current context for a thread.

// When initializing the program/library
CUcontext ctx;
cuDevicePrimaryCtxRetain(&ctx, dev);

// When the program/library launches work
cuCtxPushCurrent(ctx);
kernel<<<...>>>(...);
cuCtxPopCurrent(&ctx);

// When the program/library is finished with the context
cuDevicePrimaryCtxRelease(dev);



Note
NVIDIA-SMI can be used to configure a GPU for exclusive process mode, which limits the number of contexts per GPU to one. This context can be current to as many threads as desired within the creating process, and cuDevicePrimaryCtxRetain will fail if a non-primary context that was created with the CUDA driver API already exists on the device.





11. Instruction Optimizationï

Awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program). Best practices suggest that this optimization be performed after all higher-level optimizations have been completed.


11.1. Arithmetic Instructionsï

Single-precision floats provide the best performance, and their use is highly encouraged. The throughput of individual arithmetic operations is detailed in the CUDA C++ Programming Guide.


11.1.1. Division Modulo Operationsï


Note
Low Priority: Use shift operations to avoid expensive division and modulo calculations.

Integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: If \(n\) is a power of 2, ( \(i/n\) ) is equivalent to ( \(i \gg {log2}(n)\) ) and ( \(i\% n\) ) is equivalent to ( \(i\&\left( {n - 1} \right)\) ).
The compiler will perform these conversions if n is literal. (For further information, refer to Performance Guidelines in the CUDA C++ Programming Guide).



11.1.2. Loop Counters Signed vs. Unsignedï


Note
Low Medium Priority: Use signed integers rather than unsigned integers as loop counters.

In the C language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results. Therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic. This is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned. For slightly better performance, however, they should instead be declared as signed.
For example, consider the following code:

for (i = 0; i < n; i++) {
    out[i] = in[offset + stride*i];
}


Here, the sub-expression stride*i could overflow a 32-bit integer, so if i is declared as unsigned, the overflow semantics prevent the compiler from using some optimizations that might otherwise have applied, such as strength reduction. If instead i is declared as signed, where the overflow semantics are undefined, the compiler has more leeway to use these optimizations.



11.1.3. Reciprocal Square Rootï

The reciprocal square root should always be invoked explicitly as rsqrtf() for single precision and rsqrt() for double precision. The compiler optimizes 1.0f/sqrtf(x) into rsqrtf() only when this does not violate IEEE-754 semantics.



11.1.4. Other Arithmetic Instructionsï


Note
Low Priority: Avoid automatic conversion of doubles to floats.

The compiler must on occasion insert conversion instructions, introducing additional execution cycles. This is the case for:

Functions operating on char or short whose operands generally need to be converted to an int
Double-precision floating-point constants (defined without any type suffix) used as input to single-precision floating-point computations

The latter case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3.141592653589793f, 1.0f, 0.5f.
For single-precision code, use of the float type and the single-precision math functions are highly recommended.
It should also be noted that the CUDA math libraryâs complementary error function, erfcf(), is particularly fast with full single-precision accuracy.



11.1.5. Exponentiation With Small Fractional Argumentsï

For some fractional exponents, exponentiation can be accelerated significantly compared to the use of pow() by using square roots, cube roots, and their inverses. For those exponentiations where the exponent is not exactly representable as a floating-point number, such as 1/3, this can also provide much more accurate results, as use of pow() magnifies the initial representational error.
The formulas in the table below are valid for x >= 0, x != -0, that is, signbit(x) == 0.


Table 5. Formulae for exponentiation by small fractionsï







Computation
Formula




x1/9
r = rcbrt(rcbrt(x))


x-1/9
r = cbrt(rcbrt(x))


x1/6
r = rcbrt(rsqrt(x))


x-1/6
r = rcbrt(sqrt(x))


x1/4
r = rsqrt(rsqrt(x))


x-1/4
r = sqrt(rsqrt(x))


x1/3
r = cbrt(x)


x-1/3
r = rcbrt(x)


x1/2
r = sqrt(x)


x-1/2
r = rsqrt(x)


x2/3
r = cbrt(x); r = r*r


x-2/3
r = rcbrt(x); r = r*r


x3/4
r = sqrt(x); r = r*sqrt(r)


x-3/4
r = rsqrt(x); r = r*sqrt(r)


x7/6
r = x*rcbrt(rsqrt(x))


x-7/6
r = (1/x) * rcbrt(sqrt(x))


x5/4
r = x*rsqrt(rsqrt(x))


x-5/4
r = (1/x)*sqrt(rsqrt(x))


x4/3
r = x*cbrt(x)


x-4/3
r = (1/x)*rcbrt(x)


x3/2
r = x*sqrt(x)


x-3/2
r = (1/x)*rsqrt(x)






11.1.6. Math Librariesï


Note
Medium Priority: Use the fast math library whenever speed trumps precision.

Two types of runtime math operations are supported. They can be distinguished by their names: some have names with prepended underscores, whereas others do not (e.g., __functionName() versus functionName()). Functions following the __functionName() naming convention map directly to the hardware level. They are faster but provide somewhat lower accuracy (e.g., __sinf(x) and __expf(x)). Functions following functionName() naming convention are slower but have higher accuracy (e.g., sinf(x) and expf(x)). The throughput of __sinf(x), __cosf(x), and__expf(x) is much greater than that of sinf(x), cosf(x), and expf(x). The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument x needs to be reduced. Moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory. More details are available in the CUDA C++ Programming Guide.
Note also that whenever sine and cosine of the same argument are computed, the sincos family of instructions should be used to optimize performance:

__sincosf() for single-precision fast math (see next paragraph)
sincosf() for regular single-precision
sincos() for double precision

The -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. It also disables single-precision denormal support and lowers the precision of single-precision division in general. This is an aggressive optimization that can both reduce numerical accuracy and alter special case handling. A more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated. Note this switch is effective only on single-precision floating point.

Note
Medium Priority: Prefer faster, more specialized math functions over slower, more general ones when possible.

For small integer powers (e.g., x2 or x3), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow(). While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage. This advantage is increased when several powers of the same base are needed (e.g., where both x2 and x5 are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization.
For exponentiation using base 2 or 10, use the functions exp2() or expf2() and exp10() or expf10() rather than the functions pow() or powf(). Both pow() and powf() are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent. The functions exp2(), exp2f(), exp10(), and exp10f(), on the other hand, are similar to exp() and expf() in terms of performance, and can be as much as ten times faster than their pow()/powf() equivalents.
For exponentiation with an exponent of 1/3, use the cbrt() or cbrtf() function rather than the generic exponentiation functions pow() or powf(), as the former are significantly faster than the latter. Likewise, for exponentation with an exponent of -1/3, use rcbrt() or rcbrtf().
Replace sin(Ï*<expr>) with sinpi(<expr>), cos(Ï*<expr>) with cospi(<expr>), and sincos(Ï*<expr>) with sincospi(<expr>). This is advantageous with regard to both accuracy and performance. As a particular example, to evaluate the sine function in degrees instead of radians, use sinpi(x/180.0). Similarly, the single-precision functions sinpif(), cospif(), and sincospif() should replace calls to sinf(), cosf(), and sincosf() when the function argument is of the form Ï*<expr>. (The performance advantage sinpi() has over sin() is due to simplified argument reduction; the accuracy advantage is because sinpi() multiplies by Ï only implicitly, effectively using an infinitely precise mathematical Ï rather than a single- or double-precision approximation thereof.)



11.1.7. Precision-related Compiler Flagsï

By default, the nvcc compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster:

-ftz=true (denormalized numbers are flushed to zero)
-prec-div=false (less precise division)
-prec-sqrt=false (less precise square root)

Another, more aggressive, option is -use_fast_math, which coerces every functionName() call to the equivalent __functionName() call. This makes the code run faster at the cost of diminished precision and accuracy. See Math Libraries.




11.2. Memory Instructionsï


Note
High Priority: Minimize the use of global memory. Prefer shared memory access where possible.

Memory instructions include any instruction that reads from or writes to shared, local, or global memory. When accessing uncached local or global memory, there are hundreds of clock cycles of memory latency.
As an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory:

__shared__ float shared[32];
__device__ float device[32];
shared[threadIdx.x] = device[threadIdx.x];


Much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete. However, it is best to avoid accessing global memory whenever possible.




12. Control Flowï



12.1. Branching and Divergenceï


Note
High Priority: Avoid different execution paths within the same warp.

Flow control instructions (if, switch, do, for, while) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp.
To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps.
This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture of the CUDA C++ Programming Guide. A trivial example is when the controlling condition depends only on (threadIdx / WSIZE) where WSIZE is the warp size.
In this case, no warp diverges because the controlling condition is perfectly aligned with the warps.
For branches including just a few instructions, warp divergence generally results in marginal performance losses. For example, the compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Threads with a false predicate do not write results, and also do not evaluate addresses or read operands.
Starting with the Volta architecture, Independent Thread Scheduling allows a warp to remain diverged outside of the data-dependent conditional block. An explicit __syncwarp() can be used to guarantee that the warp has reconverged for subsequent instructions.



12.2. Branch Predicationï


Note
Low Priority: Make it easy for the compiler to use branch predication in lieu of loops or control statements.

Sometimes, the compiler may unroll loops or optimize out if or switch statements by using branch predication instead. In these cases, no warp can ever diverge. The programmer can also control loop unrolling using

#pragma unroll


For more information on this pragma, refer to the CUDA C++ Programming Guide.
When using branch predication, none of the instructions whose execution depends on the controlling condition is skipped. Instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition. Although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed. Instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands.
The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold.




13. Deploying CUDA Applicationsï

Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.
Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.



14. Understanding the Programming Environmentï

With each generation of NVIDIA processors, new features are added to the GPU that CUDA can leverage. Consequently, itâs important to understand the characteristics of the architecture.
Programmers should be aware of two version numbers. The first is the compute capability, and the second is the version number of the CUDA Runtime and CUDA Driver APIs.


14.1. CUDA Compute Capabilityï

The compute capability describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor. Higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible.
The compute capability of the GPU in the device can be queried programmatically as illustrated in the deviceQuery CUDA Sample. The output for that program is shown in Figure 16. This information is obtained by calling cudaGetDeviceProperties() and accessing the information in the structure it returns.



Sample CUDA configuration data reported by deviceQueryï


The major and minor revision numbers of the compute capability are shown on the seventh line of Figure 16. Device 0 of this system has compute capability 7.0.
More details about the compute capabilities of various GPUs are in CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming Guide. In particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device.



14.2. Additional Hardware Dataï

Certain hardware features are not described by the compute capability. For example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all GPUs irrespective of the compute capability. In such cases, call cudaGetDeviceProperties() to determine whether the device is capable of a certain feature. For example, the asyncEngineCount field of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, the canMapHostMemory field indicates whether zero-copy data transfers can be performed.



14.3. Which Compute Capability Targetï

To target specific versions of NVIDIA hardware and CUDA software, use the -arch, -code, and -gencode options of nvcc. Code that uses the warp shuffle operation, for example, must be compiled with -arch=sm_30 (or higher compute capability).
See Building for Maximum Compatibility for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously.



14.4. CUDA Runtimeï

The host runtime component of the CUDA software environment can be used only by host functions. It provides functions to handle the following:

Device management
Context management
Memory management
Code module management
Execution control
Texture reference management
Interoperability with OpenGL and Direct3D

As compared to the lower-level CUDA Driver API, the CUDA Runtime greatly eases device management by providing implicit initialization, context management, and device code module management. The C++ host code generated by nvcc utilizes the CUDA Runtime, so applications that link to this code will depend on the CUDA Runtime; similarly, any code that uses the cuBLAS, cuFFT, and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries.
The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual.
The CUDA Runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched. The implicit driver version checking, code initialization, CUDA context management, CUDA module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the CUDA Runtime.
It comprises two principal parts:

A C-style function interface (cuda_runtime_api.h).
C++-style convenience wrappers (cuda_runtime.h) built on top of the C-style functions.

For more information on the Runtime API, refer to CUDA Runtime of the CUDA C++ Programming Guide.




15. CUDA Compatibility Developerâs Guideï

CUDA Toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes. CUDA compatibility allows users to update the latest CUDA Toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack.
The CUDA software environment consists of three parts:

CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications.
CUDA driver - User-mode driver component used to run CUDA applications (e.g. libcuda.so on Linux systems).
NVIDIA GPU device driver - Kernel-mode driver component for NVIDIA GPUs.

On Linux systems, the CUDA driver and kernel mode components are delivered together in the NVIDIA display driver package. This is shown in Figure 1.



Components of CUDAï


The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA code (by splitting and steering compilation), along with the CUDA runtime, is part of the CUDA compiler toolchain. The CUDA Runtime API provides developers with high-level C++ interface for simplified management of devices, kernel executions etc., While the CUDA driver API provides (CUDA Driver API) a low-level programming interface for applications to target NVIDIA hardware.
Built on top of these technologies are CUDA libraries, some of which are included in the CUDA Toolkit, while others such as cuDNN may be released independently of the CUDA Toolkit.


15.1. CUDA Toolkit Versioningï

Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .X.Y.Z, where:

.X stands for the major version - APIs have changed and binary compatibility is broken.
.Y stands for the minor version - Introduction of new APIs, deprecation of old APIs, and source compatibility might be broken but binary compatibility is maintained.
.Z stands for the release/patch version - new updates and patches will increment this.

Each component in the toolkit is recommended to be semantically versioned. From CUDA 11.3 NVRTC is also semantically versioned. We will note some of them later on in the document. The versions of the components in the toolkit are available in this table.
Compatibility of the CUDA platform is thus intended to address a few scenarios:

NVIDIA driver upgrades to systems with GPUs running in production for enterprises or datacenters can be complex and may need advance planning. Delays in rolling out new NVIDIA drivers could mean that users of such systems may not have access to new features available in CUDA releases. Not requiring driver updates for new CUDA releases can mean that new versions of the software can be made available faster to users.
Many software libraries and applications built on top of CUDA (e.g. math libraries or deep learning frameworks) do not have a direct dependency on the CUDA runtime, compiler or driver. In such cases, users or developers can still benefit from not having to upgrade the entire CUDA Toolkit or driver to use these libraries or frameworks.
Upgrading dependencies is error-prone and time consuming, and in some corner cases, can even change the semantics of a program. Constantly recompiling with the latest CUDA Toolkit means forcing upgrades on the end-customers of an application product. Package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process.

CUDA supports several compatibility choices:

First introduced in CUDA 10, the CUDA Forward Compatible Upgrade is designed to allow users to get access to new CUDA features and run applications built with new CUDA releases on systems with older installations of the NVIDIA datacenter driver.

First introduced in CUDA 11.1, CUDA Enhanced Compatibility provides two benefits:

By leveraging semantic versioning across components in the CUDA Toolkit, an application can be built for one CUDA minor release (for example 11.1) and work across all future minor releases within the major family (i.e. 11.x).
The CUDA runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release.


The CUDA driver ensures backward Binary Compatibility is maintained for compiled CUDA applications. Applications compiled with CUDA toolkit versions as old as 3.2 will run on newer drivers.




15.2. Source Compatibilityï

We define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the SDK) will continue to build and run without errors when a newer version of the SDK is installed.
Both the CUDA driver and the CUDA runtime are not source compatible across the different SDK releases. APIs can be deprecated and removed. Therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit.
Developers are notified through deprecation and documentation mechanisms of any current or upcoming changes. This does not mean that application binaries compiled using an older toolkit will not be supported anymore. Application binaries rely on CUDA Driver API interface and even though the CUDA Driver API itself may also have changed across toolkit versions, CUDA guarantees Binary Compatibility of the CUDA Driver API interface.



15.3. Binary Compatibilityï

We define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library.
The CUDA Driver API has a versioned C-style ABI, which guarantees that applications that were running against an older driver (for example CUDA 3.2) will still run and function correctly against a modern driver (for example one shipped with CUDA 11.0). This means that even though an application source might need to be changed if it has to be recompiled against a newer CUDA Toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions.
The CUDA Driver API thus is binary-compatible (the OS loader can pick up a newer version and the application continues to work) but not source-compatible (rebuilding your application against a newer SDK might require source changes).



CUDA Toolkit and Minimum Driver Versionsï


Before we proceed further on this topic, itâs important for developers to understand the concept of Minimum Driver Version and how that may affect them.
Each version of the CUDA Toolkit (and runtime) requires a minimum version of the NVIDIA driver. Applications compiled against a CUDA Toolkit version will only run on systems with the specified minimum driver version for that toolkit version. Prior to CUDA 11.0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the CUDA Toolkit.
So, when an application is built with CUDA 11.0, it can only run on a system with an R450 or later driver. If such an application is run on a system with the R418 driver installed, CUDA initialization will return an error as can be seen in the example below.
In this example, the deviceQuery sample is compiled with CUDA 11.1 and is run on a system with R418. In this scenario, CUDA initialization returns an error due to the minimum driver requirement.

ubuntu@:~/samples/1_Utilities/deviceQuery
$ make
/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp

/usr/local/cuda-11.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o

$ nvidia-smi

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   42C    P0    28W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


$ samples/bin/x86_64/linux/release/deviceQuery
samples/bin/x86_64/linux/release/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

cudaGetDeviceCount returned 3
-> initialization error
Result = FAIL


Refer to the CUDA Toolkit Release Notes for details for the minimum driver version and the version of the driver shipped with the toolkit.


15.3.1. CUDA Binary (cubin) Compatibilityï

A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA.
CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual. It is however usually more effective to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device.
The cubins are architecture-specific. Binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions. In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where zâ¥y.
To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability. For portability, that is, to be able to execute code on future GPU architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled by the NVIDIA driver for these future devices.
More information on cubins, PTX and application compatibility can be found in the CUDA C++ Programming Guide.




15.4. CUDA Compatibility Across Minor Releasesï

By leveraging the semantic versioning, starting with CUDA 11, components in the CUDA Toolkit will remain binary compatible across the minor versions of the toolkit. In order to maintain binary compatibility across minor versions, the CUDA runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a major release is shipped.
One of the main reasons a new toolchain requires a new minimum driver is to handle the JIT compilation of PTX code and the JIT linking of binary code.
In this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the CUDA platform.


15.4.1. Existing CUDA Applications within Minor Versions of CUDAï


$ nvidia-smi

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


When our CUDA 11.1 application (i.e. cudart 11.1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11.0 version - that is, without requiring the driver or other toolkit components to be updated on the system.

$ samples/bin/x86_64/linux/release/deviceQuery
samples/bin/x86_64/linux/release/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "Tesla T4"
  CUDA Driver Version / Runtime Version          11.0 / 11.1
  CUDA Capability Major/Minor version number:    7.5

  ...<snip>...

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.0, CUDA Runtime Version = 11.1, NumDevs = 1
Result = PASS


By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features.
The following sections discuss some caveats and considerations.


15.4.1.1. Handling New CUDA Features and Driver APIsï

A subset of CUDA APIs donât need a new driver and they can all be used without any driver dependencies. For example, cuMemMap APIs or any of APIs introduced prior to CUDA 11.0, such as cudaDeviceSynchronize, do not require a driver upgrade. To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully. This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions. Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release.
When working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older CUDA driver. Users wishing to take advantage of such a feature should query its availability with a dynamic check in the code:

static bool hostRegisterFeatureSupported = false;
static bool hostRegisterIsDeviceAddress = false;

static error_t cuFooFunction(int *ptr)
{
    int *dptr = null;
    if (hostRegisterFeatureSupported) {
         cudaHostRegister(ptr, size, flags);
         if (hostRegisterIsDeviceAddress) {
              qptr = ptr;
         }
       else {
          cudaHostGetDevicePointer(&qptr, ptr, 0);
          }
       }
    else {
            // cudaMalloc();
            // cudaMemcpy();
       }
    gemm<<<1,1>>>(dptr);
    cudaDeviceSynchronize();
}

int main()
{
    // rest of code here
    cudaDeviceGetAttribute(
           &hostRegisterFeatureSupported,
           cudaDevAttrHostRegisterSupported,
           0);
    cudaDeviceGetAttribute(
           &hostRegisterIsDeviceAddress,
           cudaDevAttrCanUseHostPointerForRegisteredMem,
           0);
    cuFooFunction(/* malloced pointer */);
}


Alternatively the applicationâs interface might not work at all without a new CUDA driver and then its best to return an error right away:

#define MIN_VERSION 11010
cudaError_t foo()
{
    int version = 0;
    cudaGetDriverVersion(&version);
    if (version < MIN_VERSION) {
        return CUDA_ERROR_INSUFFICIENT_DRIVER;
    }
    // proceed as normal
}


A new error code is added to indicate that the functionality is missing from the driver you are running against: cudaErrorCallRequiresNewerDriver.



15.4.1.2. Using PTXï

PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs are translated at load time to the target hardware instruction set via the JIT Compiler which is part of the CUDA driver. As PTX is compiled by the CUDA driver, new toolchains will generate PTX that is not compatible with the older CUDA driver. This is not a problem when PTX is used for future device compatibility (the most common case), but can lead to issues when used for runtime compilation.
For codes continuing to make use of PTX, in order to support compiling on an older driver, your code must be first transformed into device code via the static ptxjitcompiler library or NVRTC with the option of generating code for a specific architecture (e.g. sm_80) rather than a virtual architecture (e.g. compute_80). For this workflow, a new nvptxcompiler_static library is shipped with the CUDA Toolkit.
We can see this usage in the following example:

char* compilePTXToNVElf()
{
    nvPTXCompilerHandle compiler = NULL;
    nvPTXCompileResult status;

    size_t elfSize, infoSize, errorSize;
    char *elf, *infoLog, *errorLog;
    int minorVer, majorVer;

    const char* compile_options[] = { "--gpu-name=sm_80",
                                      "--device-debug"
    };

    nvPTXCompilerGetVersion(&majorVer, &minorVer);
    nvPTXCompilerCreate(&compiler, (size_t)strlen(ptxCode), ptxCode);
    status = nvPTXCompilerCompile(compiler, 2, compile_options);
    if (status != NVPTXCOMPILE_SUCCESS) {
        nvPTXCompilerGetErrorLogSize(compiler, (void*)&errorSize);

        if (errorSize != 0) {
            errorLog = (char*)malloc(errorSize+1);
            nvPTXCompilerGetErrorLog(compiler, (void*)errorLog);
            printf("Error log: %s\n", errorLog);
            free(errorLog);
        }
        exit(1);
    }

    nvPTXCompilerGetCompiledProgramSize(compiler, &elfSize));
    elf = (char*)malloc(elfSize);
    nvPTXCompilerGetCompiledProgram(compiler, (void*)elf);
    nvPTXCompilerGetInfoLogSize(compiler, (void*)&infoSize);

    if (infoSize != 0) {
        infoLog = (char*)malloc(infoSize+1);
        nvPTXCompilerGetInfoLog(compiler, (void*)infoLog);
        printf("Info log: %s\n", infoLog);
        free(infoLog);
    }

    nvPTXCompilerDestroy(&compiler);
    return elf;
}





15.4.1.3. Dynamic Code Generationï

NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx.
Dealing with relocatable objects is not yet supported, therefore the cuLink* set of APIs in the CUDA driver will not work with enhanced compatibility. An upgraded driver matching the CUDA runtime version is currently required for those APIs.
As mentioned in the PTX section, the compilation of PTX to device code lives along with the CUDA driver, hence the generated PTX might be newer than what is supported by the driver on the deployment system. When using NVRTC, it is recommended that the resulting PTX code is first transformed to the final device code via the steps outlined by the PTX user workflow. This ensures your code is compatible. Alternatively, NVRTC can generate cubins directly starting with CUDA 11.1. Applications using the new API can load the final device code directly using driver APIs cuModuleLoadData and cuModuleLoadDataEx.
NVRTC used to support only virtual architectures through the option -arch, since it was only emitting PTX. It will now support actual architectures as well to emit SASS. The interface is augmented to retrieve either the PTX or cubin if an actual architecture is specified.
The example below shows how an existing example can be adapted to use the new features, guarded by the USE_CUBIN macro in this case:

#include <nvrtc.h>
#include <cuda.h>
#include <iostream>

void NVRTC_SAFE_CALL(nvrtcResult result) {
  if (result != NVRTC_SUCCESS) {
    std::cerr << "\nnvrtc error: " << nvrtcGetErrorString(result) << '\n';
    std::exit(1);
  }
}

void CUDA_SAFE_CALL(CUresult result) {
  if (result != CUDA_SUCCESS) {
    const char *msg;
    cuGetErrorName(result, &msg);
    std::cerr << "\ncuda error: " << msg << '\n';
    std::exit(1);
  }
}

const char *hello = "                                           \n\
extern \"C\" __global__ void hello() {                          \n\
  printf(\"hello world\\n\");                                   \n\
}                                                               \n";

int main()
{
  nvrtcProgram prog;
  NVRTC_SAFE_CALL(nvrtcCreateProgram(&prog, hello, "hello.cu", 0, NULL, NULL));
#ifdef USE_CUBIN
  const char *opts[] = {"-arch=sm_70"};
#else
  const char *opts[] = {"-arch=compute_70"};
#endif
  nvrtcResult compileResult = nvrtcCompileProgram(prog, 1, opts);
  size_t logSize;
  NVRTC_SAFE_CALL(nvrtcGetProgramLogSize(prog, &logSize));
  char *log = new char[logSize];
  NVRTC_SAFE_CALL(nvrtcGetProgramLog(prog, log));
  std::cout << log << '\n';
  delete[] log;
  if (compileResult != NVRTC_SUCCESS)
    exit(1);
  size_t codeSize;
#ifdef USE_CUBIN
  NVRTC_SAFE_CALL(nvrtcGetCUBINSize(prog, &codeSize));
  char *code = new char[codeSize];
  NVRTC_SAFE_CALL(nvrtcGetCUBIN(prog, code));
#else
  NVRTC_SAFE_CALL(nvrtcGetPTXSize(prog, &codeSize));
  char *code = new char[codeSize];
  NVRTC_SAFE_CALL(nvrtcGetPTX(prog, code));
#endif
  NVRTC_SAFE_CALL(nvrtcDestroyProgram(&prog));
  CUdevice cuDevice;
  CUcontext context;
  CUmodule module;
  CUfunction kernel;
  CUDA_SAFE_CALL(cuInit(0));
  CUDA_SAFE_CALL(cuDeviceGet(&cuDevice, 0));
  CUDA_SAFE_CALL(cuCtxCreate(&context, 0, cuDevice));
  CUDA_SAFE_CALL(cuModuleLoadDataEx(&module, code, 0, 0, 0));
  CUDA_SAFE_CALL(cuModuleGetFunction(&kernel, module, "hello"));
  CUDA_SAFE_CALL(cuLaunchKernel(kernel, 1, 1, 1, 1, 1, 1, 0, NULL, NULL, 0));
  CUDA_SAFE_CALL(cuCtxSynchronize());
  CUDA_SAFE_CALL(cuModuleUnload(module));
  CUDA_SAFE_CALL(cuCtxDestroy(context));
  delete[] code;
}





15.4.1.4. Recommendations for building a minor-version compatible libraryï

We recommend that the CUDA runtime be statically linked to minimize dependencies. Verify that your library doesnât leak dependencies, breakages, namespaces, etc. outside your established ABI contract.
Follow semantic versioning for your libraryâs soname. Having a semantically versioned ABI means the interfaces need to be maintained and versioned. The library should follow semantic rules and increment the version number when a change is made that affects this ABI contract. Missing dependencies is also a binary compatibility break, hence you should provide fallbacks or guards for functionality that depends on those interfaces. Increment major versions when there are ABI breaking changes such as API deprecation and modifications. New APIs can be added in minor versions.
Conditionally use features to remain compatible against older drivers. If no new features are used (or if they are used conditionally with fallbacks provided) youâll be able to remain compatible.
Donât expose ABI structures that can change. A pointer to a structure with a size embedded is a better solution.
When linking with dynamic libraries from the toolkit, the library must be equal to or newer than what is needed by any one of the components involved in the linking of your application. For example, if you link against the CUDA 11.1 dynamic runtime, and use functionality from 11.1, as well as a separate shared library that was linked against the CUDA 11.2 dynamic runtime that requires 11.2 functionality, the final link step must include a CUDA 11.2 or newer dynamic runtime.



15.4.1.5. Recommendations for taking advantage of minor version compatibility in your applicationï

Certain functionality might not be available so you should query where applicable. This is common for building applications that are GPU architecture, platform and compiler agnostic. However we now add âthe underlying driverâ to that mix.
As with the previous section on library building recommendations, if using the CUDA runtime, we recommend linking to the CUDA runtime statically when building your application. When using the driver APIs directly, we recommend using the new driver entry point access API (cuGetProcAddress) documented here: CUDA Driver API :: CUDA Toolkit Documentation.
When using a shared or static library, follow the release notes of said library to determine if the library supports minor version compatibility.






16. Preparing for Deploymentï



16.1. Testing for CUDA Availabilityï

When deploying a CUDA application, it is often desirable to ensure that the application will continue to function properly even if the target machine does not have a CUDA-capable GPU and/or a sufficient version of the NVIDIA Driver installed. (Developers targeting a single machine with known configuration may choose to skip this section.)
Detecting a CUDA-Capable GPU
When an application will be deployed to target machines of arbitrary/unknown configuration, the application should explicitly test for the existence of a CUDA-capable GPU in order to take appropriate action when no such device is available. The cudaGetDeviceCount() function can be used to query for the number of available devices. Like all CUDA Runtime API functions, this function will fail gracefully and return cudaErrorNoDevice to the application if there is no CUDA-capable GPU or cudaErrorInsufficientDriver if there is not an appropriate version of the NVIDIA Driver installed. If cudaGetDeviceCount() reports an error, the application should fall back to an alternative code path.
A system with multiple GPUs may contain GPUs of different hardware versions and capabilities. When using multiple GPUs from the same application, it is recommended to use GPUs of the same type, rather than mixing hardware generations. The cudaChooseDevice() function can be used to select the device that most closely matches a desired set of features.
Detecting Hardware and Software Configuration
When an application depends on the availability of certain hardware or software capabilities to enable certain functionality, the CUDA API can be queried for details about the configuration of the available device and for the installed software versions.
The cudaGetDeviceProperties() function reports various features of the available devices, including the CUDA Compute Capability of the device (see also the Compute Capabilities section of the CUDA C++ Programming Guide). See Version Management for details on how to query the available CUDA software API versions.



16.2. Error Handlingï

All CUDA Runtime API calls return an error code of type cudaError_t; the return value will be equal to cudaSuccess if no errors have occurred. (The exceptions to this are kernel launches, which return void, and cudaGetErrorString(), which returns a character string describing the cudaError_t code that was passed into it.) The CUDA Toolkit libraries (cuBLAS, cuFFT, etc.) likewise return their own sets of error codes.
Since some CUDA API calls and all kernel launches are asynchronous with respect to the host code, errors may be reported to the host asynchronously as well; often this occurs the next time the host and device synchronize with each other, such as during a call to cudaMemcpy() or to cudaDeviceSynchronize().
Always check the error return values on all CUDA API functions, even for functions that are not expected to fail, as this will allow the application to detect and recover from errors as soon as possible should they occur. To check for errors occurring during kernel launches using the <<<...>>> syntax, which does not return any error code, the return code of cudaGetLastError() should be checked immediately after the kernel launch. Applications that do not check for CUDA API errors could at times run to completion without having noticed that the data calculated by the GPU is incomplete, invalid, or uninitialized.

Note
The CUDA Toolkit Samples provide several helper functions for error checking with the various CUDA APIs; these helper functions are located in the samples/common/inc/helper_cuda.h file in the CUDA Toolkit.




16.3. Building for Maximum Compatibilityï

Each generation of CUDA-capable device has an associated compute capability version that indicates the feature set supported by the device (see CUDA Compute Capability). One or more compute capability versions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target GPU(s) of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of GPU.
When an application is built for multiple compute capabilities simultaneously (using several instances of the -gencode flag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the CUDA Driver selects the most appropriate binary at runtime according to the compute capability of the present device. If an appropriate native binary (cubin) is not available, but the intermediate PTX code (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiled Just In Time (JIT) (see Compiler JIT Cache Management Tools) from the PTX to the native cubin for the device. If the PTX is also not available, then the kernel launch will fail.
Windows

nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_70,code=sm_70
  -gencode=arch=compute_75,code=sm_75
  -gencode=arch=compute_75,code=compute_75
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"


Mac/Linux

/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_70,code=sm_70
  -gencode=arch=compute_75,code=sm_75
  -gencode=arch=compute_75,code=compute_75
  -O2 -o mykernel.o -c mykernel.cu


Alternatively, the nvcc command-line option -arch=sm_XX can be used as a shorthand equivalent to the following more explicit -gencode= command-line options described above:

-gencode=arch=compute_XX,code=sm_XX
-gencode=arch=compute_XX,code=compute_XX


However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default (due to the code=compute_XX target it implies), it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly.



16.4. Distributing the CUDA Runtime and Librariesï

CUDA applications are built against the CUDA Runtime library, which handles device, memory, and kernel management. Unlike the CUDA Driver, the CUDA Runtime guarantees neither forward nor backward binary compatibility across versions. It is therefore best to redistribute the CUDA Runtime library with the application when using dynamic linking or else to statically link against the CUDA Runtime. This will ensure that the executable will be able to run even if the user does not have the same CUDA Toolkit installed that the application was built against.

Note
When statically linking to the CUDA Runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously; for example, if an application uses one version of the CUDA Runtime, and a plugin to that application is statically linked to a different version, that is perfectly acceptable, as long as the installed NVIDIA Driver is sufficient for both.

Statically-linked CUDA Runtime
The easiest option is to statically link against the CUDA Runtime. This is the default if using nvcc to link in CUDA 5.5 and later. Static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the CUDA Runtime library.
Dynamically-linked CUDA Runtime
If static linking against the CUDA Runtime is impractical for some reason, then a dynamically-linked version of the CUDA Runtime library is also available. (This was the default and only option provided in CUDA versions 5.0 and earlier.)
To use dynamic linking with the CUDA Runtime when using the nvcc from CUDA 5.5 or later to link the application, add the --cudart=shared flag to the link command line; otherwise the statically-linked CUDA Runtime library is used by default.
After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be bundled with the application. It can be copied into the same directory as the application executable or into a subdirectory of that installation path.
Other CUDA Libraries
Although the CUDA Runtime provides the option of static linking, some libraries included in the CUDA Toolkit are available only in dynamically-linked form. As with the dynamically-linked version of the CUDA Runtime library, these libraries should be bundled with the application executable when distributing that application.


16.4.1. CUDA Toolkit Library Redistributionï

The CUDA Toolkitâs End-User License Agreement (EULA) allows for redistribution of many of the CUDA libraries under certain terms and conditions. This allows applications that depend on these libraries to redistribute the exact versions of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the CUDA Toolkit (or perhaps none at all) installed on their machines. Please refer to the EULA for details.

Note
This does not apply to the NVIDIA Driver; the end user must still download and install an NVIDIA Driver appropriate to their GPU(s) and operating system.



16.4.1.1. Which Files to Redistributeï

When redistributing the dynamically-linked versions of one or more CUDA libraries, it is important to identify the exact files that need to be redistributed. The following examples use the cuBLAS library from CUDA Toolkit 5.5 as an illustration:
Linux
In a shared library on Linux, there is a string field called the SONAME that indicates the binary compatibility level of the library. The SONAME of the library against which the application was built must match the filename of the library that is redistributed with the application.
For example, in the standard CUDA Toolkit installation, the files libcublas.so and libcublas.so.5.5 are both symlinks pointing to a specific build of cuBLAS, which is named like libcublas.so.5.5.x, where x is the build number (e.g., libcublas.so.5.5.17). However, the SONAME of this library is given as âlibcublas.so.5.5â:

$ objdump -p /usr/local/cuda/lib64/libcublas.so | grep SONAME
   SONAME               libcublas.so.5.5


Because of this, even if -lcublas (with no version number specified) is used when linking the application, the SONAME found at link time implies that âlibcublas.so.5.5â is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file (or a symlink to the same) that is redistributed with the application.
The ldd tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the dynamic loader would select when loading the application given the current library search path:

$ ldd a.out | grep libcublas
   libcublas.so.5.5 => /usr/local/cuda/lib64/libcublas.so.5.5


Mac
In a shared library on Mac OS X, there is a field called the install name that indicates the expected installation path and filename the library; the CUDA libraries also use this filename to indicate binary compatibility. The value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime.
For example, if the install name of the cuBLAS library is given as @rpath/libcublas.5.5.dylib, then the library is version 5.5 and the copy of this library redistributed with the application must be named libcublas.5.5.dylib, even though only -lcublas (with no version number specified) is used at link time. Furthermore, this file should be installed into the @rpath of the application; see Where to Install Redistributed CUDA Libraries.
To view a libraryâs install name, use the otool -L command:

$ otool -L a.out
a.out:
        @rpath/libcublas.5.5.dylib (...)


Windows
The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename.
For example, a 64-bit application linked to cuBLAS 5.5 will look for cublas64_55.dll at runtime, so this is the file that should be redistributed with that application, even though cublas.lib is the file that the application is linked against. For 32-bit applications, the file would be cublas32_55.dll.
To verify the exact DLL filename that the application expects to find at runtime, use the dumpbin tool from the Visual Studio command prompt:

$ dumpbin /IMPORTS a.exe
Microsoft (R) COFF/PE Dumper Version 10.00.40219.01
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file a.exe

File Type: EXECUTABLE IMAGE

  Section contains the following imports:

    ...
    cublas64_55.dll
    ...





16.4.1.2. Where to Install Redistributed CUDA Librariesï

Once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them.
On Windows, if the CUDA Runtime or other dynamically-linked CUDA Toolkit library is placed in the same directory as the executable, Windows will locate it automatically. On Linux and Mac, the -rpath linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths:
Linux/Mac

nvcc -I $(CUDA_HOME)/include
  -Xlinker "-rpath '$ORIGIN'" --cudart=shared
  -o myprogram myprogram.cu


Windows

nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" --cudart=shared
  -o "Release\myprogram.exe" "myprogram.cu"



Note
It may be necessary to adjust the value of -ccbin to reflect the location of your Visual Studio installation.

To specify an alternate path where the libraries will be distributed, use linker options similar to those below:
Linux/Mac

nvcc -I $(CUDA_HOME)/include
  -Xlinker "-rpath '$ORIGIN/lib'" --cudart=shared
  -o myprogram myprogram.cu


Windows

nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT /DELAY" --cudart=shared
  -o "Release\myprogram.exe" "myprogram.cu"


For Linux and Mac, the -rpath option is used as before. For Windows, the /DELAY option is used; this requires that the application call SetDllDirectory() before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs.

Note
For Windows 8, SetDefaultDLLDirectories() and AddDllDirectory() should be used instead of SetDllDirectory(). Please see the MSDN documentation for these routines for more information.







17. Deployment Infrastructure Toolsï



17.1. Nvidia-SMIï

The NVIDIA System Management Interface (nvidia-smi) is a command line utility that aids in the management and monitoring of NVIDIA GPU devices. This utility allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state. nvidia-smi is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs. nvidia-smi ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7. nvidia-smi can output queried information as XML or as human-readable plain text either to standard output or to a file. See the nvidia-smi documenation for details. Please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions.


17.1.1. Queryable stateï


ECC error counts

Both correctable single-bit and detectable double-bit errors are reported. Error counts are provided for both the current boot cycle and the lifetime of the GPU.

GPU utilization

Current utilization rates are reported for both the compute resources of the GPU and the memory interface.

Active compute process

The list of active processes running on the GPU is reported, along with the corresponding process name/ID and allocated GPU memory.

Clocks and performance state

Max and current clock rates are reported for several important clock domains, as well as the current GPU performance state (pstate).

Temperature and fan speed

The current GPU core temperature is reported, along with fan speeds for products with active cooling.

Power management

The current board power draw and power limits are reported for products that report these measurements.

Identification

Various dynamic and static information is reported, including board serial numbers, PCI device IDs, VBIOS/Inforom version numbers and product names.





17.1.2. Modifiable stateï


ECC mode

Enable and disable ECC reporting.

ECC reset

Clear single-bit and double-bit ECC error counts.

Compute mode

Indicate whether compute processes can run on the GPU and whether they run exclusively or concurrently with other compute processes.

Persistence mode

Indicate whether the NVIDIA driver stays loaded when no applications are connected to the GPU. It is best to enable this option in most circumstances.

GPU reset

Reinitialize the GPU hardware and software state via a secondary bus reset.






17.2. NVMLï

The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications. The NVML API is shipped with the CUDA Toolkit (since version 8.0) and is also available standalone on the NVIDIA developer website as part of the GPU Deployment Kit through a single header file accompanied by PDF documentation, stub libraries, and sample applications; see https://developer.nvidia.com/gpu-deployment-kit. Each new version of NVML is backward-compatible.
An additional set of Perl and Python bindings are provided for the NVML API. These bindings expose the same features as the C-based interface and also provide backwards compatibility. The Perl bindings are provided via CPAN and the Python bindings via PyPI.
All of these products (nvidia-smi, NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality.
See https://developer.nvidia.com/nvidia-management-library-nvml for additional information.



17.3. Cluster Management Toolsï

Managing your GPU cluster will help achieve maximum GPU utilization and help you and your users extract the best possible performance. Many of the industryâs most popular cluster management tools support CUDA GPUs via NVML. For a listing of some of these tools, see https://developer.nvidia.com/cluster-management.



17.4. Compiler JIT Cache Management Toolsï

Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver. This is called just-in-time compilation (JIT). Just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements. It is also the only way for applications to run on devices that did not exist at the time the application was compiled.
When JIT compilation of PTX device code is used, the NVIDIA driver caches the resulting binary code on disk. Some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see Just in Time Compilation of the CUDA C++ Programming Guide.



17.5. CUDA_VISIBLE_DEVICESï

It is possible to rearrange the collection of installed CUDA devices that will be visible to and enumerated by a CUDA application prior to the start of that application by way of the CUDA_VISIBLE_DEVICES environment variable.
Devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices. For example, to use only devices 0 and 2 from the system-wide list of devices, set CUDA_VISIBLE_DEVICES=0,2 before launching the application. The application will then enumerate these devices as device 0 and device 1, respectively.




18. Recommendations and Best Practicesï

This chapter contains a summary of the recommendations for optimization that are explained in this document.


18.1. Overall Performance Optimization Strategiesï

Performance optimization revolves around three basic strategies:

Maximizing parallel execution
Optimizing memory usage to achieve maximum memory bandwidth
Optimizing instruction usage to achieve maximum instruction throughput

Maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible. Once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible. This is done by carefully choosing the execution configuration of each kernel launch. The application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device.
Optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers. Kernel access to global memory also should be minimized by maximizing the use of shared memory on the device. Sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed.
The effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory. The next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns. This optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles. Shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts.
As for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided. This suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision. Finally, particular attention must be paid to control flow instructions due to the SIMT (single instruction multiple thread) nature of the device.




19. nvcc Compiler Switchesï



19.1. nvccï

The NVIDIA nvcc compiler driver converts .cu files into C++ for the host system and CUDA assembly or binary instructions for the device. It supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices:

-maxrregcount=N specifies the maximum number of registers kernels can use at a per-file level. See Register Pressure. (See also the__launch_bounds__ qualifier discussed in Execution Configuration of the CUDA C++ Programming Guide to control the number of registers used on a per-kernel basis.)
--ptxas-options=-v or -Xptxas=-v lists per-kernel register, shared, and constant memory usage.
-ftz=true (denormalized numbers are flushed to zero)
-prec-div=false (less precise division)
-prec-sqrt=false (less precise square root)
-use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. This makes the code run faster at the cost of diminished precision and accuracy. See Math Libraries.





20. Noticesï



20.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



20.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



20.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      




















Maxwell Compatibility










































1. Maxwell Compatibility
1.1. About this Document
1.2. Application Compatibility on Maxwell

1.3. Verifying Maxwell Compatibility for Existing Applications
1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier
1.3.2. Applications Using CUDA Toolkit 6.0 or Later



1.4. Building Applications with Maxwell Support
1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier
1.4.2. Applications Using CUDA Toolkit 6.0 or Later




2. Revision History

3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Maxwell Compatibility Guide






 Â»

1. Maxwell Compatibility



v12.5 |
PDF
|
Archive
Â 






Maxwell Compatibility Guide for CUDA Applications
The guide to building CUDA applications for GPUs based on the NVIDIA Maxwell Architecture.


1. Maxwell Compatibilityï



1.1. About this Documentï

This application note, Maxwell Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIAÂ® CUDAÂ® applications will run on GPUs based on the NVIDIAÂ® Maxwell Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Maxwell.



1.2. Application Compatibility on Maxwellï

The NVIDIA CUDA C++ compiler, nvcc, can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number. For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.

Note
CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes.

Applications that already include PTX versions of their kernels should work as-is on Maxwell-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Maxwell-compatible PTX or cubins.



1.3. Verifying Maxwell Compatibility for Existing Applicationsï

The first step is to check that Maxwell-compatible device code (at least PTX) is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions.


1.3.1. Applications Using CUDA Toolkit 5.5 or Earlierï

CUDA applications built using CUDA Toolkit versions 2.1 through 5.5 are compatible with Maxwell as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following:

Download and install the latest driver from https://www.nvidia.com/drivers.
Set the environment variable CUDA_FORCE_PTX_JIT=1.
Launch your application.

When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code.
If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Maxwell compatibility.

Note
Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing.




1.3.2. Applications Using CUDA Toolkit 6.0 or Laterï

CUDA applications built using CUDA Toolkit 6.0 or Later1 are compatible with Maxwell as long as they are built to include kernels in either Maxwell-native cubin format (see Building Applications with Maxwell Support) or PTX format (see Applications Using CUDA Toolkit 5.5 or Earlier) or both.




1.4. Building Applications with Maxwell Supportï

When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPUâs native cubin format before launching it. If neither is available, then the kernel launch will fail.
The method used to build your application with either native cubin or at least PTX support for Maxwell depend on the version of the CUDA Toolkit used.
The main advantages of providing native cubins are as follows:

It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy.



1.4.1. Applications Using CUDA Toolkit 5.5 or Earlierï

The compilers included in CUDA Toolkit 5.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Fermi and Kepler, but they cannot generate cubin files native to the Maxwell architecture. To allow support for Maxwell and future architectures when using version 5.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel.
Below are compiler settings that could be used to build mykernel.cu to run on Fermi or Kepler devices natively and on Maxwell devices via PTX JIT.

Note
compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Maxwell compatibility.

Windows

nvcc.exe -ccbin "C:\vs2010\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_20,code=sm_20
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_35,code=compute_35
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"


Mac/Linux

/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_20,code=sm_20
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_35,code=compute_35
  -O2 -o mykernel.o -c mykernel.cu


Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX, which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following:

-gencode=arch=compute_XX,code=sm_XX
-gencode=arch=compute_XX,code=compute_XX


However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly.



1.4.2. Applications Using CUDA Toolkit 6.0 or Laterï

With version 6.0 of the CUDA Toolkit, nvcc can generate cubin files native to the first-generation Maxwell architecture (compute capability 5.0); CUDA Toolkit 6.5 and later further add native support for second-generation Maxwell devices (compute capability 5.2). When using CUDA Toolkit 6.x or Later, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows

nvcc.exe -ccbin "C:\vs2010\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_20,code=sm_20
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_52,code=sm_52
  -gencode=arch=compute_52,code=compute_52
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"


Mac/Linux

/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_20,code=sm_20
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_52,code=sm_52
  -gencode=arch=compute_52,code=compute_52
  -O2 -o mykernel.o -c mykernel.cu



Note
compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.






2. Revision Historyï

Version 1.0

Initial public release.

Version 1.1

Updated for second-generation Maxwell (compute capability 5.2).

Version 1.2

Use CUDA C++ instead of CUDA C/C++.
Updated CUDA Toolkit reference to 6.0 and Later.




3. Noticesï



3.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



3.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



3.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.

1

Future CUDA Toolkit version might deprecate support for the Maxwell Architecture.












Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2014-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      




















Pascal Compatibility










































1. Pascal Compatibility
1.1. About this Document
1.2. Application Compatibility on Pascal

1.3. Verifying Pascal Compatibility for Existing Applications
1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier
1.3.2. Applications Using CUDA Toolkit 8.0



1.4. Building Applications with Pascal Support
1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier
1.4.2. Applications Using CUDA Toolkit 8.0




2. Revision History

3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Pascal Compatibility Guide






 Â»

1. Pascal Compatibility



v12.5 |
PDF
|
Archive
Â 






Pascal Compatibility Guide for CUDA Applications
The guide to building CUDA applications for GPUs based on the NVIDIA Pascal Architecture.


1. Pascal Compatibilityï



1.1. About this Documentï

This application note, Pascal Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIAÂ® CUDAÂ® applications will run on GPUs based on the NVIDIAÂ® Pascal Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Pascal.



1.2. Application Compatibility on Pascalï

The NVIDIA CUDA C++ compiler, nvcc, can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number. For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels.

Note
CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes.

Applications that already include PTX versions of their kernels should work as-is on Pascal-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Pascal-compatible PTX or cubins.



1.3. Verifying Pascal Compatibility for Existing Applicationsï

The first step is to check that Pascal-compatible device code (at least PTX) is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions.


1.3.1. Applications Using CUDA Toolkit 7.5 or Earlierï

CUDA applications built using CUDA Toolkit versions 2.1 through 7.5 are compatible with Pascal as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following:

Download and install the latest driver from https://www.nvidia.com/drivers.
Set the environment variable CUDA_FORCE_PTX_JIT=1.
Launch your application.

When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code.
If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Pascal compatibility.

Note
Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing.




1.3.2. Applications Using CUDA Toolkit 8.0ï

CUDA applications built using CUDA Toolkit 8.0 are compatible with Pascal as long as they are built to include kernels in either Pascal-native cubin format (see Building Applications with Pascal Support) or PTX format (see Applications Using CUDA Toolkit 7.5 or Earlier) or both.




1.4. Building Applications with Pascal Supportï

When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPUâs native cubin format before launching it. If neither is available, then the kernel launch will fail.
The method used to build your application with either native cubin or at least PTX support for Pascal depend on the version of the CUDA Toolkit used.
The main advantages of providing native cubins are as follows:

It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible.
PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy.



1.4.1. Applications Using CUDA Toolkit 7.5 or Earlierï

The compilers included in CUDA Toolkit 7.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Kepler and Maxwell, but they cannot generate cubin files native to the Pascal architecture. To allow support for Pascal and future architectures when using version 7.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel.
Below are compiler settings that could be used to build mykernel.cu to run on Kepler or Maxwell devices natively and on Pascal devices via PTX JIT.

Note
compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Pascal compatibility.

Windows

nvcc.exe -ccbin "C:\vs2010\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_52,code=sm_52
  -gencode=arch=compute_52,code=compute_52
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"


Mac/Linux

/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_52,code=sm_52
  -gencode=arch=compute_52,code=compute_52
  -O2 -o mykernel.o -c mykernel.cu


Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX, which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following:

-gencode=arch=compute_XX,code=sm_XX
-gencode=arch=compute_XX,code=compute_XX


However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly.



1.4.2. Applications Using CUDA Toolkit 8.0ï

With version 8.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Pascal architectures (compute capability 6.0 and 6.1). When using CUDA Toolkit 8.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below.
Windows

nvcc.exe -ccbin "C:\vs2010\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_52,code=sm_52
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_61,code=sm_61
  -gencode=arch=compute_61,code=compute_61
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"


Mac/Linux

/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_52,code=sm_52
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_61,code=sm_61
  -gencode=arch=compute_61,code=compute_61
  -O2 -o mykernel.o -c mykernel.cu



Note
compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.






2. Revision Historyï

Version 1.0

Initial public release.

Version 1.1

Use CUDA C++ instead of CUDA C/C++




3. Noticesï



3.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



3.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



3.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2016-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      



















CUDA 12.5 Update 1 Release Notes








































1. CUDA 12.5 Update 1 Release Notes
1.1. CUDA Toolkit Major Component Versions
1.2. New Features
1.2.1. General CUDA
1.2.2. CUDA Compiler
1.2.3. CUDA Developer Tools


1.3. Resolved Issues
1.3.1. CUDA Compiler


1.4. Known Issues and Limitations
1.5. Deprecated or Dropped Features
1.5.1. Deprecated or Dropped Architectures
1.5.2. Deprecated Operating Systems
1.5.3. Deprecated Toolchains
1.5.4. CUDA Tools




2. CUDA Libraries
2.1. cuBLAS Library
2.1.1. cuBLAS: Release 12.5 Update 1
2.1.2. cuBLAS: Release 12.5
2.1.3. cuBLAS: Release 12.4 Update 1
2.1.4. cuBLAS: Release 12.4
2.1.5. cuBLAS: Release 12.3 Update 1
2.1.6. cuBLAS: Release 12.3
2.1.7. cuBLAS: Release 12.2 Update 2
2.1.8. cuBLAS: Release 12.2
2.1.9. cuBLAS: Release 12.1 Update 1
2.1.10. cuBLAS: Release 12.0 Update 1
2.1.11. cuBLAS: Release 12.0


2.2. cuFFT Library
2.2.1. cuFFT: Release 12.5
2.2.2. cuFFT: Release 12.4 Update 1
2.2.3. cuFFT: Release 12.4
2.2.4. cuFFT: Release 12.3 Update 1
2.2.5. cuFFT: Release 12.3
2.2.6. cuFFT: Release 12.2
2.2.7. cuFFT: Release 12.1 Update 1
2.2.8. cuFFT: Release 12.1
2.2.9. cuFFT: Release 12.0 Update 1
2.2.10. cuFFT: Release 12.0


2.3. cuSOLVER Library
2.3.1. cuSOLVER: Release 12.5 Update 1
2.3.2. cuSOLVER: Release 12.5
2.3.3. cuSOLVER: Release 12.4 Update 1
2.3.4. cuSOLVER: Release 12.4
2.3.5. cuSOLVER: Release 12.2 Update 2
2.3.6. cuSOLVER: Release 12.2


2.4. cuSPARSE Library
2.4.1. cuSPARSE: Release 12.5 Update 1
2.4.2. cuSPARSE: Release 12.5
2.4.3. cuSPARSE: Release 12.4
2.4.4. cuSPARSE: Release 12.3 Update 1
2.4.5. cuSPARSE: Release 12.3
2.4.6. cuSPARSE: Release 12.2 Update 1
2.4.7. cuSPARSE: Release 12.1 Update 1
2.4.8. cuSPARSE: Release 12.0 Update 1
2.4.9. cuSPARSE: Release 12.0


2.5. Math Library
2.5.1. CUDA Math: Release 12.5
2.5.2. CUDA Math: Release 12.4
2.5.3. CUDA Math: Release 12.3
2.5.4. CUDA Math: Release 12.2
2.5.5. CUDA Math: Release 12.1
2.5.6. CUDA Math: Release 12.0


2.6. NVIDIA Performance Primitives (NPP)
2.6.1. NPP: Release 12.4
2.6.2. NPP: Release 12.0


2.7. nvJPEG Library
2.7.1. nvJPEG: Release 12.4
2.7.2. nvJPEG: Release 12.3 Update 1
2.7.3. nvJPEG: Release 12.2
2.7.4. nvJPEG: Release 12.0




3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Release Notes





 »
1. CUDA 12.5 Update 1 Release Notes



v12.5 |
PDF
|
Archive
 






NVIDIA CUDA Toolkit Release Notes
The Release Notes for the CUDA Toolkit.

1. CUDA 12.5 Update 1 Release Notesï
The release notes for the NVIDIAÂ® CUDAÂ® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html.

Note
The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.


1.1. CUDA Toolkit Major Component Versionsï

CUDA ComponentsStarting with CUDA 11, the various components in the toolkit are versioned independently.
For CUDA 12.5 Update 1, the table below indicates the versions:



Table 1 CUDA 12.5 Update 1 Component Versionsï








Component Name
Version Information
Supported Architectures
Supported Platforms



CUDA C++ Core Compute Libraries
Thrust
2.4.0
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUB
2.4.0

libcu++
2.4.0

Cooperative Groups
12.5.82

CUDA Compatibility
12.5.36505571
aarch64-jetson
Linux

CUDA Runtime (cudart)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuobjdump
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUPTI
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuxxfilt (demangler)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA Demo Suite
12.5.82
x86_64
Linux, Windows

CUDA GDB
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, WSL

CUDA Nsight Eclipse Plugin
12.5.82
x86_64
Linux

CUDA NVCC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvdisasm
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA NVML Headers
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvprof
12.5.82
x86_64
Linux, Windows

CUDA nvprune
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVRTC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

NVTX
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVVP
12.5.82
x86_64,
Linux, Windows

CUDA OpenCL
12.5.39
x86_64
Linux, Windows

CUDA Profiler API
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA Compute Sanitizer API
12.5.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuBLAS
12.5.3.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuDLA
12.5.82
aarch64-jetson
Linux

CUDA cuFFT
11.2.3.61
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuFile
1.10.1.7
x86_64, arm64-sbsa, aarch64-jetson
Linux

CUDA cuRAND
10.3.6.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSOLVER
11.6.3.83
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSPARSE
12.5.1.3
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NPP
12.3.0.159
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvFatbin
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJitLink
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJPEG
12.3.2.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

Nsight Compute
2024.2.1.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL (Windows 11)

Nsight Systems
2024.2.3.38
x86_64, arm64-sbsa,
Linux, Windows, WSL

Nsight Visual Studio Edition (VSE)
2024.2.1.24155
x86_64 (Windows)
Windows

nvidia_fs1
2.20.6
x86_64, arm64-sbsa, aarch64-jetson
Linux

Visual Studio Integration
12.5.82
x86_64 (Windows)
Windows

NVIDIA Linux Driver
555.42.06
x86_64, arm64-sbsa
Linux

NVIDIA Windows Driver
555.85
x86_64 (Windows)
Windows, WSL




CUDA DriverRunning a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.
Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.
Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html



Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibilityï






CUDA Toolkit
Minimum Required Driver Version for CUDA Minor Version Compatibility*




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.x
>=525.60.13
>=528.33

CUDA 11.8.x
CUDA 11.7.x
CUDA 11.6.x
CUDA 11.5.x
CUDA 11.4.x
CUDA 11.3.x
CUDA 11.2.x
CUDA 11.1.x
>=450.80.02
>=452.39

CUDA 11.0 (11.0.3)
>=450.36.06**
>=451.22**



* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode â please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

Table 3 CUDA Toolkit and Corresponding Driver Versionsï






CUDA Toolkit
Toolkit Driver Version




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.5 Update 1
>=555.42.06
>=555.85

CUDA 12.5 GA
>=555.42.02
>=555.85

CUDA 12.4 Update 1
>=550.54.15
>=551.78

CUDA 12.4 GA
>=550.54.14
>=551.61

CUDA 12.3 Update 1
>=545.23.08
>=546.12

CUDA 12.3 GA
>=545.23.06
>=545.84

CUDA 12.2 Update 2
>=535.104.05
>=537.13

CUDA 12.2 Update 1
>=535.86.09
>=536.67

CUDA 12.2 GA
>=535.54.03
>=536.25

CUDA 12.1 Update 1
>=530.30.02
>=531.14

CUDA 12.1 GA
>=530.30.02
>=531.14

CUDA 12.0 Update 1
>=525.85.12
>=528.33

CUDA 12.0 GA
>=525.60.13
>=527.41

CUDA 11.8 GA
>=520.61.05
>=520.06

CUDA 11.7 Update 1
>=515.48.07
>=516.31

CUDA 11.7 GA
>=515.43.04
>=516.01

CUDA 11.6 Update 2
>=510.47.03
>=511.65

CUDA 11.6 Update 1
>=510.47.03
>=511.65

CUDA 11.6 GA
>=510.39.01
>=511.23

CUDA 11.5 Update 2
>=495.29.05
>=496.13

CUDA 11.5 Update 1
>=495.29.05
>=496.13

CUDA 11.5 GA
>=495.29.05
>=496.04

CUDA 11.4 Update 4
>=470.82.01
>=472.50

CUDA 11.4 Update 3
>=470.82.01
>=472.50

CUDA 11.4 Update 2
>=470.57.02
>=471.41

CUDA 11.4 Update 1
>=470.57.02
>=471.41

CUDA 11.4.0 GA
>=470.42.01
>=471.11

CUDA 11.3.1 Update 1
>=465.19.01
>=465.89

CUDA 11.3.0 GA
>=465.19.01
>=465.89

CUDA 11.2.2 Update 2
>=460.32.03
>=461.33

CUDA 11.2.1 Update 1
>=460.32.03
>=461.09

CUDA 11.2.0 GA
>=460.27.03
>=460.82

CUDA 11.1.1 Update 1
>=455.32
>=456.81

CUDA 11.1 GA
>=455.23
>=456.38

CUDA 11.0.3 Update 1
>= 450.51.06
>= 451.82

CUDA 11.0.2 GA
>= 450.51.05
>= 451.48

CUDA 11.0.1 RC
>= 450.36.06
>= 451.22

CUDA 10.2.89
>= 440.33
>= 441.22

CUDA 10.1 (10.1.105 general release, and updates)
>= 418.39
>= 418.96

CUDA 10.0.130
>= 410.48
>= 411.31

CUDA 9.2 (9.2.148 Update 1)
>= 396.37
>= 398.26

CUDA 9.2 (9.2.88)
>= 396.26
>= 397.44

CUDA 9.1 (9.1.85)
>= 390.46
>= 391.29

CUDA 9.0 (9.0.76)
>= 384.81
>= 385.54

CUDA 8.0 (8.0.61 GA2)
>= 375.26
>= 376.51

CUDA 8.0 (8.0.44)
>= 367.48
>= 369.30

CUDA 7.5 (7.5.16)
>= 352.31
>= 353.66

CUDA 7.0 (7.0.28)
>= 346.46
>= 347.62



For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.
For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.


1.2. New Featuresï
This section lists new general CUDA and CUDA compilers features.

1.2.1. General CUDAï

In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here.



1.2.2. CUDA Compilerï

For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5.



1.2.3. CUDA Developer Toolsï

For changes to nvprof and Visual Profiler, see the changelog.
For new features, improvements, and bug fixes in Nsight Systems, see the changelog.
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.
For new features, improvements, and bug fixes in CUPTI, see the changelog.
For new features, improvements, and bug fixes in Nsight Compute, see the changelog.
For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.




1.3. Resolved Issuesï

1.3.1. CUDA Compilerï

Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to âMisaligned shared or local addressâ.
Fix to correct the calculation of write-after-read hazard latency.




1.4. Known Issues and Limitationsï

Runfile will not be supported for Amazon Linux 2023.
Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms.



1.5. Deprecated or Dropped Featuresï
Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

1.5.1. Deprecated or Dropped Architecturesï

NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.



1.5.2. Deprecated Operating Systemsï

NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
CUDA 12.5 is the last release to support Debian 10.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.



1.5.3. Deprecated Toolchainsï
CUDA Toolkit 12.4 deprecated support for the following host compilers:


Microsoft Visual C/C++ (MSVC) 2017
All GCC versions prior to GCC 7.3




1.5.4. CUDA Toolsï


Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.






2. CUDA Librariesï
This section covers CUDA Libraries release notes for 12.x releases.

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.


2.1. cuBLAS Libraryï

2.1.1. cuBLAS: Release 12.5 Update 1ï

New Features

Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.


Known Issues

The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.
cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.


Resolved Issues

Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).





2.1.2. cuBLAS: Release 12.5ï

New Features

cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details.


Known Issues

cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.


Resolved Issues

cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.





2.1.3. cuBLAS: Release 12.4 Update 1ï

Known Issues

Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.


Resolved Issues

cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4.
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.





2.1.4. cuBLAS: Release 12.4ï

New Features

cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH.  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details.


Known Issues

When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget().
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.





2.1.5. cuBLAS: Release 12.3 Update 1ï

New Features

Improved performance of heuristics cache for workloads that have a high eviction rate.


Known Issues

BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.


Resolved Issues

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute().
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.





2.1.6. cuBLAS: Release 12.3ï

New Features

Improved performance on NVIDIA L40S Ada GPUs.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release.





2.1.7. cuBLAS: Release 12.2 Update 2ï

New Features

cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.





2.1.8. cuBLAS: Release 12.2ï

Known Issues

cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE.  The kernels apply the first batchâs bias vector to all batches. This will be fixed in a future release.





2.1.9. cuBLAS: Release 12.1 Update 1ï

New Features

Support for FP8 on NVIDIA Ada GPUs.
Improved performance on NVIDIA L4 Ada GPUs.
Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.


Known Issues

When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.





2.1.10. cuBLAS: Release 12.0 Update 1ï

New Features

Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.


Known Issues

For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.


Resolved Issues

Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release.
Added forward compatible single precision complex GEMM that does not require workspace.





2.1.11. cuBLAS: Release 12.0ï

New Features

cublasLtMatmul now supports FP8 with a non-zero beta.
Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

CUBLASLT_EPILOGUE_BGRAD{A,B}
CUBLASLT_EPILOGUE_{RELU,GELU}_AUX
CUBLASLT_EPILOGUE_D{RELU,GELU}


Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.


Known Issues

There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.


Resolved Issues

Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

Deprecations

Disallow including cublas.h and cublas_v2.h in the same translation unit.
Removed:

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.
cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t.
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.
CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.








2.2. cuFFT Libraryï

2.2.1. cuFFT: Release 12.5ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API.







2.2.2. cuFFT: Release 12.4 Update 1ï

Resolved Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.





2.2.3. cuFFT: Release 12.4ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.


Known Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.


Resolved Issues

Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.





2.2.4. cuFFT: Release 12.3 Update 1ï

Known Issues

Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.


Resolved Issues

Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.





2.2.5. cuFFT: Release 12.3ï

New Features

Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes.





2.2.6. cuFFT: Release 12.2ï

New Features

cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.
Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.


Resolved Issues

cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.





2.2.7. cuFFT: Release 12.1 Update 1ï

Known Issues

cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.





2.2.8. cuFFT: Release 12.1ï

New Features

Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.


Known Issues

Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.


Resolved Issues

cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.





2.2.9. cuFFT: Release 12.0 Update 1ï

Resolved Issues

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.





2.2.10. cuFFT: Release 12.0ï

New Features

PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.


Known Issues

cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.


Resolved Issues

cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.






2.3. cuSOLVER Libraryï

2.3.1. cuSOLVER: Release 12.5 Update 1ï

Resolved Issues

The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.





2.3.2. cuSOLVER: Release 12.5ï

New Features

Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.
Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR.
Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.


Known Issues

With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with
auto ALIGN_32=[](int64_t val) {
   return ((val + 31)/32)*32;
};


and
auto sizeofCudaDataType=[](cudaDataType dt) {
   if (dt == CUDA_R_32F) return sizeof(float);
   if (dt == CUDA_R_64F) return sizeof(double);
   if (dt == CUDA_C_32F) return sizeof(cuComplex);
   if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);
};








2.3.3. cuSOLVER: Release 12.4 Update 1ï

New Features

The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.
The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.


Resolved Issues

cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.


Deprecations

Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead.





2.3.4. cuSOLVER: Release 12.4ï

New Features

cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.


Known Issues

cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.





2.3.5. cuSOLVER: Release 12.2 Update 2ï

Resolved Issues

Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to âNâ.





2.3.6. cuSOLVER: Release 12.2ï

New Features

A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp().


Known Issues

Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.






2.4. cuSPARSE Libraryï

2.4.1. cuSPARSE: Release 12.5 Update 1ï

New Features

Added support for BSR format in cusparseSpMM.


Resolved Issues

cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \*= beta. The bug behavior was not modifying C at all.
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.





2.4.2. cuSPARSE: Release 12.5ï

New Features

Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.


Resolved Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.





2.4.3. cuSPARSE: Release 12.4ï

New Features

Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess().
Added support for mixed real and complex types for cusparseSpMM().
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM().


Known Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.


Resolved Issues

cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.





2.4.4. cuSPARSE: Release 12.3 Update 1ï

New Features

Added support for block sizes of 64 and 128 in cusparseSDDMM().
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.





2.4.5. cuSPARSE: Release 12.3ï

New Features

The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.


Known Issues

The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.


Resolved Issues

cusparseSpSV() provided indeterministic results in some cases.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.





2.4.6. cuSPARSE: Release 12.2 Update 1ï

New Features

The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api.


Resolved Issues

Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
Clarified the supported operations for cusparseSDDMM().
cusparseCreateConstSlicedEll() now uses const pointers.
Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.
cusparseSpSM_bufferSize() could ask slightly less memory than needed.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.


Deprecations

Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.





2.4.7. cuSPARSE: Release 12.1 Update 1ï

New Features

Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).
Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.





2.4.8. cuSPARSE: Release 12.0 Update 1ï

New Features

cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
Improved cusparseSpMV() performance with a new load balancing algorithm.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.


Resolved Issues

cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.





2.4.9. cuSPARSE: Release 12.0ï

New Features

JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan().
Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2().
Improved cusparseSpSV() performance for both the analysis and the solving phases.
Improved cusparseSpSM() performance for both the analysis and the solving phases.
Improved cusparseSDDMM() performance and added support for batch computation.
Improved cusparseCsr2cscEx2() performance.


Resolved Issues

cusparseSpSV() and cusparseSpSM() could produce wrong results.
cusparseDnMatGetStridedBatch() did not accept batchStride == 0.


Deprecations

Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.






2.5. Math Libraryï

2.5.1. CUDA Math: Release 12.5ï

Known Issues

As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.





2.5.2. CUDA Math: Release 12.4ï

Resolved Issues

Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.





2.5.3. CUDA Math: Release 12.3ï

New Features

Performance of SIMD Integer CUDA Math APIs was improved.


Resolved Issues

The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.


Known Issues

Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.





2.5.4. CUDA Math: Release 12.2ï

New Features

CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

__CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__
__CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__




Resolved Issues

During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.
Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh().





2.5.5. CUDA Math: Release 12.1ï

New Features

Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.





2.5.6. CUDA Math: Release 12.0ï

New Features

Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.


Known Issues

Double precision inputs that cause the double precision division algorithm in the default âround to nearest even modeâ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code.


Deprecations

All previously deprecated undocumented APIs are removed from CUDA 12.0.






2.6. NVIDIA Performance Primitives (NPP)ï

2.6.1. NPP: Release 12.4ï

New Features

Enhanced large file support with size_t.





2.6.2. NPP: Release 12.0ï

Deprecations

Deprecating non-CTX API support from next release.


Resolved Issues

A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.






2.7. nvJPEG Libraryï

2.7.1. nvJPEG: Release 12.4ï

New Features

IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.





2.7.2. nvJPEG: Release 12.3 Update 1ï

New Features

New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.





2.7.3. nvJPEG: Release 12.2ï

New Features

Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T.





2.7.4. nvJPEG: Release 12.0ï

New Features

Immproved the GPU Memory optimisation for the nvJPEG codec.


Resolved Issues

An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
An issue with CMYK four component color conversion is now resolved.


Known Issues

Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.


Deprecations

The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables).




1
Only available on select Linux distros






3. Noticesï

3.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


3.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


3.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA 12.5 Update 1 Release Notes








































1. CUDA 12.5 Update 1 Release Notes
1.1. CUDA Toolkit Major Component Versions
1.2. New Features
1.2.1. General CUDA
1.2.2. CUDA Compiler
1.2.3. CUDA Developer Tools


1.3. Resolved Issues
1.3.1. CUDA Compiler


1.4. Known Issues and Limitations
1.5. Deprecated or Dropped Features
1.5.1. Deprecated or Dropped Architectures
1.5.2. Deprecated Operating Systems
1.5.3. Deprecated Toolchains
1.5.4. CUDA Tools




2. CUDA Libraries
2.1. cuBLAS Library
2.1.1. cuBLAS: Release 12.5 Update 1
2.1.2. cuBLAS: Release 12.5
2.1.3. cuBLAS: Release 12.4 Update 1
2.1.4. cuBLAS: Release 12.4
2.1.5. cuBLAS: Release 12.3 Update 1
2.1.6. cuBLAS: Release 12.3
2.1.7. cuBLAS: Release 12.2 Update 2
2.1.8. cuBLAS: Release 12.2
2.1.9. cuBLAS: Release 12.1 Update 1
2.1.10. cuBLAS: Release 12.0 Update 1
2.1.11. cuBLAS: Release 12.0


2.2. cuFFT Library
2.2.1. cuFFT: Release 12.5
2.2.2. cuFFT: Release 12.4 Update 1
2.2.3. cuFFT: Release 12.4
2.2.4. cuFFT: Release 12.3 Update 1
2.2.5. cuFFT: Release 12.3
2.2.6. cuFFT: Release 12.2
2.2.7. cuFFT: Release 12.1 Update 1
2.2.8. cuFFT: Release 12.1
2.2.9. cuFFT: Release 12.0 Update 1
2.2.10. cuFFT: Release 12.0


2.3. cuSOLVER Library
2.3.1. cuSOLVER: Release 12.5 Update 1
2.3.2. cuSOLVER: Release 12.5
2.3.3. cuSOLVER: Release 12.4 Update 1
2.3.4. cuSOLVER: Release 12.4
2.3.5. cuSOLVER: Release 12.2 Update 2
2.3.6. cuSOLVER: Release 12.2


2.4. cuSPARSE Library
2.4.1. cuSPARSE: Release 12.5 Update 1
2.4.2. cuSPARSE: Release 12.5
2.4.3. cuSPARSE: Release 12.4
2.4.4. cuSPARSE: Release 12.3 Update 1
2.4.5. cuSPARSE: Release 12.3
2.4.6. cuSPARSE: Release 12.2 Update 1
2.4.7. cuSPARSE: Release 12.1 Update 1
2.4.8. cuSPARSE: Release 12.0 Update 1
2.4.9. cuSPARSE: Release 12.0


2.5. Math Library
2.5.1. CUDA Math: Release 12.5
2.5.2. CUDA Math: Release 12.4
2.5.3. CUDA Math: Release 12.3
2.5.4. CUDA Math: Release 12.2
2.5.5. CUDA Math: Release 12.1
2.5.6. CUDA Math: Release 12.0


2.6. NVIDIA Performance Primitives (NPP)
2.6.1. NPP: Release 12.4
2.6.2. NPP: Release 12.0


2.7. nvJPEG Library
2.7.1. nvJPEG: Release 12.4
2.7.2. nvJPEG: Release 12.3 Update 1
2.7.3. nvJPEG: Release 12.2
2.7.4. nvJPEG: Release 12.0




3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Release Notes





 »
1. CUDA 12.5 Update 1 Release Notes



v12.5 |
PDF
|
Archive
 






NVIDIA CUDA Toolkit Release Notes
The Release Notes for the CUDA Toolkit.

1. CUDA 12.5 Update 1 Release Notesï
The release notes for the NVIDIAÂ® CUDAÂ® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html.

Note
The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.


1.1. CUDA Toolkit Major Component Versionsï

CUDA ComponentsStarting with CUDA 11, the various components in the toolkit are versioned independently.
For CUDA 12.5 Update 1, the table below indicates the versions:



Table 1 CUDA 12.5 Update 1 Component Versionsï








Component Name
Version Information
Supported Architectures
Supported Platforms



CUDA C++ Core Compute Libraries
Thrust
2.4.0
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUB
2.4.0

libcu++
2.4.0

Cooperative Groups
12.5.82

CUDA Compatibility
12.5.36505571
aarch64-jetson
Linux

CUDA Runtime (cudart)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuobjdump
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUPTI
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuxxfilt (demangler)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA Demo Suite
12.5.82
x86_64
Linux, Windows

CUDA GDB
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, WSL

CUDA Nsight Eclipse Plugin
12.5.82
x86_64
Linux

CUDA NVCC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvdisasm
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA NVML Headers
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvprof
12.5.82
x86_64
Linux, Windows

CUDA nvprune
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVRTC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

NVTX
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVVP
12.5.82
x86_64,
Linux, Windows

CUDA OpenCL
12.5.39
x86_64
Linux, Windows

CUDA Profiler API
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA Compute Sanitizer API
12.5.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuBLAS
12.5.3.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuDLA
12.5.82
aarch64-jetson
Linux

CUDA cuFFT
11.2.3.61
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuFile
1.10.1.7
x86_64, arm64-sbsa, aarch64-jetson
Linux

CUDA cuRAND
10.3.6.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSOLVER
11.6.3.83
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSPARSE
12.5.1.3
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NPP
12.3.0.159
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvFatbin
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJitLink
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJPEG
12.3.2.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

Nsight Compute
2024.2.1.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL (Windows 11)

Nsight Systems
2024.2.3.38
x86_64, arm64-sbsa,
Linux, Windows, WSL

Nsight Visual Studio Edition (VSE)
2024.2.1.24155
x86_64 (Windows)
Windows

nvidia_fs1
2.20.6
x86_64, arm64-sbsa, aarch64-jetson
Linux

Visual Studio Integration
12.5.82
x86_64 (Windows)
Windows

NVIDIA Linux Driver
555.42.06
x86_64, arm64-sbsa
Linux

NVIDIA Windows Driver
555.85
x86_64 (Windows)
Windows, WSL




CUDA DriverRunning a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.
Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.
Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html



Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibilityï






CUDA Toolkit
Minimum Required Driver Version for CUDA Minor Version Compatibility*




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.x
>=525.60.13
>=528.33

CUDA 11.8.x
CUDA 11.7.x
CUDA 11.6.x
CUDA 11.5.x
CUDA 11.4.x
CUDA 11.3.x
CUDA 11.2.x
CUDA 11.1.x
>=450.80.02
>=452.39

CUDA 11.0 (11.0.3)
>=450.36.06**
>=451.22**



* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode â please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

Table 3 CUDA Toolkit and Corresponding Driver Versionsï






CUDA Toolkit
Toolkit Driver Version




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.5 Update 1
>=555.42.06
>=555.85

CUDA 12.5 GA
>=555.42.02
>=555.85

CUDA 12.4 Update 1
>=550.54.15
>=551.78

CUDA 12.4 GA
>=550.54.14
>=551.61

CUDA 12.3 Update 1
>=545.23.08
>=546.12

CUDA 12.3 GA
>=545.23.06
>=545.84

CUDA 12.2 Update 2
>=535.104.05
>=537.13

CUDA 12.2 Update 1
>=535.86.09
>=536.67

CUDA 12.2 GA
>=535.54.03
>=536.25

CUDA 12.1 Update 1
>=530.30.02
>=531.14

CUDA 12.1 GA
>=530.30.02
>=531.14

CUDA 12.0 Update 1
>=525.85.12
>=528.33

CUDA 12.0 GA
>=525.60.13
>=527.41

CUDA 11.8 GA
>=520.61.05
>=520.06

CUDA 11.7 Update 1
>=515.48.07
>=516.31

CUDA 11.7 GA
>=515.43.04
>=516.01

CUDA 11.6 Update 2
>=510.47.03
>=511.65

CUDA 11.6 Update 1
>=510.47.03
>=511.65

CUDA 11.6 GA
>=510.39.01
>=511.23

CUDA 11.5 Update 2
>=495.29.05
>=496.13

CUDA 11.5 Update 1
>=495.29.05
>=496.13

CUDA 11.5 GA
>=495.29.05
>=496.04

CUDA 11.4 Update 4
>=470.82.01
>=472.50

CUDA 11.4 Update 3
>=470.82.01
>=472.50

CUDA 11.4 Update 2
>=470.57.02
>=471.41

CUDA 11.4 Update 1
>=470.57.02
>=471.41

CUDA 11.4.0 GA
>=470.42.01
>=471.11

CUDA 11.3.1 Update 1
>=465.19.01
>=465.89

CUDA 11.3.0 GA
>=465.19.01
>=465.89

CUDA 11.2.2 Update 2
>=460.32.03
>=461.33

CUDA 11.2.1 Update 1
>=460.32.03
>=461.09

CUDA 11.2.0 GA
>=460.27.03
>=460.82

CUDA 11.1.1 Update 1
>=455.32
>=456.81

CUDA 11.1 GA
>=455.23
>=456.38

CUDA 11.0.3 Update 1
>= 450.51.06
>= 451.82

CUDA 11.0.2 GA
>= 450.51.05
>= 451.48

CUDA 11.0.1 RC
>= 450.36.06
>= 451.22

CUDA 10.2.89
>= 440.33
>= 441.22

CUDA 10.1 (10.1.105 general release, and updates)
>= 418.39
>= 418.96

CUDA 10.0.130
>= 410.48
>= 411.31

CUDA 9.2 (9.2.148 Update 1)
>= 396.37
>= 398.26

CUDA 9.2 (9.2.88)
>= 396.26
>= 397.44

CUDA 9.1 (9.1.85)
>= 390.46
>= 391.29

CUDA 9.0 (9.0.76)
>= 384.81
>= 385.54

CUDA 8.0 (8.0.61 GA2)
>= 375.26
>= 376.51

CUDA 8.0 (8.0.44)
>= 367.48
>= 369.30

CUDA 7.5 (7.5.16)
>= 352.31
>= 353.66

CUDA 7.0 (7.0.28)
>= 346.46
>= 347.62



For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.
For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.


1.2. New Featuresï
This section lists new general CUDA and CUDA compilers features.

1.2.1. General CUDAï

In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here.



1.2.2. CUDA Compilerï

For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5.



1.2.3. CUDA Developer Toolsï

For changes to nvprof and Visual Profiler, see the changelog.
For new features, improvements, and bug fixes in Nsight Systems, see the changelog.
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.
For new features, improvements, and bug fixes in CUPTI, see the changelog.
For new features, improvements, and bug fixes in Nsight Compute, see the changelog.
For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.




1.3. Resolved Issuesï

1.3.1. CUDA Compilerï

Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to âMisaligned shared or local addressâ.
Fix to correct the calculation of write-after-read hazard latency.




1.4. Known Issues and Limitationsï

Runfile will not be supported for Amazon Linux 2023.
Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms.



1.5. Deprecated or Dropped Featuresï
Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

1.5.1. Deprecated or Dropped Architecturesï

NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.



1.5.2. Deprecated Operating Systemsï

NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
CUDA 12.5 is the last release to support Debian 10.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.



1.5.3. Deprecated Toolchainsï
CUDA Toolkit 12.4 deprecated support for the following host compilers:


Microsoft Visual C/C++ (MSVC) 2017
All GCC versions prior to GCC 7.3




1.5.4. CUDA Toolsï


Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.






2. CUDA Librariesï
This section covers CUDA Libraries release notes for 12.x releases.

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.


2.1. cuBLAS Libraryï

2.1.1. cuBLAS: Release 12.5 Update 1ï

New Features

Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.


Known Issues

The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.
cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.


Resolved Issues

Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).





2.1.2. cuBLAS: Release 12.5ï

New Features

cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details.


Known Issues

cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.


Resolved Issues

cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.





2.1.3. cuBLAS: Release 12.4 Update 1ï

Known Issues

Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.


Resolved Issues

cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4.
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.





2.1.4. cuBLAS: Release 12.4ï

New Features

cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH.  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details.


Known Issues

When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget().
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.





2.1.5. cuBLAS: Release 12.3 Update 1ï

New Features

Improved performance of heuristics cache for workloads that have a high eviction rate.


Known Issues

BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.


Resolved Issues

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute().
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.





2.1.6. cuBLAS: Release 12.3ï

New Features

Improved performance on NVIDIA L40S Ada GPUs.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release.





2.1.7. cuBLAS: Release 12.2 Update 2ï

New Features

cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.





2.1.8. cuBLAS: Release 12.2ï

Known Issues

cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE.  The kernels apply the first batchâs bias vector to all batches. This will be fixed in a future release.





2.1.9. cuBLAS: Release 12.1 Update 1ï

New Features

Support for FP8 on NVIDIA Ada GPUs.
Improved performance on NVIDIA L4 Ada GPUs.
Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.


Known Issues

When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.





2.1.10. cuBLAS: Release 12.0 Update 1ï

New Features

Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.


Known Issues

For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.


Resolved Issues

Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release.
Added forward compatible single precision complex GEMM that does not require workspace.





2.1.11. cuBLAS: Release 12.0ï

New Features

cublasLtMatmul now supports FP8 with a non-zero beta.
Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

CUBLASLT_EPILOGUE_BGRAD{A,B}
CUBLASLT_EPILOGUE_{RELU,GELU}_AUX
CUBLASLT_EPILOGUE_D{RELU,GELU}


Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.


Known Issues

There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.


Resolved Issues

Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

Deprecations

Disallow including cublas.h and cublas_v2.h in the same translation unit.
Removed:

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.
cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t.
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.
CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.








2.2. cuFFT Libraryï

2.2.1. cuFFT: Release 12.5ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API.







2.2.2. cuFFT: Release 12.4 Update 1ï

Resolved Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.





2.2.3. cuFFT: Release 12.4ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.


Known Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.


Resolved Issues

Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.





2.2.4. cuFFT: Release 12.3 Update 1ï

Known Issues

Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.


Resolved Issues

Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.





2.2.5. cuFFT: Release 12.3ï

New Features

Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes.





2.2.6. cuFFT: Release 12.2ï

New Features

cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.
Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.


Resolved Issues

cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.





2.2.7. cuFFT: Release 12.1 Update 1ï

Known Issues

cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.





2.2.8. cuFFT: Release 12.1ï

New Features

Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.


Known Issues

Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.


Resolved Issues

cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.





2.2.9. cuFFT: Release 12.0 Update 1ï

Resolved Issues

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.





2.2.10. cuFFT: Release 12.0ï

New Features

PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.


Known Issues

cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.


Resolved Issues

cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.






2.3. cuSOLVER Libraryï

2.3.1. cuSOLVER: Release 12.5 Update 1ï

Resolved Issues

The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.





2.3.2. cuSOLVER: Release 12.5ï

New Features

Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.
Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR.
Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.


Known Issues

With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with
auto ALIGN_32=[](int64_t val) {
   return ((val + 31)/32)*32;
};


and
auto sizeofCudaDataType=[](cudaDataType dt) {
   if (dt == CUDA_R_32F) return sizeof(float);
   if (dt == CUDA_R_64F) return sizeof(double);
   if (dt == CUDA_C_32F) return sizeof(cuComplex);
   if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);
};








2.3.3. cuSOLVER: Release 12.4 Update 1ï

New Features

The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.
The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.


Resolved Issues

cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.


Deprecations

Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead.





2.3.4. cuSOLVER: Release 12.4ï

New Features

cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.


Known Issues

cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.





2.3.5. cuSOLVER: Release 12.2 Update 2ï

Resolved Issues

Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to âNâ.





2.3.6. cuSOLVER: Release 12.2ï

New Features

A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp().


Known Issues

Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.






2.4. cuSPARSE Libraryï

2.4.1. cuSPARSE: Release 12.5 Update 1ï

New Features

Added support for BSR format in cusparseSpMM.


Resolved Issues

cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \*= beta. The bug behavior was not modifying C at all.
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.





2.4.2. cuSPARSE: Release 12.5ï

New Features

Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.


Resolved Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.





2.4.3. cuSPARSE: Release 12.4ï

New Features

Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess().
Added support for mixed real and complex types for cusparseSpMM().
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM().


Known Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.


Resolved Issues

cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.





2.4.4. cuSPARSE: Release 12.3 Update 1ï

New Features

Added support for block sizes of 64 and 128 in cusparseSDDMM().
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.





2.4.5. cuSPARSE: Release 12.3ï

New Features

The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.


Known Issues

The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.


Resolved Issues

cusparseSpSV() provided indeterministic results in some cases.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.





2.4.6. cuSPARSE: Release 12.2 Update 1ï

New Features

The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api.


Resolved Issues

Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
Clarified the supported operations for cusparseSDDMM().
cusparseCreateConstSlicedEll() now uses const pointers.
Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.
cusparseSpSM_bufferSize() could ask slightly less memory than needed.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.


Deprecations

Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.





2.4.7. cuSPARSE: Release 12.1 Update 1ï

New Features

Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).
Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.





2.4.8. cuSPARSE: Release 12.0 Update 1ï

New Features

cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
Improved cusparseSpMV() performance with a new load balancing algorithm.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.


Resolved Issues

cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.





2.4.9. cuSPARSE: Release 12.0ï

New Features

JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan().
Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2().
Improved cusparseSpSV() performance for both the analysis and the solving phases.
Improved cusparseSpSM() performance for both the analysis and the solving phases.
Improved cusparseSDDMM() performance and added support for batch computation.
Improved cusparseCsr2cscEx2() performance.


Resolved Issues

cusparseSpSV() and cusparseSpSM() could produce wrong results.
cusparseDnMatGetStridedBatch() did not accept batchStride == 0.


Deprecations

Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.






2.5. Math Libraryï

2.5.1. CUDA Math: Release 12.5ï

Known Issues

As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.





2.5.2. CUDA Math: Release 12.4ï

Resolved Issues

Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.





2.5.3. CUDA Math: Release 12.3ï

New Features

Performance of SIMD Integer CUDA Math APIs was improved.


Resolved Issues

The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.


Known Issues

Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.





2.5.4. CUDA Math: Release 12.2ï

New Features

CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

__CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__
__CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__




Resolved Issues

During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.
Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh().





2.5.5. CUDA Math: Release 12.1ï

New Features

Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.





2.5.6. CUDA Math: Release 12.0ï

New Features

Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.


Known Issues

Double precision inputs that cause the double precision division algorithm in the default âround to nearest even modeâ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code.


Deprecations

All previously deprecated undocumented APIs are removed from CUDA 12.0.






2.6. NVIDIA Performance Primitives (NPP)ï

2.6.1. NPP: Release 12.4ï

New Features

Enhanced large file support with size_t.





2.6.2. NPP: Release 12.0ï

Deprecations

Deprecating non-CTX API support from next release.


Resolved Issues

A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.






2.7. nvJPEG Libraryï

2.7.1. nvJPEG: Release 12.4ï

New Features

IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.





2.7.2. nvJPEG: Release 12.3 Update 1ï

New Features

New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.





2.7.3. nvJPEG: Release 12.2ï

New Features

Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T.





2.7.4. nvJPEG: Release 12.0ï

New Features

Immproved the GPU Memory optimisation for the nvJPEG codec.


Resolved Issues

An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
An issue with CMYK four component color conversion is now resolved.


Known Issues

Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.


Deprecations

The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables).




1
Only available on select Linux distros






3. Noticesï

3.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


3.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


3.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA 12.5 Update 1 Release Notes








































1. CUDA 12.5 Update 1 Release Notes
1.1. CUDA Toolkit Major Component Versions
1.2. New Features
1.2.1. General CUDA
1.2.2. CUDA Compiler
1.2.3. CUDA Developer Tools


1.3. Resolved Issues
1.3.1. CUDA Compiler


1.4. Known Issues and Limitations
1.5. Deprecated or Dropped Features
1.5.1. Deprecated or Dropped Architectures
1.5.2. Deprecated Operating Systems
1.5.3. Deprecated Toolchains
1.5.4. CUDA Tools




2. CUDA Libraries
2.1. cuBLAS Library
2.1.1. cuBLAS: Release 12.5 Update 1
2.1.2. cuBLAS: Release 12.5
2.1.3. cuBLAS: Release 12.4 Update 1
2.1.4. cuBLAS: Release 12.4
2.1.5. cuBLAS: Release 12.3 Update 1
2.1.6. cuBLAS: Release 12.3
2.1.7. cuBLAS: Release 12.2 Update 2
2.1.8. cuBLAS: Release 12.2
2.1.9. cuBLAS: Release 12.1 Update 1
2.1.10. cuBLAS: Release 12.0 Update 1
2.1.11. cuBLAS: Release 12.0


2.2. cuFFT Library
2.2.1. cuFFT: Release 12.5
2.2.2. cuFFT: Release 12.4 Update 1
2.2.3. cuFFT: Release 12.4
2.2.4. cuFFT: Release 12.3 Update 1
2.2.5. cuFFT: Release 12.3
2.2.6. cuFFT: Release 12.2
2.2.7. cuFFT: Release 12.1 Update 1
2.2.8. cuFFT: Release 12.1
2.2.9. cuFFT: Release 12.0 Update 1
2.2.10. cuFFT: Release 12.0


2.3. cuSOLVER Library
2.3.1. cuSOLVER: Release 12.5 Update 1
2.3.2. cuSOLVER: Release 12.5
2.3.3. cuSOLVER: Release 12.4 Update 1
2.3.4. cuSOLVER: Release 12.4
2.3.5. cuSOLVER: Release 12.2 Update 2
2.3.6. cuSOLVER: Release 12.2


2.4. cuSPARSE Library
2.4.1. cuSPARSE: Release 12.5 Update 1
2.4.2. cuSPARSE: Release 12.5
2.4.3. cuSPARSE: Release 12.4
2.4.4. cuSPARSE: Release 12.3 Update 1
2.4.5. cuSPARSE: Release 12.3
2.4.6. cuSPARSE: Release 12.2 Update 1
2.4.7. cuSPARSE: Release 12.1 Update 1
2.4.8. cuSPARSE: Release 12.0 Update 1
2.4.9. cuSPARSE: Release 12.0


2.5. Math Library
2.5.1. CUDA Math: Release 12.5
2.5.2. CUDA Math: Release 12.4
2.5.3. CUDA Math: Release 12.3
2.5.4. CUDA Math: Release 12.2
2.5.5. CUDA Math: Release 12.1
2.5.6. CUDA Math: Release 12.0


2.6. NVIDIA Performance Primitives (NPP)
2.6.1. NPP: Release 12.4
2.6.2. NPP: Release 12.0


2.7. nvJPEG Library
2.7.1. nvJPEG: Release 12.4
2.7.2. nvJPEG: Release 12.3 Update 1
2.7.3. nvJPEG: Release 12.2
2.7.4. nvJPEG: Release 12.0




3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Release Notes





 »
1. CUDA 12.5 Update 1 Release Notes



v12.5 |
PDF
|
Archive
 






NVIDIA CUDA Toolkit Release Notes
The Release Notes for the CUDA Toolkit.

1. CUDA 12.5 Update 1 Release Notesï
The release notes for the NVIDIAÂ® CUDAÂ® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html.

Note
The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.


1.1. CUDA Toolkit Major Component Versionsï

CUDA ComponentsStarting with CUDA 11, the various components in the toolkit are versioned independently.
For CUDA 12.5 Update 1, the table below indicates the versions:



Table 1 CUDA 12.5 Update 1 Component Versionsï








Component Name
Version Information
Supported Architectures
Supported Platforms



CUDA C++ Core Compute Libraries
Thrust
2.4.0
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUB
2.4.0

libcu++
2.4.0

Cooperative Groups
12.5.82

CUDA Compatibility
12.5.36505571
aarch64-jetson
Linux

CUDA Runtime (cudart)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuobjdump
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUPTI
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuxxfilt (demangler)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA Demo Suite
12.5.82
x86_64
Linux, Windows

CUDA GDB
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, WSL

CUDA Nsight Eclipse Plugin
12.5.82
x86_64
Linux

CUDA NVCC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvdisasm
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA NVML Headers
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvprof
12.5.82
x86_64
Linux, Windows

CUDA nvprune
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVRTC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

NVTX
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVVP
12.5.82
x86_64,
Linux, Windows

CUDA OpenCL
12.5.39
x86_64
Linux, Windows

CUDA Profiler API
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA Compute Sanitizer API
12.5.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuBLAS
12.5.3.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuDLA
12.5.82
aarch64-jetson
Linux

CUDA cuFFT
11.2.3.61
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuFile
1.10.1.7
x86_64, arm64-sbsa, aarch64-jetson
Linux

CUDA cuRAND
10.3.6.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSOLVER
11.6.3.83
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSPARSE
12.5.1.3
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NPP
12.3.0.159
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvFatbin
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJitLink
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJPEG
12.3.2.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

Nsight Compute
2024.2.1.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL (Windows 11)

Nsight Systems
2024.2.3.38
x86_64, arm64-sbsa,
Linux, Windows, WSL

Nsight Visual Studio Edition (VSE)
2024.2.1.24155
x86_64 (Windows)
Windows

nvidia_fs1
2.20.6
x86_64, arm64-sbsa, aarch64-jetson
Linux

Visual Studio Integration
12.5.82
x86_64 (Windows)
Windows

NVIDIA Linux Driver
555.42.06
x86_64, arm64-sbsa
Linux

NVIDIA Windows Driver
555.85
x86_64 (Windows)
Windows, WSL




CUDA DriverRunning a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.
Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.
Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html



Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibilityï






CUDA Toolkit
Minimum Required Driver Version for CUDA Minor Version Compatibility*




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.x
>=525.60.13
>=528.33

CUDA 11.8.x
CUDA 11.7.x
CUDA 11.6.x
CUDA 11.5.x
CUDA 11.4.x
CUDA 11.3.x
CUDA 11.2.x
CUDA 11.1.x
>=450.80.02
>=452.39

CUDA 11.0 (11.0.3)
>=450.36.06**
>=451.22**



* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode â please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

Table 3 CUDA Toolkit and Corresponding Driver Versionsï






CUDA Toolkit
Toolkit Driver Version




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.5 Update 1
>=555.42.06
>=555.85

CUDA 12.5 GA
>=555.42.02
>=555.85

CUDA 12.4 Update 1
>=550.54.15
>=551.78

CUDA 12.4 GA
>=550.54.14
>=551.61

CUDA 12.3 Update 1
>=545.23.08
>=546.12

CUDA 12.3 GA
>=545.23.06
>=545.84

CUDA 12.2 Update 2
>=535.104.05
>=537.13

CUDA 12.2 Update 1
>=535.86.09
>=536.67

CUDA 12.2 GA
>=535.54.03
>=536.25

CUDA 12.1 Update 1
>=530.30.02
>=531.14

CUDA 12.1 GA
>=530.30.02
>=531.14

CUDA 12.0 Update 1
>=525.85.12
>=528.33

CUDA 12.0 GA
>=525.60.13
>=527.41

CUDA 11.8 GA
>=520.61.05
>=520.06

CUDA 11.7 Update 1
>=515.48.07
>=516.31

CUDA 11.7 GA
>=515.43.04
>=516.01

CUDA 11.6 Update 2
>=510.47.03
>=511.65

CUDA 11.6 Update 1
>=510.47.03
>=511.65

CUDA 11.6 GA
>=510.39.01
>=511.23

CUDA 11.5 Update 2
>=495.29.05
>=496.13

CUDA 11.5 Update 1
>=495.29.05
>=496.13

CUDA 11.5 GA
>=495.29.05
>=496.04

CUDA 11.4 Update 4
>=470.82.01
>=472.50

CUDA 11.4 Update 3
>=470.82.01
>=472.50

CUDA 11.4 Update 2
>=470.57.02
>=471.41

CUDA 11.4 Update 1
>=470.57.02
>=471.41

CUDA 11.4.0 GA
>=470.42.01
>=471.11

CUDA 11.3.1 Update 1
>=465.19.01
>=465.89

CUDA 11.3.0 GA
>=465.19.01
>=465.89

CUDA 11.2.2 Update 2
>=460.32.03
>=461.33

CUDA 11.2.1 Update 1
>=460.32.03
>=461.09

CUDA 11.2.0 GA
>=460.27.03
>=460.82

CUDA 11.1.1 Update 1
>=455.32
>=456.81

CUDA 11.1 GA
>=455.23
>=456.38

CUDA 11.0.3 Update 1
>= 450.51.06
>= 451.82

CUDA 11.0.2 GA
>= 450.51.05
>= 451.48

CUDA 11.0.1 RC
>= 450.36.06
>= 451.22

CUDA 10.2.89
>= 440.33
>= 441.22

CUDA 10.1 (10.1.105 general release, and updates)
>= 418.39
>= 418.96

CUDA 10.0.130
>= 410.48
>= 411.31

CUDA 9.2 (9.2.148 Update 1)
>= 396.37
>= 398.26

CUDA 9.2 (9.2.88)
>= 396.26
>= 397.44

CUDA 9.1 (9.1.85)
>= 390.46
>= 391.29

CUDA 9.0 (9.0.76)
>= 384.81
>= 385.54

CUDA 8.0 (8.0.61 GA2)
>= 375.26
>= 376.51

CUDA 8.0 (8.0.44)
>= 367.48
>= 369.30

CUDA 7.5 (7.5.16)
>= 352.31
>= 353.66

CUDA 7.0 (7.0.28)
>= 346.46
>= 347.62



For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.
For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.


1.2. New Featuresï
This section lists new general CUDA and CUDA compilers features.

1.2.1. General CUDAï

In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here.



1.2.2. CUDA Compilerï

For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5.



1.2.3. CUDA Developer Toolsï

For changes to nvprof and Visual Profiler, see the changelog.
For new features, improvements, and bug fixes in Nsight Systems, see the changelog.
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.
For new features, improvements, and bug fixes in CUPTI, see the changelog.
For new features, improvements, and bug fixes in Nsight Compute, see the changelog.
For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.




1.3. Resolved Issuesï

1.3.1. CUDA Compilerï

Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to âMisaligned shared or local addressâ.
Fix to correct the calculation of write-after-read hazard latency.




1.4. Known Issues and Limitationsï

Runfile will not be supported for Amazon Linux 2023.
Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms.



1.5. Deprecated or Dropped Featuresï
Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

1.5.1. Deprecated or Dropped Architecturesï

NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.



1.5.2. Deprecated Operating Systemsï

NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
CUDA 12.5 is the last release to support Debian 10.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.



1.5.3. Deprecated Toolchainsï
CUDA Toolkit 12.4 deprecated support for the following host compilers:


Microsoft Visual C/C++ (MSVC) 2017
All GCC versions prior to GCC 7.3




1.5.4. CUDA Toolsï


Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.






2. CUDA Librariesï
This section covers CUDA Libraries release notes for 12.x releases.

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.


2.1. cuBLAS Libraryï

2.1.1. cuBLAS: Release 12.5 Update 1ï

New Features

Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.


Known Issues

The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.
cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.


Resolved Issues

Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).





2.1.2. cuBLAS: Release 12.5ï

New Features

cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details.


Known Issues

cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.


Resolved Issues

cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.





2.1.3. cuBLAS: Release 12.4 Update 1ï

Known Issues

Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.


Resolved Issues

cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4.
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.





2.1.4. cuBLAS: Release 12.4ï

New Features

cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH.  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details.


Known Issues

When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget().
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.





2.1.5. cuBLAS: Release 12.3 Update 1ï

New Features

Improved performance of heuristics cache for workloads that have a high eviction rate.


Known Issues

BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.


Resolved Issues

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute().
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.





2.1.6. cuBLAS: Release 12.3ï

New Features

Improved performance on NVIDIA L40S Ada GPUs.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release.





2.1.7. cuBLAS: Release 12.2 Update 2ï

New Features

cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.





2.1.8. cuBLAS: Release 12.2ï

Known Issues

cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE.  The kernels apply the first batchâs bias vector to all batches. This will be fixed in a future release.





2.1.9. cuBLAS: Release 12.1 Update 1ï

New Features

Support for FP8 on NVIDIA Ada GPUs.
Improved performance on NVIDIA L4 Ada GPUs.
Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.


Known Issues

When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.





2.1.10. cuBLAS: Release 12.0 Update 1ï

New Features

Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.


Known Issues

For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.


Resolved Issues

Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release.
Added forward compatible single precision complex GEMM that does not require workspace.





2.1.11. cuBLAS: Release 12.0ï

New Features

cublasLtMatmul now supports FP8 with a non-zero beta.
Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

CUBLASLT_EPILOGUE_BGRAD{A,B}
CUBLASLT_EPILOGUE_{RELU,GELU}_AUX
CUBLASLT_EPILOGUE_D{RELU,GELU}


Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.


Known Issues

There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.


Resolved Issues

Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

Deprecations

Disallow including cublas.h and cublas_v2.h in the same translation unit.
Removed:

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.
cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t.
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.
CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.








2.2. cuFFT Libraryï

2.2.1. cuFFT: Release 12.5ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API.







2.2.2. cuFFT: Release 12.4 Update 1ï

Resolved Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.





2.2.3. cuFFT: Release 12.4ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.


Known Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.


Resolved Issues

Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.





2.2.4. cuFFT: Release 12.3 Update 1ï

Known Issues

Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.


Resolved Issues

Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.





2.2.5. cuFFT: Release 12.3ï

New Features

Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes.





2.2.6. cuFFT: Release 12.2ï

New Features

cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.
Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.


Resolved Issues

cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.





2.2.7. cuFFT: Release 12.1 Update 1ï

Known Issues

cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.





2.2.8. cuFFT: Release 12.1ï

New Features

Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.


Known Issues

Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.


Resolved Issues

cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.





2.2.9. cuFFT: Release 12.0 Update 1ï

Resolved Issues

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.





2.2.10. cuFFT: Release 12.0ï

New Features

PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.


Known Issues

cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.


Resolved Issues

cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.






2.3. cuSOLVER Libraryï

2.3.1. cuSOLVER: Release 12.5 Update 1ï

Resolved Issues

The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.





2.3.2. cuSOLVER: Release 12.5ï

New Features

Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.
Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR.
Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.


Known Issues

With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with
auto ALIGN_32=[](int64_t val) {
   return ((val + 31)/32)*32;
};


and
auto sizeofCudaDataType=[](cudaDataType dt) {
   if (dt == CUDA_R_32F) return sizeof(float);
   if (dt == CUDA_R_64F) return sizeof(double);
   if (dt == CUDA_C_32F) return sizeof(cuComplex);
   if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);
};








2.3.3. cuSOLVER: Release 12.4 Update 1ï

New Features

The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.
The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.


Resolved Issues

cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.


Deprecations

Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead.





2.3.4. cuSOLVER: Release 12.4ï

New Features

cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.


Known Issues

cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.





2.3.5. cuSOLVER: Release 12.2 Update 2ï

Resolved Issues

Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to âNâ.





2.3.6. cuSOLVER: Release 12.2ï

New Features

A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp().


Known Issues

Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.






2.4. cuSPARSE Libraryï

2.4.1. cuSPARSE: Release 12.5 Update 1ï

New Features

Added support for BSR format in cusparseSpMM.


Resolved Issues

cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \*= beta. The bug behavior was not modifying C at all.
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.





2.4.2. cuSPARSE: Release 12.5ï

New Features

Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.


Resolved Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.





2.4.3. cuSPARSE: Release 12.4ï

New Features

Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess().
Added support for mixed real and complex types for cusparseSpMM().
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM().


Known Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.


Resolved Issues

cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.





2.4.4. cuSPARSE: Release 12.3 Update 1ï

New Features

Added support for block sizes of 64 and 128 in cusparseSDDMM().
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.





2.4.5. cuSPARSE: Release 12.3ï

New Features

The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.


Known Issues

The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.


Resolved Issues

cusparseSpSV() provided indeterministic results in some cases.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.





2.4.6. cuSPARSE: Release 12.2 Update 1ï

New Features

The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api.


Resolved Issues

Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
Clarified the supported operations for cusparseSDDMM().
cusparseCreateConstSlicedEll() now uses const pointers.
Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.
cusparseSpSM_bufferSize() could ask slightly less memory than needed.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.


Deprecations

Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.





2.4.7. cuSPARSE: Release 12.1 Update 1ï

New Features

Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).
Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.





2.4.8. cuSPARSE: Release 12.0 Update 1ï

New Features

cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
Improved cusparseSpMV() performance with a new load balancing algorithm.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.


Resolved Issues

cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.





2.4.9. cuSPARSE: Release 12.0ï

New Features

JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan().
Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2().
Improved cusparseSpSV() performance for both the analysis and the solving phases.
Improved cusparseSpSM() performance for both the analysis and the solving phases.
Improved cusparseSDDMM() performance and added support for batch computation.
Improved cusparseCsr2cscEx2() performance.


Resolved Issues

cusparseSpSV() and cusparseSpSM() could produce wrong results.
cusparseDnMatGetStridedBatch() did not accept batchStride == 0.


Deprecations

Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.






2.5. Math Libraryï

2.5.1. CUDA Math: Release 12.5ï

Known Issues

As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.





2.5.2. CUDA Math: Release 12.4ï

Resolved Issues

Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.





2.5.3. CUDA Math: Release 12.3ï

New Features

Performance of SIMD Integer CUDA Math APIs was improved.


Resolved Issues

The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.


Known Issues

Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.





2.5.4. CUDA Math: Release 12.2ï

New Features

CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

__CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__
__CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__




Resolved Issues

During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.
Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh().





2.5.5. CUDA Math: Release 12.1ï

New Features

Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.





2.5.6. CUDA Math: Release 12.0ï

New Features

Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.


Known Issues

Double precision inputs that cause the double precision division algorithm in the default âround to nearest even modeâ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code.


Deprecations

All previously deprecated undocumented APIs are removed from CUDA 12.0.






2.6. NVIDIA Performance Primitives (NPP)ï

2.6.1. NPP: Release 12.4ï

New Features

Enhanced large file support with size_t.





2.6.2. NPP: Release 12.0ï

Deprecations

Deprecating non-CTX API support from next release.


Resolved Issues

A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.






2.7. nvJPEG Libraryï

2.7.1. nvJPEG: Release 12.4ï

New Features

IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.





2.7.2. nvJPEG: Release 12.3 Update 1ï

New Features

New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.





2.7.3. nvJPEG: Release 12.2ï

New Features

Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T.





2.7.4. nvJPEG: Release 12.0ï

New Features

Immproved the GPU Memory optimisation for the nvJPEG codec.


Resolved Issues

An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
An issue with CMYK four component color conversion is now resolved.


Known Issues

Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.


Deprecations

The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables).




1
Only available on select Linux distros






3. Noticesï

3.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


3.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


3.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA 12.5 Update 1 Release Notes








































1. CUDA 12.5 Update 1 Release Notes
1.1. CUDA Toolkit Major Component Versions
1.2. New Features
1.2.1. General CUDA
1.2.2. CUDA Compiler
1.2.3. CUDA Developer Tools


1.3. Resolved Issues
1.3.1. CUDA Compiler


1.4. Known Issues and Limitations
1.5. Deprecated or Dropped Features
1.5.1. Deprecated or Dropped Architectures
1.5.2. Deprecated Operating Systems
1.5.3. Deprecated Toolchains
1.5.4. CUDA Tools




2. CUDA Libraries
2.1. cuBLAS Library
2.1.1. cuBLAS: Release 12.5 Update 1
2.1.2. cuBLAS: Release 12.5
2.1.3. cuBLAS: Release 12.4 Update 1
2.1.4. cuBLAS: Release 12.4
2.1.5. cuBLAS: Release 12.3 Update 1
2.1.6. cuBLAS: Release 12.3
2.1.7. cuBLAS: Release 12.2 Update 2
2.1.8. cuBLAS: Release 12.2
2.1.9. cuBLAS: Release 12.1 Update 1
2.1.10. cuBLAS: Release 12.0 Update 1
2.1.11. cuBLAS: Release 12.0


2.2. cuFFT Library
2.2.1. cuFFT: Release 12.5
2.2.2. cuFFT: Release 12.4 Update 1
2.2.3. cuFFT: Release 12.4
2.2.4. cuFFT: Release 12.3 Update 1
2.2.5. cuFFT: Release 12.3
2.2.6. cuFFT: Release 12.2
2.2.7. cuFFT: Release 12.1 Update 1
2.2.8. cuFFT: Release 12.1
2.2.9. cuFFT: Release 12.0 Update 1
2.2.10. cuFFT: Release 12.0


2.3. cuSOLVER Library
2.3.1. cuSOLVER: Release 12.5 Update 1
2.3.2. cuSOLVER: Release 12.5
2.3.3. cuSOLVER: Release 12.4 Update 1
2.3.4. cuSOLVER: Release 12.4
2.3.5. cuSOLVER: Release 12.2 Update 2
2.3.6. cuSOLVER: Release 12.2


2.4. cuSPARSE Library
2.4.1. cuSPARSE: Release 12.5 Update 1
2.4.2. cuSPARSE: Release 12.5
2.4.3. cuSPARSE: Release 12.4
2.4.4. cuSPARSE: Release 12.3 Update 1
2.4.5. cuSPARSE: Release 12.3
2.4.6. cuSPARSE: Release 12.2 Update 1
2.4.7. cuSPARSE: Release 12.1 Update 1
2.4.8. cuSPARSE: Release 12.0 Update 1
2.4.9. cuSPARSE: Release 12.0


2.5. Math Library
2.5.1. CUDA Math: Release 12.5
2.5.2. CUDA Math: Release 12.4
2.5.3. CUDA Math: Release 12.3
2.5.4. CUDA Math: Release 12.2
2.5.5. CUDA Math: Release 12.1
2.5.6. CUDA Math: Release 12.0


2.6. NVIDIA Performance Primitives (NPP)
2.6.1. NPP: Release 12.4
2.6.2. NPP: Release 12.0


2.7. nvJPEG Library
2.7.1. nvJPEG: Release 12.4
2.7.2. nvJPEG: Release 12.3 Update 1
2.7.3. nvJPEG: Release 12.2
2.7.4. nvJPEG: Release 12.0




3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Release Notes





 »
1. CUDA 12.5 Update 1 Release Notes



v12.5 |
PDF
|
Archive
 






NVIDIA CUDA Toolkit Release Notes
The Release Notes for the CUDA Toolkit.

1. CUDA 12.5 Update 1 Release Notesï
The release notes for the NVIDIAÂ® CUDAÂ® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html.

Note
The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.


1.1. CUDA Toolkit Major Component Versionsï

CUDA ComponentsStarting with CUDA 11, the various components in the toolkit are versioned independently.
For CUDA 12.5 Update 1, the table below indicates the versions:



Table 1 CUDA 12.5 Update 1 Component Versionsï








Component Name
Version Information
Supported Architectures
Supported Platforms



CUDA C++ Core Compute Libraries
Thrust
2.4.0
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUB
2.4.0

libcu++
2.4.0

Cooperative Groups
12.5.82

CUDA Compatibility
12.5.36505571
aarch64-jetson
Linux

CUDA Runtime (cudart)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuobjdump
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUPTI
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuxxfilt (demangler)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA Demo Suite
12.5.82
x86_64
Linux, Windows

CUDA GDB
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, WSL

CUDA Nsight Eclipse Plugin
12.5.82
x86_64
Linux

CUDA NVCC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvdisasm
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA NVML Headers
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvprof
12.5.82
x86_64
Linux, Windows

CUDA nvprune
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVRTC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

NVTX
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVVP
12.5.82
x86_64,
Linux, Windows

CUDA OpenCL
12.5.39
x86_64
Linux, Windows

CUDA Profiler API
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA Compute Sanitizer API
12.5.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuBLAS
12.5.3.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuDLA
12.5.82
aarch64-jetson
Linux

CUDA cuFFT
11.2.3.61
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuFile
1.10.1.7
x86_64, arm64-sbsa, aarch64-jetson
Linux

CUDA cuRAND
10.3.6.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSOLVER
11.6.3.83
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSPARSE
12.5.1.3
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NPP
12.3.0.159
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvFatbin
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJitLink
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJPEG
12.3.2.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

Nsight Compute
2024.2.1.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL (Windows 11)

Nsight Systems
2024.2.3.38
x86_64, arm64-sbsa,
Linux, Windows, WSL

Nsight Visual Studio Edition (VSE)
2024.2.1.24155
x86_64 (Windows)
Windows

nvidia_fs1
2.20.6
x86_64, arm64-sbsa, aarch64-jetson
Linux

Visual Studio Integration
12.5.82
x86_64 (Windows)
Windows

NVIDIA Linux Driver
555.42.06
x86_64, arm64-sbsa
Linux

NVIDIA Windows Driver
555.85
x86_64 (Windows)
Windows, WSL




CUDA DriverRunning a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.
Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.
Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html



Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibilityï






CUDA Toolkit
Minimum Required Driver Version for CUDA Minor Version Compatibility*




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.x
>=525.60.13
>=528.33

CUDA 11.8.x
CUDA 11.7.x
CUDA 11.6.x
CUDA 11.5.x
CUDA 11.4.x
CUDA 11.3.x
CUDA 11.2.x
CUDA 11.1.x
>=450.80.02
>=452.39

CUDA 11.0 (11.0.3)
>=450.36.06**
>=451.22**



* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode â please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

Table 3 CUDA Toolkit and Corresponding Driver Versionsï






CUDA Toolkit
Toolkit Driver Version




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.5 Update 1
>=555.42.06
>=555.85

CUDA 12.5 GA
>=555.42.02
>=555.85

CUDA 12.4 Update 1
>=550.54.15
>=551.78

CUDA 12.4 GA
>=550.54.14
>=551.61

CUDA 12.3 Update 1
>=545.23.08
>=546.12

CUDA 12.3 GA
>=545.23.06
>=545.84

CUDA 12.2 Update 2
>=535.104.05
>=537.13

CUDA 12.2 Update 1
>=535.86.09
>=536.67

CUDA 12.2 GA
>=535.54.03
>=536.25

CUDA 12.1 Update 1
>=530.30.02
>=531.14

CUDA 12.1 GA
>=530.30.02
>=531.14

CUDA 12.0 Update 1
>=525.85.12
>=528.33

CUDA 12.0 GA
>=525.60.13
>=527.41

CUDA 11.8 GA
>=520.61.05
>=520.06

CUDA 11.7 Update 1
>=515.48.07
>=516.31

CUDA 11.7 GA
>=515.43.04
>=516.01

CUDA 11.6 Update 2
>=510.47.03
>=511.65

CUDA 11.6 Update 1
>=510.47.03
>=511.65

CUDA 11.6 GA
>=510.39.01
>=511.23

CUDA 11.5 Update 2
>=495.29.05
>=496.13

CUDA 11.5 Update 1
>=495.29.05
>=496.13

CUDA 11.5 GA
>=495.29.05
>=496.04

CUDA 11.4 Update 4
>=470.82.01
>=472.50

CUDA 11.4 Update 3
>=470.82.01
>=472.50

CUDA 11.4 Update 2
>=470.57.02
>=471.41

CUDA 11.4 Update 1
>=470.57.02
>=471.41

CUDA 11.4.0 GA
>=470.42.01
>=471.11

CUDA 11.3.1 Update 1
>=465.19.01
>=465.89

CUDA 11.3.0 GA
>=465.19.01
>=465.89

CUDA 11.2.2 Update 2
>=460.32.03
>=461.33

CUDA 11.2.1 Update 1
>=460.32.03
>=461.09

CUDA 11.2.0 GA
>=460.27.03
>=460.82

CUDA 11.1.1 Update 1
>=455.32
>=456.81

CUDA 11.1 GA
>=455.23
>=456.38

CUDA 11.0.3 Update 1
>= 450.51.06
>= 451.82

CUDA 11.0.2 GA
>= 450.51.05
>= 451.48

CUDA 11.0.1 RC
>= 450.36.06
>= 451.22

CUDA 10.2.89
>= 440.33
>= 441.22

CUDA 10.1 (10.1.105 general release, and updates)
>= 418.39
>= 418.96

CUDA 10.0.130
>= 410.48
>= 411.31

CUDA 9.2 (9.2.148 Update 1)
>= 396.37
>= 398.26

CUDA 9.2 (9.2.88)
>= 396.26
>= 397.44

CUDA 9.1 (9.1.85)
>= 390.46
>= 391.29

CUDA 9.0 (9.0.76)
>= 384.81
>= 385.54

CUDA 8.0 (8.0.61 GA2)
>= 375.26
>= 376.51

CUDA 8.0 (8.0.44)
>= 367.48
>= 369.30

CUDA 7.5 (7.5.16)
>= 352.31
>= 353.66

CUDA 7.0 (7.0.28)
>= 346.46
>= 347.62



For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.
For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.


1.2. New Featuresï
This section lists new general CUDA and CUDA compilers features.

1.2.1. General CUDAï

In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here.



1.2.2. CUDA Compilerï

For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5.



1.2.3. CUDA Developer Toolsï

For changes to nvprof and Visual Profiler, see the changelog.
For new features, improvements, and bug fixes in Nsight Systems, see the changelog.
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.
For new features, improvements, and bug fixes in CUPTI, see the changelog.
For new features, improvements, and bug fixes in Nsight Compute, see the changelog.
For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.




1.3. Resolved Issuesï

1.3.1. CUDA Compilerï

Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to âMisaligned shared or local addressâ.
Fix to correct the calculation of write-after-read hazard latency.




1.4. Known Issues and Limitationsï

Runfile will not be supported for Amazon Linux 2023.
Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms.



1.5. Deprecated or Dropped Featuresï
Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

1.5.1. Deprecated or Dropped Architecturesï

NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.



1.5.2. Deprecated Operating Systemsï

NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
CUDA 12.5 is the last release to support Debian 10.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.



1.5.3. Deprecated Toolchainsï
CUDA Toolkit 12.4 deprecated support for the following host compilers:


Microsoft Visual C/C++ (MSVC) 2017
All GCC versions prior to GCC 7.3




1.5.4. CUDA Toolsï


Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.






2. CUDA Librariesï
This section covers CUDA Libraries release notes for 12.x releases.

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.


2.1. cuBLAS Libraryï

2.1.1. cuBLAS: Release 12.5 Update 1ï

New Features

Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.


Known Issues

The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.
cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.


Resolved Issues

Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).





2.1.2. cuBLAS: Release 12.5ï

New Features

cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details.


Known Issues

cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.


Resolved Issues

cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.





2.1.3. cuBLAS: Release 12.4 Update 1ï

Known Issues

Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.


Resolved Issues

cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4.
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.





2.1.4. cuBLAS: Release 12.4ï

New Features

cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH.  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details.


Known Issues

When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget().
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.





2.1.5. cuBLAS: Release 12.3 Update 1ï

New Features

Improved performance of heuristics cache for workloads that have a high eviction rate.


Known Issues

BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.


Resolved Issues

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute().
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.





2.1.6. cuBLAS: Release 12.3ï

New Features

Improved performance on NVIDIA L40S Ada GPUs.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release.





2.1.7. cuBLAS: Release 12.2 Update 2ï

New Features

cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.





2.1.8. cuBLAS: Release 12.2ï

Known Issues

cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE.  The kernels apply the first batchâs bias vector to all batches. This will be fixed in a future release.





2.1.9. cuBLAS: Release 12.1 Update 1ï

New Features

Support for FP8 on NVIDIA Ada GPUs.
Improved performance on NVIDIA L4 Ada GPUs.
Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.


Known Issues

When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.





2.1.10. cuBLAS: Release 12.0 Update 1ï

New Features

Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.


Known Issues

For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.


Resolved Issues

Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release.
Added forward compatible single precision complex GEMM that does not require workspace.





2.1.11. cuBLAS: Release 12.0ï

New Features

cublasLtMatmul now supports FP8 with a non-zero beta.
Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

CUBLASLT_EPILOGUE_BGRAD{A,B}
CUBLASLT_EPILOGUE_{RELU,GELU}_AUX
CUBLASLT_EPILOGUE_D{RELU,GELU}


Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.


Known Issues

There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.


Resolved Issues

Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

Deprecations

Disallow including cublas.h and cublas_v2.h in the same translation unit.
Removed:

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.
cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t.
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.
CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.








2.2. cuFFT Libraryï

2.2.1. cuFFT: Release 12.5ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API.







2.2.2. cuFFT: Release 12.4 Update 1ï

Resolved Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.





2.2.3. cuFFT: Release 12.4ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.


Known Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.


Resolved Issues

Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.





2.2.4. cuFFT: Release 12.3 Update 1ï

Known Issues

Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.


Resolved Issues

Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.





2.2.5. cuFFT: Release 12.3ï

New Features

Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes.





2.2.6. cuFFT: Release 12.2ï

New Features

cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.
Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.


Resolved Issues

cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.





2.2.7. cuFFT: Release 12.1 Update 1ï

Known Issues

cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.





2.2.8. cuFFT: Release 12.1ï

New Features

Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.


Known Issues

Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.


Resolved Issues

cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.





2.2.9. cuFFT: Release 12.0 Update 1ï

Resolved Issues

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.





2.2.10. cuFFT: Release 12.0ï

New Features

PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.


Known Issues

cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.


Resolved Issues

cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.






2.3. cuSOLVER Libraryï

2.3.1. cuSOLVER: Release 12.5 Update 1ï

Resolved Issues

The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.





2.3.2. cuSOLVER: Release 12.5ï

New Features

Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.
Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR.
Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.


Known Issues

With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with
auto ALIGN_32=[](int64_t val) {
   return ((val + 31)/32)*32;
};


and
auto sizeofCudaDataType=[](cudaDataType dt) {
   if (dt == CUDA_R_32F) return sizeof(float);
   if (dt == CUDA_R_64F) return sizeof(double);
   if (dt == CUDA_C_32F) return sizeof(cuComplex);
   if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);
};








2.3.3. cuSOLVER: Release 12.4 Update 1ï

New Features

The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.
The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.


Resolved Issues

cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.


Deprecations

Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead.





2.3.4. cuSOLVER: Release 12.4ï

New Features

cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.


Known Issues

cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.





2.3.5. cuSOLVER: Release 12.2 Update 2ï

Resolved Issues

Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to âNâ.





2.3.6. cuSOLVER: Release 12.2ï

New Features

A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp().


Known Issues

Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.






2.4. cuSPARSE Libraryï

2.4.1. cuSPARSE: Release 12.5 Update 1ï

New Features

Added support for BSR format in cusparseSpMM.


Resolved Issues

cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \*= beta. The bug behavior was not modifying C at all.
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.





2.4.2. cuSPARSE: Release 12.5ï

New Features

Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.


Resolved Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.





2.4.3. cuSPARSE: Release 12.4ï

New Features

Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess().
Added support for mixed real and complex types for cusparseSpMM().
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM().


Known Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.


Resolved Issues

cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.





2.4.4. cuSPARSE: Release 12.3 Update 1ï

New Features

Added support for block sizes of 64 and 128 in cusparseSDDMM().
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.





2.4.5. cuSPARSE: Release 12.3ï

New Features

The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.


Known Issues

The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.


Resolved Issues

cusparseSpSV() provided indeterministic results in some cases.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.





2.4.6. cuSPARSE: Release 12.2 Update 1ï

New Features

The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api.


Resolved Issues

Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
Clarified the supported operations for cusparseSDDMM().
cusparseCreateConstSlicedEll() now uses const pointers.
Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.
cusparseSpSM_bufferSize() could ask slightly less memory than needed.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.


Deprecations

Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.





2.4.7. cuSPARSE: Release 12.1 Update 1ï

New Features

Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).
Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.





2.4.8. cuSPARSE: Release 12.0 Update 1ï

New Features

cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
Improved cusparseSpMV() performance with a new load balancing algorithm.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.


Resolved Issues

cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.





2.4.9. cuSPARSE: Release 12.0ï

New Features

JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan().
Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2().
Improved cusparseSpSV() performance for both the analysis and the solving phases.
Improved cusparseSpSM() performance for both the analysis and the solving phases.
Improved cusparseSDDMM() performance and added support for batch computation.
Improved cusparseCsr2cscEx2() performance.


Resolved Issues

cusparseSpSV() and cusparseSpSM() could produce wrong results.
cusparseDnMatGetStridedBatch() did not accept batchStride == 0.


Deprecations

Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.






2.5. Math Libraryï

2.5.1. CUDA Math: Release 12.5ï

Known Issues

As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.





2.5.2. CUDA Math: Release 12.4ï

Resolved Issues

Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.





2.5.3. CUDA Math: Release 12.3ï

New Features

Performance of SIMD Integer CUDA Math APIs was improved.


Resolved Issues

The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.


Known Issues

Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.





2.5.4. CUDA Math: Release 12.2ï

New Features

CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

__CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__
__CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__




Resolved Issues

During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.
Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh().





2.5.5. CUDA Math: Release 12.1ï

New Features

Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.





2.5.6. CUDA Math: Release 12.0ï

New Features

Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.


Known Issues

Double precision inputs that cause the double precision division algorithm in the default âround to nearest even modeâ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code.


Deprecations

All previously deprecated undocumented APIs are removed from CUDA 12.0.






2.6. NVIDIA Performance Primitives (NPP)ï

2.6.1. NPP: Release 12.4ï

New Features

Enhanced large file support with size_t.





2.6.2. NPP: Release 12.0ï

Deprecations

Deprecating non-CTX API support from next release.


Resolved Issues

A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.






2.7. nvJPEG Libraryï

2.7.1. nvJPEG: Release 12.4ï

New Features

IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.





2.7.2. nvJPEG: Release 12.3 Update 1ï

New Features

New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.





2.7.3. nvJPEG: Release 12.2ï

New Features

Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T.





2.7.4. nvJPEG: Release 12.0ï

New Features

Immproved the GPU Memory optimisation for the nvJPEG codec.


Resolved Issues

An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
An issue with CMYK four component color conversion is now resolved.


Known Issues

Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.


Deprecations

The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables).




1
Only available on select Linux distros






3. Noticesï

3.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


3.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


3.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA 12.5 Update 1 Release Notes








































1. CUDA 12.5 Update 1 Release Notes
1.1. CUDA Toolkit Major Component Versions
1.2. New Features
1.2.1. General CUDA
1.2.2. CUDA Compiler
1.2.3. CUDA Developer Tools


1.3. Resolved Issues
1.3.1. CUDA Compiler


1.4. Known Issues and Limitations
1.5. Deprecated or Dropped Features
1.5.1. Deprecated or Dropped Architectures
1.5.2. Deprecated Operating Systems
1.5.3. Deprecated Toolchains
1.5.4. CUDA Tools




2. CUDA Libraries
2.1. cuBLAS Library
2.1.1. cuBLAS: Release 12.5 Update 1
2.1.2. cuBLAS: Release 12.5
2.1.3. cuBLAS: Release 12.4 Update 1
2.1.4. cuBLAS: Release 12.4
2.1.5. cuBLAS: Release 12.3 Update 1
2.1.6. cuBLAS: Release 12.3
2.1.7. cuBLAS: Release 12.2 Update 2
2.1.8. cuBLAS: Release 12.2
2.1.9. cuBLAS: Release 12.1 Update 1
2.1.10. cuBLAS: Release 12.0 Update 1
2.1.11. cuBLAS: Release 12.0


2.2. cuFFT Library
2.2.1. cuFFT: Release 12.5
2.2.2. cuFFT: Release 12.4 Update 1
2.2.3. cuFFT: Release 12.4
2.2.4. cuFFT: Release 12.3 Update 1
2.2.5. cuFFT: Release 12.3
2.2.6. cuFFT: Release 12.2
2.2.7. cuFFT: Release 12.1 Update 1
2.2.8. cuFFT: Release 12.1
2.2.9. cuFFT: Release 12.0 Update 1
2.2.10. cuFFT: Release 12.0


2.3. cuSOLVER Library
2.3.1. cuSOLVER: Release 12.5 Update 1
2.3.2. cuSOLVER: Release 12.5
2.3.3. cuSOLVER: Release 12.4 Update 1
2.3.4. cuSOLVER: Release 12.4
2.3.5. cuSOLVER: Release 12.2 Update 2
2.3.6. cuSOLVER: Release 12.2


2.4. cuSPARSE Library
2.4.1. cuSPARSE: Release 12.5 Update 1
2.4.2. cuSPARSE: Release 12.5
2.4.3. cuSPARSE: Release 12.4
2.4.4. cuSPARSE: Release 12.3 Update 1
2.4.5. cuSPARSE: Release 12.3
2.4.6. cuSPARSE: Release 12.2 Update 1
2.4.7. cuSPARSE: Release 12.1 Update 1
2.4.8. cuSPARSE: Release 12.0 Update 1
2.4.9. cuSPARSE: Release 12.0


2.5. Math Library
2.5.1. CUDA Math: Release 12.5
2.5.2. CUDA Math: Release 12.4
2.5.3. CUDA Math: Release 12.3
2.5.4. CUDA Math: Release 12.2
2.5.5. CUDA Math: Release 12.1
2.5.6. CUDA Math: Release 12.0


2.6. NVIDIA Performance Primitives (NPP)
2.6.1. NPP: Release 12.4
2.6.2. NPP: Release 12.0


2.7. nvJPEG Library
2.7.1. nvJPEG: Release 12.4
2.7.2. nvJPEG: Release 12.3 Update 1
2.7.3. nvJPEG: Release 12.2
2.7.4. nvJPEG: Release 12.0




3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Release Notes





 »
1. CUDA 12.5 Update 1 Release Notes



v12.5 |
PDF
|
Archive
 






NVIDIA CUDA Toolkit Release Notes
The Release Notes for the CUDA Toolkit.

1. CUDA 12.5 Update 1 Release Notesï
The release notes for the NVIDIAÂ® CUDAÂ® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html.

Note
The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.


1.1. CUDA Toolkit Major Component Versionsï

CUDA ComponentsStarting with CUDA 11, the various components in the toolkit are versioned independently.
For CUDA 12.5 Update 1, the table below indicates the versions:



Table 1 CUDA 12.5 Update 1 Component Versionsï








Component Name
Version Information
Supported Architectures
Supported Platforms



CUDA C++ Core Compute Libraries
Thrust
2.4.0
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUB
2.4.0

libcu++
2.4.0

Cooperative Groups
12.5.82

CUDA Compatibility
12.5.36505571
aarch64-jetson
Linux

CUDA Runtime (cudart)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuobjdump
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUPTI
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuxxfilt (demangler)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA Demo Suite
12.5.82
x86_64
Linux, Windows

CUDA GDB
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, WSL

CUDA Nsight Eclipse Plugin
12.5.82
x86_64
Linux

CUDA NVCC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvdisasm
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA NVML Headers
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvprof
12.5.82
x86_64
Linux, Windows

CUDA nvprune
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVRTC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

NVTX
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVVP
12.5.82
x86_64,
Linux, Windows

CUDA OpenCL
12.5.39
x86_64
Linux, Windows

CUDA Profiler API
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA Compute Sanitizer API
12.5.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuBLAS
12.5.3.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuDLA
12.5.82
aarch64-jetson
Linux

CUDA cuFFT
11.2.3.61
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuFile
1.10.1.7
x86_64, arm64-sbsa, aarch64-jetson
Linux

CUDA cuRAND
10.3.6.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSOLVER
11.6.3.83
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSPARSE
12.5.1.3
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NPP
12.3.0.159
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvFatbin
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJitLink
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJPEG
12.3.2.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

Nsight Compute
2024.2.1.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL (Windows 11)

Nsight Systems
2024.2.3.38
x86_64, arm64-sbsa,
Linux, Windows, WSL

Nsight Visual Studio Edition (VSE)
2024.2.1.24155
x86_64 (Windows)
Windows

nvidia_fs1
2.20.6
x86_64, arm64-sbsa, aarch64-jetson
Linux

Visual Studio Integration
12.5.82
x86_64 (Windows)
Windows

NVIDIA Linux Driver
555.42.06
x86_64, arm64-sbsa
Linux

NVIDIA Windows Driver
555.85
x86_64 (Windows)
Windows, WSL




CUDA DriverRunning a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.
Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.
Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html



Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibilityï






CUDA Toolkit
Minimum Required Driver Version for CUDA Minor Version Compatibility*




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.x
>=525.60.13
>=528.33

CUDA 11.8.x
CUDA 11.7.x
CUDA 11.6.x
CUDA 11.5.x
CUDA 11.4.x
CUDA 11.3.x
CUDA 11.2.x
CUDA 11.1.x
>=450.80.02
>=452.39

CUDA 11.0 (11.0.3)
>=450.36.06**
>=451.22**



* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode â please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

Table 3 CUDA Toolkit and Corresponding Driver Versionsï






CUDA Toolkit
Toolkit Driver Version




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.5 Update 1
>=555.42.06
>=555.85

CUDA 12.5 GA
>=555.42.02
>=555.85

CUDA 12.4 Update 1
>=550.54.15
>=551.78

CUDA 12.4 GA
>=550.54.14
>=551.61

CUDA 12.3 Update 1
>=545.23.08
>=546.12

CUDA 12.3 GA
>=545.23.06
>=545.84

CUDA 12.2 Update 2
>=535.104.05
>=537.13

CUDA 12.2 Update 1
>=535.86.09
>=536.67

CUDA 12.2 GA
>=535.54.03
>=536.25

CUDA 12.1 Update 1
>=530.30.02
>=531.14

CUDA 12.1 GA
>=530.30.02
>=531.14

CUDA 12.0 Update 1
>=525.85.12
>=528.33

CUDA 12.0 GA
>=525.60.13
>=527.41

CUDA 11.8 GA
>=520.61.05
>=520.06

CUDA 11.7 Update 1
>=515.48.07
>=516.31

CUDA 11.7 GA
>=515.43.04
>=516.01

CUDA 11.6 Update 2
>=510.47.03
>=511.65

CUDA 11.6 Update 1
>=510.47.03
>=511.65

CUDA 11.6 GA
>=510.39.01
>=511.23

CUDA 11.5 Update 2
>=495.29.05
>=496.13

CUDA 11.5 Update 1
>=495.29.05
>=496.13

CUDA 11.5 GA
>=495.29.05
>=496.04

CUDA 11.4 Update 4
>=470.82.01
>=472.50

CUDA 11.4 Update 3
>=470.82.01
>=472.50

CUDA 11.4 Update 2
>=470.57.02
>=471.41

CUDA 11.4 Update 1
>=470.57.02
>=471.41

CUDA 11.4.0 GA
>=470.42.01
>=471.11

CUDA 11.3.1 Update 1
>=465.19.01
>=465.89

CUDA 11.3.0 GA
>=465.19.01
>=465.89

CUDA 11.2.2 Update 2
>=460.32.03
>=461.33

CUDA 11.2.1 Update 1
>=460.32.03
>=461.09

CUDA 11.2.0 GA
>=460.27.03
>=460.82

CUDA 11.1.1 Update 1
>=455.32
>=456.81

CUDA 11.1 GA
>=455.23
>=456.38

CUDA 11.0.3 Update 1
>= 450.51.06
>= 451.82

CUDA 11.0.2 GA
>= 450.51.05
>= 451.48

CUDA 11.0.1 RC
>= 450.36.06
>= 451.22

CUDA 10.2.89
>= 440.33
>= 441.22

CUDA 10.1 (10.1.105 general release, and updates)
>= 418.39
>= 418.96

CUDA 10.0.130
>= 410.48
>= 411.31

CUDA 9.2 (9.2.148 Update 1)
>= 396.37
>= 398.26

CUDA 9.2 (9.2.88)
>= 396.26
>= 397.44

CUDA 9.1 (9.1.85)
>= 390.46
>= 391.29

CUDA 9.0 (9.0.76)
>= 384.81
>= 385.54

CUDA 8.0 (8.0.61 GA2)
>= 375.26
>= 376.51

CUDA 8.0 (8.0.44)
>= 367.48
>= 369.30

CUDA 7.5 (7.5.16)
>= 352.31
>= 353.66

CUDA 7.0 (7.0.28)
>= 346.46
>= 347.62



For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.
For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.


1.2. New Featuresï
This section lists new general CUDA and CUDA compilers features.

1.2.1. General CUDAï

In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here.



1.2.2. CUDA Compilerï

For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5.



1.2.3. CUDA Developer Toolsï

For changes to nvprof and Visual Profiler, see the changelog.
For new features, improvements, and bug fixes in Nsight Systems, see the changelog.
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.
For new features, improvements, and bug fixes in CUPTI, see the changelog.
For new features, improvements, and bug fixes in Nsight Compute, see the changelog.
For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.




1.3. Resolved Issuesï

1.3.1. CUDA Compilerï

Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to âMisaligned shared or local addressâ.
Fix to correct the calculation of write-after-read hazard latency.




1.4. Known Issues and Limitationsï

Runfile will not be supported for Amazon Linux 2023.
Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms.



1.5. Deprecated or Dropped Featuresï
Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

1.5.1. Deprecated or Dropped Architecturesï

NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.



1.5.2. Deprecated Operating Systemsï

NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
CUDA 12.5 is the last release to support Debian 10.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.



1.5.3. Deprecated Toolchainsï
CUDA Toolkit 12.4 deprecated support for the following host compilers:


Microsoft Visual C/C++ (MSVC) 2017
All GCC versions prior to GCC 7.3




1.5.4. CUDA Toolsï


Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.






2. CUDA Librariesï
This section covers CUDA Libraries release notes for 12.x releases.

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.


2.1. cuBLAS Libraryï

2.1.1. cuBLAS: Release 12.5 Update 1ï

New Features

Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.


Known Issues

The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.
cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.


Resolved Issues

Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).





2.1.2. cuBLAS: Release 12.5ï

New Features

cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details.


Known Issues

cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.


Resolved Issues

cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.





2.1.3. cuBLAS: Release 12.4 Update 1ï

Known Issues

Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.


Resolved Issues

cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4.
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.





2.1.4. cuBLAS: Release 12.4ï

New Features

cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH.  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details.


Known Issues

When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget().
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.





2.1.5. cuBLAS: Release 12.3 Update 1ï

New Features

Improved performance of heuristics cache for workloads that have a high eviction rate.


Known Issues

BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.


Resolved Issues

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute().
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.





2.1.6. cuBLAS: Release 12.3ï

New Features

Improved performance on NVIDIA L40S Ada GPUs.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release.





2.1.7. cuBLAS: Release 12.2 Update 2ï

New Features

cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.





2.1.8. cuBLAS: Release 12.2ï

Known Issues

cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE.  The kernels apply the first batchâs bias vector to all batches. This will be fixed in a future release.





2.1.9. cuBLAS: Release 12.1 Update 1ï

New Features

Support for FP8 on NVIDIA Ada GPUs.
Improved performance on NVIDIA L4 Ada GPUs.
Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.


Known Issues

When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.





2.1.10. cuBLAS: Release 12.0 Update 1ï

New Features

Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.


Known Issues

For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.


Resolved Issues

Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release.
Added forward compatible single precision complex GEMM that does not require workspace.





2.1.11. cuBLAS: Release 12.0ï

New Features

cublasLtMatmul now supports FP8 with a non-zero beta.
Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

CUBLASLT_EPILOGUE_BGRAD{A,B}
CUBLASLT_EPILOGUE_{RELU,GELU}_AUX
CUBLASLT_EPILOGUE_D{RELU,GELU}


Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.


Known Issues

There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.


Resolved Issues

Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

Deprecations

Disallow including cublas.h and cublas_v2.h in the same translation unit.
Removed:

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.
cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t.
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.
CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.








2.2. cuFFT Libraryï

2.2.1. cuFFT: Release 12.5ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API.







2.2.2. cuFFT: Release 12.4 Update 1ï

Resolved Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.





2.2.3. cuFFT: Release 12.4ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.


Known Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.


Resolved Issues

Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.





2.2.4. cuFFT: Release 12.3 Update 1ï

Known Issues

Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.


Resolved Issues

Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.





2.2.5. cuFFT: Release 12.3ï

New Features

Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes.





2.2.6. cuFFT: Release 12.2ï

New Features

cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.
Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.


Resolved Issues

cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.





2.2.7. cuFFT: Release 12.1 Update 1ï

Known Issues

cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.





2.2.8. cuFFT: Release 12.1ï

New Features

Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.


Known Issues

Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.


Resolved Issues

cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.





2.2.9. cuFFT: Release 12.0 Update 1ï

Resolved Issues

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.





2.2.10. cuFFT: Release 12.0ï

New Features

PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.


Known Issues

cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.


Resolved Issues

cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.






2.3. cuSOLVER Libraryï

2.3.1. cuSOLVER: Release 12.5 Update 1ï

Resolved Issues

The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.





2.3.2. cuSOLVER: Release 12.5ï

New Features

Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.
Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR.
Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.


Known Issues

With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with
auto ALIGN_32=[](int64_t val) {
   return ((val + 31)/32)*32;
};


and
auto sizeofCudaDataType=[](cudaDataType dt) {
   if (dt == CUDA_R_32F) return sizeof(float);
   if (dt == CUDA_R_64F) return sizeof(double);
   if (dt == CUDA_C_32F) return sizeof(cuComplex);
   if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);
};








2.3.3. cuSOLVER: Release 12.4 Update 1ï

New Features

The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.
The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.


Resolved Issues

cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.


Deprecations

Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead.





2.3.4. cuSOLVER: Release 12.4ï

New Features

cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.


Known Issues

cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.





2.3.5. cuSOLVER: Release 12.2 Update 2ï

Resolved Issues

Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to âNâ.





2.3.6. cuSOLVER: Release 12.2ï

New Features

A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp().


Known Issues

Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.






2.4. cuSPARSE Libraryï

2.4.1. cuSPARSE: Release 12.5 Update 1ï

New Features

Added support for BSR format in cusparseSpMM.


Resolved Issues

cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \*= beta. The bug behavior was not modifying C at all.
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.





2.4.2. cuSPARSE: Release 12.5ï

New Features

Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.


Resolved Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.





2.4.3. cuSPARSE: Release 12.4ï

New Features

Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess().
Added support for mixed real and complex types for cusparseSpMM().
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM().


Known Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.


Resolved Issues

cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.





2.4.4. cuSPARSE: Release 12.3 Update 1ï

New Features

Added support for block sizes of 64 and 128 in cusparseSDDMM().
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.





2.4.5. cuSPARSE: Release 12.3ï

New Features

The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.


Known Issues

The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.


Resolved Issues

cusparseSpSV() provided indeterministic results in some cases.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.





2.4.6. cuSPARSE: Release 12.2 Update 1ï

New Features

The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api.


Resolved Issues

Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
Clarified the supported operations for cusparseSDDMM().
cusparseCreateConstSlicedEll() now uses const pointers.
Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.
cusparseSpSM_bufferSize() could ask slightly less memory than needed.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.


Deprecations

Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.





2.4.7. cuSPARSE: Release 12.1 Update 1ï

New Features

Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).
Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.





2.4.8. cuSPARSE: Release 12.0 Update 1ï

New Features

cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
Improved cusparseSpMV() performance with a new load balancing algorithm.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.


Resolved Issues

cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.





2.4.9. cuSPARSE: Release 12.0ï

New Features

JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan().
Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2().
Improved cusparseSpSV() performance for both the analysis and the solving phases.
Improved cusparseSpSM() performance for both the analysis and the solving phases.
Improved cusparseSDDMM() performance and added support for batch computation.
Improved cusparseCsr2cscEx2() performance.


Resolved Issues

cusparseSpSV() and cusparseSpSM() could produce wrong results.
cusparseDnMatGetStridedBatch() did not accept batchStride == 0.


Deprecations

Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.






2.5. Math Libraryï

2.5.1. CUDA Math: Release 12.5ï

Known Issues

As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.





2.5.2. CUDA Math: Release 12.4ï

Resolved Issues

Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.





2.5.3. CUDA Math: Release 12.3ï

New Features

Performance of SIMD Integer CUDA Math APIs was improved.


Resolved Issues

The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.


Known Issues

Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.





2.5.4. CUDA Math: Release 12.2ï

New Features

CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

__CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__
__CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__




Resolved Issues

During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.
Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh().





2.5.5. CUDA Math: Release 12.1ï

New Features

Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.





2.5.6. CUDA Math: Release 12.0ï

New Features

Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.


Known Issues

Double precision inputs that cause the double precision division algorithm in the default âround to nearest even modeâ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code.


Deprecations

All previously deprecated undocumented APIs are removed from CUDA 12.0.






2.6. NVIDIA Performance Primitives (NPP)ï

2.6.1. NPP: Release 12.4ï

New Features

Enhanced large file support with size_t.





2.6.2. NPP: Release 12.0ï

Deprecations

Deprecating non-CTX API support from next release.


Resolved Issues

A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.






2.7. nvJPEG Libraryï

2.7.1. nvJPEG: Release 12.4ï

New Features

IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.





2.7.2. nvJPEG: Release 12.3 Update 1ï

New Features

New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.





2.7.3. nvJPEG: Release 12.2ï

New Features

Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T.





2.7.4. nvJPEG: Release 12.0ï

New Features

Immproved the GPU Memory optimisation for the nvJPEG codec.


Resolved Issues

An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
An issue with CMYK four component color conversion is now resolved.


Known Issues

Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.


Deprecations

The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables).




1
Only available on select Linux distros






3. Noticesï

3.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


3.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


3.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA 12.5 Update 1 Release Notes








































1. CUDA 12.5 Update 1 Release Notes
1.1. CUDA Toolkit Major Component Versions
1.2. New Features
1.2.1. General CUDA
1.2.2. CUDA Compiler
1.2.3. CUDA Developer Tools


1.3. Resolved Issues
1.3.1. CUDA Compiler


1.4. Known Issues and Limitations
1.5. Deprecated or Dropped Features
1.5.1. Deprecated or Dropped Architectures
1.5.2. Deprecated Operating Systems
1.5.3. Deprecated Toolchains
1.5.4. CUDA Tools




2. CUDA Libraries
2.1. cuBLAS Library
2.1.1. cuBLAS: Release 12.5 Update 1
2.1.2. cuBLAS: Release 12.5
2.1.3. cuBLAS: Release 12.4 Update 1
2.1.4. cuBLAS: Release 12.4
2.1.5. cuBLAS: Release 12.3 Update 1
2.1.6. cuBLAS: Release 12.3
2.1.7. cuBLAS: Release 12.2 Update 2
2.1.8. cuBLAS: Release 12.2
2.1.9. cuBLAS: Release 12.1 Update 1
2.1.10. cuBLAS: Release 12.0 Update 1
2.1.11. cuBLAS: Release 12.0


2.2. cuFFT Library
2.2.1. cuFFT: Release 12.5
2.2.2. cuFFT: Release 12.4 Update 1
2.2.3. cuFFT: Release 12.4
2.2.4. cuFFT: Release 12.3 Update 1
2.2.5. cuFFT: Release 12.3
2.2.6. cuFFT: Release 12.2
2.2.7. cuFFT: Release 12.1 Update 1
2.2.8. cuFFT: Release 12.1
2.2.9. cuFFT: Release 12.0 Update 1
2.2.10. cuFFT: Release 12.0


2.3. cuSOLVER Library
2.3.1. cuSOLVER: Release 12.5 Update 1
2.3.2. cuSOLVER: Release 12.5
2.3.3. cuSOLVER: Release 12.4 Update 1
2.3.4. cuSOLVER: Release 12.4
2.3.5. cuSOLVER: Release 12.2 Update 2
2.3.6. cuSOLVER: Release 12.2


2.4. cuSPARSE Library
2.4.1. cuSPARSE: Release 12.5 Update 1
2.4.2. cuSPARSE: Release 12.5
2.4.3. cuSPARSE: Release 12.4
2.4.4. cuSPARSE: Release 12.3 Update 1
2.4.5. cuSPARSE: Release 12.3
2.4.6. cuSPARSE: Release 12.2 Update 1
2.4.7. cuSPARSE: Release 12.1 Update 1
2.4.8. cuSPARSE: Release 12.0 Update 1
2.4.9. cuSPARSE: Release 12.0


2.5. Math Library
2.5.1. CUDA Math: Release 12.5
2.5.2. CUDA Math: Release 12.4
2.5.3. CUDA Math: Release 12.3
2.5.4. CUDA Math: Release 12.2
2.5.5. CUDA Math: Release 12.1
2.5.6. CUDA Math: Release 12.0


2.6. NVIDIA Performance Primitives (NPP)
2.6.1. NPP: Release 12.4
2.6.2. NPP: Release 12.0


2.7. nvJPEG Library
2.7.1. nvJPEG: Release 12.4
2.7.2. nvJPEG: Release 12.3 Update 1
2.7.3. nvJPEG: Release 12.2
2.7.4. nvJPEG: Release 12.0




3. Notices
3.1. Notice
3.2. OpenCL
3.3. Trademarks








Release Notes





 »
1. CUDA 12.5 Update 1 Release Notes



v12.5 |
PDF
|
Archive
 






NVIDIA CUDA Toolkit Release Notes
The Release Notes for the CUDA Toolkit.

1. CUDA 12.5 Update 1 Release Notesï
The release notes for the NVIDIAÂ® CUDAÂ® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html.

Note
The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.


1.1. CUDA Toolkit Major Component Versionsï

CUDA ComponentsStarting with CUDA 11, the various components in the toolkit are versioned independently.
For CUDA 12.5 Update 1, the table below indicates the versions:



Table 1 CUDA 12.5 Update 1 Component Versionsï








Component Name
Version Information
Supported Architectures
Supported Platforms



CUDA C++ Core Compute Libraries
Thrust
2.4.0
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUB
2.4.0

libcu++
2.4.0

Cooperative Groups
12.5.82

CUDA Compatibility
12.5.36505571
aarch64-jetson
Linux

CUDA Runtime (cudart)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuobjdump
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUPTI
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuxxfilt (demangler)
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA Demo Suite
12.5.82
x86_64
Linux, Windows

CUDA GDB
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, WSL

CUDA Nsight Eclipse Plugin
12.5.82
x86_64
Linux

CUDA NVCC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvdisasm
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows

CUDA NVML Headers
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvprof
12.5.82
x86_64
Linux, Windows

CUDA nvprune
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVRTC
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

NVTX
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NVVP
12.5.82
x86_64,
Linux, Windows

CUDA OpenCL
12.5.39
x86_64
Linux, Windows

CUDA Profiler API
12.5.39
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA Compute Sanitizer API
12.5.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuBLAS
12.5.3.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

cuDLA
12.5.82
aarch64-jetson
Linux

CUDA cuFFT
11.2.3.61
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuFile
1.10.1.7
x86_64, arm64-sbsa, aarch64-jetson
Linux

CUDA cuRAND
10.3.6.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSOLVER
11.6.3.83
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA cuSPARSE
12.5.1.3
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA NPP
12.3.0.159
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvFatbin
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJitLink
12.5.82
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

CUDA nvJPEG
12.3.2.81
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL

Nsight Compute
2024.2.1.2
x86_64, arm64-sbsa, aarch64-jetson
Linux, Windows, WSL (Windows 11)

Nsight Systems
2024.2.3.38
x86_64, arm64-sbsa,
Linux, Windows, WSL

Nsight Visual Studio Edition (VSE)
2024.2.1.24155
x86_64 (Windows)
Windows

nvidia_fs1
2.20.6
x86_64, arm64-sbsa, aarch64-jetson
Linux

Visual Studio Integration
12.5.82
x86_64 (Windows)
Windows

NVIDIA Linux Driver
555.42.06
x86_64, arm64-sbsa
Linux

NVIDIA Windows Driver
555.85
x86_64 (Windows)
Windows, WSL




CUDA DriverRunning a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3. For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus.
Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.
More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades.
Note: Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.
The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html



Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibilityï






CUDA Toolkit
Minimum Required Driver Version for CUDA Minor Version Compatibility*




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.x
>=525.60.13
>=528.33

CUDA 11.8.x
CUDA 11.7.x
CUDA 11.6.x
CUDA 11.5.x
CUDA 11.4.x
CUDA 11.3.x
CUDA 11.2.x
CUDA 11.1.x
>=450.80.02
>=452.39

CUDA 11.0 (11.0.3)
>=450.36.06**
>=451.22**



* Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode â please read the CUDA Compatibility Guide for details.
** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.
The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.

Table 3 CUDA Toolkit and Corresponding Driver Versionsï






CUDA Toolkit
Toolkit Driver Version




Linux x86_64 Driver Version
Windows x86_64 Driver Version

CUDA 12.5 Update 1
>=555.42.06
>=555.85

CUDA 12.5 GA
>=555.42.02
>=555.85

CUDA 12.4 Update 1
>=550.54.15
>=551.78

CUDA 12.4 GA
>=550.54.14
>=551.61

CUDA 12.3 Update 1
>=545.23.08
>=546.12

CUDA 12.3 GA
>=545.23.06
>=545.84

CUDA 12.2 Update 2
>=535.104.05
>=537.13

CUDA 12.2 Update 1
>=535.86.09
>=536.67

CUDA 12.2 GA
>=535.54.03
>=536.25

CUDA 12.1 Update 1
>=530.30.02
>=531.14

CUDA 12.1 GA
>=530.30.02
>=531.14

CUDA 12.0 Update 1
>=525.85.12
>=528.33

CUDA 12.0 GA
>=525.60.13
>=527.41

CUDA 11.8 GA
>=520.61.05
>=520.06

CUDA 11.7 Update 1
>=515.48.07
>=516.31

CUDA 11.7 GA
>=515.43.04
>=516.01

CUDA 11.6 Update 2
>=510.47.03
>=511.65

CUDA 11.6 Update 1
>=510.47.03
>=511.65

CUDA 11.6 GA
>=510.39.01
>=511.23

CUDA 11.5 Update 2
>=495.29.05
>=496.13

CUDA 11.5 Update 1
>=495.29.05
>=496.13

CUDA 11.5 GA
>=495.29.05
>=496.04

CUDA 11.4 Update 4
>=470.82.01
>=472.50

CUDA 11.4 Update 3
>=470.82.01
>=472.50

CUDA 11.4 Update 2
>=470.57.02
>=471.41

CUDA 11.4 Update 1
>=470.57.02
>=471.41

CUDA 11.4.0 GA
>=470.42.01
>=471.11

CUDA 11.3.1 Update 1
>=465.19.01
>=465.89

CUDA 11.3.0 GA
>=465.19.01
>=465.89

CUDA 11.2.2 Update 2
>=460.32.03
>=461.33

CUDA 11.2.1 Update 1
>=460.32.03
>=461.09

CUDA 11.2.0 GA
>=460.27.03
>=460.82

CUDA 11.1.1 Update 1
>=455.32
>=456.81

CUDA 11.1 GA
>=455.23
>=456.38

CUDA 11.0.3 Update 1
>= 450.51.06
>= 451.82

CUDA 11.0.2 GA
>= 450.51.05
>= 451.48

CUDA 11.0.1 RC
>= 450.36.06
>= 451.22

CUDA 10.2.89
>= 440.33
>= 441.22

CUDA 10.1 (10.1.105 general release, and updates)
>= 418.39
>= 418.96

CUDA 10.0.130
>= 410.48
>= 411.31

CUDA 9.2 (9.2.148 Update 1)
>= 396.37
>= 398.26

CUDA 9.2 (9.2.88)
>= 396.26
>= 397.44

CUDA 9.1 (9.1.85)
>= 390.46
>= 391.29

CUDA 9.0 (9.0.76)
>= 384.81
>= 385.54

CUDA 8.0 (8.0.61 GA2)
>= 375.26
>= 376.51

CUDA 8.0 (8.0.44)
>= 367.48
>= 369.30

CUDA 7.5 (7.5.16)
>= 352.31
>= 353.66

CUDA 7.0 (7.0.28)
>= 346.46
>= 347.62



For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.
For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers.
During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).
For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software.
For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas.


1.2. New Featuresï
This section lists new general CUDA and CUDA compilers features.

1.2.1. General CUDAï

In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.
End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.
MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here.



1.2.2. CUDA Compilerï

For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5.



1.2.3. CUDA Developer Toolsï

For changes to nvprof and Visual Profiler, see the changelog.
For new features, improvements, and bug fixes in Nsight Systems, see the changelog.
For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog.
For new features, improvements, and bug fixes in CUPTI, see the changelog.
For new features, improvements, and bug fixes in Nsight Compute, see the changelog.
For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog.
For new features, improvements, and bug fixes in CUDA-GDB, see the changelog.




1.3. Resolved Issuesï

1.3.1. CUDA Compilerï

Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.
Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.
Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag.
Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.
Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to âMisaligned shared or local addressâ.
Fix to correct the calculation of write-after-read hazard latency.




1.4. Known Issues and Limitationsï

Runfile will not be supported for Amazon Linux 2023.
Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features.
Launching Cooperative Group kernels with MPS is not supported on Tegra platforms.



1.5. Deprecated or Dropped Featuresï
Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software.

1.5.1. Deprecated or Dropped Architecturesï

NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.



1.5.2. Deprecated Operating Systemsï

NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.
CUDA 12.5 is the last release to support Debian 10.
Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.



1.5.3. Deprecated Toolchainsï
CUDA Toolkit 12.4 deprecated support for the following host compilers:


Microsoft Visual C/C++ (MSVC) 2017
All GCC versions prior to GCC 7.3




1.5.4. CUDA Toolsï


Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.






2. CUDA Librariesï
This section covers CUDA Libraries release notes for 12.x releases.

CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.


2.1. cuBLAS Libraryï

2.1.1. cuBLAS: Release 12.5 Update 1ï

New Features

Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.


Known Issues

The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually. This will be fixed in a future release.
cublasGemmGroupedBatchedEx and cublas<t>gemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release.


Resolved Issues

Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.
cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).





2.1.2. cuBLAS: Release 12.5ï

New Features

cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.  This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details.


Known Issues

cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.


Resolved Issues

cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.
cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.





2.1.3. cuBLAS: Release 12.4 Update 1ï

Known Issues

Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.  This will be fixed in an upcoming release.
cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. This will be fixed in an upcoming release.


Resolved Issues

cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 (CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2).
Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul(), cublasLtMatmulAlgoCheck(), and cublasLtMatmulAlgoGetHeuristic(). The issue was introduced in CUDA Toolkit 12.4.
cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8.





2.1.4. cuBLAS: Release 12.4ï

New Features

cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.  Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH.  Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).  Please see gemmGroupedBatched for more details.


Known Issues

When the current context has been created using cuGreenCtxCreate(), cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget().
BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. This is the same known issue documented in cuBLAS 12.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1.
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6.
When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync. However, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.





2.1.5. cuBLAS: Release 12.3 Update 1ï

New Features

Improved performance of heuristics cache for workloads that have a high eviction rate.


Known Issues

BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE. The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST.


Resolved Issues

cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute().
Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).
cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.





2.1.6. cuBLAS: Release 12.3ï

New Features

Improved performance on NVIDIA L40S Ada GPUs.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.
When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute(). To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit(). This will be fixed in an upcoming release.





2.1.7. cuBLAS: Release 12.2 Update 2ï

New Features

cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.  It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.  This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.


Known Issues

cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.





2.1.8. cuBLAS: Release 12.2ï

Known Issues

cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue.
Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE.  The kernels apply the first batchâs bias vector to all batches. This will be fixed in a future release.





2.1.9. cuBLAS: Release 12.1 Update 1ï

New Features

Support for FP8 on NVIDIA Ada GPUs.
Improved performance on NVIDIA L4 Ada GPUs.
Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions.


Known Issues

When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t. The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.





2.1.10. cuBLAS: Release 12.0 Update 1ï

New Features

Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.


Known Issues

For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.


Resolved Issues

Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.  This began in the CUDA Toolkit 12.0 release.
Added forward compatible single precision complex GEMM that does not require workspace.





2.1.11. cuBLAS: Release 12.0ï

New Features

cublasLtMatmul now supports FP8 with a non-zero beta.
Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface.
Added more Hopper-specific kernels for cublasLtMatmul with epilogues:

CUBLASLT_EPILOGUE_BGRAD{A,B}
CUBLASLT_EPILOGUE_{RELU,GELU}_AUX
CUBLASLT_EPILOGUE_D{RELU,GELU}


Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.


Known Issues

There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release.


Resolved Issues

Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE) could return incorrect results for the bias gradient.
cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.

Deprecations

Disallow including cublas.h and cublas_v2.h in the same translation unit.
Removed:

CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t. No kernels utilize these stages anymore.
cublasLt3mMode_t, CUBLASLT_MATMUL_PREF_MATH_MODE_MASK, and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t. Instead, use the corresponding flags from cublasLtNumericalImplFlags_t.
CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK, CUBLASLT_MATMUL_PREF_EPILOGUE_MASK, and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t. The corresponding parameters are taken directly from cublasLtMatmulDesc_t.
CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t. This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.








2.2. cuFFT Libraryï

2.2.1. cuFFT: Release 12.5ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes.

We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API.







2.2.2. cuFFT: Release 12.4 Update 1ï

Resolved Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h) in CUDA 12.4. This routine has now been removed from the header.





2.2.3. cuFFT: Release 12.4ï

New Features

Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.
Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs.
Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.


Known Issues

A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header (cufftXt.h). This routine is not supported by cuFFT, and will be removed from the header in a future release.


Resolved Issues

Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e. using the ostride component of the Advanced Data Layout API).
Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL. From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.





2.2.4. cuFFT: Release 12.3 Update 1ï

Known Issues

Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT.


Resolved Issues

Complex-to-complex (C2C) execution functions (cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.





2.2.5. cuFFT: Release 12.3ï

New Features

Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.
Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.
Slightly improved planning times for some FFT sizes.





2.2.6. cuFFT: Release 12.2ï

New Features

cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs.
Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.
Reduced the size of the static libraries when compared to cuFFT in the 12.1 release.


Resolved Issues

cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.
cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.





2.2.7. cuFFT: Release 12.1 Update 1ï

Known Issues

cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy) and another thread calls any API (except cufftCreate or cufftDestroy), and when the total number of plans alive exceeds 1023.
cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.





2.2.8. cuFFT: Release 12.1ï

New Features

Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.


Known Issues

Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.


Resolved Issues

cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.





2.2.9. cuFFT: Release 12.0 Update 1ï

Resolved Issues

Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.





2.2.10. cuFFT: Release 12.0ï

New Features

PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.


Known Issues

cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme.


Resolved Issues

cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.






2.3. cuSOLVER Libraryï

2.3.1. cuSOLVER: Release 12.5 Update 1ï

Resolved Issues

The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.





2.3.2. cuSOLVER: Release 12.5ï

New Features

Performance improvements of cusolverDnXgesvd and cusolverDn<t>gesvd if jobu != 'N' or jobvt != 'N'.
Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR.
Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.


Known Issues

With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice. As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)), with
auto ALIGN_32=[](int64_t val) {
   return ((val + 31)/32)*32;
};


and
auto sizeofCudaDataType=[](cudaDataType dt) {
   if (dt == CUDA_R_32F) return sizeof(float);
   if (dt == CUDA_R_64F) return sizeof(double);
   if (dt == CUDA_C_32F) return sizeof(cuComplex);
   if (dt == CUDA_C_64F) return sizeof(cuDoubleComplex);
};








2.3.3. cuSOLVER: Release 12.4 Update 1ï

New Features

The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDn<t>ormqr, cusolverDn<t>ormtr, and cusolverDnXsyevd.
The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster.


Resolved Issues

cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.


Deprecations

Using long-deprecated cusolverDnPotrf, cusolverDnPotrs, cusolverDnGeqrf, cusolverDnGetrf, cusolverDnGetrs, cusolverDnSyevd, cusolverDnSyevdx, cusolverDnGesvd,  and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf, cusolverDnXpotrs, cusolverDnXgeqrf, cusolverDnXgetrf, cusolverDnXgetrs, cusolverDnXsyevd, cusolverDnXsyevdx, cusolverDnXgesvd, and the corresponding bufferSize functions instead.





2.3.4. cuSOLVER: Release 12.4ï

New Features

cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.


Known Issues

cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.





2.3.5. cuSOLVER: Release 12.2 Update 2ï

Resolved Issues

Fixed an issue with cusolverDn<t>gesvd(), cusolverDnGesvd(), and cusolverDnXgesvd(), which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to âNâ.





2.3.6. cuSOLVER: Release 12.2ï

New Features

A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode(). Affected functions are: cusolverDn<t>geqrf(), cusolverDn<t>syevd(), cusolverDn<t>syevdx(), cusolverDn<t>gesvdj(), cusolverDnXgeqrf(), cusolverDnXsyevd(), cusolverDnXsyevdx(), cusolverDnXgesvdr(), and cusolverDnXgesvdp().


Known Issues

Concurrent executions of cusolverDn<t>getrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.






2.4. cuSPARSE Libraryï

2.4.1. cuSPARSE: Release 12.5 Update 1ï

New Features

Added support for BSR format in cusparseSpMM.


Resolved Issues

cusparseSpMM() would sometimes get incorrect results when alpha=0, num_batches>1, batch_stride indicates that there is padding between batches.
cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).
cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \*= beta. The bug behavior was not modifying C at all.
cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.
Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.
Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.





2.4.2. cuSPARSE: Release 12.5ï

New Features

Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.


Resolved Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.





2.4.3. cuSPARSE: Release 12.4ï

New Features

Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess().
Added support for mixed real and complex types for cusparseSpMM().
Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM().


Known Issues

cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.


Resolved Issues

cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.





2.4.4. cuSPARSE: Release 12.3 Update 1ï

New Features

Added support for block sizes of 64 and 128 in cusparseSDDMM().
Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.





2.4.5. cuSPARSE: Release 12.3ï

New Features

The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.
The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.


Known Issues

The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.
Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.


Resolved Issues

cusparseSpSV() provided indeterministic results in some cases.
Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.
Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.





2.4.6. cuSPARSE: Release 12.2 Update 1ï

New Features

The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api.


Resolved Issues

Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.
Clarified the supported operations for cusparseSDDMM().
cusparseCreateConstSlicedEll() now uses const pointers.
Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.
cusparseSpSM_bufferSize() could ask slightly less memory than needed.
cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.


Deprecations

Several legacy APIs have been officially deprecated. A compile-time warning has been added to all of them.





2.4.7. cuSPARSE: Release 12.1 Update 1ï

New Features

Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine (cusparseSDDMM).
Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication (cusparseSpMV) and triangular solver with a single right-hand side (cusparseSpSV).
Added a new API call (cusparseSpSV_updateMatrix) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.





2.4.8. cuSPARSE: Release 12.0 Update 1ï

New Features

cusparseSDDMM() now supports mixed precision computation.
Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.
Improved cusparseSpMV() performance with a new load balancing algorithm.
cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.


Resolved Issues

cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.





2.4.9. cuSPARSE: Release 12.0ï

New Features

JIT LTO functionalities (cusparseSpMMOp()) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so, see cuSPARSE documentation. JIT LTO performance has also been improved for cusparseSpMMOpPlan().
Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet(). Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.
Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.
Added int8_t support to cusparseGather(), cusparseScatter(), and cusparseCsr2cscEx2().
Improved cusparseSpSV() performance for both the analysis and the solving phases.
Improved cusparseSpSM() performance for both the analysis and the solving phases.
Improved cusparseSDDMM() performance and added support for batch computation.
Improved cusparseCsr2cscEx2() performance.


Resolved Issues

cusparseSpSV() and cusparseSpSM() could produce wrong results.
cusparseDnMatGetStridedBatch() did not accept batchStride == 0.


Deprecations

Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.






2.5. Math Libraryï

2.5.1. CUDA Math: Release 12.5ï

Known Issues

As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.





2.5.2. CUDA Math: Release 12.4ï

Resolved Issues

Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.





2.5.3. CUDA Math: Release 12.3ï

New Features

Performance of SIMD Integer CUDA Math APIs was improved.


Resolved Issues

The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.


Known Issues

Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half, __half2, __nv_bfloat16, __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing. This behavior may improve in future versions of the headers.





2.5.4. CUDA Math: Release 12.2ï

New Features

CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side <emulated> support for many of the arithmetic operations and conversions.
__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution. Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release):

__CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__
__CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__




Resolved Issues

During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware.
Updated the observed worst case error bounds for single precision intrinsic functions __expf(), __exp10f() and double precision functions asinh(), acosh().





2.5.5. CUDA Math: Release 12.1ï

New Features

Performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf.





2.5.6. CUDA Math: Release 12.0ï

New Features

Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html.


Known Issues

Double precision inputs that cause the double precision division algorithm in the default âround to nearest even modeâ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn(). Affected CUDA language operation: double precision / operation in the device code.


Deprecations

All previously deprecated undocumented APIs are removed from CUDA 12.0.






2.6. NVIDIA Performance Primitives (NPP)ï

2.6.1. NPP: Release 12.4ï

New Features

Enhanced large file support with size_t.





2.6.2. NPP: Release 12.0ï

Deprecations

Deprecating non-CTX API support from next release.


Resolved Issues

A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.






2.7. nvJPEG Libraryï

2.7.1. nvJPEG: Release 12.4ï

New Features

IDCT performance optimizations for single image CUDA decode.
Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE.





2.7.2. nvJPEG: Release 12.3 Update 1ï

New Features

New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.





2.7.3. nvJPEG: Release 12.2ï

New Features

Added support for JPEG Lossless decode (process 14, FO prediction).
nvJPEG is now supported on L4T.





2.7.4. nvJPEG: Release 12.0ï

New Features

Immproved the GPU Memory optimisation for the nvJPEG codec.


Resolved Issues

An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.
An issue with CMYK four component color conversion is now resolved.


Known Issues

Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.


Deprecations

The reuse of Huffman table in Encoder (nvjpegEncoderParamsCopyHuffmanTables).




1
Only available on select Linux distros






3. Noticesï

3.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


3.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


3.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      

















CUDA Features Archive










































1. CUDA 11.6 Features

1.1. Compiler
1.1.1. VS2022 Support
1.1.2. New instructions in public PTX
1.1.3. Unused Kernel Optimization
1.1.4. New -arch=native option
1.1.5. Generate PTX from nvlink:
1.1.6. Bullseye support
1.1.7. INT128 developer tool support





2. Notices
2.1. Notice
2.2. OpenCL
2.3. Trademarks








CUDA Features Archive






 Â»

1. CUDA 11.6 Features



v12.5 |
PDF
|
Archive
Â 






NVIDIA CUDA Features Archive
The list of CUDA features by release.


1. CUDA 11.6 Featuresï



1.1. Compilerï



1.1.1. VS2022 Supportï

CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here. A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it.



1.1.2. New instructions in public PTXï

New instructions for bit mask creationâBMSK, and sign extensionâSZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT.



1.1.3. Unused Kernel Optimizationï

In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an opt-in feature but in 11.6, this feature is enabled by default. As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.

$ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info





1.1.4. New -arch=native optionï

In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1. This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in.



1.1.5. Generate PTX from nvlink:ï

Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN:

nvcc -dlto -dlink -ptx


Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.
With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization.



1.1.6. Bullseye supportï

NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye.



1.1.7. INT128 developer tool supportï

In 11.5, CUDA C++ support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu++, int 128 data datype is supported by math functions.





2. Noticesï



2.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



2.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



2.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Features Archive










































1. CUDA 11.6 Features

1.1. Compiler
1.1.1. VS2022 Support
1.1.2. New instructions in public PTX
1.1.3. Unused Kernel Optimization
1.1.4. New -arch=native option
1.1.5. Generate PTX from nvlink:
1.1.6. Bullseye support
1.1.7. INT128 developer tool support





2. Notices
2.1. Notice
2.2. OpenCL
2.3. Trademarks








CUDA Features Archive






 Â»

1. CUDA 11.6 Features



v12.5 |
PDF
|
Archive
Â 






NVIDIA CUDA Features Archive
The list of CUDA features by release.


1. CUDA 11.6 Featuresï



1.1. Compilerï



1.1.1. VS2022 Supportï

CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here. A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it.



1.1.2. New instructions in public PTXï

New instructions for bit mask creationâBMSK, and sign extensionâSZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT.



1.1.3. Unused Kernel Optimizationï

In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an opt-in feature but in 11.6, this feature is enabled by default. As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.

$ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info





1.1.4. New -arch=native optionï

In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1. This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in.



1.1.5. Generate PTX from nvlink:ï

Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN:

nvcc -dlto -dlink -ptx


Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.
With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization.



1.1.6. Bullseye supportï

NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye.



1.1.7. INT128 developer tool supportï

In 11.5, CUDA C++ support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu++, int 128 data datype is supported by math functions.





2. Noticesï



2.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



2.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



2.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Features Archive










































1. CUDA 11.6 Features

1.1. Compiler
1.1.1. VS2022 Support
1.1.2. New instructions in public PTX
1.1.3. Unused Kernel Optimization
1.1.4. New -arch=native option
1.1.5. Generate PTX from nvlink:
1.1.6. Bullseye support
1.1.7. INT128 developer tool support





2. Notices
2.1. Notice
2.2. OpenCL
2.3. Trademarks








CUDA Features Archive






 Â»

1. CUDA 11.6 Features



v12.5 |
PDF
|
Archive
Â 






NVIDIA CUDA Features Archive
The list of CUDA features by release.


1. CUDA 11.6 Featuresï



1.1. Compilerï



1.1.1. VS2022 Supportï

CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here. A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it.



1.1.2. New instructions in public PTXï

New instructions for bit mask creationâBMSK, and sign extensionâSZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT.



1.1.3. Unused Kernel Optimizationï

In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an opt-in feature but in 11.6, this feature is enabled by default. As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.

$ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info





1.1.4. New -arch=native optionï

In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1. This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in.



1.1.5. Generate PTX from nvlink:ï

Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN:

nvcc -dlto -dlink -ptx


Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.
With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization.



1.1.6. Bullseye supportï

NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye.



1.1.7. INT128 developer tool supportï

In 11.5, CUDA C++ support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu++, int 128 data datype is supported by math functions.





2. Noticesï



2.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



2.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



2.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Features Archive










































1. CUDA 11.6 Features

1.1. Compiler
1.1.1. VS2022 Support
1.1.2. New instructions in public PTX
1.1.3. Unused Kernel Optimization
1.1.4. New -arch=native option
1.1.5. Generate PTX from nvlink:
1.1.6. Bullseye support
1.1.7. INT128 developer tool support





2. Notices
2.1. Notice
2.2. OpenCL
2.3. Trademarks








CUDA Features Archive






 Â»

1. CUDA 11.6 Features



v12.5 |
PDF
|
Archive
Â 






NVIDIA CUDA Features Archive
The list of CUDA features by release.


1. CUDA 11.6 Featuresï



1.1. Compilerï



1.1.1. VS2022 Supportï

CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here. A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it.



1.1.2. New instructions in public PTXï

New instructions for bit mask creationâBMSK, and sign extensionâSZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT.



1.1.3. Unused Kernel Optimizationï

In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an opt-in feature but in 11.6, this feature is enabled by default. As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.

$ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info





1.1.4. New -arch=native optionï

In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1. This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in.



1.1.5. Generate PTX from nvlink:ï

Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN:

nvcc -dlto -dlink -ptx


Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.
With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization.



1.1.6. Bullseye supportï

NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye.



1.1.7. INT128 developer tool supportï

In 11.5, CUDA C++ support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu++, int 128 data datype is supported by math functions.





2. Noticesï



2.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



2.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



2.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















PTX ISA 8.5










































1. Introduction
1.1. Scalable Data-Parallel Computing using GPUs
1.2. Goals of PTX
1.3. PTX ISA Version 8.5
1.4. Document Structure



2. Programming Model
2.1. A Highly Multithreaded Coprocessor

2.2. Thread Hierarchy
2.2.1. Cooperative Thread Arrays
2.2.2. Cluster of Cooperative Thread Arrays
2.2.3. Grid of Clusters


2.3. Memory Hierarchy



3. PTX Machine Model
3.1. A Set of SIMT Multiprocessors
3.2. Independent Thread Scheduling
3.3. On-chip Shared Memory



4. Syntax
4.1. Source Format
4.2. Comments

4.3. Statements
4.3.1. Directive Statements
4.3.2. Instruction Statements


4.4. Identifiers

4.5. Constants
4.5.1. Integer Constants
4.5.2. Floating-Point Constants
4.5.3. Predicate Constants
4.5.4. Constant Expressions
4.5.5. Integer Constant Expression Evaluation
4.5.6. Summary of Constant Expression Evaluation Rules





5. State Spaces, Types, and Variables

5.1. State Spaces
5.1.1. Register State Space
5.1.2. Special Register State Space

5.1.3. Constant State Space
5.1.3.1. Banked Constant State Space (deprecated)


5.1.4. Global State Space
5.1.5. Local State Space

5.1.6. Parameter State Space
5.1.6.1. Kernel Function Parameters
5.1.6.2. Kernel Function Parameter Attributes
5.1.6.3. Kernel Parameter Attribute: .ptr
5.1.6.4. Device Function Parameters


5.1.7. Shared State Space
5.1.8. Texture State Space (deprecated)



5.2. Types
5.2.1. Fundamental Types
5.2.2. Restricted Use of Sub-Word Sizes
5.2.3. Alternate Floating-Point Data Formats

5.2.4. Packed Data Types
5.2.4.1. Packed Floating Point Data Types
5.2.4.2. Packed Integer Data Types





5.3. Texture Sampler and Surface Types
5.3.1. Texture and Surface Properties
5.3.2. Sampler Properties
5.3.3. Channel Data Type and Channel Order Fields



5.4. Variables
5.4.1. Variable Declarations
5.4.2. Vectors
5.4.3. Array Declarations
5.4.4. Initializers
5.4.5. Alignment
5.4.6. Parameterized Variable Names
5.4.7. Variable Attributes
5.4.8. Variable and Function Attribute Directive: .attribute



5.5. Tensors
5.5.1. Tensor Dimension, size and format
5.5.2. Tensor Access Modes

5.5.3. Tiled Mode
5.5.3.1. Bounding Box
5.5.3.2. Traversal-Stride
5.5.3.3. Out of Boundary Access



5.5.4. Im2col mode
5.5.4.1. Bounding Box
5.5.4.2. Traversal Stride
5.5.4.3. Out of Boundary Access


5.5.5. Interleave layout
5.5.6. Swizzling Modes
5.5.7. Tensor-map





6. Instruction Operands
6.1. Operand Type Information
6.2. Source Operands
6.3. Destination Operands

6.4. Using Addresses, Arrays, and Vectors

6.4.1. Addresses as Operands
6.4.1.1. Generic Addressing


6.4.2. Arrays as Operands
6.4.3. Vectors as Operands
6.4.4. Labels and Function Names as Operands



6.5. Type Conversion
6.5.1. Scalar Conversions
6.5.2. Rounding Modifiers


6.6. Operand Costs



7. Abstracting the ABI

7.1. Function Declarations and Definitions
7.1.1. Changes from PTX ISA Version 1.x


7.2. Variadic Functions
7.3. Alloca



8. Memory Consistency Model

8.1. Scope and applicability of the model
8.1.1. Limitations on atomicity at system scope



8.2. Memory operations
8.2.1. Overlap
8.2.2. Aliases
8.2.3. Multimem Addresses
8.2.4. Memory Operations on Vector Data Types
8.2.5. Memory Operations on Packed Data Types
8.2.6. Initialization


8.3. State spaces

8.4. Operation types
8.4.1. mmio Operation


8.5. Scope
8.6. Proxies

8.7. Morally strong operations
8.7.1. Conflict and Data-races
8.7.2. Limitations on Mixed-size Data-races


8.8. Release and Acquire Patterns

8.9. Ordering of memory operations

8.9.1. Program Order
8.9.1.1. Asynchronous Operations


8.9.2. Observation Order
8.9.3. Fence-SC Order
8.9.4. Memory synchronization
8.9.5. Causality Order
8.9.6. Coherence Order
8.9.7. Communication Order



8.10. Axioms
8.10.1. Coherence
8.10.2. Fence-SC
8.10.3. Atomicity
8.10.4. No Thin Air
8.10.5. Sequential Consistency Per Location
8.10.6. Causality





9. Instruction Set
9.1. Format and Semantics of Instruction Descriptions
9.2. PTX Instructions

9.3. Predicated Execution

9.3.1. Comparisons
9.3.1.1. Integer and Bit-Size Comparisons
9.3.1.2. Floating Point Comparisons


9.3.2. Manipulating Predicates



9.4. Type Information for Instructions and Operands
9.4.1. Operand Size Exceeding Instruction-Type Size


9.5. Divergence of Threads in Control Constructs

9.6. Semantics
9.6.1. Machine-Specific Semantics of 16-bit Code



9.7. Instructions

9.7.1. Integer Arithmetic Instructions
9.7.1.1. Integer Arithmetic Instructions: add
9.7.1.2. Integer Arithmetic Instructions: sub
9.7.1.3. Integer Arithmetic Instructions: mul
9.7.1.4. Integer Arithmetic Instructions: mad
9.7.1.5. Integer Arithmetic Instructions: mul24
9.7.1.6. Integer Arithmetic Instructions: mad24
9.7.1.7. Integer Arithmetic Instructions: sad
9.7.1.8. Integer Arithmetic Instructions: div
9.7.1.9. Integer Arithmetic Instructions: rem
9.7.1.10. Integer Arithmetic Instructions: abs
9.7.1.11. Integer Arithmetic Instructions: neg
9.7.1.12. Integer Arithmetic Instructions: min
9.7.1.13. Integer Arithmetic Instructions: max
9.7.1.14. Integer Arithmetic Instructions: popc
9.7.1.15. Integer Arithmetic Instructions: clz
9.7.1.16. Integer Arithmetic Instructions: bfind
9.7.1.17. Integer Arithmetic Instructions: fns
9.7.1.18. Integer Arithmetic Instructions: brev
9.7.1.19. Integer Arithmetic Instructions: bfe
9.7.1.20. Integer Arithmetic Instructions: bfi
9.7.1.21. Integer Arithmetic Instructions: szext
9.7.1.22. Integer Arithmetic Instructions: bmsk
9.7.1.23. Integer Arithmetic Instructions: dp4a
9.7.1.24. Integer Arithmetic Instructions: dp2a



9.7.2. Extended-Precision Integer Arithmetic Instructions
9.7.2.1. Extended-Precision Arithmetic Instructions: add.cc
9.7.2.2. Extended-Precision Arithmetic Instructions: addc
9.7.2.3. Extended-Precision Arithmetic Instructions: sub.cc
9.7.2.4. Extended-Precision Arithmetic Instructions: subc
9.7.2.5. Extended-Precision Arithmetic Instructions: mad.cc
9.7.2.6. Extended-Precision Arithmetic Instructions: madc



9.7.3. Floating-Point Instructions
9.7.3.1. Floating Point Instructions: testp
9.7.3.2. Floating Point Instructions: copysign
9.7.3.3. Floating Point Instructions: add
9.7.3.4. Floating Point Instructions: sub
9.7.3.5. Floating Point Instructions: mul
9.7.3.6. Floating Point Instructions: fma
9.7.3.7. Floating Point Instructions: mad
9.7.3.8. Floating Point Instructions: div
9.7.3.9. Floating Point Instructions: abs
9.7.3.10. Floating Point Instructions: neg
9.7.3.11. Floating Point Instructions: min
9.7.3.12. Floating Point Instructions: max
9.7.3.13. Floating Point Instructions: rcp
9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64
9.7.3.15. Floating Point Instructions: sqrt
9.7.3.16. Floating Point Instructions: rsqrt
9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64
9.7.3.18. Floating Point Instructions: sin
9.7.3.19. Floating Point Instructions: cos
9.7.3.20. Floating Point Instructions: lg2
9.7.3.21. Floating Point Instructions: ex2
9.7.3.22. Floating Point Instructions: tanh



9.7.4. Half Precision Floating-Point Instructions
9.7.4.1. Half Precision Floating Point Instructions: add
9.7.4.2. Half Precision Floating Point Instructions: sub
9.7.4.3. Half Precision Floating Point Instructions: mul
9.7.4.4. Half Precision Floating Point Instructions: fma
9.7.4.5. Half Precision Floating Point Instructions: neg
9.7.4.6. Half Precision Floating Point Instructions: abs
9.7.4.7. Half Precision Floating Point Instructions: min
9.7.4.8. Half Precision Floating Point Instructions: max
9.7.4.9. Half Precision Floating Point Instructions: tanh
9.7.4.10. Half Precision Floating Point Instructions: ex2



9.7.5. Comparison and Selection Instructions
9.7.5.1. Comparison and Selection Instructions: set
9.7.5.2. Comparison and Selection Instructions: setp
9.7.5.3. Comparison and Selection Instructions: selp
9.7.5.4. Comparison and Selection Instructions: slct



9.7.6. Half Precision Comparison Instructions
9.7.6.1. Half Precision Comparison Instructions: set
9.7.6.2. Half Precision Comparison Instructions: setp



9.7.7. Logic and Shift Instructions
9.7.7.1. Logic and Shift Instructions: and
9.7.7.2. Logic and Shift Instructions: or
9.7.7.3. Logic and Shift Instructions: xor
9.7.7.4. Logic and Shift Instructions: not
9.7.7.5. Logic and Shift Instructions: cnot
9.7.7.6. Logic and Shift Instructions: lop3
9.7.7.7. Logic and Shift Instructions: shf
9.7.7.8. Logic and Shift Instructions: shl
9.7.7.9. Logic and Shift Instructions: shr



9.7.8. Data Movement and Conversion Instructions
9.7.8.1. Cache Operators
9.7.8.2. Cache Eviction Priority Hints
9.7.8.3. Data Movement and Conversion Instructions: mov
9.7.8.4. Data Movement and Conversion Instructions: mov
9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated)
9.7.8.6. Data Movement and Conversion Instructions: shfl.sync
9.7.8.7. Data Movement and Conversion Instructions: prmt
9.7.8.8. Data Movement and Conversion Instructions: ld
9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc
9.7.8.10. Data Movement and Conversion Instructions: ldu
9.7.8.11. Data Movement and Conversion Instructions: st
9.7.8.12. Data Movement and Conversion Instructions: st.async
9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red
9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu
9.7.8.15. Data Movement and Conversion Instructions: applypriority
9.7.8.16. Data Movement and Conversion Instructions: discard
9.7.8.17. Data Movement and Conversion Instructions: createpolicy
9.7.8.18. Data Movement and Conversion Instructions: isspacep
9.7.8.19. Data Movement and Conversion Instructions: cvta
9.7.8.20. Data Movement and Conversion Instructions: cvt
9.7.8.21. Data Movement and Conversion Instructions: cvt.pack
9.7.8.22. Data Movement and Conversion Instructions: mapa
9.7.8.23. Data Movement and Conversion Instructions: getctarank

9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copy
9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operations
9.7.8.24.2. Async Proxy
9.7.8.24.3. Data Movement and Conversion Instructions: cp.async
9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_group
9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all
9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulk
9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk
9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch
9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor
9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor
9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor
9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group
9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group


9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace



9.7.9. Texture Instructions
9.7.9.1. Texturing Modes
9.7.9.2. Mipmaps
9.7.9.3. Texture Instructions: tex
9.7.9.4. Texture Instructions: tld4
9.7.9.5. Texture Instructions: txq
9.7.9.6. Texture Instructions: istypep



9.7.10. Surface Instructions
9.7.10.1. Surface Instructions: suld
9.7.10.2. Surface Instructions: sust
9.7.10.3. Surface Instructions: sured
9.7.10.4. Surface Instructions: suq



9.7.11. Control Flow Instructions
9.7.11.1. Control Flow Instructions: {}
9.7.11.2. Control Flow Instructions: @
9.7.11.3. Control Flow Instructions: bra
9.7.11.4. Control Flow Instructions: brx.idx
9.7.11.5. Control Flow Instructions: call
9.7.11.6. Control Flow Instructions: ret
9.7.11.7. Control Flow Instructions: exit



9.7.12. Parallel Synchronization and Communication Instructions
9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrier
9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.sync
9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.cluster
9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fence
9.7.12.5. Parallel Synchronization and Communication Instructions: atom
9.7.12.6. Parallel Synchronization and Communication Instructions: red
9.7.12.7. Parallel Synchronization and Communication Instructions: red.async
9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated)
9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync
9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync
9.7.12.11. Parallel Synchronization and Communication Instructions: activemask
9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync
9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol
9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync

9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier
9.7.12.15.1. Size and alignment of mbarrier object
9.7.12.15.2. Contents of the mbarrier object
9.7.12.15.3. Lifecycle of the mbarrier object
9.7.12.15.4. Phase of the mbarrier object

9.7.12.15.5. Tracking asynchronous operations by the mbarrier object
9.7.12.15.5.1. expect-tx operation
9.7.12.15.5.2. complete-tx operation


9.7.12.15.6. Phase Completion of the mbarrier object
9.7.12.15.7. Arrive-on operation on mbarrier object
9.7.12.15.8. mbarrier support with shared memory
9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init
9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval
9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx
9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx
9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive
9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop
9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive
9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait
9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count
9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy





9.7.13. Warp Level Matrix Multiply-Accumulate Instructions
9.7.13.1. Matrix Shape
9.7.13.2. Matrix Data-types

9.7.13.3. Matrix multiply-accumulate operation using wmma instructions
9.7.13.3.1. Matrix Fragments for WMMA
9.7.13.3.2. Matrix Storage for WMMA
9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load
9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store
9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma



9.7.13.4. Matrix multiply-accumulate operation using mma instruction
9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type
9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point type
9.7.13.4.3. Matrix Fragments for mma.m8n8k16
9.7.13.4.4. Matrix Fragments for mma.m8n8k32
9.7.13.4.5. Matrix Fragments for mma.m8n8k128
9.7.13.4.6. Matrix Fragments for mma.m16n8k4
9.7.13.4.7. Matrix Fragments for mma.m16n8k8
9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type
9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type
9.7.13.4.10. Matrix Fragments for mma.m16n8k32
9.7.13.4.11. Matrix Fragments for mma.m16n8k64
9.7.13.4.12. Matrix Fragments for mma.m16n8k128
9.7.13.4.13. Matrix Fragments for mma.m16n8k256
9.7.13.4.14. Multiply-and-Accumulate Instruction: mma
9.7.13.4.15. Warp-level matrix load instruction: ldmatrix
9.7.13.4.16. Warp-level matrix store instruction: stmatrix
9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix



9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A
9.7.13.5.1. Sparse matrix storage

9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A
9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types
9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types
9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type
9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type
9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type
9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 type
9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type
9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer type


9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata





9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions
9.7.14.1. Warpgroup
9.7.14.2. Matrix Shape
9.7.14.3. Matrix Data-types
9.7.14.4. Async Proxy

9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction

9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts

9.7.14.5.1.1. Register Fragments
9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16
9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8
9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32
9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256



9.7.14.5.1.2. Shared Memory Matrix Layout
9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16
9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8
9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32
9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256
9.7.14.5.1.2.5. Strides
9.7.14.5.1.2.6. Swizzling Modes
9.7.14.5.1.2.7. Matrix Descriptor Format




9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async



9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction
9.7.14.6.1. Sparse matrix storage

9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A
9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32
9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16
9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64



9.7.14.6.3. Shared Memory Matrix Layout
9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32
9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16
9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64


9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp



9.7.14.7. Asynchronous wgmma Proxy Operations
9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence
9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group
9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group





9.7.15. Stack Manipulation Instructions
9.7.15.1. Stack Manipulation Instructions: stacksave
9.7.15.2. Stack Manipulation Instructions: stackrestore
9.7.15.3. Stack Manipulation Instructions: alloca



9.7.16. Video Instructions

9.7.16.1. Scalar Video Instructions
9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax
9.7.16.1.2. Scalar Video Instructions: vshl, vshr
9.7.16.1.3. Scalar Video Instructions: vmad
9.7.16.1.4. Scalar Video Instructions: vset



9.7.16.2. SIMD Video Instructions
9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2
9.7.16.2.2. SIMD Video Instructions: vset2
9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4
9.7.16.2.4. SIMD Video Instructions: vset4





9.7.17. Miscellaneous Instructions
9.7.17.1. Miscellaneous Instructions: brkpt
9.7.17.2. Miscellaneous Instructions: nanosleep
9.7.17.3. Miscellaneous Instructions: pmevent
9.7.17.4. Miscellaneous Instructions: trap
9.7.17.5. Miscellaneous Instructions: setmaxnreg







10. Special Registers
10.1. Special Registers: %tid
10.2. Special Registers: %ntid
10.3. Special Registers: %laneid
10.4. Special Registers: %warpid
10.5. Special Registers: %nwarpid
10.6. Special Registers: %ctaid
10.7. Special Registers: %nctaid
10.8. Special Registers: %smid
10.9. Special Registers: %nsmid
10.10. Special Registers: %gridid
10.11. Special Registers: %is_explicit_cluster
10.12. Special Registers: %clusterid
10.13. Special Registers: %nclusterid
10.14. Special Registers: %cluster_ctaid
10.15. Special Registers: %cluster_nctaid
10.16. Special Registers: %cluster_ctarank
10.17. Special Registers: %cluster_nctarank
10.18. Special Registers: %lanemask_eq
10.19. Special Registers: %lanemask_le
10.20. Special Registers: %lanemask_lt
10.21. Special Registers: %lanemask_ge
10.22. Special Registers: %lanemask_gt
10.23. Special Registers: %clock, %clock_hi
10.24. Special Registers: %clock64
10.25. Special Registers: %pm0..%pm7
10.26. Special Registers: %pm0_64..%pm7_64
10.27. Special Registers: %envreg<32>
10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi
10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2>
10.30. Special Registers: %total_smem_size
10.31. Special Registers: %aggr_smem_size
10.32. Special Registers: %dynamic_smem_size
10.33. Special Registers: %current_graph_exec



11. Directives

11.1. PTX Module Directives
11.1.1. PTX Module Directives: .version
11.1.2. PTX Module Directives: .target
11.1.3. PTX Module Directives: .address_size



11.2. Specifying Kernel Entry Points and Functions
11.2.1. Kernel and Function Directives: .entry
11.2.2. Kernel and Function Directives: .func
11.2.3. Kernel and Function Directives: .alias



11.3. Control Flow Directives
11.3.1. Control Flow Directives: .branchtargets
11.3.2. Control Flow Directives: .calltargets
11.3.3. Control Flow Directives: .callprototype



11.4. Performance-Tuning Directives
11.4.1. Performance-Tuning Directives: .maxnreg
11.4.2. Performance-Tuning Directives: .maxntid
11.4.3. Performance-Tuning Directives: .reqntid
11.4.4. Performance-Tuning Directives: .minnctapersm
11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated)
11.4.6. Performance-Tuning Directives: .noreturn
11.4.7. Performance-Tuning Directives: .pragma



11.5. Debugging Directives
11.5.1. Debugging Directives: @@dwarf
11.5.2. Debugging Directives: .section
11.5.3. Debugging Directives: .file
11.5.4. Debugging Directives: .loc



11.6. Linking Directives
11.6.1. Linking Directives: .extern
11.6.2. Linking Directives: .visible
11.6.3. Linking Directives: .weak
11.6.4. Linking Directives: .common



11.7. Cluster Dimension Directives
11.7.1. Cluster Dimension Directives: .reqnctapercluster
11.7.2. Cluster Dimension Directives: .explicitcluster
11.7.3. Cluster Dimension Directives: .maxclusterrank





12. Release Notes
12.1. Changes in PTX ISA Version 8.5
12.2. Changes in PTX ISA Version 8.4
12.3. Changes in PTX ISA Version 8.3
12.4. Changes in PTX ISA Version 8.2
12.5. Changes in PTX ISA Version 8.1
12.6. Changes in PTX ISA Version 8.0
12.7. Changes in PTX ISA Version 7.8
12.8. Changes in PTX ISA Version 7.7
12.9. Changes in PTX ISA Version 7.6
12.10. Changes in PTX ISA Version 7.5
12.11. Changes in PTX ISA Version 7.4
12.12. Changes in PTX ISA Version 7.3
12.13. Changes in PTX ISA Version 7.2
12.14. Changes in PTX ISA Version 7.1
12.15. Changes in PTX ISA Version 7.0
12.16. Changes in PTX ISA Version 6.5
12.17. Changes in PTX ISA Version 6.4
12.18. Changes in PTX ISA Version 6.3
12.19. Changes in PTX ISA Version 6.2
12.20. Changes in PTX ISA Version 6.1
12.21. Changes in PTX ISA Version 6.0
12.22. Changes in PTX ISA Version 5.0
12.23. Changes in PTX ISA Version 4.3
12.24. Changes in PTX ISA Version 4.2
12.25. Changes in PTX ISA Version 4.1
12.26. Changes in PTX ISA Version 4.0
12.27. Changes in PTX ISA Version 3.2
12.28. Changes in PTX ISA Version 3.1
12.29. Changes in PTX ISA Version 3.0
12.30. Changes in PTX ISA Version 2.3
12.31. Changes in PTX ISA Version 2.2
12.32. Changes in PTX ISA Version 2.1
12.33. Changes in PTX ISA Version 2.0



14. Descriptions of .pragma Strings
14.1. Pragma Strings: ânounrollâ
14.2. Pragma Strings: âused_bytes_maskâ



15. Notices
15.1. Notice
15.2. OpenCL
15.3. Trademarks








PTX ISA






 Â»

1. Introduction



v8.5 |
PDF
|
Archive
Â 






Parallel Thread Execution ISA Version 8.5
The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture).


1. Introductionï

This document describes PTX, a low-level parallel thread execution virtual machine and instruction
set architecture (ISA). PTX exposes the GPU as a data-parallel computing device.


1.1. Scalable Data-Parallel Computing using GPUsï

Driven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable
GPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous
computational horsepower and very high memory bandwidth. The GPU is especially well-suited to
address problems that can be expressed as data-parallel computations - the same program is executed
on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic
operations to memory operations. Because the same program is executed for each data element, there
is a lower requirement for sophisticated flow control; and because it is executed on many data
elements and has high arithmetic intensity, the memory access latency can be hidden with
calculations instead of big data caches.
Data-parallel processing maps data elements to parallel processing threads. Many applications that
process large data sets can use a data-parallel programming model to speed up the computations. In
3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and
media processing applications such as post-processing of rendered images, video encoding and
decoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to
parallel processing threads. In fact, many algorithms outside the field of image rendering and
processing are accelerated by data-parallel processing, from general signal processing or physics
simulation to computational finance or computational biology.
PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs
are translated at install time to the target hardware instruction set. The PTX-to-GPU translator
and driver enable NVIDIA GPUs to be used as programmable parallel computers.



1.2. Goals of PTXï

PTX provides a stable programming model and instruction set for general purpose parallel
programming. It is designed to be efficient on NVIDIA GPUs supporting the computation features
defined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA
and C/C++ generate PTX instructions, which are optimized for and translated to native
target-architecture instructions.
The goals for PTX include the following:

Provide a stable ISA that spans multiple GPU generations.
Achieve performance in compiled applications comparable to native GPU performance.
Provide a machine-independent ISA for C/C++ and other compilers to target.
Provide a code distribution ISA for application and middleware developers.
Provide a common source-level ISA for optimizing code generators and translators, which map PTX to
specific target machines.
Facilitate hand-coding of libraries, performance kernels, and architecture tests.
Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units.




1.3. PTX ISA Version 8.5ï

PTX ISA version 8.5 introduces the following new features:

Adds support for mma.sp::ordered_metadata instruction.




1.4. Document Structureï

The information in this document is organized into the following Chapters:

Programming Model outlines the programming model.
PTX Machine Model gives an overview of the PTX virtual machine model.
Syntax describes the basic syntax of the PTX language.
State Spaces, Types, and Variables describes
state spaces, types, and variable declarations.
Instruction Operands describes instruction operands.
Abstracting the ABI describes the function and call syntax,
calling convention, and PTX support for abstracting the Application Binary Interface (ABI).
Instruction Set describes the instruction set.
Special Registers lists special registers.
Directives lists the assembly directives supported in PTX.
Release Notes provides release notes for PTX ISA versions 2.x and
beyond.

References


754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008.
http://ieeexplore.ieee.org/servlet/opac?punumber=4610933


The OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011.
http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf


CUDA Programming Guide.
https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html


CUDA Dynamic Parallelism Programming Guide.
https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism


CUDA Atomicity Requirements.
https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity


PTX Writers Guide to Interoperability.
https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html






2. Programming Modelï



2.1. A Highly Multithreaded Coprocessorï

The GPU is a compute device capable of executing a very large number of threads in parallel. It
operates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive
portions of applications running on the host are off-loaded onto the device.
More precisely, a portion of an application that is executed many times, but independently on
different data, can be isolated into a kernel function that is executed on the GPU as many different
threads. To that effect, such a function is compiled to the PTX instruction set and the resulting
kernel is translated at install time to the target GPU instruction set.



2.2. Thread Hierarchyï

The batch of threads that executes a kernel is organized as a grid. A grid consists of either
cooperative thread arrays or clusters of cooperative thread arrays as described in this section and
illustrated in Figure 1 and
Figure 2. Cooperative thread arrays (CTAs) implement CUDA
thread blocks and clusters implement CUDA thread block clusters.


2.2.1. Cooperative Thread Arraysï

The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program
specifies the execution of a given thread of a parallel thread array. A cooperative thread array,
or CTA, is an array of threads that execute a kernel concurrently or in parallel.
Threads within a CTA can communicate with each other. To coordinate the communication of the threads
within the CTA, one can specify synchronization points where threads wait until all threads in the
CTA have arrived.
Each thread has a unique thread identifier within the CTA. Programs use a data parallel
decomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread
uses its thread identifier to determine its assigned role, assign specific input and output
positions, compute addresses, and select work to perform. The thread identifier is a three-element
vector tid, (with elements tid.x, tid.y, and tid.z) that specifies the threadâs
position within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the
number of thread ids in that CTA dimension.
Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements
ntid.x, ntid.y, and ntid.z). The vector ntid specifies the number of threads in each
CTA dimension.
Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called
warps. A warp is a maximal subset of threads from a single CTA, such that the threads execute
the same instructions at the same time. Threads within a warp are sequentially numbered. The warp
size is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be
able to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate
constant, WARP_SZ, which may be used in any instruction where an immediate operand is allowed.



2.2.2. Cluster of Cooperative Thread Arraysï

Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate
with each other via shared memory. The executing CTA has to make sure that the shared memory of the
peer CTA exists before communicating with it via shared memory and the peer CTA hasnât exited before
completing the shared memory operation.
Threads within the different CTAs in a cluster can synchronize and communicate with each other via
shared memory. Cluster-wide barriers can be used to synchronize all the threads within the
cluster. Each CTA in a cluster has a unique CTA identifier within its cluster
(cluster_ctaid). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter
cluster_nctaid. Each CTA in the cluster also has a unique CTA identifier (cluster_ctarank)
across all dimensions. The total number of CTAs across all the dimensions in the cluster is
specified by cluster_nctarank. Threads may read and use these values through predefined, read-only
special registers %cluster_ctaid, %cluster_nctaid, %cluster_ctarank,
%cluster_nctarank.
Cluster level is applicable only on target architecture sm_90 or higher. Specifying cluster
level during launch time is optional. If the user specifies the cluster dimensions at launch time
then it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster
launch with default dimension 1x1x1. PTX provides read-only special register
%is_explicit_cluster to differentiate between explicit and implicit cluster launch.



2.2.3. Grid of Clustersï

There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a
cluster can contain. However, clusters with CTAs that execute the same kernel can be batched
together into a grid of clusters, so that the total number of threads that can be launched in a
single kernel invocation is very large. This comes at the expense of reduced thread communication
and synchronization, because threads in different clusters cannot communicate and synchronize with
each other.
Each cluster has a unique cluster identifier (clusterid) within a grid of clusters. Each grid of
clusters has a 1D, 2D , or 3D shape specified by the parameter nclusterid. Each grid also has a
unique temporal grid identifier (gridid). Threads may read and use these values through
predefined, read-only special registers %tid, %ntid, %clusterid, %nclusterid, and
%gridid.
Each CTA has a unique identifier (ctaid) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape
specified by the parameter nctaid. Thread may use and read these values through predefined,
read-only special registers %ctaid and %nctaid.
Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs
where cluster is optional level and is applicable only for target architectures sm_90 and
higher. Figure 1 shows a grid consisting of CTAs and
Figure 2 shows a grid consisting of clusters.
Grids may be launched with dependencies between one another - a grid may be a dependent grid and/or
a prerequisite grid. To understand how grid dependencies may be defined, refer to the section on
CUDA Graphs in the Cuda Programming Guide.



Figure 1 Grid with CTAsï





Figure 2 Grid with clustersï

A cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads
that execute the same kernel program. A grid is a set of clusters consisting of CTAs that
execute independently.







2.3. Memory Hierarchyï

PTX threads may access data from multiple state spaces during their execution as illustrated by
Figure 3 where cluster level is introduced from
target architecture sm_90 onwards. Each thread has a private local memory. Each thread block
(CTA) has a shared memory visible to all threads of the block and to all active blocks in the
cluster and with the same lifetime as the block. Finally, all threads have access to the same global
memory.
There are additional state spaces accessible by all threads: the constant, param, texture, and
surface state spaces.  Constant and texture memory are read-only; surface memory is readable and
writable. The global, constant, param, texture, and surface state spaces are optimized for different
memory usages. For example, texture memory offers different addressing modes as well as data
filtering for specific data formats. Note that texture and surface memory is cached, and within the
same kernel call, the cache is not kept coherent with respect to global memory writes and surface
memory writes, so any texture fetch or surface read to an address that has been written to via a
global or a surface write in the same kernel call returns undefined data. In other words, a thread
can safely read some texture or surface memory location only if this memory location has been
updated by a previous kernel call or memory copy, but not if it has been previously updated by the
same thread or another thread from the same kernel call.
The global, constant, and texture state spaces are persistent across kernel launches by the same
application.
Both the host and the device maintain their own local memory, referred to as host memory and
device memory, respectively. The device memory may be mapped and read or written by the host, or,
for more efficient transfer, copied from the host memory through optimized API calls that utilize
the deviceâs high-performance Direct Memory Access (DMA) engine.



Figure 3 Memory Hierarchyï






3. PTX Machine Modelï



3.1. A Set of SIMT Multiprocessorsï

The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming
Multiprocessors (SMs). When a host program invokes a kernel grid, the blocks of the grid are
enumerated and distributed to multiprocessors with available execution capacity. The threads of a
thread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are
launched on the vacated multiprocessors.
A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction
unit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent
threads in hardware with zero scheduling overhead. It implements a single-instruction barrier
synchronization. Fast barrier synchronization together with lightweight thread creation and
zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for
example, a low granularity decomposition of problems by assigning one thread to each data element
(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).
To manage hundreds of threads running several different programs, the multiprocessor employs an
architecture we call SIMT (single-instruction, multiple-thread). The multiprocessor maps each
thread to one scalar processor core, and each scalar thread executes independently with its own
instruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,
and executes threads in groups of parallel threads called warps. (This term originates from
weaving, the first parallel thread technology.) Individual threads composing a SIMT warp start
together at the same program address but are otherwise free to branch and execute independently.
When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that
get scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp
contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.
At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues
the next instruction to the active threads of the warp. A warp executes one common instruction at a
time, so full efficiency is realized when all threads of a warp agree on their execution path. If
threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each
branch path taken, disabling threads that are not on that path, and when all paths complete, the
threads converge back to the same execution path. Branch divergence occurs only within a warp;
different warps execute independently regardless of whether they are executing common or disjointed
code paths.
SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a
single instruction controls multiple processing elements. A key difference is that SIMD vector
organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution
and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables
programmers to write thread-level parallel code for independent, scalar threads, as well as
data-parallel code for coordinated threads. For the purposes of correctness, the programmer can
essentially ignore the SIMT behavior; however, substantial performance improvements can be realized
by taking care that the code seldom requires threads in a warp to diverge. In practice, this is
analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when
designing for correctness but must be considered in the code structure when designing for peak
performance. Vector architectures, on the other hand, require the software to coalesce loads into
vectors and manage divergence manually.
How many blocks a multiprocessor can process at once depends on how many registers per thread and
how much shared memory per block are required for a given kernel since the multiprocessorâs
registers and shared memory are split among all the threads of the batch of blocks. If there are not
enough registers or shared memory available per multiprocessor to process at least one block, the
kernel will fail to launch.



Figure 4 Hardware Modelï


A set of SIMT multiprocessors with on-chip shared memory.



3.2. Independent Thread Schedulingï

On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads
in the warp together with an active mask specifying the active threads of the warp. As a result,
threads from the same warp in divergent regions or different states of execution cannot signal each
other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or
mutexes can easily lead to deadlock, depending on which warp the contending threads come from.
Starting with the Volta architecture, Independent Thread Scheduling allows full concurrency
between threads, regardless of warp. With Independent Thread Scheduling, the GPU maintains
execution state per thread, including a program counter and call stack, and can yield execution at a
per-thread granularity, either to make better use of execution resources or to allow one thread to
wait for data to be produced by another. A schedule optimizer determines how to group active threads
from the same warp together into SIMT units. This retains the high throughput of SIMT execution as
in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at
sub-warp granularity.
Independent Thread Scheduling can lead to a rather different set of threads participating in the
executed code than intended if the developer made assumptions about warp-synchronicity of previous
hardware architectures. In particular, any warp-synchronous code (such as synchronization-free,
intra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the
section on Compute Capability 7.x in the Cuda Programming Guide for further details.



3.3. On-chip Shared Memoryï

As illustrated by Figure 4, each multiprocessor has
on-chip memory of the four following types:

One set of local 32-bit registers per processor,
A parallel data cache or shared memory that is shared by all scalar processor cores and is where
the shared memory space resides,
A read-only constant cache that is shared by all scalar processor cores and speeds up reads from
the constant memory space, which is a read-only region of device memory,
A read-only texture cache that is shared by all scalar processor cores and speeds up reads from
the texture memory space, which is a read-only region of device memory; each multiprocessor
accesses the texture cache via a texture unit that implements the various addressing modes and
data filtering.

The local and global memory spaces are read-write regions of device memory.




4. Syntaxï

PTX programs are a collection of text source modules (files). PTX source modules have an
assembly-language style syntax with instruction operation codes and operands. Pseudo-operations
specify symbol and addressing management. The ptxas optimizing backend compiler optimizes and
assembles PTX source modules to produce corresponding binary object files.


4.1. Source Formatï

Source modules are ASCII text. Lines are separated by the newline character (\n).
All whitespace characters are equivalent; whitespace is ignored except for its use in separating
tokens in the language.
The C preprocessor cpp may be used to process PTX source modules. Lines beginning with # are
preprocessor directives. The following are common preprocessor directives:
#include, #define, #if, #ifdef, #else, #endif, #line, #file
C: A Reference Manual by Harbison and Steele provides a good description of the C preprocessor.
PTX is case sensitive and uses lowercase for keywords.
Each PTX module must begin with a .version directive specifying the PTX language version,
followed by a .target directive specifying the target architecture assumed. See PTX Module
Directives for a more information on these directives.



4.2. Commentsï

Comments in PTX follow C/C++ syntax, using non-nested /* and */ for comments that may span
multiple lines, and using // to begin a comment that extends up to the next newline character,
which terminates the current line. Comments cannot occur within character constants, string
literals, or within other comments.
Comments in PTX are treated as whitespace.



4.3. Statementsï

A PTX statement is either a directive or an instruction. Statements begin with an optional label and
end with a semicolon.
Examples

        .reg     .b32 r1, r2;
        .global  .f32  array[N];

start:  mov.b32   r1, %tid.x;
        shl.b32   r1, r1, 2;          // shift thread id by 2 bits
        ld.global.b32 r2, array[r1];  // thread[tid] gets array[tid]
        add.f32   r2, r2, 0.5;        // add 1/2




4.3.1. Directive Statementsï

Directive keywords begin with a dot, so no conflict is possible with user-defined identifiers. The
directives in PTX are listed in Table 1 and
described in State Spaces, Types, and Variables
and Directives.


Table 1 PTX Directivesï









.address_size
.explicitcluster
.maxnreg
.section


.alias
.extern
.maxntid
.shared


.align
.file
.minnctapersm
.sreg


.branchtargets
.func
.noreturn
.target


.callprototype
.global
.param
.tex


.calltargets
.loc
.pragma
.version


.common
.local
.reg
.visible


.const
.maxclusterrank
.reqnctapercluster
.weak


.entry
.maxnctapersm
.reqntid







4.3.2. Instruction Statementsï

Instructions are formed from an instruction opcode followed by a comma-separated list of zero or
more operands, and terminated with a semicolon. Operands may be register variables, constant
expressions, address expressions, or label names. Instructions have an optional guard predicate
which controls conditional execution. The guard predicate follows the optional label and precedes
the opcode, and is written as @p, where p is a predicate register. The guard predicate may
be optionally negated, written as @!p.
The destination operand is first, followed by source operands.
Instruction keywords are listed in
Table 2. All instruction keywords are
reserved tokens in PTX.


Table 2 Reserved Instruction Keywordsï










abs
discard
min
shf
vadd


activemask
div
mma
shfl
vadd2


add
dp2a
mov
shl
vadd4


addc
dp4a
movmatrix
shr
vavrg2


alloca
elect
mul
sin
vavrg4


and
ex2
mul24
slct
vmad


applypriority
exit
multimem
sqrt
vmax


atom
fence
nanosleep
st
vmax2


bar
fma
neg
stackrestore
vmax4


barrier
fns
not
stacksave
vmin


bfe
getctarank
or
stmatrix
vmin2


bfi
griddepcontrol
pmevent
sub
vmin4


bfind
isspacep
popc
subc
vote


bmsk
istypep
prefetch
suld
vset


bra
ld
prefetchu
suq
vset2


brev
ldmatrix
prmt
sured
vset4


brkpt
ldu
rcp
sust
vshl


brx
lg2
red
szext
vshr


call
lop3
redux
tanh
vsub


clz
mad
rem
testp
vsub2


cnot
mad24
ret
tex
vsub4


copysign
madc
rsqrt
tld4
wgmma


cos
mapa
sad
trap
wmma


cp
match
selp
txq
xor


createpolicy
max
set
vabsdiff



cvt
mbarrier
setmaxnreg
vabsdiff2



cvta
membar
setp
vabsdiff4








4.4. Identifiersï

User-defined identifiers follow extended C++ rules: they either start with a letter followed by zero
or more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar,
or percentage character followed by one or more letters, digits, underscore, or dollar characters:

followsym:   [a-zA-Z0-9_$]
identifier:  [a-zA-Z]{followsym}* | {[_$%]{followsym}+


PTX does not specify a maximum length for identifiers and suggests that all implementations support
a minimum length of at least 1024 characters.
Many high-level languages such as C and C++ follow similar rules for identifier names, except that
the percentage sign is not allowed. PTX allows the percentage sign as the first character of an
identifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined
variable names and compiler-generated names.
PTX predefines one constant and a small number of special registers that begin with the percentage
sign, listed in Table 3.


Table 3 Predefined Identifiersï









%clock
%laneid
%lanemask_gt
%pm0, ..., %pm7


%clock64
%lanemask_eq
%nctaid
%smid


%ctaid
%lanemask_le
%ntid
%tid


%envreg<32>
%lanemask_lt
%nsmid
%warpid


%gridid
%lanemask_ge
%nwarpid
WARP_SZ






4.5. Constantsï

PTX supports integer and floating-point constants and constant expressions. These constants may be
used in data initialization and as operands to instructions. Type checking rules remain the same for
integer, floating-point, and bit-size types. For predicate-type data and instructions, integer
constants are allowed and are interpreted as in C, i.e., zero values are False and non-zero
values are True.


4.5.1. Integer Constantsï

Integer constants are 64-bits in size and are either signed or unsigned, i.e., every integer
constant has type .s64 or .u64. The signed/unsigned nature of an integer constant is needed
to correctly evaluate constant expressions containing operations such as division and ordered
comparisons, where the behavior of the operation depends on the operand types. When used in an
instruction or data initialization, each integer constant is converted to the appropriate size based
on the data or instruction type at its use.
Integer literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax
follows that of C. Integer literals may be followed immediately by the letter U to indicate that
the literal is unsigned.

hexadecimal literal:  0[xX]{hexdigit}+U?
octal literal:        0{octal digit}+U?
binary literal:       0[bB]{bit}+U?
decimal literal       {nonzero-digit}{digit}*U?


Integer literals are non-negative and have a type determined by their magnitude and optional type
suffix as follows: literals are signed (.s64) unless the value cannot be fully represented in
.s64 or the unsigned suffix is specified, in which case the literal is unsigned (.u64).
The predefined integer constant WARP_SZ specifies the number of threads per warp for the target
platform; to date, all target architectures have a WARP_SZ value of 32.



4.5.2. Floating-Point Constantsï

Floating-point constants are represented as 64-bit double-precision values, and all floating-point
constant expressions are evaluated using 64-bit double precision arithmetic. The only exception is
the 32-bit hex notation for expressing an exact single-precision floating-point value; such values
retain their exact 32-bit single-precision value and may not be used in constant expressions. Each
64-bit floating-point constant is converted to the appropriate floating-point size based on the data
or instruction type at its use.
Floating-point literals may be written with an optional decimal point and an optional signed
exponent. Unlike C and C++, there is no suffix letter to specify size; literals are always
represented in 64-bit double-precision format.
PTX includes a second representation of floating-point constants for specifying the exact machine
representation using a hexadecimal constant. To specify IEEE 754 double-precision floating point
values, the constant begins with 0d or 0D followed by 16 hex digits. To specify IEEE 754
single-precision floating point values, the constant begins with 0f or 0F followed by 8 hex
digits.

0[fF]{hexdigit}{8}      // single-precision floating point
0[dD]{hexdigit}{16}     // double-precision floating point


Example

mov.f32  $f3, 0F3f800000;       //  1.0





4.5.3. Predicate Constantsï

In PTX, integer constants may be used as predicates. For predicate-type data initializers and
instruction operands, integer constants are interpreted as in C, i.e., zero values are False and
non-zero values are True.



4.5.4. Constant Expressionsï

In PTX, constant expressions are formed using operators as in C and are evaluated using rules
similar to those in C, but simplified by restricting types and sizes, removing most casts, and
defining full semantics to eliminate cases where expression evaluation in C is implementation
dependent.
Constant expressions are formed from constant literals, unary plus and minus, basic arithmetic
operators (addition, subtraction, multiplication, division), comparison operators, the conditional
ternary operator ( ?: ), and parentheses. Integer constant expressions also allow unary logical
negation (!), bitwise complement (~), remainder (%), shift operators (<< and
>>), bit-type operators (&, |, and ^), and logical operators (&&, ||).
Constant expressions in PTX do not support casts between integer and floating-point.
Constant expressions are evaluated using the same operator precedence as
in C. Table 4 gives operator precedence and
associativity. Operator precedence is highest for unary operators and decreases with each line in
the chart. Operators on the same line have the same precedence and are evaluated right-to-left for
unary operators and left-to-right for binary operators.


Table 4 Operator Precedenceï









Kind
Operator Symbols
Operator Names
Associates




Primary
()
parenthesis
n/a


Unary
+- ! ~
plus, minus, negation, complement
right


(.s64)(.u64)
casts
right


Binary
*/ %
multiplication, division, remainder
left


+-
addition, subtraction


>> <<
shifts


< > <= >=
ordered comparisons


== !=
equal, not equal


&
bitwise AND


^
bitwise XOR


|
bitwise OR


&&
logical AND


||
logical OR


Ternary
?:
conditional
right






4.5.5. Integer Constant Expression Evaluationï

Integer constant expressions are evaluated at compile time according to a set of rules that
determine the type (signed .s64 versus unsigned .u64) of each sub-expression. These rules
are based on the rules in C, but theyâve been simplified to apply only to 64-bit integers, and
behavior is fully defined in all cases (specifically, for remainder and shift operators).


Literals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a
U suffix. For example:

42, 0x1234, 0123 are signed.
0xfabc123400000000, 42U, 0x1234U are unsigned.



Unary plus and minus preserve the type of the input operand. For example:

+123, -1, -(-42) are signed.
-1U, -0xfabc123400000000 are unsigned.


Unary logical negation (!) produces a signed result with value 0 or 1.
Unary bitwise complement (~) interprets the source operand as unsigned and produces an
unsigned result.
Some binary operators require normalization of source operands. This normalization is known as
the usual arithmetic conversions and simply converts both operands to unsigned type if either
operand is unsigned.
Addition, subtraction, multiplication, and division perform the usual arithmetic conversions and
produce a result with the same type as the converted operands. That is, the operands and result
are unsigned if either source operand is unsigned, and is otherwise signed.
Remainder (%) interprets the operands as unsigned. Note that this differs from C, which allows
a negative divisor but defines the behavior to be implementation dependent.
Left and right shift interpret the second operand as unsigned and produce a result with the same
type as the first operand. Note that the behavior of right-shift is determined by the type of the
first operand: right shift of a signed value is arithmetic and preserves the sign, and right shift
of an unsigned value is logical and shifts in a zero bit.
AND (&), OR (|), and XOR (^) perform the usual arithmetic conversions and produce a
result with the same type as the converted operands.
AND_OP (&&), OR_OP (||), Equal (==), and Not_Equal (!=) produce a signed
result. The result value is 0 or 1.
Ordered comparisons (<, <=, >, >=) perform the usual arithmetic conversions on
source operands and produce a signed result. The result value is 0 or 1.
Casting of expressions to signed or unsigned is supported using (.s64) and (.u64) casts.
For the conditional operator ( ? : ) , the first operand must be an integer, and the second
and third operands are either both integers or both floating-point. The usual arithmetic
conversions are performed on the second and third operands, and the result type is the same as the
converted type.




4.5.6. Summary of Constant Expression Evaluation Rulesï

Table 5
contains a summary of the constant expression evaluation rules.


Table 5 Constant Expression Evaluation Rulesï










Kind
Operator
Operand Types
Operand Interpretation
Result Type




Primary
()
any type
same as source
same as source


constant literal
n/a
n/a
.u64, .s64, or .f64


Unary
+-
any type
same as source
same as source


!
integer
zero or non-zero
.s64


~
integer
.u64
.u64


Cast
(.u64)
integer
.u64
.u64


(.s64)
integer
.s64
.s64


Binary
+- * /
.f64
.f64
.f64


integer
use usual conversions
converted type


< > <= >=
.f64
.f64
.s64


integer
use usual conversions
.s64


== !=
.f64
.f64
.s64


integer
use usual conversions
.s64


%
integer
.u64
.s64


>> <<
integer
1st unchanged, 2nd is .u64
same as 1st operand


& | ^
integer
.u64
.u64


&& ||
integer
zero or non-zero
.s64


Ternary
?:
int ? .f64 : .f64
same as sources
.f64


int ? int : int
use usual conversions
converted type








5. State Spaces, Types, and Variablesï

While the specific resources available in a given target GPU will vary, the kinds of resources will
be common across platforms, and these resources are abstracted in PTX through state spaces and data
types.


5.1. State Spacesï

A state space is a storage area with particular characteristics. All variables reside in some state
space. The characteristics of a state space include its size, addressability, access speed, access
rights, and level of sharing between threads.
The state spaces defined in PTX are a byproduct of parallel programming and graphics
programming. The list of state spaces is shown in Table 6,and
properties of state spaces are shown in Table 7.


Table 6 State Spacesï







Name
Description




.reg
Registers, fast.


.sreg
Special registers. Read-only; pre-defined; platform-specific.


.const
Shared, read-only memory.


.global
Global memory, shared by all threads.


.local
Local memory, private to each thread.


.param

Kernel parameters, defined per-grid; or
Function or local parameters, defined per-thread.



.shared
Addressable memory, defined per CTA, accessible to all threads in the cluster
throughout the lifetime of the CTA that defines it.


.tex
Global texture memory (deprecated).





Table 7 Properties of State Spacesï










Name
Addressable
Initializable
Access
Sharing




.reg
No
No
R/W
per-thread


.sreg
No
No
RO
per-CTA


.const
Yes
Yes1
RO
per-grid


.global
Yes
Yes1
R/W
Context


.local
Yes
No
R/W
per-thread


.param
(as input to kernel)
Yes2
No
RO
per-grid


.param
(used in functions)
Restricted3
No
R/W
per-thread


.shared
Yes
No
R/W
per-cluster5


.tex
No4
Yes, via driver
RO
Context



Notes:
1 Variables in .const and .global state spaces are initialized to zero by default.
2 Accessible only via the ld.param{::entry} instruction. Address may be taken via
mov instruction.
3 Accessible via ld.param{::func} and st.param{::func} instructions. Device function
input and return parameters may have their address taken via mov; the parameter is then located
on the stack frame and its address is in the .local state space.
4 Accessible only via the tex instruction.
5 Visible to the owning CTA and other active CTAs in the cluster.






5.1.1. Register State Spaceï

Registers (.reg state space) are fast storage locations. The number of registers is limited, and
will vary from platform to platform. When the limit is exceeded, register variables will be spilled
to memory, causing changes in performance. For each architecture, there is a recommended maximum
number of registers to use (see the CUDA Programming Guide for details).
Registers may be typed (signed integer, unsigned integer, floating point, predicate) or
untyped. Register size is restricted; aside from predicate registers which are 1-bit, scalar
registers have a width of 8-, 16-, 32-, 64-, or 128-bits, and vector registers have a width of
16-, 32-, 64-, or 128-bits. The most common use of 8-bit registers is with ld, st, and cvt
instructions, or as elements of vector tuples.
Registers differ from the other state spaces in that they are not fully addressable, i.e., it is not
possible to refer to the address of a register. When compiling to use the Application Binary
Interface (ABI), register variables are restricted to function scope and may not be declared at
module scope. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped
.reg variables, the compiler silently disables use of the ABI. Registers may have alignment
boundaries required by multi-word loads and stores.



5.1.2. Special Register State Spaceï

The special register (.sreg) state space holds predefined, platform-specific registers, such as
grid, cluster, CTA, and thread parameters, clock counters, and performance monitoring registers. All
special registers are predefined.



5.1.3. Constant State Spaceï

The constant (.const) state space is a read-only memory initialized by the host. Constant memory
is accessed with a ld.const instruction. Constant memory is restricted in size, currently
limited to 64 KB which can be used to hold statically-sized constant variables. There is an
additional 640 KB of constant memory, organized as ten independent 64 KB regions. The driver may
allocate and initialize constant buffers in these regions and pass pointers to the buffers as kernel
function parameters. Since the ten regions are not contiguous, the driver must ensure that constant
buffers are allocated so that each buffer fits entirely within a 64 KB region and does not span a
region boundary.
Statically-sized constant variables have an optional variable initializer; constant variables with
no explicit initializer are initialized to zero by default. Constant buffers allocated by the driver
are initialized by the host, and pointers to such buffers are passed to the kernel as
parameters. See the description of kernel parameter attributes in Kernel Function Parameter
Attributes for more details on passing pointers
to constant buffers as kernel parameters.


5.1.3.1. Banked Constant State Space (deprecated)ï

Previous versions of PTX exposed constant memory as a set of eleven 64 KB banks, with explicit bank
numbers required for variable declaration and during access.
Prior to PTX ISA version 2.2, the constant memory was organized into fixed size banks. There were
eleven 64 KB banks, and banks were specified using the .const[bank] modifier, where bank
ranged from 0 to 10. If no bank number was given, bank zero was assumed.
By convention, bank zero was used for all statically-sized constant variables. The remaining banks
were used to declare incomplete constant arrays (as in C, for example), where the size is not
known at compile time. For example, the declaration

.extern .const[2] .b32 const_buffer[];


resulted in const_buffer pointing to the start of constant bank two. This pointer could then be
used to access the entire 64 KB constant bank. Multiple incomplete array variables declared in the
same bank were aliased, with each pointing to the start address of the specified constant bank.
To access data in contant banks 1 through 10, the bank number was required in the state space of the
load instruction. For example, an incomplete array in bank 2 was accessed as follows:

.extern .const[2] .b32 const_buffer[];
ld.const[2].b32  %r1, [const_buffer+4]; // load second word


In PTX ISA version 2.2, we eliminated explicit banks and replaced the incomplete array
representation of driver-allocated constant buffers with kernel parameter attributes that allow
pointers to constant buffers to be passed as kernel parameters.




5.1.4. Global State Spaceï

The global (.global) state space is memory that is accessible by all threads in a context. It is
the mechanism by which threads in different CTAs, clusters, and grids can communicate. Use
ld.global, st.global, and atom.global to access global variables.
Global variables have an optional variable initializer; global variables with no explicit
initializer are initialized to zero by default.



5.1.5. Local State Spaceï

The local state space (.local) is private memory for each thread to keep its own data. It is
typically standard memory with cache. The size is limited, as it must be allocated on a per-thread
basis. Use ld.local and st.local to access local variables.
When compiling to use the Application Binary Interface (ABI), .local state-space variables
must be declared within function scope and are allocated on the stack. In implementations that do
not support a stack, all local memory variables are stored at fixed addresses, recursive function
calls are not supported, and .local variables may be declared at module scope. When compiling
legacy PTX code (ISA versions prior to 3.0) containing module-scoped .local variables, the
compiler silently disables use of the ABI.



5.1.6. Parameter State Spaceï

The parameter (.param) state space is used (1) to pass input arguments from the host to the
kernel, (2a) to declare formal input and return parameters for device functions called from within
kernel execution, and (2b) to declare locally-scoped byte array variables that serve as function
call arguments, typically for passing large structures by value to a function. Kernel function
parameters differ from device function parameters in terms of access and sharing (read-only versus
read-write, per-kernel versus per-thread). Note that PTX ISA versions 1.x supports only kernel
function parameters in .param space; device function parameters were previously restricted to the
register state space. The use of parameter state space for device function parameters was introduced
in PTX ISA version 2.0 and requires target architecture sm_20 or higher. Additional sub-qualifiers
::entry or ::func can be specified on instructions with .param state space to indicate
whether the address refers to kernel function parameter or device function parameter. If no
sub-qualifier is specified with the .param state space, then the default sub-qualifier is specific
to and dependent on the exact instruction. For example, st.param is equivalent to st.param::func
whereas isspacep.param is equivalent to isspacep.param::entry. Refer to the instruction
description for more details on default sub-qualifier assumption.

Note
The location of parameter space is implementation specific. For example, in some implementations
kernel parameters reside in global memory. No access protection is provided between parameter and
global space in this case. Though the exact location of the kernel parameter space is
implementation specific, the kernel parameter space window is always contained within the global
space window. Similarly, function parameters are mapped to parameter passing registers and/or
stack locations based on the function calling conventions of the Application Binary Interface
(ABI). Therefore, PTX code should make no assumptions about the relative locations or ordering
of .param space variables.



5.1.6.1. Kernel Function Parametersï

Each kernel function definition includes an optional list of parameters. These parameters are
addressable, read-only variables declared in the .param state space. Values passed from the host
to the kernel are accessed through these parameter variables using ld.param instructions. The
kernel parameter variables are shared across all CTAs from all clusters within a grid.
The address of a kernel parameter may be moved into a register using the mov instruction. The
resulting address is in the .param state space and is accessed using ld.param instructions.
Example

.entry foo ( .param .b32 N, .param .align 8 .b8 buffer[64] )
{
    .reg .u32 %n;
    .reg .f64 %d;

    ld.param.u32 %n, [N];
    ld.param.f64 %d, [buffer];
    ...


Example

.entry bar ( .param .b32 len )
{
    .reg .u32 %ptr, %n;

    mov.u32      %ptr, len;
    ld.param.u32 %n, [%ptr];
    ...


Kernel function parameters may represent normal data values, or they may hold addresses to objects
in constant, global, local, or shared state spaces. In the case of pointers, the compiler and
runtime system need information about which parameters are pointers, and to which state space they
point. Kernel parameter attribute directives are used to provide this information at the PTX
level. See Kernel Function Parameter Attributes for a description of kernel parameter attribute
directives.

Note
The current implementation does not allow creation of generic pointers to constant variables
(cvta.const) in programs that have pointers to constant buffers passed as kernel parameters.




5.1.6.2. Kernel Function Parameter Attributesï

Kernel function parameters may be declared with an optional .ptr attribute to indicate that a
parameter is a pointer to memory, and also indicate the state space and alignment of the memory
being pointed to. Kernel Parameter Attribute: .ptr
describes the .ptr kernel parameter attribute.



5.1.6.3. Kernel Parameter Attribute: .ptrï

.ptr
Kernel parameter alignment attribute.
Syntax

.param .type .ptr .space .align N  varname
.param .type .ptr        .align N  varname

.space = { .const, .global, .local, .shared };


Description
Used to specify the state space and, optionally, the alignment of memory pointed to by a pointer
type kernel parameter. The alignment value N, if present, must be a power of two. If no state
space is specified, the pointer is assumed to be a generic address pointing to one of const, global,
local, or shared memory. If no alignment is specified, the memory pointed to is assumed to be
aligned to a 4 byte boundary.
Spaces between .ptr, .space, and .align may be eliminated to improve readability.
PTX ISA Notes

Introduced in PTX ISA version 2.2.
Support for generic addressing of .const space added in PTX ISA version 3.1.

Target ISA Notes

Supported on all target architectures.

Examples

.entry foo ( .param .u32 param1,
             .param .u32 .ptr.global.align 16 param2,
             .param .u32 .ptr.const.align 8 param3,
             .param .u32 .ptr.align 16 param4  // generic address
                                               // pointer
) { .. }





5.1.6.4. Device Function Parametersï

PTX ISA version 2.0 extended the use of parameter space to device function parameters. The most
common use is for passing objects by value that do not fit within a PTX register, such as C
structures larger than 8 bytes. In this case, a byte array in parameter space is used. Typically,
the caller will declare a locally-scoped .param byte array variable that represents a flattened
C structure or union. This will be passed by value to a callee, which declares a .param formal
parameter having the same size and alignment as the passed argument.
Example

// pass object of type struct { double d; int y; };
.func foo ( .reg .b32 N, .param .align 8 .b8 buffer[12] )
{
    .reg .f64 %d;
    .reg .s32 %y;

    ld.param.f64 %d, [buffer];
    ld.param.s32 %y, [buffer+8];
    ...
}

// code snippet from the caller
// struct { double d; int y; } mystruct; is flattened, passed to foo
    ...
    .reg .f64 dbl;
    .reg .s32 x;
    .param .align 8 .b8 mystruct;
    ...
    st.param.f64 [mystruct+0], dbl;
    st.param.s32 [mystruct+8], x;
    call foo, (4, mystruct);
    ...


See the section on function call syntax for more details.
Function input parameters may be read via ld.param and function return parameters may be written
using st.param; it is illegal to write to an input parameter or read from a return parameter.
Aside from passing structures by value, .param space is also required whenever a formal
parameter has its address taken within the called function. In PTX, the address of a function input
parameter may be moved into a register using the mov instruction. Note that the parameter will
be copied to the stack if necessary, and so the address will be in the .local state space and is
accessed via ld.local and st.local instructions. It is not possible to use mov to get
the address of or a locally-scoped .param space variable. Starting PTX ISA version 6.0, it is
possible to use mov instruction to get address of return parameter of device function.
Example

// pass array of up to eight floating-point values in buffer
.func foo ( .param .b32 N, .param .b32 buffer[32] )
{
    .reg .u32  %n, %r;
    .reg .f32  %f;
    .reg .pred %p;

    ld.param.u32 %n, [N];
    mov.u32      %r, buffer;  // forces buffer to .local state space
Loop:
    setp.eq.u32  %p, %n, 0;
@%p: bra         Done;
    ld.local.f32 %f, [%r];
    ...
    add.u32      %r, %r, 4;
    sub.u32      %n, %n, 1;
    bra          Loop;
Done:
    ...
}






5.1.7. Shared State Spaceï

The shared (.shared) state space is a memory that is owned by an executing CTA and is accessible
to the threads of all the CTAs within a cluster. An address in shared memory can be read and written
by any thread in a CTA cluster.
Additional sub-qualifiers ::cta or ::cluster can be specified on instructions with
.shared state space to indicate whether the address belongs to the shared memory window of the
executing CTA or of any CTA in the cluster respectively. The addresses in the .shared::cta
window also fall within the .shared::cluster window. If no sub-qualifier is specified with the
.shared state space, then it defaults to ::cta. For example, ld.shared is equivalent to
ld.shared::cta.
Variables declared in .shared state space refer to the memory addresses in the current
CTA. Instruction mapa gives the .shared::cluster address of the corresponding variable in
another CTA in the cluster.
Shared memory typically has some optimizations to support the sharing. One example is broadcast;
where all threads read from the same address. Another is sequential access from sequential threads.



5.1.8. Texture State Space (deprecated)ï

The texture (.tex) state space is global memory accessed via the texture instruction. It is
shared by all threads in a context. Texture memory is read-only and cached, so accesses to texture
memory are not coherent with global memory stores to the texture image.
The GPU hardware has a fixed number of texture bindings that can be accessed within a single kernel
(typically 128). The .tex directive will bind the named texture memory variable to a hardware
texture identifier, where texture identifiers are allocated sequentially beginning with
zero. Multiple names may be bound to the same physical texture identifier. An error is generated if
the maximum number of physical resources is exceeded. The texture name must be of type .u32 or
.u64.
Physical texture resources are allocated on a per-kernel granularity, and .tex variables are
required to be defined in the global scope.
Texture memory is read-only. A textureâs base address is assumed to be aligned to a 16 byte
boundary.
Example

.tex .u32 tex_a;         // bound to physical texture 0
.tex .u32 tex_c, tex_d;  // both bound to physical texture 1
.tex .u32 tex_d;         // bound to physical texture 2
.tex .u32 tex_f;         // bound to physical texture 3



Note
Explicit declarations of variables in the texture state space is deprecated, and programs should
instead reference texture memory through variables of type .texref. The .tex directive is
retained for backward compatibility, and variables declared in the .tex state space are
equivalent to module-scoped .texref variables in the .global state space.

For example, a legacy PTX definitions such as

.tex .u32 tex_a;


is equivalent to:

.global .texref tex_a;


See Texture Sampler and Surface Types for the
description of the .texref type and Texture Instructions
for its use in texture instructions.




5.2. Typesï



5.2.1. Fundamental Typesï

In PTX, the fundamental types reflect the native data types supported by the target architectures. A
fundamental type specifies both a basic type and a size. Register variables are always of a
fundamental type, and instructions operate on these types. The same type-size specifiers are used
for both variable definitions and for typing instructions, so their names are intentionally short.
Table 8 lists the fundamental type specifiers for
each basic type:


Table 8 Fundamental Type Specifiersï







Basic Type
Fundamental Type Specifiers




Signed integer
.s8, .s16, .s32, .s64


Unsigned integer
.u8, .u16, .u32, .u64


Floating-point
.f16, .f16x2, .f32, .f64


Bits (untyped)
.b8, .b16, .b32, .b64, .b128


Predicate
.pred



Most instructions have one or more type specifiers, needed to fully specify instruction
behavior. Operand types and sizes are checked against instruction types for compatibility.
Two fundamental types are compatible if they have the same basic type and are the same size. Signed
and unsigned integer types are compatible if they have the same size. The bit-size type is
compatible with any fundamental type having the same size.
In principle, all variables (aside from predicates) could be declared using only bit-size types, but
typed variables enhance program readability and allow for better operand type checking.



5.2.2. Restricted Use of Sub-Word Sizesï

The .u8, .s8, and .b8 instruction types are restricted to ld, st, and cvt
instructions. The .f16 floating-point type is allowed only in conversions to and from .f32,
.f64 types, in half precision floating point instructions and texture fetch instructions. The
.f16x2 floating point type is allowed only in half precision floating point arithmetic
instructions and texture fetch instructions.
For convenience, ld, st, and cvt instructions permit source and destination data
operands to be wider than the instruction-type size, so that narrow values may be loaded, stored,
and converted using regular-width registers. For example, 8-bit or 16-bit values may be held
directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and
sizes.



5.2.3. Alternate Floating-Point Data Formatsï

The fundamental floating-point types supported in PTX have implicit bit representations that
indicate the number of bits used to store exponent and mantissa. For example, the .f16 type
indicates 5 bits reserved for exponent and 10 bits reserved for mantissa. In addition to the
floating-point representations assumed by the fundamental types, PTX allows the following alternate
floating-point data formats:


bf16 data format:

This data format is a 16-bit floating point format with 8 bits for exponent and 7 bits for
mantissa. A register variable containing bf16 data must be declared with .b16 type.


e4m3 data format:

This data format is an 8-bit floating point format with 4 bits for exponent and 3 bits for
mantissa. The e4m3 encoding does not support infinity and NaN values are limited to
0x7f and 0xff. A register variable containing e4m3 value must be declared using
bit-size type.


e5m2 data format:

This data format is an 8-bit floating point format with 5 bits for exponent and 2 bits for
mantissa. A register variable containing e5m2 value must be declared using bit-size type.


tf32 data format:

This data format is a special 32-bit floating point format supported by the matrix
multiply-and-accumulate instructions, with the same range as .f32 and reduced precision (>=10
bits). The internal layout of tf32 format is implementation defined. PTX facilitates
conversion from single precision .f32 type to tf32 format. A register variable containing
tf32 data must be declared with .b32 type.


Alternate data formats cannot be used as fundamental types. They are supported as source or
destination formats by certain instructions.



5.2.4. Packed Data Typesï

Certain PTX instructions operate on two sets of inputs in parallel, and produce two outputs. Such
instructions can use the data stored in a packed format. PTX supports packing two values of the same
scalar data type into a single, larger value. The packed value is considered as a value of a packed
data type. In this section we describe the packed data types supported in PTX.


5.2.4.1. Packed Floating Point Data Typesï

PTX supports the following four variants of packed floating point data types:

.f16x2 packed type containing two .f16 floating point values.
.bf16x2 packed type containing two .bf16 alternate floating point values.
.e4m3x2 packed type containing two .e4m3 alternate floating point values.
.e5m2x2 packed type containing two .e5m2 alternate floating point values.

.f16x2 is supported as a fundamental type. .bf16x2, .e4m3x2 and .e5m2x2 cannot be
used as fundamental types - they are supported as instruction types on certain instructions. A
register variable containing .bf16x2 data must be declared with .b32 type. A register
variable containing .e4m3x2 or .e5m2x2 data must be declared with .b16 type.



5.2.4.2. Packed Integer Data Typesï

PTX supports two variants of packed integer data types: .u16x2 and .s16x2. The packed data
type consists of two .u16 or .s16 values. A register variable containing .u16x2 or
.s16x2 data must be declared with .b32 type. Packed integer data types cannot be used as
fundamental types. They are supported as instruction types on certain instructions.





5.3. Texture Sampler and Surface Typesï

PTX includes built-in opaque types for defining texture, sampler, and surface descriptor
variables. These types have named fields similar to structures, but all information about layout,
field ordering, base address, and overall size is hidden to a PTX program, hence the term
opaque. The use of these opaque types is limited to:

Variable definition within global (module) scope and in kernel entry parameter lists.
Static initialization of module-scope variables using comma-delimited static assignment
expressions for the named members of the type.
Referencing textures, samplers, or surfaces via texture and surface load/store instructions
(tex, suld, sust, sured).
Retrieving the value of a named member via query instructions (txq, suq).
Creating pointers to opaque variables using mov, e.g., mov.u64 reg, opaque_var;. The
resulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and
de-referenced by texture and surface load, store, and query instructions, but the pointer cannot
otherwise be treated as an address, i.e., accessing the pointer with ld and st
instructions, or performing pointer arithmetic will result in undefined results.
Opaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque
variable.


Note
Indirect access to textures and surfaces using pointers to opaque variables is supported
beginning with PTX ISA version 3.1 and requires target sm_20 or later.
Indirect access to textures is supported only in unified texture mode (see below).

The three built-in types are .texref, .samplerref, and .surfref. For working with
textures and samplers, PTX has two modes of operation. In the unified mode, texture and sampler
information is accessed through a single .texref handle. In the independent mode, texture and
sampler information each have their own handle, allowing them to be defined separately and combined
at the site of usage in the program. In independent mode, the fields of the .texref type that
describe sampler properties are ignored, since these properties are defined by .samplerref
variables.
Table 9 and
Table 10 list the named members
of each type for unified and independent texture modes. These members and their values have
precise mappings to methods and values defined in the texture HW class as well as
exposed values via the API.


Table 9 Opaque Type Fields in Unified Texture Modeï








Member
.texref values
.surfref values




width
in elements


height
in elements


depth
in elements


channel_data_type
enum type corresponding to source language API


channel_order
enum type corresponding to source language API


normalized_coords
0, 1
N/A


filter_mode
nearest, linear
N/A


addr_mode_0, addr_mode_1,
addr_mode_2
wrap,mirror, clamp_ogl,
clamp_to_edge, clamp_to_border
N/A


array_size
as number of textures in a texture
array
as number of surfaces in a surface array


num_mipmap_levels
as number of levels in a mipmapped
texture
N/A


num_samples
as number of samples in a multi-sample
texture
N/A


memory_layout
N/A
1 for linear memory layout; 0 otherwise





5.3.1. Texture and Surface Propertiesï

Fields width, height, and depth specify the size of the texture or surface in number of
elements in each dimension.
The channel_data_type and channel_order fields specify these properties of the texture or
surface using enumeration types corresponding to the source language API. For example, see Channel
Data Type and Channel Order Fields for
the OpenCL enumeration types currently supported in PTX.



5.3.2. Sampler Propertiesï

The normalized_coords field indicates whether the texture or surface uses normalized coordinates
in the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is
specified, the default is set by the runtime system based on the source language.
The filter_mode field specifies how the values returned by texture reads are computed based on
the input texture coordinates.
The addr_mode_{0,1,2} fields define the addressing mode in each dimension, which determine how
out-of-range coordinates are handled.
See the CUDA C++ Programming Guide for more details of these properties.


Table 10 Opaque Type Fields in Independent Texture Modeï









Member
.samplerref values
.texref values
.surfref values




width
N/A
in elements


height
N/A
in elements


depth
N/A
in elements


channel_data_type
N/A
enum type corresponding to source
language API


channel_order
N/A
enum type corresponding to source
language AP


normalized_coords
N/A
0, 1
N/A


force_unnormalized_coords
0, 1
N/A
N/A


filter_mode
nearest, linear
ignored
N/A


addr_mode_0,
addr_mode_1,
addr_mode_2
wrap,mirror, clamp_ogl,
clamp_to_edge, clamp_to_border
N/A
N/A


array_size
N/A
as number of textures
in a texture array
as number of surfaces in
a surface array


num_mipmap_levels
N/A
as number of levels
in a mipmapped
texture
N/A


num_samples
N/A
as number of samples
in a multi-sample
texture
N/A


memory_layout
N/A
N/A
1 for linear memory
layout; 0 otherwise



In independent texture mode, the sampler properties are carried in an independent .samplerref
variable, and these fields are disabled in the .texref variables. One additional sampler
property, force_unnormalized_coords, is available in independent texture mode.
The force_unnormalized_coords field is a property of .samplerref variables that allows the
sampler to override the texture header normalized_coords property. This field is defined only in
independent texture mode. When True, the texture header setting is overridden and unnormalized
coordinates are used; when False, the texture header setting is used.
The force_unnormalized_coords property is used in compiling OpenCL; in OpenCL, the property of
normalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are
always initialized with normalized_coords set to True, and the OpenCL sampler-based
normalized_coords flag maps (negated) to the PTX-level force_unnormalized_coords flag.
Variables using these types may be declared at module scope or within kernel entry parameter
lists. At module scope, these variables must be in the .global state space. As kernel
parameters, these variables are declared in the .param state space.
Example

.global .texref     my_texture_name;
.global .samplerref my_sampler_name;
.global .surfref    my_surface_name;


When declared at module scope, the types may be initialized using a list of static expressions
assigning values to the named members.
Example

.global .texref tex1;
.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,
                               filter_mode = nearest
                             };





5.3.3. Channel Data Type and Channel Order Fieldsï

The channel_data_type and channel_order fields have enumeration types corresponding to the
source language API. Currently, OpenCL is the only source language that defines these
fields. Table 12 and
Table 11 show the
enumeration values defined in OpenCL version 1.0 for channel data type and channel order.


Table 11 OpenCL 1.0 Channel Data Type Definitionï







CL_SNORM_INT8
0x10D0


CL_SNORM_INT16
0x10D1


CL_UNORM_INT8
0x10D2


CL_UNORM_INT16
0x10D3


CL_UNORM_SHORT_565
0x10D4


CL_UNORM_SHORT_555
0x10D5


CL_UNORM_INT_101010
0x10D6


CL_SIGNED_INT8
0x10D7


CL_SIGNED_INT16
0x10D8


CL_SIGNED_INT32
0x10D9


CL_UNSIGNED_INT8
0x10DA


CL_UNSIGNED_INT16
0x10DB


CL_UNSIGNED_INT32
0x10DC


CL_HALF_FLOAT
0x10DD


CL_FLOAT
0x10DE





Table 12 OpenCL 1.0 Channel Order Definitionï







CL_R
0x10B0


CL_A
0x10B1


CL_RG
0x10B2


CL_RA
0x10B3


CL_RGB
0x10B4


CL_RGBA
0x10B5


CL_BGRA
0x10B6


CL_ARGB
0x10B7


CL_INTENSITY
0x10B8


CL_LUMINANCE
0x10B9







5.4. Variablesï

In PTX, a variable declaration describes both the variableâs type and its state space. In addition
to fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays.


5.4.1. Variable Declarationsï

All storage for data is specified with variable declarations. Every variable must reside in one of
the state spaces enumerated in the previous section.
A variable declaration names the space in which the variable resides, its type and size, its name,
an optional array size, an optional initializer, and an optional fixed address for the variable.
Predicate variables may only be declared in the register state space.
Examples

.global .u32 loc;
.reg    .s32 i;
.const  .f32 bias[] = {-1.0, 1.0};
.global .u8  bg[4] = {0, 0, 0, 0};
.reg    .v4 .f32 accel;
.reg    .pred p, q, r;





5.4.2. Vectorsï

Limited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate
fundamental type can be declared by prefixing the type with .v2 or .v4. Vectors must be
based on a fundamental type, and they may reside in the register space. Vectors cannot exceed
128-bits in length; for example, .v4 .f64 is not allowed. Three-element vectors may be
handled by using a .v4 vector, where the fourth element provides padding. This is a common case
for three-dimensional grids, textures, etc.
Examples

.global .v4 .f32 V;   // a length-4 vector of floats
.shared .v2 .u16 uv;  // a length-2 vector of unsigned ints
.global .v4 .b8  v;   // a length-4 vector of bytes


By default, vector variables are aligned to a multiple of their overall size (vector length times
base-type size), to enable vector load and store instructions which require addresses aligned to a
multiple of the access size.



5.4.3. Array Declarationsï

Array declarations are provided to allow the programmer to reserve space. To declare an array, the
variable name is followed with dimensional declarations similar to fixed-size array declarations
in C. The size of each dimension is a constant expression.
Examples

.local  .u16 kernel[19][19];
.shared .u8  mailbox[128];


The size of the array specifies how many elements should be reserved. For the declaration of array
kernel above, 19*19 = 361 halfwords are reserved, for a total of 722 bytes.
When declared with an initializer, the first dimension of the array may be omitted. The size of the
first array dimension is determined by the number of elements in the array initializer.
Examples

.global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 };
.global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} };


Array index has eight elements, and array offset is a 4x2 array.



5.4.4. Initializersï

Declared variables may specify an initial value using a syntax similar to C/C++, where the variable
name is followed by an equals sign and the initial value or values for the variable. A scalar takes
a single value, while vectors and arrays take nested lists of values inside of curly braces (the
nesting matches the dimensionality of the declaration).
As in C, array initializers may be incomplete, i.e., the number of initializer elements may be less
than the extent of the corresponding array dimension, with remaining array locations initialized to
the default value for the specified array type.
Examples

.const  .f32 vals[8] = { 0.33, 0.25, 0.125 };
.global .s32 x[3][2] = { {1,2}, {3} };


is equivalent to

.const  .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 };
.global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} };


Currently, variable initialization is supported only for constant and global state spaces. Variables
in constant and global state spaces with no explicit initializer are initialized to zero by
default. Initializers are not allowed in external variable declarations.
Variable names appearing in initializers represent the address of the variable; this can be used to
statically initialize a pointer to a variable. Initializers may also contain var+offset
expressions, where offset is a byte offset added to the address of var. Only variables in
.global or .const state spaces may be used in initializers. By default, the resulting
address is the offset in the variableâs state space (as is the case when taking the address of a
variable with a mov instruction). An operator, generic(), is provided to create a generic
address for variables used in initializers.
Starting PTX ISA version 7.1, an operator mask() is provided, where mask is an integer
immediate. The only allowed expressions in the mask() operator are integer constant expression
and symbol expression representing address of variable. The mask() operator extracts n
consecutive bits from the expression used in initializers and inserts these bits at the lowest
position of the initialized variable. The number n and the starting position of the bits to be
extracted is specified by the integer immediate mask. PTX ISA version 7.1 only supports
extracting a single byte starting at byte boundary from the address of the variable. PTX ISA version
7.3 supports Integer constant expression as an operand in the mask() operator.
Supported values for mask are: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000,
0xFF000000000000, 0xFF00000000000000.
Examples

.const  .u32 foo = 42;
.global .u32 bar[] = { 2, 3, 5 };
.global .u32 p1 = foo;          // offset of foo in .const space
.global .u32 p2 = generic(foo); // generic address of foo

// array of generic-address pointers to elements of bar
.global .u32 parr[] = { generic(bar), generic(bar)+4,
generic(bar)+8 };

// examples using mask() operator are pruned for brevity
.global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...};

.global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...}

.global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...}

.global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...}

// mask() operator with integer const expression
.global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...};



Note
PTX 3.1 redefines the default addressing for global variables in initializers, from generic
addresses to offsets in the global state space. Legacy PTX code is treated as having an implicit
generic() operator for each global variable used in an initializer. PTX 3.1 code should
either include explicit generic() operators in initializers, use cvta.global to form
generic addresses at runtime, or load from the non-generic address using ld.global.

Device function names appearing in initializers represent the address of the first instruction in
the function; this can be used to initialize a table of function pointers to be used with indirect
calls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to
initialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch
kernels from GPU. See the CUDA Dynamic Parallelism Programming Guide for details.
Labels cannot be used in initializers.
Variables that hold addresses of variables or functions should be of type .u8 or .u32 or
.u64.
Type .u8 is allowed only if the mask() operator is used.
Initializers are allowed for all types except .f16, .f16x2 and .pred.
Examples

.global .s32 n = 10;
.global .f32 blur_kernel[][3]
               = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}};

.global .u32 foo[] = { 2, 3, 5, 7, 9, 11 };
.global .u64 ptr = generic(foo);   // generic address of foo[0]
.global .u64 ptr = generic(foo)+8; // generic address of foo[2]





5.4.5. Alignmentï

Byte alignment of storage for all addressable variables can be specified in the variable
declaration. Alignment is specified using an optional .alignbyte-count specifier immediately
following the state-space specifier. The variable will be aligned to an address which is an integer
multiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment
specifies the address alignment for the starting address of the entire array, not for individual
elements.
The default alignment for scalar and array variables is to a multiple of the base-type size. The
default alignment for vector variables is to a multiple of the overall vector size.
Examples

 // allocate array at 4-byte aligned address.  Elements are bytes.
.const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0};


Note that all PTX instructions that access memory require that the address be aligned to a multiple
of the access size. The access size of a memory instruction is the total number of bytes accessed in
memory. For example, the access size of ld.v4.b32 is 16 bytes, while the access size of
atom.f16x2 is 4 bytes.



5.4.6. Parameterized Variable Namesï

Since PTX supports virtual registers, it is quite common for a compiler frontend to generate a large
number of register names. Rather than require explicit declaration of every name, PTX supports a
syntax for creating a set of variables having a common prefix string appended with integer suffixes.
For example, suppose a program uses a large number, say one hundred, of .b32 variables, named
%r0, %r1, â¦, %r99. These 100 register variables can be declared as follows:

.reg .b32 %r<100>;    // declare %r0, %r1, ..., %r99


This shorthand syntax may be used with any of the fundamental types and with any state space, and
may be preceded by an alignment specifier. Array variables cannot be declared this way, nor are
initializers permitted.



5.4.7. Variable Attributesï

Variables may be declared with an optional .attribute directive which allows specifying special
attributes of variables. Keyword .attribute is followed by attribute specification inside
parenthesis. Multiple attributes are separated by comma.
Variable and Function Attribute Directive: .attribute describes the .attribute
directive.



5.4.8. Variable and Function Attribute Directive: .attributeï

.attribute
Variable and function attributes
Description
Used to specify special attributes of a variable or a function.
The following attributes are supported.

.managed

.managed attribute specifies that variable will be allocated at a location in unified virtual
memory environment where host and other devices in the system can reference the variable
directly. This attribute can only be used with variables in .global state space. See the CUDA
UVM-Lite Programming Guide for details.

.unified

.unified attribute specifies that function has the same memory address on the host and on
other devices in the system. Integer constants uuid1 and uuid2 respectively specify upper
and lower 64 bits of the unique identifier associated with the function or the variable. This
attribute can only be used on device functions or on variables in the .global state
space. Variables with .unified attribute are read-only and must be loaded by specifying
.unified qualifier on the address operand of ld instruction, otherwise the behavior is
undefined.


PTX ISA Notes

Introduced in PTX ISA version 4.0.
Support for function attributes introduced in PTX ISA version 8.0.

Target ISA Notes

.managed attribute requires sm_30 or higher.
.unified attribute requires sm_90 or higher.

Examples

.global .attribute(.managed) .s32 g;
.global .attribute(.managed) .u64 x;

.global .attribute(.unified(19,95)) .f32 f;

.func .attribute(.unified(0xAB, 0xCD)) bar() { ... }






5.5. Tensorsï

A tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following
properties:

Dimensionality
Dimension sizes across each dimension
Individual element types
Tensor stride across each dimension

PTX supports instructions which can operate on the tensor data. PTX Tensor instructions include:

Copying data between global and shared memories
Reducing the destination tensor data with the source.

The Tensor data can be operated on by various wmma.mma, mma and wgmma.mma_async
instructions.
PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure
and treat the data in the shared memory as a linear data.


5.5.1. Tensor Dimension, size and formatï

Tensors can have dimensions: 1D, 2D, 3D, 4D or 5D.
Each dimension has a size which represents the number of elements along the dimension. The elements
can have one the following types:

Bit-sized type: .b32, .b64
Integer: .u8, .u16, .u32, .s32, .u64, .s64
Floating point and alternate floating point: .f16, .bf16, .tf32, .f32, .f64
(rounded to nearest even).

Tensor can have padding at the end in each of the dimensions to provide alignment for the data in
the subsequent dimensions. Tensor stride can be used to specify the amount of padding in each
dimension.



5.5.2. Tensor Access Modesï

Tensor data can be accessed in two modes:


Tiled mode:
In tiled mode, the source multi-dimensional tensor layout is preserved at the destination.


Im2col mode:
In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns
at the destination. Refer here for more details.





5.5.3. Tiled Modeï

This section talks about how Tensor and Tensor access work in tiled mode.


5.5.3.1. Bounding Boxï

A tensor can be accessed in chunks known as Bounding Box. The Bounding Box has the same
dimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple
of 16 bytes. The address of the bounding Box must also be aligned to 16 bytes.
Bounding Box has the following access properties:

Bounding Box dimension sizes
Out of boundary access mode
Traversal strides

The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the
bounding box. Starting offset of the bounding box along with the rest of the bounding box
information together are used to determine the elements which are to be accessed.



5.5.3.2. Traversal-Strideï

While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies
the exact number of elements to be skipped. If no jump over is required, default value of 1 must be
specified.
The traversal stride in dimension 0 can be used for the Interleave layout. For non-interleaved layout, the traversal stride in
dimension 0 must always be 1.
Figure 5 illustrates tensor, tensor size, tensor stride,
Bounding Box size and traversal stride.



Figure 5 Tiled mode bounding box, tensor size and traversal strideï





5.5.3.3. Out of Boundary Accessï

PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor
boundary in any dimension. There are 2 modes:


Zero fill mode:
Elements in the Bounding Box which fall outside of the tensor boundary are set to 0.


OOB-NaN fill mode:
Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN
called OOB-NaN.


Figure 6 shows an example of the out of boundary access.



Figure 6 Out of boundary accessï






5.5.4. Im2col modeï

Im2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data
is treated as a batch of images with the following properties:

N : number of images in the batch
D, H, W : size of a 3D image (depth, height and width)
C: channels per image element

The above properties are associated with 3D, 4D and 5D tensors as follows:







Dimension
N/D/H/W/C applicability




3D
NWC


4D
NHWC


5D
NDHWC





5.5.4.1. Bounding Boxï

In im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are
specified by Pixels-per-Column and Channels-per-Pixel parameters as described below.
The dimensionality of the Bounding Box is two less than the tensor dimensionality.
The following properties describe how to access of the elements in im2col mode:

Bounding-Box Lower-Corner
Bounding-Box Upper-Corner
Pixels-per-Column
Channels-per-Pixel

Bounding-box Lower-Corner and Bounding-box Upper-Corner specify the two opposite corners of the
Bounding Box in the DHW space. Bounding-box Lower-Corner specifies the corner with the smallest
coordinate and Bounding-box Upper-Corner specifies the corner with the largest coordinate.
Bounding-box Upper- and Lower-Corners are 16-bit signed values whose limits varies across the
dimensions and are as shown below:










3D
4D
5D




Upper- / Lower- Corner sizes
[-215, 215-1]
[-27, 27-1]
[-24, 24-1]



Figure 7 and Figure 8 show the Upper-Corners and Lower-Corners.



Figure 7 im2col mode bounding box example 1ï





Figure 8 im2col mode bounding box example 2ï


The Bounding-box Upper- and Lower- Corners specify only the boundaries and not the number of
elements to be accessed. Pixels-per-Column specifies the number of elements to be accessed in the
NDHW space.
Channels-per-Pixel specifies the number of elements to access across the C dimension.
The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different
dimensions:

Across N and C dimensions: specify the starting offsets along the dimension, similar to the tiled
mode.
Across DHW dimensions: specify the location of the convolution filter base in the tensor
space. The filter corner location must be within the bounding box.

The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter
base coordinates to determine the starting location in the tensor space from where the elements are
accessed.
The size of the im2col offsets varies across the dimensions and their valid ranges are as shown
below:










3D
4D
5D




im2col offsets range
[0, 216-1]
[0, 28-1]
[0, 25-1]



Following are some examples of the im2col mode accesses:


Example 1 (Figure 9):

Tensor Size[0] = 64
Tensor Size[1] = 9
Tensor Size[2] = 14
Tensor Size[3] = 64
Pixels-per-Column = 64
channels-per-pixel = 8
Bounding-Box Lower-Corner W = -1
Bounding-Box Lower-Corner H = -1
Bounding-Box Upper-Corner W = -1
Bounding-Box Upper-Corner H = -1.

tensor coordinates = (7, 7, 4, 0)
im2col offsets : (0, 0)





Figure 9 im2col mode example 1ï




Example 2 (Figure 10):

Tensor Size[0] = 64
Tensor Size[1] = 9
Tensor Size[2] = 14
Tensor Size[3] = 64
Pixels-per-Column = 64
channels-per-pixel = 8
Bounding-Box Lower-Corner W = 0
Bounding-Box Lower-Corner H = 0
Bounding-Box Upper-Corner W = -2
Bounding-Box Upper-Corner H = -2

tensor coordinates = (7, 7, 4, 0)
im2col offsets: (2, 2)





Figure 10 im2col mode example 2ï







5.5.4.2. Traversal Strideï

The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being
accessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being
accessed, in im2col mode.
The number of elements traversed along the D, H and W dimensions is strided by the traversal stride
for that dimension.
The following example with Figure 11 illustrates accesse with traversal-strides:

Tensor Size[0] = 64
Tensor Size[1] = 8
Tensor Size[2] = 14
Tensor Size[3] = 64
Traversal Stride = 2
Pixels-per-Column = 32
channels-per-pixel = 16
Bounding-Box Lower-Corner W = -1
Bounding-Box Lower-Corner H = -1
Bounding-Box Upper-Corner W = -1
Bounding-Box Upper-Corner H = -1.
Tensor coordinates in the instruction = (7, 7, 5, 0)
Im2col offsets in the instruction : (1, 1)





Figure 11 im2col mode traversal stride exampleï





5.5.4.3. Out of Boundary Accessï

In im2col mode, when the number of requested pixels in NDHW space specified by Pixels-per-Column
exceeds the number of available pixels in the image batch then out-of-bounds access is performed.
Similar to tiled mode, zero fill or OOB-NaN fill can be performed based on the Fill-Mode
specified.




5.5.5. Interleave layoutï

Tensor can be interleaved and the following interleave layouts are supported:

No interleave (NDHWC)
8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel.
16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel.

The C information is organized in slices where sequential C elements are grouped in 16 byte or 32
byte quantities.
If the total number of channels is not a multiple of the number of channels per slice, then the last
slice must be padded with zeros to make it complete 16B or 32B slice.
Interleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D.



5.5.6. Swizzling Modesï

The layout of the data in the shared memory can be different to that of global memory, for access
performance reasons. The following describes various swizzling modes:


No swizzle mode:
There is no swizzling in this mode and the destination data layout is exactly similar to the
source data layout.













0
1
2
3
4
5
6
7


0
1
2
3
4
5
6
7


â¦ Pattern repeats â¦





32 byte swizzle mode:
The following table, where each elements (numbered cell) is 16 byte and the starting address is
256 bytes aligned, shows the pattern of the destination data layout:













0
1
2
3
4
5
6
7


1
0
3
2
5
4
7
6


â¦ Pattern repeats â¦



An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,
with the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in
Figure 12.



Figure 12 32-byte swizzle mode exampleï


Figure 13 shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1.



Figure 13 32-byte swizzle mode fragmentsï


Figure 14 shows the destination data layout with 32 byte swizzling.



Figure 14 32-byte swizzle mode destination data layoutï




64 byte swizzle mode:
The following table, where each elements (numbered cell) is 16 byte and the starting address is
512 bytes aligned, shows the pattern of the destination data layout:













0
1
2
3
4
5
6
7


1
0
3
2
5
4
7
6


2
3
0
1
6
7
4
5


3
2
1
0
7
6
5
4


â¦ Pattern repeats â¦



An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /
channel and 32 channels, is shown in Figure 15.



Figure 15 64-byte swizzle mode exampleï


Each colored cell represents 8 channels. Figure 16 shows the source data layout.



Figure 16 64-byte swizzle mode source data layoutï


Figure 17 shows the destination data layout with 64 byte swizzling.



Figure 17 64-byte swizzle mode destination data layoutï




128 byte swizzle mode:
The following table, where each elements (numbered cell) is 16 byte and the starting address is
1024 bytes aligned, shows the pattern of the destination data layout:













0
1
2
3
4
5
6
7


1
0
3
2
5
4
7
6


2
3
0
1
6
7
4
5


3
2
1
0
7
6
5
4


4
5
6
7
0
1
2
3


5
4
7
6
1
0
3
2


6
7
4
5
2
3
0
1


â¦ Pattern repeats â¦



An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /
channel and 64 channels, is shown in Figure 18.



Figure 18 128-byte swizzle mode exampleï


Each colored cell represents 8 channels. Figure 19 shows the source data layout.



Figure 19 128-byte swizzle mode source data layoutï


Figure 20 shows the destination data layout with 128 byte swizzling.



Figure 20 128-byte swizzle mode destination data layoutï







5.5.7. Tensor-mapï

The tensor-map is a 128-byte opaque object either in .const space or .param (kernel function
parameter) space or .global space which describes the tensor properties and the access properties
of the tensor data described in previous sections.
Tensor-Map can be created using CUDA APIs. Refer to CUDA programming guide for more details.





6. Instruction Operandsï



6.1. Operand Type Informationï

All operands in instructions have a known type from their declarations. Each operand type must be
compatible with the type determined by the instruction template and instruction type. There is no
automatic conversion between types.
The bit-size type is compatible with every type having the same size. Integer types of a common size
are compatible with each other. Operands having type different from but compatible with the
instruction type are silently cast to the instruction type.



6.2. Source Operandsï

The source operands are denoted in the instruction descriptions by the names a, b, and
c. PTX describes a load-store machine, so operands for ALU instructions must all be in variables
declared in the .reg register state space. For most operations, the sizes of the operands must
be consistent.
The cvt (convert) instruction takes a variety of operand types and sizes, as its job is to
convert from nearly any data type to any other data type (and size).
The ld, st, mov, and cvt instructions copy data from one location to
another. Instructions ld and st move data from/to addressable state spaces to/from
registers. The mov instruction copies data between registers.
Most instructions have an optional predicate guard that controls conditional execution, and a few
instructions have additional predicate source operands. Predicate operands are denoted by the names
p, q, r, s.



6.3. Destination Operandsï

PTX instructions that produce a single result store the result in the field denoted by d (for
destination) in the instruction descriptions. The result operand is a scalar or vector variable in
the register state space.



6.4. Using Addresses, Arrays, and Vectorsï

Using scalar variables as operands is straightforward. The interesting capabilities begin with
addresses, arrays, and vectors.


6.4.1. Addresses as Operandsï

All the memory instructions take an address operand that specifies the memory location being
accessed. This addressable operand is one of:

[var]

the name of an addressable variable var.

[reg]

an integer or bit-size type register reg containing a byte address.

[reg+immOff]

a sum of register reg containing a byte address plus a constant integer byte offset (signed, 32-bit).

[var+immOff]

a sum of address of addressable variable var containing a byte address plus a constant integer
byte offset (signed, 32-bit).

[immAddr]

an immediate absolute byte address (unsigned, 32-bit).

var[immOff]

an array element as described in Arrays as Operands.


The register containing an address may be declared as a bit-size type or integer type.
The access size of a memory instruction is the total number of bytes accessed in memory. For
example, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4
bytes.
The address must be naturally aligned to a multiple of the access size. If an address is not
properly aligned, the resulting behavior is undefined. For example, among other things, the access
may proceed by silently masking off low-order address bits to achieve proper rounding, or the
instruction may fault.
The address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are
zero-extended to the specified width as needed, and truncated if the register width exceeds the
state space address width for the target architecture.
Address arithmetic is performed using integer arithmetic and logical instructions. Examples include
pointer arithmetic and pointer comparisons. All addresses and address computations are byte-based;
there is no support for C-style pointer arithmetic.
The mov instruction can be used to move the address of a variable into a pointer. The address is
an offset in the state space in which the variable is declared. Load and store operations move data
between registers and locations in addressable state spaces. The syntax is similar to that used in
many assembly languages, where scalar variables are simply named and addresses are de-referenced by
enclosing the address expression in square brackets. Address expressions include variable names,
address registers, address register plus byte offset, and immediate address expressions which
evaluate at compile-time to a constant address.
Here are a few examples:

.shared .u16 x;
.reg    .u16 r0;
.global .v4 .f32 V;
.reg    .v4 .f32 W;
.const  .s32 tbl[256];
.reg    .b32 p;
.reg    .s32 q;

ld.shared.u16   r0,[x];
ld.global.v4.f32 W, [V];
ld.const.s32    q, [tbl+12];
mov.u32         p, tbl;




6.4.1.1. Generic Addressingï

If a memory instruction does not specify a state space, the operation is performed using generic
addressing. The state spaces .const, Kernel Function Parameters (.param), .local and .shared are modeled as
windows within the generic address space. Each window is defined by a window base and a window size
that is equal to the size of the corresponding state space. A generic address maps to global
memory unless it falls within the window for const, local, or shared memory. The Kernel
Function Parameters (.param) window is contained
within the .global window. Within each window, a generic address maps to an address in the
underlying state space by subtracting the window base from the generic address.




6.4.2. Arrays as Operandsï

Arrays of all types can be declared, and the identifier becomes an address constant in the space
where the array is declared. The size of the array is a constant in the program.
Array elements can be accessed using an explicitly calculated byte address, or by indexing into the
array using square-bracket notation. The expression within square brackets is either a constant
integer, a register variable, or a simple register with constant offset expression, where the
offset is a constant expression that is either added or subtracted from a register variable. If more
complicated indexing is desired, it must be written as an address calculation prior to use. Examples
are:

ld.global.u32  s, a[0];
ld.global.u32  s, a[N-1];
mov.u32        s, a[1];  // move address of a[1] into s





6.4.3. Vectors as Operandsï

Vector operands are supported by a limited subset of instructions, which include mov, ld,
st, atom, red and tex. Vectors may also be passed as arguments to called functions.
Vector elements can be extracted from the vector with the suffixes .x, .y, .z and
.w, as well as the typical color fields .r, .g, .b and .a.
A brace-enclosed list is used for pattern matching to pull apart vectors.

.reg .v4 .f32 V;
.reg .f32     a, b, c, d;

mov.v4.f32 {a,b,c,d}, V;


Vector loads and stores can be used to implement wide loads and stores, which may improve memory
performance. The registers in the load/store operations can be a vector, or a brace-enclosed list of
similarly typed scalars. Here are examples:

ld.global.v4.f32  {a,b,c,d}, [addr+16];
ld.global.v2.u32  V2, [addr+8];


Elements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows:

Ra = V.x = V.r
Rb = V.y = V.g
Rc = V.z = V.b
Rd = V.w = V.a





6.4.4. Labels and Function Names as Operandsï

Labels and function names can be used only in bra/brx.idx and call instructions
respectively. Function names can be used in mov instruction to get the address of the function
into a register, for use in an indirect call.
Beginning in PTX ISA version 3.1, the mov instruction may be used to take the address of kernel
functions, to be passed to a system call that initiates a kernel launch from the GPU. This feature
is part of the support for CUDA Dynamic Parallelism. See the CUDA Dynamic Parallelism Programming
Guide for details.




6.5. Type Conversionï

All operands to all arithmetic, logic, and data movement instruction must be of the same type and
size, except for operations where changing the size and/or type is part of the definition of the
instruction. Operands of different sizes or types must be converted prior to the operation.


6.5.1. Scalar Conversionsï

Table 13 shows what
precision and format the cvt instruction uses given operands of differing types. For example, if a
cvt.s32.u16 instruction is given a u16 source operand and s32 as a destination operand,
the u16 is zero-extended to s32.
Conversions to floating-point that are beyond the range of floating-point numbers are represented
with the maximum floating-point value (IEEE 754 Inf for f32 and f64, and ~131,000 for
f16).


Table 13 Convert Instruction Precision and Formatï



















Destination Format


s8
s16
s32
s64
u8
u16
u32
u64
f16
f32
f64




Source
Format
s8
â
sext
sext
sext
â
sext
sext
sext
s2f
s2f
s2f


s16
chop1
â
sext
sext
chop1
â
sext
sext
s2f
s2f
s2f


s32
chop1
chop1
â
sext
chop1
chop1
â
sext
s2f
s2f
s2f


s64
chop1
chop1
chop
â
chop1
chop1
chop
â
s2f
s2f
s2f


u8
â
zext
zext
zext
â
zext
zext
zext
u2f
u2f
u2f


u16
chop1
â
zext
zext
chop1
â
zext
zext
u2f
u2f
u2f


u32
chop1
chop1
â
zext
chop1
chop1
â
zext
u2f
u2f
u2f


u64
chop1
chop1
chop
â
chop1
chop1
chop
â
u2f
u2f
u2f


f16
f2s
f2s
f2s
f2s
f2u
f2u
f2u
f2u
â
f2f
f2f


f32
f2s
f2s
f2s
f2s
f2u
f2u
f2u
f2u
f2f
â
f2f


f64
f2s
f2s
f2s
f2s
f2u
f2u
f2u
f2u
f2f
f2f
â


Notes

sext = sign-extend; zext = zero-extend; chop = keep only low bits that fit;
s2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float;
f2u = float-to-unsigned; f2f = float-to-float.
1 If the destination register is wider than the destination format, the result is extended to the
destination register width after chopping. The type of extension (sign or zero) is based on the
destination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then
sign-extends to 32-bit.







6.5.2. Rounding Modifiersï

Conversion instructions may specify a rounding modifier. In PTX, there are four integer rounding
modifiers and four floating-point rounding
modifiers. Table 14 and
Table 15 summarize the rounding modifiers.


Table 14 Floating-Point Rounding Modifiersï







Modifier
Description




.rn
mantissa LSB rounds to nearest even


.rna
mantissa LSB rounds to nearest, ties away from zero


.rz
mantissa LSB rounds towards zero


.rm
mantissa LSB rounds towards negative infinity


.rp
mantissa LSB rounds towards positive infinity





Table 15 Integer Rounding Modifiersï







Modifier
Description




.rni
round to nearest integer, choosing even integer if source is equidistant between two integers.


.rzi
round to nearest integer in the direction of zero


.rmi
round to nearest integer in direction of negative infinity


.rpi
round to nearest integer in direction of positive infinity







6.6. Operand Costsï

Operands from different state spaces affect the speed of an operation. Registers are fastest, while
global memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first
is to have multiple threads of execution so that the hardware can issue a memory operation and then
switch to other execution. Another way to hide latency is to issue the load instructions as early as
possible, as execution is not blocked until the desired result is used in a subsequent (in time)
instruction. The register in a store operation is available much more
quickly. Table 16 gives estimates of the
costs of using different kinds of memory.


Table 16 Cost Estimates for Accessing State-Spacesï








Space
Time
Notes




Register
0



Shared
0



Constant
0
Amortized cost is low, first access is high


Local
> 100 clocks



Parameter
0



Immediate
0



Global
> 100 clocks



Texture
> 100 clocks



Surface
> 100 clocks








7. Abstracting the ABIï

Rather than expose details of a particular calling convention, stack layout, and Application Binary
Interface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI
implementations. In this section, we describe the features of PTX needed to achieve this hiding of
the ABI. These include syntax for function definitions, function calls, parameter passing, support
for variadic functions (varargs), and memory allocated on the stack (alloca).
Refer to PTX Writers Guide to Interoperability for details on generating PTX compliant with
Application Binary Interface (ABI) for the CUDAÂ® architecture.


7.1. Function Declarations and Definitionsï

In PTX, functions are declared and defined using the .func directive. A function declaration
specifies an optional list of return parameters, the function name, and an optional list of input
parameters; together these specify the functionâs interface, or prototype. A function definition
specifies both the interface and the body of the function. A function must be declared or defined
prior to being called.
The simplest function has no parameters or return values, and is represented in PTX as follows:

.func foo
{
    ...
    ret;
}

    ...
    call foo;
    ...


Here, execution of the call instruction transfers control to foo, implicitly saving the
return address. Execution of the ret instruction within foo transfers control to the
instruction following the call.
Scalar and vector base-type input and return parameters may be represented simply as register
variables. At the call, arguments may be register variables or constants, and return values may be
placed directly into register variables. The arguments and return variables at the call must have
type and size that match the calleeâs corresponding formal parameters.
Example

.func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc )
{
    add.u32 %res, %ptr, %inc;
    ret;
}

    ...
    call (%r1), inc_ptr, (%r1,4);
    ...


When using the ABI, .reg state space parameters must be at least 32-bits in size. Subword scalar
objects in the source language should be promoted to 32-bit registers in PTX, or use .param
state space byte arrays described next.
Objects such as C structures and unions are flattened into registers or byte arrays in PTX and are
represented using .param space memory. For example, consider the following C structure, passed
by value to a function:

struct {
    double dbl;
    char   c[4];
};


In PTX, this structure will be flattened into a byte array. Since memory accesses are required to be
aligned to a multiple of the access size, the structure in this example will be a 12 byte array with
8 byte alignment so that accesses to the .f64 field are aligned. The .param state space is
used to pass the structure by value:
Example

.func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12])
{
    .reg .f64 f1;
    .reg .b32 c1, c2, c3, c4;
    ...
    ld.param.f64 f1, [y+0];
    ld.param.b8  c1, [y+8];
    ld.param.b8  c2, [y+9];
    ld.param.b8  c3, [y+10];
    ld.param.b8  c4, [y+11];
    ...
    ... // computation using x,f1,c1,c2,c3,c4;
}

{
     .param .b8 .align 8 py[12];
     ...
     st.param.b64 [py+ 0], %rd;
     st.param.b8  [py+ 8], %rc1;
     st.param.b8  [py+ 9], %rc2;
     st.param.b8  [py+10], %rc1;
     st.param.b8  [py+11], %rc2;
     // scalar args in .reg space, byte array in .param space
     call (%out), bar, (%x, py);
     ...


In this example, note that .param space variables are used in two ways. First, a .param
variable y is used in function definition bar to represent a formal parameter. Second, a
.param variable py is declared in the body of the calling function and used to set up the
structure being passed to bar.
The following is a conceptual way to think about the .param state space use in device functions.
For a caller,

The .param state space is used to set values that will be passed to a called function and/or
to receive return values from a called function. Typically, a .param byte array is used to
collect together fields of a structure being passed by value.

For a callee,

The .param state space is used to receive parameter values and/or pass return values back to
the caller.

The following restrictions apply to parameter passing.
For a caller,

Arguments may be .param variables, .reg variables, or constants.
In the case of .param space formal parameters that are byte arrays, the argument must also be
a .param space byte array with matching type, size, and alignment. A .param argument must
be declared within the local scope of the caller.
In the case of .param space formal parameters that are base-type scalar or vector variables,
the corresponding argument may be either a .param or .reg space variable with matching
type and size, or a constant that can be represented in the type of the formal parameter.
In the case of .reg space formal parameters, the corresponding argument may be either a
.param or .reg space variable of matching type and size, or a constant that can be
represented in the type of the formal parameter.
In the case of .reg space formal parameters, the register must be at least 32-bits in size.
All st.param instructions used for passing arguments to function call must immediately precede
the corresponding call instruction and ld.param instruction used for collecting return
value must immediately follow the call instruction without any control flow
alteration. st.param and ld.param instructions used for argument passing cannot be
predicated. This enables compiler optimization and ensures that the .param variable does not
consume extra space in the callerâs frame beyond that needed by the ABI. The .param variable
simply allows a mapping to be made at the call site between data that may be in multiple
locations (e.g., structure being manipulated by caller is located in registers and memory) to
something that can be passed as a parameter or return value to the callee.

For a callee,

Input and return parameters may be .param variables or .reg variables.
Parameters in .param memory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes.
Parameters in the .reg state space must be at least 32-bits in size.
The .reg state space can be used to receive and return base-type scalar and vector values,
including sub-word size objects when compiling in non-ABI mode. Supporting the .reg state
space provides legacy support.

Note that the choice of .reg or .param state space for parameter passing has no impact on
whether the parameter is ultimately passed in physical registers or on the stack. The mapping of
parameters to physical registers and stack locations depends on the ABI definition and the order,
size, and alignment of parameters.


7.1.1. Changes from PTX ISA Version 1.xï

In PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no
support for array parameters. Objects such as C structures were flattened and passed or returned
using multiple registers. PTX ISA version 1.x supports multiple return values for this purpose.
Beginning with PTX ISA version 2.0, formal parameters may be in either .reg or .param state
space, and .param space parameters support arrays. For targets sm_20 or higher, PTX
restricts functions to a single return value, and a .param byte array should be used to return
objects that do not fit into a register. PTX continues to support multiple return registers for
sm_1x targets.

Note
PTX implements a stack-based ABI only for targets sm_20 or higher.

PTX ISA versions prior to 3.0 permitted variables in .reg and .local state spaces to be
defined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows
module-scoped .reg and .local variables and restricts their use to within function
scope. When compiling without use of the ABI, module-scoped .reg and .local variables are
supported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing
module-scoped .reg or .local variables, the compiler silently disables use of the ABI.




7.2. Variadic Functionsï


Note
Support for variadic functions which was unimplemented has been removed from the spec.

PTX version 6.0 supports passing unsized array parameter to a function which can be used to
implement variadic functions.
Refer to Kernel and Function Directives: .func
for details



7.3. Allocaï

PTX provides alloca instruction for allocating storage at runtime on the per-thread local memory
stack. The allocated stack memory can be accessed with ld.local and st.local instructions
using the pointer returned by alloca.
In order to facilitate deallocation of memory allocated with alloca, PTX provides two additional
instructions: stacksave which allows reading the value of stack pointer in a local variable, and
stackrestore which can restore the stack pointer with the saved value.
alloca, stacksave, and stackrestore instructions are described in Stack Manipulation
Instructions.

Preview Feature:

Stack manipulation instructions alloca, stacksave and stackrestore are preview features
in PTX ISA version 7.3. All details are subject to change with no guarantees of backward
compatibility on future PTX ISA versions or SM architectures.






8. Memory Consistency Modelï

In multi-threaded executions, the side-effects of memory operations performed by each thread become
visible to other threads in a partial and non-identical order. This means that any two operations
may appear to happen in no order, or in different orders, to different threads. The axioms
introduced by the memory consistency model specify exactly which contradictions are forbidden
between the orders observed by different threads.
In the absence of any constraint, each read operation returns the value committed by some write
operation to the same memory location, including the initial write to that memory location. The
memory consistency model effectively constrains the set of such candidate writes from which a read
operation can return a value.


8.1. Scope and applicability of the modelï

The constraints specified under this model apply to PTX programs with any PTX ISA version number,
running on sm_70 or later architectures.
The memory consistency model does not apply to texture (including ld.global.nc) and surface
accesses.


8.1.1. Limitations on atomicity at system scopeï

When communicating with the host CPU, certain strong operations with system scope may not be
performed atomically on some systems. For more details on atomicity guarantees to host memory, see
the CUDA Atomicity Requirements.




8.2. Memory operationsï

The fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state
space available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX
state space has a unique address relative to all threads that have access to the same state space.
Each PTX memory instruction specifies an address operand and a data type. The address operand
contains a virtual address that gets converted to a physical address during memory access. The
physical address and the size of the data type together define a physical memory location, which is
the range of bytes starting from the physical address and extending up to the size of the data type
in bytes.
The memory consistency model specification uses the terms âaddressâ or âmemory addressâ to indicate
a virtual address, and the term âmemory locationâ to indicate a physical memory location.
Each PTX memory instruction also specifies the operation â either a read, a write or an atomic
read-modify-write â to be performed on all the bytes in the corresponding memory location.


8.2.1. Overlapï

Two memory locations are said to overlap when the starting address of one location is within the
range of bytes constituting the other location. Two memory operations are said to overlap when they
specify the same virtual address and the corresponding memory locations overlap. The overlap is said
to be complete when both memory locations are identical, and it is said to be partial otherwise.



8.2.2. Aliasesï

Two distinct virtual addresses are said to be aliases if they map to the same memory location.



8.2.3. Multimem Addressesï

A multimem address is a virtual address which points to multiple distinct memory locations across
devices.
Only multimem.* operations are valid on multimem addresses. That is, the behavior of accessing
a multimem address in any other memory operation is undefined.



8.2.4. Memory Operations on Vector Data Typesï

The memory consistency model relates operations executed on memory locations with scalar data types,
which have a maximum size and alignment of 64 bits. Memory operations with a vector data type are
modelled as a set of equivalent memory operations with a scalar data type, executed in an
unspecified order on the elements in the vector.



8.2.5. Memory Operations on Packed Data Typesï

A packed data type consists of two values of the same scalar data type, as described in Packed Data
Types. These values are accessed in adjacent memory locations. A
memory operation on a packed data type is modelled as a pair of equivalent memory operations on the
scalar data type, executed in an unspecified order on each element of the packed data.



8.2.6. Initializationï

Each byte in memory is initialized by a hypothetical write W0 executed before starting any thread
in the program. If the byte is included in a program variable, and that variable has an initial
value, then W0 writes the corresponding initial value for that byte; else W0 is assumed to have
written an unknown but constant value to the byte.




8.3. State spacesï

The relations defined in the memory consistency model are independent of state spaces. In
particular, causality order closes over all memory operations across all the state spaces. But the
side-effect of a memory operation in one state space can be observed directly only by operations
that also have access to the same state space. This further constrains the synchronizing effect of a
memory operation in addition to scope. For example, the synchronizing effect of the PTX instruction
ld.relaxed.shared.sys is identical to that of ld.relaxed.shared.cluster, since no thread
outside the same cluster can execute an operation that accesses the same memory location.



8.4. Operation typesï

For simplicity, the rest of the document refers to the following operation types, instead of
mentioning specific instructions that give rise to them.


Table 17 Operation Typesï







Operation Type
Instruction/Operation




atomic operation
atom or red instruction.


read operation
All variants of ld instruction and atom instruction (but not
red instruction).


write operation
All variants of st instruction, and atomic operations if they result
in a write.


memory operation
A read or write operation.


volatile operation
An instruction with .volatile qualifier.


acquire operation
A memory operation with .acquire or .acq_rel qualifier.


release operation
A memory operation with .release or .acq_rel qualifier.


mmio operation
An ld or st instruction with .mmio qualifier.


memory fence operation
A membar, fence.sc or fence.acq_rel instruction.


proxy fence operation
A fence.proxy or a membar.proxy instruction.


strong operation
A memory fence operation, or a memory operation with a .relaxed,
.acquire, .release, .acq_rel, .volatile, or .mmio
qualifier.


weak operation
An ld or st instruction with a .weak qualifier.


synchronizing operation
A barrier instruction, fence operation, release operation or
acquire operation.





8.4.1. mmio Operationï

An mmio operation is a memory operation with .mmio qualifier specified. It is usually performed
on a memory location which is mapped to the control registers of peer I/O devices. It can also be
used for communication between threads but has poor performance relative to non-mmio operations.
The semantic meaning of mmio operations cannot be defined precisely as it is defined by the
underlying I/O device. For formal specification of semantics of mmio operation from Memory
Consistency Model perspective, it is equivalent to the semantics of a strong operation. But it
follows a few implementation-specific properties, if it meets the CUDA atomicity requirements at
the specified scope:

Writes are always performed and are never combined within the scope specified.

Reads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any
cache within the scope specified.

As an exception, in some implementations, the surrounding locations may also be loaded. In such
cases the amount of data loaded is implementation specific and varies between 32 and 128 bytes
in size.







8.5. Scopeï

Each strong operation must specify a scope, which is the set of threads that may interact
directly with that operation and establish any of the relations described in the memory consistency
model. There are four scopes:


Table 18 Scopesï







Scope
Description




.cta
The set of all threads executing in the same CTA as the current thread.


.cluster
The set of all threads executing in the same cluster as the current thread.


.gpu
The set of all threads in the current program executing on the same compute
device as the current thread. This also includes other kernel grids invoked by
the host program on the same compute device.


.sys
The set of all threads in the current program, including all kernel grids
invoked by the host program on all compute devices, and all threads
constituting the host program itself.



Note that the warp is not a scope; the CTA is the smallest collection of threads that qualifies as
a scope in the memory consistency model.



8.6. Proxiesï

A memory proxy, or a proxy is an abstract label applied to a method of memory access. When two
memory operations use distinct methods of memory access, they are said to be different proxies.
Memory operations as defined in Operation types use generic
method of memory access, i.e. a generic proxy. Other operations such as textures and surfaces all
use distinct methods of memory access, also distinct from the generic method.
A proxy fence is required to synchronize memory operations across different proxies. Although
virtual aliases use the generic method of memory access, since using distinct virtual addresses
behaves as if using different proxies, they require a proxy fence to establish memory ordering.



8.7. Morally strong operationsï

Two operations are said to be morally strong relative to each other if they satisfy all of the
following conditions:

The operations are related in program order (i.e, they are both executed by the same thread),
or each operation is strong and specifies a scope that includes the thread executing the
other operation.
Both operations are performed via the same proxy.
If both are memory operations, then they overlap completely.

Most (but not all) of the axioms in the memory consistency model depend on relations between
morally strong operations.


8.7.1. Conflict and Data-racesï

Two overlapping memory operations are said to conflict when at least one of them is a write.
Two conflicting memory operations are said to be in a data-race if they are not related in
causality order and they are not morally strong.



8.7.2. Limitations on Mixed-size Data-racesï

A data-race between operations that overlap completely is called a uniform-size data-race,
while a data-race between operations that overlap partially is called a mixed-size data-race.
The axioms in the memory consistency model do not apply if a PTX program contains one or more
mixed-size data-races. But these axioms are sufficient to describe the behavior of a PTX program
with only uniform-size data-races.
Atomicity of mixed-size RMW operations
In any program with or without mixed-size data-races, the following property holds for every pair
of overlapping atomic operations A1 and A2 such that each specifies a scope that includes the
other: Either the read-modify-write operation specified by A1 is performed completely before A2 is
initiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2
overlap partially or completely.




8.8. Release and Acquire Patternsï

Some sequences of instructions give rise to patterns that participate in memory synchronization as
described later. The release pattern makes prior operations from the current thread1
visible to some operations from other threads. The acquire pattern makes some operations from
other threads visible to later operations from the current thread.
A release pattern on a location M consists of one of the following:


A release operation on M
E.g.: st.release [M]; or atom.acq_rel [M]; or mbarrier.arrive.release [M];


Or a release operation on M followed by a strong write on M in program order
E.g.: st.release [M]; st.relaxed [M];


Or a memory fence followed by a strong write on M in program order
E.g.: fence; st.relaxed [M];


Any memory synchronization established by a release pattern only affects operations occurring in
program order before the first instruction in that pattern.
An acquire pattern on a location M consists of one of the following:


An acquire operation on M
E.g.: ld.acquire [M]; or atom.acq_rel [M]; or mbarrier.test_wait.acquire [M];


Or a strong read on M followed by an acquire operation on M in program order
E.g.: ld.relaxed [M]; ld.acquire [M];


Or a strong read on M followed by a memory fence in program order
E.g.: ld.relaxed [M]; fence;


Any memory synchronization established by an acquire pattern only affects operations occurring
in program order after the last instruction in that pattern.
1 For both release and acquire patterns, this effect is further extended to operations in
other threads through the transitive nature of causality order.



8.9. Ordering of memory operationsï

The sequence of operations performed by each thread is captured as program order while memory
synchronization across threads is captured as causality order. The visibility of the side-effects
of memory operations to other memory operations is captured as communication order. The memory
consistency model defines contradictions that are disallowed between communication order on the one
hand, and causality order and program order on the other.


8.9.1. Program Orderï

The program order relates all operations performed by a thread to the order in which a sequential
processor will execute instructions in the corresponding PTX source. It is a transitive relation
that forms a total order over the operations performed by the thread, but does not relate operations
from different threads.


8.9.1.1. Asynchronous Operationsï

Some PTX instructions (all variants of cp.async, cp.async.bulk, cp.reduce.async.bulk,
wgmma.mma_async) perform operations that are asynchronous to the thread that executed the
instruction. These asynchronous operations are ordered after prior instructions in the same thread
(except in the case of wgmma.mma_async), but they are not part of the program order for that
thread. Instead, they provide weaker ordering guarantees as documented in the instruction
description.
For example, the loads and stores performed as part of a cp.async are ordered with respect to
each other, but not to those of any other cp.async instructions initiated by the same thread,
nor any other instruction subsequently issued by the thread with the exception of
cp.async.commit_group or cp.async.mbarrier.arrive. The asynchronous mbarrier arrive-on operation
performed by a cp.async.mbarrier.arrive instruction is ordered with respect to the memory
operations performed by all prior cp.async operations initiated by the same thread, but not to
those of any other instruction issued by the thread. The implicit mbarrier complete-tx
operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk
instructions is ordered only with respect to the memory operations performed by the same
asynchronous instruction, and in particular it does not transitively establish ordering with respect
to prior instructions from the issuing thread.




8.9.2. Observation Orderï

Observation order relates a write W to a read R through an optional sequence of atomic
read-modify-write operations.
A write W precedes a read R in observation order if:

R and W are morally strong and R reads the value written by W, or
For some atomic operation Z, W precedes Z and Z precedes R in observation order.




8.9.3. Fence-SC Orderï

The Fence-SC order is an acyclic partial order, determined at runtime, that relates every pair of
morally strong fence.sc operations.



8.9.4. Memory synchronizationï

Synchronizing operations performed by different threads synchronize with each other at runtime as
described here. The effect of such synchronization is to establish causality order across threads.

A fence.sc operation X synchronizes with a fence.sc operation Y if X precedes Y in the
Fence-SC order.
A bar{.cta}.sync or bar{.cta}.red or bar{.cta}.arrive operation synchronizes with a
bar{.cta}.sync or bar{.cta}.red operation executed on the same barrier.
A barrier.cluster.arrive operation synchronizes with a barrier.cluster.wait operation.
A release pattern X synchronizes with an acquire pattern Y, if a write operation in X
precedes a read operation in Y in observation order, and the first operation in X and the
last operation in Y are morally strong.

API synchronization
A synchronizes relation can also be established by certain CUDA APIs.

Completion of a task enqueued in a CUDA stream synchronizes with the start of the following
task in the same stream, if any.
For purposes of the above, recording or waiting on a CUDA event in a stream, or causing a
cross-stream barrier to be inserted due to cudaStreamLegacy, enqueues tasks in the associated
streams even if there are no direct side effects. An event record task synchronizes with
matching event wait tasks, and a barrier arrival task synchronizes with matching barrier wait
tasks.
Start of a CUDA kernel synchronizes with start of all threads in the kernel. End of all threads
in a kernel synchronize with end of the kernel.
Start of a CUDA graph synchronizes with start of all source nodes in the graph. Completion of
all sink nodes in a CUDA graph synchronizes with completion of the graph. Completion of a graph
node synchronizes with start of all nodes with a direct dependency.
Start of a CUDA API call to enqueue a task synchronizes with start of the task.
Completion of the last task queued to a stream, if any, synchronizes with return from
cudaStreamSynchronize. Completion of the most recently queued matching event record task, if
any, synchronizes with return from cudaEventSynchronize. Synchronizing a CUDA device or
context behaves as if synchronizing all streams in the context, including ones that have been
destroyed.
Returning cudaSuccess from an API to query a CUDA handle, such as a stream or event, behaves
the same as return from the matching synchronization API.

In addition to establishing a synchronizes relation, the CUDA API synchronization mechanisms above
also participate in proxy-preserved base causality order.



8.9.5. Causality Orderï

Causality order captures how memory operations become visible across threads through synchronizing
operations. The axiom âCausalityâ uses this order to constrain the set of write operations from
which a read operation may read a value.
Relations in the causality order primarily consist of relations in Base causality order1 , which is a transitive order, determined at runtime.
Base causality order
An operation X precedes an operation Y in base causality order if:

X precedes Y in program order, or
X synchronizes with Y, or

For some operation Z,

X precedes Z in program order and Z precedes Y in base causality order, or
X precedes Z in base causality order and Z precedes Y in program order, or
X precedes Z in base causality order and Z precedes Y in base causality order.



Proxy-preserved base causality order
A memory operation X precedes a memory operation Y in proxy-preserved base causality order if X
precedes Y in base causality order, and:

X and Y are performed to the same address, using the generic proxy, or
X and Y are performed to the same address, using the same proxy, and by the same thread block,
or
X and Y are aliases and there is an alias proxy fence along the base causality path from X
to Y.

Causality order
Causality order combines base causality order with some non-transitive relations as follows:
An operation X precedes an operation Y in causality order if:

X precedes Y in proxy-preserved base causality order, or
For some operation Z, X precedes Z in observation order, and Z precedes Y in proxy-preserved
base causality order.

1 The transitivity of base causality order accounts for the âcumulativityâ of synchronizing
operations.



8.9.6. Coherence Orderï

There exists a partial transitive order that relates overlapping write operations, determined at
runtime, called the coherence order1. Two overlapping write operations are related in
coherence order if they are morally strong or if they are related in causality order. Two
overlapping writes are unrelated in coherence order if they are in a data-race, which gives
rise to the partial nature of coherence order.
1 Coherence order cannot be observed directly since it consists entirely of write
operations. It may be observed indirectly by its use in constraining the set of candidate
writes that a read operation may read from.



8.9.7. Communication Orderï

The communication order is a non-transitive order, determined at runtime, that relates write
operations to other overlapping memory operations.

A write W precedes an overlapping read R in communication order if R returns the value of any
byte that was written by W.
A write W precedes a write Wâ in communication order if W precedes Wâ in coherence order.
A read R precedes an overlapping write W in communication order if, for any byte accessed by
both R and W, R returns the value written by a write Wâ that precedes W in coherence order.

Communication order captures the visibility of memory operations â when a memory operation X1
precedes a memory operation X2 in communication order, X1 is said to be visible to X2.




8.10. Axiomsï



8.10.1. Coherenceï

If a write W precedes an overlapping write Wâ in causality order, then W must precede Wâ in
coherence order.



8.10.2. Fence-SCï

Fence-SC order cannot contradict causality order. For a pair of morally strong fence.sc
operations F1 and F2, if F1 precedes F2 in causality order, then F1 must precede F2 in Fence-SC
order.



8.10.3. Atomicityï

Single-Copy Atomicity
Conflicting morally strong operations are performed with single-copy atomicity. When a read R
and a write W are morally strong, then the following two communications cannot both exist in the
same execution, for the set of bytes accessed by both R and W:

R reads any byte from W.
R reads any byte from any write Wâ which precedes W in coherence order.

Atomicity of read-modify-write (RMW) operations
When an atomic operation A and a write W overlap and are morally strong, then the following
two communications cannot both exist in the same execution, for the set of bytes accessed by both A
and W:

A reads any byte from a write Wâ that precedes W in coherence order.
A follows W in coherence order.

Litmus Test 1:









.global .u32 x = 0;





T1
T2




A1: atom.sys.inc.u32 %r0, [x];





A2: atom.sys.inc.u32 %r0, [x];







FINAL STATE: x == 2






Atomicity is guaranteed when the operations are morally strong.
Litmus Test 2:









.global .u32 x = 0;





T1
T2 (In a different CTA)




A1: atom.cta.inc.u32 %r0, [x];





A2: atom.gpu.inc.u32 %r0, [x];







FINAL STATE: x == 1 OR x == 2






Atomicity is not guaranteed if the operations are not morally strong.



8.10.4. No Thin Airï

Values may not appear âout of thin airâ: an execution cannot speculatively produce a value in such a
way that the speculation becomes self-satisfying through chains of instruction dependencies and
inter-thread communication. This matches both programmer intuition and hardware reality, but is
necessary to state explicitly when performing formal analysis.
Litmus Test: Load Buffering









.global .u32 x = 0;
.global .u32 y = 0;





T1
T2




A1: ld.global.u32 %r0, [x];
B1: st.global.u32 [y], %r0;





A2: ld.global.u32 %r1, [y];
B2: st.global.u32 [x], %r1;







FINAL STATE: x == 0 AND y == 0






The litmus test known as âLBâ (Load Buffering) checks such forbidden values that may arise out of
thin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a
second variable, with the first and second variable exchanged between the threads. If each variable
is initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1,
then values passing through the memory operations in this example form a cycle:
A1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of
the memory operations in this example were to speculatively associate a different value with the
corresponding memory location, then such a speculation would become self-fulfilling, and hence
forbidden.



8.10.5. Sequential Consistency Per Locationï

Within any set of overlapping memory operations that are pairwise morally strong, communication
order cannot contradict program order, i.e., a concatenation of program order between
overlapping operations and morally strong relations in communication order cannot result in a
cycle. This ensures that each program slice of overlapping pairwise morally strong operations is
strictly sequentially-consistent.
Litmus Test: CoRR









.global .u32 x = 0;





T1
T2




W1: st.global.relaxed.sys.u32 [x], 1;





R1: ld.global.relaxed.sys.u32 %r0, [x];
R2: ld.global.relaxed.sys.u32 %r1, [x];







IF %r0 == 1 THEN %r1 == 1






The litmus test âCoRRâ (Coherent Read-Read), demonstrates one consequence of this guarantee. A
thread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence
of) reads R1 and R2 on the same location x. No other writes are executed on x, except the one
modelling the initial value. The operations W1, R1 and R2 are pairwise morally strong. If R1 reads
from W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial
value of x instead, then this would form a sequence of morally-strong relations R2->W1->R1 in
communication order that contradicts the program order R1->R2 in thread T2. Hence R2 cannot read
the initial value of x in such an execution.



8.10.6. Causalityï

Relations in communication order cannot contradict causality order. This constrains the set of
candidate write operations that a read operation may read from:

If a read R precedes an overlapping write W in causality order, then R cannot read from W.
If a write W precedes an overlapping read R in causality order, then for any byte accessed by
both R and W, R cannot read from any write Wâ that precedes W in coherence order.

Litmus Test: Message Passing









.global .u32 data = 0;
.global .u32 flag = 0;





T1
T2




W1: st.global.u32 [data], 1;
F1: fence.sys;
W2: st.global.relaxed.sys.u32 [flag], 1;





R1: ld.global.relaxed.sys.u32 %r0, [flag];
F2: fence.sys;
R2: ld.global.u32 %r1, [data];







IF %r0 == 1 THEN %r1 == 1






The litmus test known as âMPâ (Message Passing) represents the essence of typical synchronization
algorithms. A vast majority of useful programs can be reduced to sequenced applications of this
pattern.
Thread T1 first writes to a data variable and then to a flag variable while a second thread T2 first
reads from the flag variable and then from the data variable. The operations on the flag are
morally strong and the memory operations in each thread are separated by a fence, and these
fences are morally strong.
If R1 observes W2, then the release pattern âF1; W2â synchronizes with the acquire pattern âR1;
F2â. This establishes the causality order W1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom causality
guarantees that R2 cannot read from any write that precedes W1 in coherence order. In the absence
of any other writes in this example, R2 must read from W1.
Litmus Test: CoWR








// These addresses are aliases
.global .u32 data_alias_1;
.global .u32 data_alias_2;





T1




W1: st.global.u32 [data_alias_1], 1;
F1: fence.proxy.alias;
R1: ld.global.u32 %r1, [data_alias_2];







%r1 == 1






Virtual aliases require an alias proxy fence along the synchronization path.
Litmus Test: Store Buffering
The litmus test known as âSBâ (Store Buffering) demonstrates the sequential consistency enforced
by the fence.sc. A thread T1 writes to a first variable, and then reads the value of a second
variable, while a second thread T2 writes to the second variable and then reads the value of the
first variable. The memory operations in each thread are separated by fence.sc instructions,
and these fences are morally strong.









.global .u32 x = 0;
.global .u32 y = 0;





T1
T2




W1: st.global.u32 [x], 1;
F1: fence.sc.sys;
R1: ld.global.u32 %r0, [y];





W2: st.global.u32 [y], 1;
F2: fence.sc.sys;
R2: ld.global.u32 %r1, [x];







%r0 == 1 OR %r1 == 1






In any execution, either F1 precedes F2 in Fence-SC order, or vice versa. If F1 precedes F2 in
Fence-SC order, then F1 synchronizes with F2. This establishes the causality order in W1 -> F1
-> F2 -> R2. Axiom causality ensures that R2 cannot read from any write that precedes W1 in
coherence order. In the absence of any other write to that variable, R2 must read from
W1. Similarly, in the case where F2 precedes F1 in Fence-SC order, R1 must read from W2. If each
fence.sc in this example were replaced by a fence.acq_rel instruction, then this outcome is
not guaranteed. There may be an execution where the write from each thread remains unobserved from
the other thread, i.e., an execution is possible, where both R1 and R2 return the initial value â0â
for variables y and x respectively.





9. Instruction Setï



9.1. Format and Semantics of Instruction Descriptionsï

This section describes each PTX instruction. In addition to the name and the format of the
instruction, the semantics are described, followed by some examples that attempt to show several
possible instantiations of the instruction.



9.2. PTX Instructionsï

PTX instructions generally have from zero to four operands, plus an optional guard predicate
appearing after an @ symbol to the left of the opcode:

@pÂ Â  opcode;
@pÂ Â  opcode a;
@pÂ Â  opcode d, a;
@pÂ Â  opcode d, a, b;
@pÂ Â  opcode d, a, b, c;

For instructions that create a result value, the d operand is the destination operand, while
a, b, and c are source operands.
The setp instruction writes two destination registers. We use a | symbol to separate
multiple destination registers.

setp.lt.s32  p|q, a, b;  // p = (a < b); q = !(a < b);


For some instructions the destination operand is optional. A bit bucket operand denoted with an
underscore (_) may be used in place of a destination register.



9.3. Predicated Executionï

In PTX, predicate registers are virtual and have .pred as the type specifier. So, predicate
registers can be declared as

.reg .pred p, q, r;


All instructions have an optional guard predicate which controls conditional execution of the
instruction. The syntax to specify conditional execution is to prefix an instruction with @{!}p,
where p is a predicate variable, optionally negated. Instructions without a guard predicate are
executed unconditionally.
Predicates are most commonly set as the result of a comparison performed by the setp
instruction.
As an example, consider the high-level code

if (i < n)
    j = j + 1;


This can be written in PTX as

      setp.lt.s32  p, i, n;    // p = (i < n)
@p    add.s32      j, j, 1;    // if i < n, add 1 to j


To get a conditional branch or conditional function call, use a predicate to control the execution
of the branch or call instructions. To implement the above example as a true conditional branch, the
following PTX instruction sequence might be used:

      setp.lt.s32  p, i, n;    // compare i to n
@!p   bra  L1;                 // if False, branch over
      add.s32      j, j, 1;
L1:     ...




9.3.1. Comparisonsï



9.3.1.1. Integer and Bit-Size Comparisonsï

The signed integer comparisons are the traditional eq (equal), ne (not-equal), lt
(less-than), le (less-than-or-equal), gt (greater-than), and ge
(greater-than-or-equal). The unsigned comparisons are eq, ne, lo (lower), ls
(lower-or-same), hi (higher), and hs (higher-or-same). The bit-size comparisons are eq
and ne; ordering comparisons are not defined for bit-size types.
Table 19
shows the operators for signed integer, unsigned integer, and bit-size types.


Table 19 Operators for Signed Integer, Unsigned Integer, and Bit-Size Typesï









Meaning
Signed Operator
Unsigned Operator
Bit-Size Operator




a == b
eq
eq
eq


a != b
ne
ne
ne


a < b
lt
lo
n/a


a <= b
le
ls
n/a


a > b
gt
hi
n/a


a >= b
ge
hs
n/a






9.3.1.2. Floating Point Comparisonsï

The ordered floating-point comparisons are eq, ne, lt, le, gt, and ge. If
either operand is NaN, the result is
False. Table 20 lists the floating-point
comparison operators.


Table 20 Floating-Point Comparison Operatorsï







Meaning
Floating-Point Operator




a == b && !isNaN(a) && !isNaN(b)
eq


a != b && !isNaN(a) && !isNaN(b)
ne


a < b && !isNaN(a) && !isNaN(b)
lt


a <= b && !isNaN(a) && !isNaN(b)
le


a > b && !isNaN(a) && !isNaN(b)
gt


a >= b && !isNaN(a) && !isNaN(b)
ge



To aid comparison operations in the presence of NaN values, unordered floating-point comparisons
are provided: equ, neu, ltu, leu, gtu, and geu. If both operands are numeric
values (not NaN), then the comparison has the same result as its ordered counterpart. If either
operand is NaN, then the result of the comparison is True.
Table 21 lists the floating-point
comparison operators accepting NaN values.


Table 21 Floating-Point Comparison Operators Accepting NaNï







Meaning
Floating-Point Operator




a == b || isNaN(a) || isNaN(b)
equ


a != b || isNaN(a) || isNaN(b)
neu


a < b || isNaN(a) || isNaN(b)
ltu


a <= b || isNaN(a) || isNaN(b)
leu


a > b || isNaN(a) || isNaN(b)
gtu


a >= b || isNaN(a) || isNaN(b)
geu



To test for NaN values, two operators num (numeric) and nan (isNaN) are
provided. num returns True if both operands are numeric values (not NaN), and nan
returns True if either operand is
NaN. Table 22 lists the
floating-point comparison operators testing for NaN values.


Table 22 Floating-Point Comparison Operators Testing for NaNï







Meaning
Floating-Point Operator




!isNaN(a) && !isNaN(b)
num


isNaN(a) || isNaN(b)
nan







9.3.2. Manipulating Predicatesï

Predicate values may be computed and manipulated using the following instructions: and, or,
xor, not, and mov.
There is no direct conversion between predicates and integer values, and no direct way to load or
store predicate register values. However, setp can be used to generate a predicate from an
integer, and the predicate-based select (selp) instruction can be used to generate an integer
value based on the value of a predicate; for example:

selp.u32 %r1,1,0,%p;    // convert predicate to 32-bit value






9.4. Type Information for Instructions and Operandsï

Typed instructions must have a type-size modifier. For example, the add instruction requires
type and size information to properly perform the addition operation (signed, unsigned, float,
different sizes), and this information must be specified as a suffix to the opcode.
Example

.reg .u16 d, a, b;

add.u16 d, a, b;    // perform a 16-bit unsigned add


Some instructions require multiple type-size modifiers, most notably the data conversion instruction
cvt. It requires separate type-size modifiers for the result and source, and these are placed in
the same order as the operands. For example:

.reg .u16 a;
.reg .f32 d;

cvt.f32.u16 d, a;   // convert 16-bit unsigned to 32-bit float


In general, an operandâs type must agree with the corresponding instruction-type modifier. The rules
for operand and instruction type conformance are as follows:

Bit-size types agree with any type of the same size.
Signed and unsigned integer types agree provided they have the same size, and integer operands are
silently cast to the instruction type if needed. For example, an unsigned integer operand used in
a signed integer instruction will be treated as a signed integer by the instruction.
Floating-point types agree only if they have the same size; i.e., they must match exactly.

Table 23 summarizes these type
checking rules.


Table 23 Type Checking Rulesï












Operand Type





.bX
.sX
.uX
.fX


Instruction Type
.bX
okay
okay
okay
okay


.sX
okay
okay
okay
invalid


.uX
okay
okay
okay
invalid


.fX
okay
invalid
invalid
okay




Note
Some operands have their type and size defined independently from the instruction type-size. For
example, the shift amount operand for left and right shift instructions always has type .u32,
while the remaining operands have their type and size determined by the instruction type.

Example

// 64-bit arithmetic right shift; shift amount 'b' is .u32
    shr.s64 d,a,b;




9.4.1. Operand Size Exceeding Instruction-Type Sizeï

For convenience, ld, st, and cvt instructions permit source and destination data
operands to be wider than the instruction-type size, so that narrow values may be loaded, stored,
and converted using regular-width registers. For example, 8-bit or 16-bit values may be held
directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and
sizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned)
instruction types; floating-point instruction types still require that the operand type-size matches
exactly, unless the operand is of bit-size type.
When a source operand has a size that exceeds the instruction-type size, the source data is
truncated (chopped) to the appropriate number of bits specified by the instruction type-size.
Table 24
summarizes the relaxed type-checking rules for source operands. Note that some combinations may
still be invalid for a particular instruction; for example, the cvt instruction does not support
.bX instruction types, so those rows are invalid for cvt.


Table 24 Relaxed Type-checking Rules for Source Operandsï
























Source Operand Type


b8
b16
b32
b64
b128
s8
s16
s32
s64
u8
u16
u32
u64
f16
f32
f64




Instruction Type
b8
â
chop
chop
chop
chop
â
chop
chop
chop
â
chop
chop
chop
chop
chop
chop


b16
inv
â
chop
chop
chop
inv
â
chop
chop
inv
â
chop
chop
â
chop
chop


b32
inv
inv
â
chop
chop
inv
inv
â
chop
inv
inv
â
chop
inv
â
chop


b64
inv
inv
inv
â
chop
inv
inv
inv
â
inv
inv
inv
â
inv
inv
â


b128
inv
inv
inv
inv
â
inv
inv
inv
inv
inv
inv
inv
inv
inv
inv
inv


s8
â
chop
chop
chop
chop
â
chop
chop
chop
â
chop
chop
chop
inv
inv
inv


s16
inv
â
chop
chop
chop
inv
â
chop
chop
inv
â
chop
chop
inv
inv
inv


s32
inv
inv
â
chop
chop
inv
inv
â
chop
inv
inv
â
chop
inv
inv
inv


s64
inv
inv
inv
â
chop
inv
inv
inv
â
inv
inv
inv
â
inv
inv
inv


u8
â
chop
chop
chop
chop
â
chop
chop
chop
â
chop
chop
chop
inv
inv
inv


u16
inv
â
chop
chop
chop
inv
â
chop
chop
inv
â
chop
chop
inv
inv
inv


u32
inv
inv
â
chop
chop
inv
inv
â
chop
inv
inv
â
chop
inv
inv
inv


u64
inv
inv
inv
â
chop
inv
inv
inv
â
inv
inv
inv
â
inv
inv
inv


f16
inv
â
chop
chop
chop
inv
inv
inv
inv
inv
inv
inv
inv
â
inv
inv


f32
inv
inv
â
chop
chop
inv
inv
inv
inv
inv
inv
inv
inv
inv
â
inv


f64
inv
inv
inv
â
chop
inv
inv
inv
inv
inv
inv
inv
inv
inv
inv
â


Notes

chop = keep only low bits that fit; âââ = allowed, but no conversion needed;
inv = invalid, parse error.

Source register size must be of equal or greater size than the instruction-type size.
Bit-size source registers may be used with any appropriately-sized instruction type. The data are
truncated (âchoppedâ) to the instruction-type size and interpreted according to the instruction
type.
Integer source registers may be used with any appropriately-sized bit-size or integer instruction
type. The data are truncated to the instruction-type size and interpreted according to the
instruction type.
Floating-point source registers can only be used with bit-size or floating-point instruction types.
When used with a narrower bit-size instruction type, the data are truncated. When used with a
floating-point instruction type, the size must match exactly.





When a destination operand has a size that exceeds the instruction-type size, the destination data
is zero- or sign-extended to the size of the destination register. If the corresponding instruction
type is signed integer, the data is sign-extended; otherwise, the data is zero-extended.
Table 25
summarizes the relaxed type-checking rules for destination operands.


Table 25 Relaxed Type-checking Rules for Destination Operandsï
























Destination Operand Type


b8
b16
b32
b64
b128
s8
s16
s32
s64
u8
u16
u32
u64
f16
f32
f64




Instruction Type
b8
â
zext
zext
zext
zext
â
zext
zext
zext
â
zext
zext
zext
zext
zext
zext


b16
inv
â
zext
zext
zext
inv
â
zext
zext
inv
â
zext
zext
â
zext
zext


b32
inv
inv
â
zext
zext
inv
inv
â
zext
inv
inv
â
zext
inv
â
zext


b64
inv
inv
inv
â
zext
inv
inv
inv
â
inv
inv
inv
â
inv
inv
â


b128
inv
inv
inv
inv
â
inv
inv
inv
inv
inv
inv
inv
inv
inv
inv
inv


s8
â
sext
sext
sext
sext
â
sext
sext
sext
â
sext
sext
sext
inv
inv
inv


s16
inv
â
sext
sext
sext
inv
â
sext
sext
inv
â
sext
sext
inv
inv
inv


s32
inv
inv
â
sext
sext
inv
inv
â
sext
inv
inv
â
sext
inv
inv
inv


s64
inv
inv
inv
â
sext
inv
inv
inv
â
inv
inv
inv
â
inv
inv
inv


u8
â
zext
zext
zext
zext
â
zext
zext
zext
â
zext
zext
zext
inv
inv
inv


u16
inv
â
zext
zext
zext
inv
â
zext
zext
inv
â
zext
zext
inv
inv
inv


u32
inv
inv
â
zext
zext
inv
inv
â
zext
inv
inv
â
zext
inv
inv
inv


u64
inv
inv
inv
â
zext
inv
inv
inv
â
inv
inv
inv
â
inv
inv
inv


f16
inv
â
zext
zext
zext
inv
inv
inv
inv
inv
inv
inv
inv
â
inv
inv


f32
inv
inv
â
zext
zext
inv
inv
inv
inv
inv
inv
inv
inv
inv
â
inv


f64
inv
inv
inv
â
zext
inv
inv
inv
inv
inv
inv
inv
inv
inv
inv
â


Notes

sext = sign-extend; zext = zero-extend; âââ = allowed, but no conversion needed;
inv = invalid, parse error.

Destination register size must be of equal or greater size than the instruction-type size.
Bit-size destination registers may be used with any appropriately-sized instruction type. The data
are sign-extended to the destination register width for signed integer instruction types, and are
zero-extended to the destination register width otherwise.
Integer destination registers may be used with any appropriately-sized bit-size or integer
instruction type. The data are sign-extended to the destination register width for signed integer
instruction types, and are zero-extended to the destination register width for bit-size an d
unsigned integer instruction types.
Floating-point destination registers can only be used with bit-size or floating-point instruction
types. When used with a narrower bit-size instruction type, the data are zero-extended. When used
with a floating-point instruction type, the size must match exactly.









9.5. Divergence of Threads in Control Constructsï

Threads in a CTA execute together, at least in appearance, until they come to a conditional control
construct such as a conditional branch, conditional function call, or conditional return. If threads
execute down different control flow paths, the threads are called divergent. If all of the threads
act in unison and follow a single control flow path, the threads are called uniform. Both
situations occur often in programs.
A CTA with divergent threads may have lower performance than a CTA with uniformly executing threads,
so it is important to have divergent threads re-converge as soon as possible. All control constructs
are assumed to be divergent points unless the control-flow instruction is marked as uniform, using
the .uni suffix. For divergent control flow, the optimizing code generator automatically
determines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore
the issue of divergent threads, but has the opportunity to improve performance by marking branch
points as uniform when the compiler or author can guarantee that the branch point is non-divergent.



9.6. Semanticsï

The goal of the semantic description of an instruction is to describe the results in all cases in as
simple language as possible. The semantics are described using C, until C is not expressive enough.


9.6.1. Machine-Specific Semantics of 16-bit Codeï

A PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a
32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit
computations are promoted to 32-bit computations. This can lead to computational differences
between code run on a 16-bit machine versus the same code run on a 32-bit machine, since the
promoted computation may have bits in the high-order half-word of registers that are not present in
16-bit physical registers. These extra precision bits can become visible at the application level,
for example, by a right-shift instruction.
At the PTX language level, one solution would be to define semantics for 16-bit code that is
consistent with execution on a 16-bit data path. This approach introduces a performance penalty for
16-bit code executing on a 32-bit data path, since the translated code would require many additional
masking instructions to suppress extra precision bits in the high-order half-word of 32-bit
registers.
Rather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of
16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce
portable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at
appropriate points in the program to guarantee portability of the code. However, for many
performance-critical applications, this is not desirable, and for many applications the difference
in execution is preferable to limiting performance.




9.7. Instructionsï

All PTX instructions may be predicated. In the following descriptions, the optional guard predicate
is omitted from the syntax.


9.7.1. Integer Arithmetic Instructionsï

Integer arithmetic instructions operate on the integer types in register and constant immediate
forms. The integer arithmetic instructions are:

add
sub
mul
mad
mul24
mad24
sad
div
rem
abs
neg
min
max
popc
clz
bfind
fns
brev
bfe
bfi
bmsk
szext
dp4a
dp2a



9.7.1.1. Integer Arithmetic Instructions: addï

add
Add two values.
Syntax

add.type       d, a, b;
add{.sat}.s32  d, a, b;     // .sat applies only to .s32

.type = { .u16, .u32, .u64,
          .s16, .s32, .s64,
          .u16x2, .s16x2 };


Description
Performs addition and writes the resulting value into a destination register.
For .u16x2, .s16x2 instruction types, forms input vectors by half word values from source
operands. Half-word operands are then added in parallel to produce .u16x2, .s16x2 result in
destination.
Operands d, a and b have type .type. For instruction types .u16x2, .s16x2,
operands d, a and b have type .b32.
Semantics

if (type == u16x2 || type == s16x2) {
    iA[0] = a[0:15];
    iA[1] = a[16:31];
    iB[0] = b[0:15];
    iB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = iA[i] + iB[i];
    }
} else {
    d = a + b;
}


Notes
Saturation modifier:

.sat

limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to
.s32 type.


PTX ISA Notes
Introduced in PTX ISA version 1.0.
add.u16x2 and add.s16x2 introduced in PTX ISA version 8.0.
Target ISA Notes
Supported on all target architectures.
add.u16x2 and add.s16x2 require sm_90 or higher.
Examples

@p  add.u32     x,y,z;
    add.sat.s32 c,c,1;
    add.u16x2   u,v,w;





9.7.1.2. Integer Arithmetic Instructions: subï

sub
Subtract one value from another.
Syntax

sub.type       d, a, b;
sub{.sat}.s32  d, a, b;     // .sat applies only to .s32

.type = { .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Performs subtraction and writes the resulting value into a destination register.
Semantics

d = a - b;


Notes
Saturation modifier:

.sat

limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to
.s32 type.


PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

sub.s32 c,a,b;





9.7.1.3. Integer Arithmetic Instructions: mulï

mul
Multiply two values.
Syntax

mul.mode.type  d, a, b;

.mode = { .hi, .lo, .wide };
.type = { .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Compute the product of two values.
Semantics

t = a * b;
n = bitwidth of type;
d = t;            // for .wide
d = t<2n-1..n>;   // for .hi variant
d = t<n-1..0>;    // for .lo variant


Notes
The type of the operation represents the types of the a and b operands. If .hi or
.lo is specified, then d is the same size as a and b, and either the upper or lower
half of the result is written to the destination register. If .wide is specified, then d is
twice as wide as a and b to receive the full result of the multiplication.
The .wide suffix is supported only for 16- and 32-bit integer types.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

mul.wide.s16 fa,fxs,fys;   // 16*16 bits yields 32 bits
mul.lo.s16 fa,fxs,fys;     // 16*16 bits, save only the low 16 bits
mul.wide.s32 z,x,y;        // 32*32 bits, creates 64 bit result





9.7.1.4. Integer Arithmetic Instructions: madï

mad
Multiply two values, optionally extract the high or low half of the intermediate result, and add a third value.
Syntax

mad.mode.type  d, a, b, c;
mad.hi.sat.s32 d, a, b, c;

.mode = { .hi, .lo, .wide };
.type = { .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Multiplies two values, optionally extracts the high or low half of the intermediate result, and adds
a third value. Writes the result into a destination register.
Semantics

t = a * b;
n = bitwidth of type;
d = t + c;           // for .wide
d = t<2n-1..n> + c;  // for .hi variant
d = t<n-1..0> + c;   // for .lo variant


Notes
The type of the operation represents the types of the a and b operands. If .hi or .lo is
specified, then d and c are the same size as a and b, and either the upper or lower
half of the result is written to the destination register. If .wide is specified, then d and
c are twice as wide as a and b to receive the result of the multiplication.
The .wide suffix is supported only for 16-bit and 32-bit integer types.
Saturation modifier:

.sat

limits result to MININT..MAXINT (no overflow) for the size of the operation.
Applies only to .s32 type in .hi mode.


PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

@p  mad.lo.s32 d,a,b,c;
    mad.lo.s32 r,p,q,r;





9.7.1.5. Integer Arithmetic Instructions: mul24ï

mul24
Multiply two 24-bit integer values.
Syntax

mul24.mode.type  d, a, b;

.mode = { .hi, .lo };
.type = { .u32, .s32 };


Description
Compute the product of two 24-bit integer values held in 32-bit source registers, and return either
the high or low 32-bits of the 48-bit result.
Semantics

t = a * b;
d = t<47..16>;    // for .hi variant
d = t<31..0>;     // for .lo variant


Notes
Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.
mul24.hi performs a 24x24-bit multiply and returns the high 32 bits of the 48-bit result.
mul24.lo performs a 24x24-bit multiply and returns the low 32 bits of the 48-bit result.
All operands are of the same type and size.
mul24.hi may be less efficient on machines without hardware support for 24-bit multiply.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

mul24.lo.s32 d,a,b;   // low 32-bits of 24x24-bit signed multiply.





9.7.1.6. Integer Arithmetic Instructions: mad24ï

mad24
Multiply two 24-bit integer values and add a third value.
Syntax

mad24.mode.type  d, a, b, c;
mad24.hi.sat.s32 d, a, b, c;

.mode = { .hi, .lo };
.type = { .u32, .s32 };


Description
Compute the product of two 24-bit integer values held in 32-bit source registers, and add a third,
32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low
32-bits of the 48-bit result.
Semantics

t = a * b;
d = t<47..16> + c;   // for .hi variant
d = t<31..0> + c;    // for .lo variant


Notes
Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits.
mad24.hi performs a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third
value.
mad24.lo performs a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third
value.
All operands are of the same type and size.
Saturation modifier:

.sat

limits result of 32-bit signed addition to MININT..MAXINT (no overflow). Applies only to
.s32 type in .hi mode.


mad24.hi may be less efficient on machines without hardware support for 24-bit multiply.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

mad24.lo.s32 d,a,b,c;   // low 32-bits of 24x24-bit signed multiply.





9.7.1.7. Integer Arithmetic Instructions: sadï

sad
Sum of absolute differences.
Syntax

sad.type  d, a, b, c;

.type = { .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Adds the absolute value of a-b to c and writes the resulting value into d.
Semantics

d = c + ((a<b) ? b-a : a-b);


PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

sad.s32  d,a,b,c;
sad.u32  d,a,b,d;  // running sum





9.7.1.8. Integer Arithmetic Instructions: divï

div
Divide one value by another.
Syntax

div.type  d, a, b;

.type = { .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Divides a by b, stores result in d.
Semantics

d = a / b;


Notes
Division by zero yields an unspecified, machine-specific value.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

div.s32  b,n,i;





9.7.1.9. Integer Arithmetic Instructions: remï

rem
The remainder of integer division.
Syntax

rem.type  d, a, b;

.type = { .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Divides a by b, store the remainder in d.
Semantics

d = a % b;


Notes
The behavior for negative numbers is machine-dependent and depends on whether divide rounds towards
zero or negative infinity.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

rem.s32  x,x,8;    // x = x%8;





9.7.1.10. Integer Arithmetic Instructions: absï

abs
Absolute value.
Syntax

abs.type  d, a;

.type = { .s16, .s32, .s64 };


Description
Take the absolute value of a and store it in d.
Semantics

d = |a|;


Notes
Only for signed integers.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

abs.s32  r0,a;





9.7.1.11. Integer Arithmetic Instructions: negï

neg
Arithmetic negate.
Syntax

neg.type  d, a;

.type = { .s16, .s32, .s64 };


Description
Negate the sign of a and store the result in d.
Semantics

d = -a;


Notes
Only for signed integers.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

neg.s32  r0,a;





9.7.1.12. Integer Arithmetic Instructions: minï

min
Find the minimum of two values.
Syntax

min.atype         d, a, b;
min{.relu}.btype  d, a, b;

.atype = { .u16, .u32, .u64,
           .u16x2, .s16, .s64 };
.btype = { .s16x2, .s32 };


Description
Store the minimum of a and b in d.
For .u16x2, .s16x2 instruction types, forms input vectors by half word values from source
operands. Half-word operands are then processed in parallel to produce .u16x2, .s16x2 result
in destination.
Operands d, a and b have the same type as the instruction type. For instruction types
.u16x2, .s16x2, operands d, a and b have type .b32.
Semantics

if (type == u16x2 || type == s16x2) {
    iA[0] = a[0:15];
    iA[1] = a[16:31];
    iB[0] = b[0:15];
    iB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = (iA[i] < iB[i]) ? iA[i] : iB[i];
    }
} else {
    d = (a < b) ? a : b; // Integer (signed and unsigned)
}


Notes
Signed and unsigned differ.

Saturation modifier:

min.relu.{s16x2, s32} clamps the result to 0 if negative.


PTX ISA Notes
Introduced in PTX ISA version 1.0.
min.u16x2, min{.relu}.s16x2 and min.relu.s32 introduced in PTX ISA version 8.0.
Target ISA Notes
Supported on all target architectures.
min.u16x2, min{.relu}.s16x2 and min.relu.s32 require sm_90 or higher.
Examples

    min.s32  r0,a,b;
@p  min.u16  h,i,j;
    min.s16x2.relu u,v,w;





9.7.1.13. Integer Arithmetic Instructions: maxï

max
Find the maximum of two values.
Syntax

max.atype         d, a, b;
max{.relu}.btype  d, a, b;

.atype = { .u16, .u32, .u64,
           .u16x2, .s16, .s64 };
.btype = { .s16x2, .s32 };


Description
Store the maximum of a and b in d.
For .u16x2, .s16x2 instruction types, forms input vectors by half word values from source
operands. Half-word operands are then processed in parallel to produce .u16x2, .s16x2 result
in destination.
Operands d, a and b have the same type as the instruction type. For instruction types
.u16x2, .s16x2, operands d, a and b have type .b32.
Semantics

if (type == u16x2 || type == s16x2) {
    iA[0] = a[0:15];
    iA[1] = a[16:31];
    iB[0] = b[0:15];
    iB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = (iA[i] > iB[i]) ? iA[i] : iB[i];
    }
} else {
    d = (a > b) ? a : b; // Integer (signed and unsigned)
}


Notes
Signed and unsigned differ.

Saturation modifier:

max.relu.{s16x2, s32} clamps the result to 0 if negative.


PTX ISA Notes
Introduced in PTX ISA version 1.0.
max.u16x2, max{.relu}.s16x2 and max.relu.s32 introduced in PTX ISA version 8.0.
Target ISA Notes
Supported on all target architectures.
max.u16x2, max{.relu}.s16x2 and max.relu.s32 require sm_90 or higher.
Examples

max.u32  d,a,b;
max.s32  q,q,0;
max.relu.s16x2 t,t,u;





9.7.1.14. Integer Arithmetic Instructions: popcï

popc
Population count.
Syntax

popc.type  d, a;

.type = { .b32, .b64 };


Description
Count the number of one bits in a and place the resulting population count in 32-bit
destination register d. Operand a has the instruction type and destination d has type
.u32.
Semantics

.u32  d = 0;
while (a != 0) {
   if (a & 0x1)  d++;
   a = a >> 1;
}


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
popc requires sm_20 or higher.
Examples

popc.b32  d, a;
popc.b64  cnt, X;  // cnt is .u32





9.7.1.15. Integer Arithmetic Instructions: clzï

clz
Count leading zeros.
Syntax

clz.type  d, a;

.type = { .b32, .b64 };


Description
Count the number of leading zeros in a starting with the most-significant bit and place the
result in 32-bit destination register d.Â Operand a has the instruction type, and destination
d has type .u32. For .b32 type, the number of leading zeros is between 0 and 32,
inclusively. For.b64 type, the number of leading zeros is between 0 and 64, inclusively.
Semantics

.u32  d = 0;
if (.type == .b32)   { max = 32; mask = 0x80000000; }
else                 { max = 64; mask = 0x8000000000000000; }

while (d < max && (a&mask == 0) ) {
    d++;
    a = a << 1;
}


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
clz requires sm_20 or higher.
Examples

clz.b32  d, a;
clz.b64  cnt, X;  // cnt is .u32





9.7.1.16. Integer Arithmetic Instructions: bfindï

bfind
Find most significant non-sign bit.
Syntax

bfind.type           d, a;
bfind.shiftamt.type  d, a;

.type = { .u32, .u64,
          .s32, .s64 };


Description
Find the bit position of the most significant non-sign bit in a and place the result in
d. Operand a has the instruction type, and destination d has type .u32. For unsigned
integers, bfind returns the bit position of the most significant 1. For signed integers,
bfind returns the bit position of the most significant 0 for negative inputs and the most
significant 1 for non-negative inputs.
If .shiftamt is specified, bfind returns the shift amount needed to left-shift the found bit
into the most-significant bit position.
bfind returns 0xffffffff if no non-sign bit is found.
Semantics

msb = (.type==.u32 || .type==.s32) ? 31 : 63;
// negate negative signed inputs
if ( (.type==.s32 || .type==.s64) && (a & (1<<msb)) ) {
    a = ~a;
}
.u32  d = 0xffffffff;
for (.s32 i=msb; i>=0; i--) {
    if (a & (1<<i))  { d = i; break; }
}
if (.shiftamt && d != 0xffffffff)  { d = msb - d; }


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
bfind requires sm_20 or higher.
Examples

bfind.u32  d, a;
bfind.shiftamt.s64  cnt, X;  // cnt is .u32





9.7.1.17. Integer Arithmetic Instructions: fnsï

fns
Find the n-th set bit
Syntax

fns.b32 d, mask, base, offset;


Description
Given a 32-bit value mask and an integer value base (between 0 and 31), find the n-th (given
by offset) set bit in mask from the base bit, and store the bit position in d. If not
found, store 0xffffffff in d.
Operand mask has a 32-bit type. Operand base has .b32, .u32 or .s32
type. Operand offset has .s32 type. Destination d has type .b32.
Operand base must be <= 31, otherwise behavior is undefined.
Semantics

d = 0xffffffff;
if (offset == 0) {
    if (mask[base] == 1) {
        d = base;
    }
} else {
    pos = base;
    count = |offset| - 1;
    inc = (offset > 0) ? 1 : -1;

    while ((pos >= 0) && (pos < 32)) {
        if (mask[pos] == 1) {
            if (count == 0) {
              d = pos;
              break;
           } else {
               count = count â 1;
           }
        }
        pos = pos + inc;
    }
}


PTX ISA Notes
Introduced in PTX ISA version 6.0.
Target ISA Notes
fns requires sm_30 or higher.
Examples

fns.b32 d, 0xaaaaaaaa, 3, 1;   // d = 3
fns.b32 d, 0xaaaaaaaa, 3, -1;  // d = 3
fns.b32 d, 0xaaaaaaaa, 2, 1;   // d = 3
fns.b32 d, 0xaaaaaaaa, 2, -1;  // d = 1





9.7.1.18. Integer Arithmetic Instructions: brevï

brev
Bit reverse.
Syntax

brev.type  d, a;

.type = { .b32, .b64 };


Description
Perform bitwise reversal of input.
Semantics

msb = (.type==.b32) ? 31 : 63;

for (i=0; i<=msb; i++) {
    d[i] = a[msb-i];
}


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
brev requires sm_20 or higher.
Examples

brev.b32  d, a;





9.7.1.19. Integer Arithmetic Instructions: bfeï

bfe
Bit Field Extract.
Syntax

bfe.type  d, a, b, c;

.type = { .u32, .u64,
          .s32, .s64 };


Description
Extract bit field from a and place the zero or sign-extended result in d. Source b gives
the bit field starting bit position, and source c gives the bit field length in bits.
Operands a and d have the same type as the instruction type. Operands b and c are
type .u32, but are restricted to the 8-bit value range 0..255.
The sign bit of the extracted field is defined as:


.u32, .u64:

zero


.s32, .s64:

msb of input a if the extracted field extends beyond the msb of a msb of extracted
field, otherwise


If the bit field length is zero, the result is zero.
The destination d is padded with the sign bit of the extracted field. If the start position is
beyond the msb of the input, the destination d is filled with the replicated sign bit of the
extracted field.
Semantics

msb = (.type==.u32 || .type==.s32) ? 31 : 63;
pos = b & 0xff;  // pos restricted to 0..255 range
len = c & 0xff;  // len restricted to 0..255 range

if (.type==.u32 || .type==.u64 || len==0)
    sbit = 0;
else
    sbit = a[min(pos+len-1,msb)];

d = 0;
for (i=0; i<=msb; i++) {
    d[i] = (i<len && pos+i<=msb) ? a[pos+i] : sbit;
}


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
bfe requires sm_20 or higher.
Examples

bfe.b32  d,a,start,len;





9.7.1.20. Integer Arithmetic Instructions: bfiï

bfi
Bit Field Insert.
Syntax

bfi.type  f, a, b, c, d;

.type = { .b32, .b64 };


Description
Align and insert a bit field from a into b, and place the result in f. Source c
gives the starting bit position for the insertion, and source d gives the bit field length in
bits.
Operands a, b, and f have the same type as the instruction type. Operands c and
d are type .u32, but are restricted to the 8-bit value range 0..255.
If the bit field length is zero, the result is b.
If the start position is beyond the msb of the input, the result is b.
Semantics

msb = (.type==.b32) ? 31 : 63;
pos = c & 0xff;  // pos restricted to 0..255 range
len = d & 0xff;  // len restricted to 0..255 range

f = b;
for (i=0; i<len && pos+i<=msb; i++) {
    f[pos+i] = a[i];
}


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
bfi requires sm_20 or higher.
Examples

bfi.b32  d,a,b,start,len;





9.7.1.21. Integer Arithmetic Instructions: szextï

szext
Sign-extend or Zero-extend.
Syntax

szext.mode.type  d, a, b;

.mode = { .clamp, .wrap };
.type = { .u32, .s32 };


Description
Sign-extends or zero-extends an N-bit value from operand a where N is specified in operand
b. The resulting value is stored in the destination operand d.
For the .s32 instruction type, the value in a is treated as an N-bit signed value and the
most significant bit of this N-bit value is replicated up to bit 31. For the .u32 instruction
type, the value in a is treated as an N-bit unsigned number and is zero-extended to 32
bits. Operand b is an unsigned 32-bit value.
If the value of N is 0, then the result of szext is 0. If the value of N is 32 or higher, then
the result of szext depends upon the value of the .mode qualifier as follows:

If .mode is .clamp, then the result is the same as the source operand a.
If .mode is .wrap, then the result is computed using the wrapped value of N.

Semantics

b1        = b & 0x1f;
too_large = (b >= 32 && .mode == .clamp) ? true : false;
mask      = too_large ? 0 : (~0) << b1;
sign_pos  = (b1 - 1) & 0x1f;

if (b1 == 0 || too_large || .type != .s32) {
    sign_bit = false;
} else {
    sign_bit = (a >> sign_pos) & 1;
}
d = (a & ~mask) | (sign_bit ? mask | 0);


PTX ISA Notes
Introduced in PTX ISA version 7.6.
Target ISA Notes
szext requires sm_70 or higher.
Examples

szext.clamp.s32 rd, ra, rb;
szext.wrap.u32  rd, 0xffffffff, 0; // Result is 0.





9.7.1.22. Integer Arithmetic Instructions: bmskï

bmsk
Bit Field Mask.
Syntax

bmsk.mode.b32  d, a, b;

.mode = { .clamp, .wrap };


Description
Generates a 32-bit mask starting from the bit position specified in operand a, and of the width
specified in operand b. The generated bitmask is stored in the destination operand d.
The resulting bitmask is 0 in the following cases:

When the value of a is 32 or higher and .mode is .clamp.
When either the specified value of b or the wrapped value of b (when .mode is
specified as .wrap) is 0.

Semantics

a1    = a & 0x1f;
mask0 = (~0) << a1;
b1    = b & 0x1f;
sum   = a1 + b1;
mask1 = (~0) << sum;

sum-overflow          = sum >= 32 ? true : false;
bit-position-overflow = false;
bit-width-overflow    = false;

if (.mode == .clamp) {
    if (a >= 32) {
        bit-position-overflow = true;
        mask0 = 0;
    }
    if (b >= 32) {
        bit-width-overflow = true;
    }
}

if (sum-overflow || bit-position-overflow || bit-width-overflow) {
    mask1 = 0;
} else if (b1 == 0) {
    mask1 = ~0;
}
d = mask0 & ~mask1;


Notes
The bitmask width specified by operand b is limited to range 0..32 in .clamp mode and to
range 0..31 in .wrap mode.
PTX ISA Notes
Introduced in PTX ISA version 7.6.
Target ISA Notes
bmsk requires sm_70 or higher.
Examples

bmsk.clamp.b32  rd, ra, rb;
bmsk.wrap.b32   rd, 1, 2; // Creates a bitmask of 0x00000006.





9.7.1.23. Integer Arithmetic Instructions: dp4aï

dp4a
Four-way byte dot product-accumulate.
Syntax

dp4a.atype.btype  d, a, b, c;

.atype = .btype = { .u32, .s32 };


Description
Four-way byte dot product which is accumulated in 32-bit result.
Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product.
Operand c has type .u32 if both .atype and .btype are .u32 else operand c
has type .s32.
Semantics

d = c;

// Extract 4 bytes from a 32bit input and sign or zero extend
// based on input type.
Va = extractAndSignOrZeroExt_4(a, .atype);
Vb = extractAndSignOrZeroExt_4(b, .btype);

for (i = 0; i < 4; ++i) {
    d += Va[i] * Vb[i];
}


PTX ISA Notes
Introduced in PTX ISA version 5.0.
Target ISA Notes
Requires sm_61 or higher.
Examples

dp4a.u32.u32           d0, a0, b0, c0;
dp4a.u32.s32           d1, a1, b1, c1;





9.7.1.24. Integer Arithmetic Instructions: dp2aï

dp2a
Two-way dot product-accumulate.
Syntax

dp2a.mode.atype.btype  d, a, b, c;

.atype = .btype = { .u32, .s32 };
.mode = { .lo, .hi };


Description
Two-way 16-bit to 8-bit dot product which is accumulated in 32-bit result.
Operand a and b are 32-bit inputs. Operand a holds two 16-bits inputs in packed form and
operand b holds 4 byte inputs in packed form for dot product.
Depending on the .mode specified, either lower half or upper half of operand b will be used
for dot product.
Operand c has type .u32 if both .atype and .btype are .u32 else operand c
has type .s32.
Semantics

d = c;
// Extract two 16-bit values from a 32-bit input and sign or zero extend
// based on input type.
Va = extractAndSignOrZeroExt_2(a, .atype);

// Extract four 8-bit values from a 32-bit input and sign or zer extend
// based on input type.
Vb = extractAndSignOrZeroExt_4(b, .btype);

b_select = (.mode == .lo) ? 0 : 2;

for (i = 0; i < 2; ++i) {
    d += Va[i] * Vb[b_select + i];
}


PTX ISA Notes
Introduced in PTX ISA version 5.0.
Target ISA Notes
Requires sm_61 or higher.
Examples

dp2a.lo.u32.u32           d0, a0, b0, c0;
dp2a.hi.u32.s32           d1, a1, b1, c1;






9.7.2. Extended-Precision Integer Arithmetic Instructionsï

Instructions add.cc, addc, sub.cc, subc, mad.cc and madc reference an
implicitly specified condition code register (CC) having a single carry flag bit (CC.CF)
holding carry-in/carry-out or borrow-in/borrow-out. These instructions support extended-precision
integer addition, subtraction, and multiplication. No other instructions access the condition code,
and there is no support for setting, clearing, or testing the condition code. The condition code
register is not preserved across calls and is mainly intended for use in straight-line code
sequences for computing extended-precision integer addition, subtraction, and multiplication.
The extended-precision arithmetic instructions are:

add.cc, addc
sub.cc, subc
mad.cc, madc



9.7.2.1. Extended-Precision Arithmetic Instructions: add.ccï

add.cc
Add two values with carry-out.
Syntax

add.cc.type  d, a, b;

.type = { .u32, .s32, .u64, .s64 };


Description
Performs integer addition and writes the carry-out value into the condition code register.
Semantics

d = a + b;


carry-out written to CC.CF
Notes
No integer rounding modifiers.
No saturation.
Behavior is the same for unsigned and signed integers.
PTX ISA Notes
32-bit add.cc introduced in PTX ISA version 1.2.
64-bit add.cc introduced in PTX ISA version 4.3.
Target ISA Notes
32-bit add.cc is supported on all target architectures.
64-bit add.cc requires sm_20 or higher.
Examples

@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of
@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values
@p  addc.cc.u32  x3,y3,z3;
@p  addc.u32     x4,y4,z4;





9.7.2.2. Extended-Precision Arithmetic Instructions: addcï

addc
Add two values with carry-in and optional carry-out.
Syntax

addc{.cc}.type  d, a, b;

.type = { .u32, .s32, .u64, .s64 };


Description
Performs integer addition with carry-in and optionally writes the carry-out value into the condition
code register.
Semantics

d = a + b + CC.CF;


if .cc specified, carry-out written to CC.CF
Notes
No integer rounding modifiers.
No saturation.
Behavior is the same for unsigned and signed integers.
PTX ISA Notes
32-bit addc introduced in PTX ISA version 1.2.
64-bit addc introduced in PTX ISA version 4.3.
Target ISA Notes
32-bit addc is supported on all target architectures.
64-bit addc requires sm_20 or higher.
Examples

@p  add.cc.u32   x1,y1,z1;   // extended-precision addition of
@p  addc.cc.u32  x2,y2,z2;   // two 128-bit values
@p  addc.cc.u32  x3,y3,z3;
@p  addc.u32     x4,y4,z4;





9.7.2.3. Extended-Precision Arithmetic Instructions: sub.ccï

sub.cc
Subtract one value from another, with borrow-out.
Syntax

sub.cc.type  d, a, b;

.type = { .u32, .s32, .u64, .s64 };


Description
Performs integer subtraction and writes the borrow-out value into the condition code register.
Semantics

d = a - b;


borrow-out written to CC.CF
Notes
No integer rounding modifiers.
No saturation.
Behavior is the same for unsigned and signed integers.
PTX ISA Notes
32-bit sub.cc introduced in PTX ISA version 1.2.
64-bit sub.cc introduced in PTX ISA version 4.3.
Target ISA Notes
32-bit sub.cc is supported on all target architectures.
64-bit sub.cc requires sm_20 or higher.
Examples

@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction
@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values
@p  subc.cc.u32  x3,y3,z3;
@p  subc.u32     x4,y4,z4;





9.7.2.4. Extended-Precision Arithmetic Instructions: subcï

subc
Subtract one value from another, with borrow-in and optional borrow-out.
Syntax

subc{.cc}.type  d, a, b;

.type = { .u32, .s32, .u64, .s64 };


Description
Performs integer subtraction with borrow-in and optionally writes the borrow-out value into the
condition code register.
Semantics

d = a  - (b + CC.CF);


if .cc specified, borrow-out written to CC.CF
Notes
No integer rounding modifiers.
No saturation.
Behavior is the same for unsigned and signed integers.
PTX ISA Notes
32-bit subc introduced in PTX ISA version 1.2.
64-bit subc introduced in PTX ISA version 4.3.
Target ISA Notes
32-bit subc is supported on all target architectures.
64-bit subc requires sm_20 or higher.
Examples

@p  sub.cc.u32   x1,y1,z1;   // extended-precision subtraction
@p  subc.cc.u32  x2,y2,z2;   // of two 128-bit values
@p  subc.cc.u32  x3,y3,z3;
@p  subc.u32     x4,y4,z4;





9.7.2.5. Extended-Precision Arithmetic Instructions: mad.ccï

mad.cc
Multiply two values, extract high or low half of result, and add a third value with carry-out.
Syntax

mad{.hi,.lo}.cc.type  d, a, b, c;

.type = { .u32, .s32, .u64, .s64 };


Description
Multiplies two values, extracts either the high or low part of the result, and adds a third
value. Writes the result to the destination register and the carry-out from the addition into the
condition code register.
Semantics

t = a * b;
d = t<63..32> + c;    // for .hi variant
d = t<31..0> + c;     // for .lo variant


carry-out from addition is written to CC.CF
Notes
Generally used in combination with madc and addc to implement extended-precision multi-word
multiplication. See madc for an example.
PTX ISA Notes
32-bit mad.cc introduced in PTX ISA version 3.0.
64-bit mad.cc introduced in PTX ISA version 4.3.
Target ISA Notes
Requires target sm_20 or higher.
Examples

@p  mad.lo.cc.u32 d,a,b,c;
    mad.lo.cc.u32 r,p,q,r;





9.7.2.6. Extended-Precision Arithmetic Instructions: madcï

madc
Multiply two values, extract high or low half of result, and add a third value with carry-in and
optional carry-out.
Syntax

madc{.hi,.lo}{.cc}.type  d, a, b, c;

.type = { .u32, .s32, .u64, .s64 };


Description
Multiplies two values, extracts either the high or low part of the result, and adds a third value
along with carry-in. Writes the result to the destination register and optionally writes the
carry-out from the addition into the condition code register.
Semantics

t = a * b;
d = t<63..32> + c + CC.CF;     // for .hi variant
d = t<31..0> + c + CC.CF;      // for .lo variant


if .cc specified, carry-out from addition is written to CC.CF
Notes
Generally used in combination with mad.cc and addc to implement extended-precision
multi-word multiplication. See example below.
PTX ISA Notes
32-bit madc introduced in PTX ISA version 3.0.
64-bit madc introduced in PTX ISA version 4.3.
Target ISA Notes
Requires target sm_20 or higher.
Examples

// extended-precision multiply:  [r3,r2,r1,r0] = [r5,r4] * [r7,r6]
mul.lo.u32     r0,r4,r6;      // r0=(r4*r6).[31:0], no carry-out
mul.hi.u32     r1,r4,r6;      // r1=(r4*r6).[63:32], no carry-out
mad.lo.cc.u32  r1,r5,r6,r1;   // r1+=(r5*r6).[31:0], may carry-out
madc.hi.u32    r2,r5,r6,0;    // r2 =(r5*r6).[63:32]+carry-in,
                              // no carry-out
mad.lo.cc.u32   r1,r4,r7,r1;  // r1+=(r4*r7).[31:0], may carry-out
madc.hi.cc.u32  r2,r4,r7,r2;  // r2+=(r4*r7).[63:32]+carry-in,
                              // may carry-out
addc.u32        r3,0,0;       // r3 = carry-in, no carry-out
mad.lo.cc.u32   r2,r5,r7,r2;  // r2+=(r5*r7).[31:0], may carry-out
madc.hi.u32     r3,r5,r7,r3;  // r3+=(r5*r7).[63:32]+carry-in






9.7.3. Floating-Point Instructionsï

Floating-point instructions operate on .f32 and .f64 register operands and constant
immediate values. The floating-point instructions are:

testp
copysign
add
sub
mul
fma
mad
div
abs
neg
min
max
rcp
sqrt
rsqrt
sin
cos
lg2
ex2
tanh

Instructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions
support subnormal inputs and results. Single-precision instructions support subnormal inputs and
results by default for sm_20 and subsequent targets, and flush subnormal inputs and results to
sign-preserving zero for sm_1x targets. The optional .ftz modifier on single-precision
instructions provides backward compatibility with sm_1x targets by flushing subnormal inputs and
results to sign-preserving zero regardless of the target architecture.
Single-precision add, sub, mul, and mad support saturation of results to the range
[0.0, 1.0], with NaNs being flushed to positive zero. NaN payloads are supported for
double-precision instructions (except for rcp.approx.ftz.f64 and rsqrt.approx.ftz.f64, which
maps input NaNs to a canonical NaN). Single-precision instructions return an unspecified
NaN. Note that future implementations may support NaN payloads for single-precision
instructions, so PTX programs should not rely on the specific single-precision NaNs being
generated.
Table 26 summarizes
floating-point instructions in PTX.


Table 26 Summary of Floating-Point Instructionsï













Instruction
.rn
.rz
.rm
.rp
.ftz
.sat
Notes




{add,sub,mul}.rnd.f32
x
x
x
x
x
x
If no rounding modifier is specified,
default is .rn and instructions may
be folded into a multiply-add.


{add,sub,mul}.rnd.f64
x
x
x
x
n/a
n/a
If no rounding modifier is specified,
default is .rn and instructions may
be folded into a multiply-add.


mad.f32
n/a
n/a
n/a
n/a
x
x

.target sm_1x
No rounding modifier.



{mad,fma}.rnd.f32
x
x
x
x
x
x

.target sm_20 or higher
mad.f32 and fma.f32 are the same.



{mad,fma}.rnd.f64
x
x
x
x
n/a
n/a
mad.f64 and fma.f64 are the same.


div.full.f32
n/a
n/a
n/a
n/a
x
n/a
No rounding modifier.


{div,rcp,sqrt}.approx.f32
n/a
n/a
n/a
n/a
x
n/a
n/a


rcp.approx.ftz.f64
n/a
n/a
n/a
n/a
x
n/a
.target sm_20 or higher


{div,rcp,sqrt}.rnd.f32
x
x
x
x
x
n/a
.target sm_20 or higher


{div,rcp,sqrt}.rnd.f64
x
x
x
x
n/a
n/a
.target sm_20 or higher


{abs,neg,min,max}.f32
n/a
n/a
n/a
n/a
x
n/a



{abs,neg,min,max}.f64
n/a
n/a
n/a
n/a
n/a
n/a



rsqrt.approx.f32
n/a
n/a
n/a
n/a
x
n/a



rsqrt.approx.f64
n/a
n/a
n/a
n/a
n/a
n/a



rsqrt.approx.ftz.f64
n/a
n/a
n/a
n/a
x
n/a
.target sm_20 or higher


{sin,cos,lg2,ex2}.approx.f32
n/a
n/a
n/a
n/a
x
n/a



tanh.approx.f32
n/a
n/a
n/a
n/a
n/a
n/a
.target sm_75 or higher





9.7.3.1. Floating Point Instructions: testpï

testp
Test floating-point property.
Syntax

testp.op.type  p, a;  // result is .pred

.op   = { .finite, .infinite,
          .number, .notanumber,
          .normal, .subnormal };
.type = { .f32, .f64 };


Description
testp tests common properties of floating-point numbers and returns a predicate value of 1
if True and 0 if False.

testp.finite

True if the input is not infinite or NaN

testp.infinite

True if the input is positive or negative infinity

testp.number

True if the input is not NaN

testp.notanumber

True if the input is NaN

testp.normal

True if the input is a normal number (not NaN, not infinity)

testp.subnormal

True if the input is a subnormal number (not NaN, not infinity)


As a special case, positive and negative zero are considered normal numbers.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
Requires sm_20 or higher.
Examples

testp.notanumber.f32  isnan, f0;
testp.infinite.f64    p, X;





9.7.3.2. Floating Point Instructions: copysignï

copysign
Copy sign of one input to another.
Syntax

copysign.type  d, a, b;

.type = { .f32, .f64 };


Description
Copy sign bit of a into value of b, and return the result as d.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
Requires sm_20 or higher.
Examples

copysign.f32  x, y, z;
copysign.f64  A, B, C;





9.7.3.3. Floating Point Instructions: addï

add
Add two values.
Syntax

add{.rnd}{.ftz}{.sat}.f32  d, a, b;
add{.rnd}.f64              d, a, b;

.rnd = { .rn, .rz, .rm, .rp };


Description
Performs addition and writes the resulting value into a destination register.
Semantics

d = a + b;


Notes
Rounding modifiers:

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


The default value of rounding modifier is .rn. Note that an add instruction with an explicit
rounding modifier is treated conservatively by the code optimizer. An add instruction with no
rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code
optimizer. In particular, mul/add sequences with no rounding modifiers may be optimized to
use fused-multiply-add instructions on the target device.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
add.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

add.f64 supports subnormal numbers.
add.f32 flushes subnormal inputs and results to sign-preserving zero.


Saturation modifier:
add.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
add.f32 supported on all target architectures.
add.f64 requires sm_13 or higher.
Rounding modifiers have the following target requirements:


.rn, .rz


available for all targets


.rm, .rp


for add.f64, requires sm_13 or higher.
for add.f32, requires sm_20 or higher.


Examples

@p  add.rz.ftz.f32  f1,f2,f3;





9.7.3.4. Floating Point Instructions: subï

sub
Subtract one value from another.
Syntax

sub{.rnd}{.ftz}{.sat}.f32  d, a, b;
sub{.rnd}.f64              d, a, b;

.rnd = { .rn, .rz, .rm, .rp };


Description
Performs subtraction and writes the resulting value into a destination register.
Semantics

d = a - b;


Notes
Rounding modifiers:

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


The default value of rounding modifier is .rn. Note that a sub instruction with an explicit
rounding modifier is treated conservatively by the code optimizer. A sub instruction with no
rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code
optimizer. In particular, mul/sub sequences with no rounding modifiers may be optimized to
use fused-multiply-add instructions on the target device.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
sub.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

sub.f64 supports subnormal numbers.
sub.f32 flushes subnormal inputs and results to sign-preserving zero.


Saturation modifier:
sub.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
sub.f32 supported on all target architectures.
sub.f64 requires sm_13 or higher.
Rounding modifiers have the following target requirements:


.rn, .rz


available for all targets


.rm, .rp


for sub.f64, requires sm_13 or higher.
for sub.f32, requires sm_20 or higher.


Examples

sub.f32 c,a,b;
sub.rn.ftz.f32  f1,f2,f3;





9.7.3.5. Floating Point Instructions: mulï

mul
Multiply two values.
Syntax

mul{.rnd}{.ftz}{.sat}.f32  d, a, b;
mul{.rnd}.f64              d, a, b;

.rnd = { .rn, .rz, .rm, .rp };


Description
Compute the product of two values.
Semantics

d = a * b;


Notes
For floating-point multiplication, all operands must be the same size.
Rounding modifiers:

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


The default value of rounding modifier is .rn. Note that a mul instruction with an explicit
rounding modifier is treated conservatively by the code optimizer. A mul instruction with no
rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code
optimizer. In particular, mul/add and mul/sub sequences with no rounding modifiers may be
optimized to use fused-multiply-add instructions on the target device.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
mul.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

mul.f64 supports subnormal numbers.
mul.f32 flushes subnormal inputs and results to sign-preserving zero.


Saturation modifier:
mul.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
mul.f32 supported on all target architectures.
mul.f64 requires sm_13 or higher.
Rounding modifiers have the following target requirements:


.rn, .rz


available for all targets


.rm, .rp


for mul.f64, requires sm_13 or higher.
for mul.f32, requires sm_20 or higher.


Examples

mul.ftz.f32 circumf,radius,pi  // a single-precision multiply





9.7.3.6. Floating Point Instructions: fmaï

fma
Fused multiply-add.
Syntax

fma.rnd{.ftz}{.sat}.f32  d, a, b, c;
fma.rnd.f64              d, a, b, c;

.rnd = { .rn, .rz, .rm, .rp };


Description
Performs a fused multiply-add with no loss of precision in the intermediate product and addition.
Semantics

d = a*b + c;


Notes
fma.f32 computes the product of a and b to infinite precision and then adds c to
this product, again in infinite precision. The resulting value is then rounded to single precision
using the rounding mode specified by .rnd.
fma.f64 computes the product of a and b to infinite precision and then adds c to
this product, again in infinite precision. The resulting value is then rounded to double precision
using the rounding mode specified by .rnd.
fma.f64 is the same as mad.f64.
Rounding modifiers (no default):

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
fma.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

fma.f64 supports subnormal numbers.
fma.f32 is unimplemented for sm_1x targets.


Saturation:
fma.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.
PTX ISA Notes
fma.f64 introduced in PTX ISA version 1.4.
fma.f32 introduced in PTX ISA version 2.0.
Target ISA Notes
fma.f32 requires sm_20 or higher.
fma.f64 requires sm_13 or higher.
Examples

    fma.rn.ftz.f32  w,x,y,z;
@p  fma.rn.f64      d,a,b,c;





9.7.3.7. Floating Point Instructions: madï

mad
Multiply two values and add a third value.
Syntax

mad{.ftz}{.sat}.f32      d, a, b, c;    // .target sm_1x
mad.rnd{.ftz}{.sat}.f32  d, a, b, c;    // .target sm_20
mad.rnd.f64              d, a, b, c;    // .target sm_13 and higher

.rnd = { .rn, .rz, .rm, .rp };


Description
Multiplies two values and adds a third, and then writes the resulting value into a destination
register.
Semantics

d = a*b + c;


Notes
For .target sm_20 and higher:

mad.f32 computes the product of a and b to infinite precision and then adds c to
this product, again in infinite precision. The resulting value is then rounded to single precision
using the rounding mode specified by .rnd.
mad.f64 computes the product of a and b to infinite precision and then adds c to
this product, again in infinite precision. The resulting value is then rounded to double precision
using the rounding mode specified by .rnd.
mad.{f32,f64} is the same as fma.{f32,f64}.

For .target sm_1x:

mad.f32 computes the product of a and b at double precision, and then the mantissa is
truncated to 23 bits, but the exponent is preserved. Note that this is different from computing
the product with mul, where the mantissa can be rounded and the exponent will be clamped. The
exception for mad.f32 is when c = +/-0.0, mad.f32 is identical to the result computed
using separate mul and add instructions. When JIT-compiled for SM 2.0 devices, mad.f32 is
implemented as a fused multiply-add (i.e., fma.rn.ftz.f32). In this case, mad.f32 can
produce slightly different numeric results and backward compatibility is not guaranteed in this
case.
mad.f64 computes the product of a and b to infinite precision and then adds c to
this product, again in infinite precision. The resulting value is then rounded to double precision
using the rounding mode specified by .rnd. Unlike mad.f32, the treatment of subnormal
inputs and output follows IEEE 754 standard.
mad.f64 is the same as fma.f64.

Rounding modifiers (no default):

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
mad.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

mad.f64 supports subnormal numbers.
mad.f32 flushes subnormal inputs and results to sign-preserving zero.


Saturation modifier:
mad.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
In PTX ISA versions 1.4 and later, a rounding modifier is required for mad.f64.
Legacy mad.f64 instructions having no rounding modifier will map to mad.rn.f64.
In PTX ISA versions 2.0 and later, a rounding modifier is required for mad.f32 for sm_20 and higher targets.
Errata
mad.f32 requires a rounding modifier for sm_20 and higher targets. However for PTX ISA
version 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently defaults
to mad.rn.f32. For PTX ISA version 3.1, ptxas generates a warning and defaults to
mad.rn.f32, and in subsequent releases ptxas will enforce the requirement for PTX ISA version
3.2 and later.
Target ISA Notes
mad.f32 supported on all target architectures.
mad.f64 requires sm_13 or higher.
Rounding modifiers have the following target requirements:

.rn,.rz,.rm,.rp for mad.f64, requires sm_13 or higher.
.rn,.rz,.rm,.rp for mad.f32, requires sm_20 or higher.

Examples

@p  mad.f32  d,a,b,c;





9.7.3.8. Floating Point Instructions: divï

div
Divide one value by another.
Syntax

div.approx{.ftz}.f32  d, a, b;  // fast, approximate divide
div.full{.ftz}.f32    d, a, b;  // full-range approximate divide
div.rnd{.ftz}.f32     d, a, b;  // IEEE 754 compliant rounding
div.rnd.f64           d, a, b;  // IEEE 754 compliant rounding

.rnd = { .rn, .rz, .rm, .rp };


Description
Divides a by b, stores result in d.
Semantics

d = a / b;


Notes
Fast, approximate single-precision divides:

div.approx.f32 implements a fast approximation to divide, computed as d = a * (1/b). For
|b| in [2-126, 2126], the maximum ulp error is 2. For 2126 <
|b| < 2128, if a is infinity, div.approx.f32 returns NaN, otherwise it
returns 0.
div.full.f32 implements a relatively fast, full-range approximation that scales operands to
achieve better accuracy, but is not fully IEEE 754 compliant and does not support rounding
modifiers. The maximum ulp error is 2 across the full range of inputs.
Subnormal inputs and results are flushed to sign-preserving zero. Fast, approximate division by
zero creates a value of infinity (with same sign as a).

Divide with IEEE 754 compliant rounding:
Rounding modifiers (no default):

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
div.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

div.f64 supports subnormal numbers.
div.f32 flushes subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
div.f32 and div.f64 introduced in PTX ISA version 1.0.
Explicit modifiers .approx, .full, .ftz, and rounding introduced in PTX ISA version 1.4.
For PTX ISA version 1.4 and later, one of .approx, .full, or .rnd is required.
For PTX ISA versions 1.0 through 1.3, div.f32 defaults to div.approx.ftz.f32, and
div.f64 defaults to div.rn.f64.
Target ISA Notes
div.approx.f32 and div.full.f32 supported on all target architectures.
div.rnd.f32 requires sm_20 or higher.
div.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32.
div.{rz,rm,rp}.f64 requires sm_20 or higher.
Examples

div.approx.ftz.f32  diam,circum,3.14159;
div.full.ftz.f32    x, y, z;
div.rn.f64          xd, yd, zd;





9.7.3.9. Floating Point Instructions: absï

abs
Absolute value.
Syntax

abs{.ftz}.f32  d, a;
abs.f64        d, a;


Description
Take the absolute value of a and store the result in d.
Semantics

d = |a|;


Notes
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
abs.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

abs.f64 supports subnormal numbers.
abs.f32 flushes subnormal inputs and results to sign-preserving zero.


For abs.f32, NaN input yields unspecified NaN. For abs.f64, NaN input is passed
through unchanged. Future implementations may comply with the IEEE 754 standard by preserving
payload and modifying only the sign bit.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
abs.f32 supported on all target architectures.
abs.f64 requires sm_13 or higher.
Examples

abs.ftz.f32  x,f0;





9.7.3.10. Floating Point Instructions: negï

neg
Arithmetic negate.
Syntax

neg{.ftz}.f32  d, a;
neg.f64        d, a;


Description
Negate the sign of a and store the result in d.
Semantics

d = -a;


Notes
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
neg.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

neg.f64 supports subnormal numbers.
neg.f32 flushes subnormal inputs and results to sign-preserving zero.


NaN inputs yield an unspecified NaN. Future implementations may comply with the IEEE 754
standard by preserving payload and modifying only the sign bit.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
neg.f32 supported on all target architectures.
neg.f64 requires sm_13 or higher.
Examples

neg.ftz.f32  x,f0;





9.7.3.11. Floating Point Instructions: minï

min
Find the minimum of two values.
Syntax

min{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;
min.f64                            d, a, b;


Description
Store the minimum of a and b in d.
If .NaN modifier is specified, then the result is canonical NaN if either of the inputs is
NaN.
If .abs modifier is specified, the magnitude of destination operand d is the minimum of
absolute values of both the input arguments.
If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the
sign bits of both the inputs.
Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign
bit of both inputs before applying .abs operation.
If the result of min is NaN then the .xorsign and .abs modifiers will be ignored.
Semantics

if (.xorsign) {
    xorsign = getSignBit(a) ^ getSignBit(b);
    if (.abs) {
        a = |a|;
        b = |b|;
   }
}
if (isNaN(a) && isNaN(b))                 d = NaN;
else if (.NaN && (isNaN(a) || isNaN(b)))  d = NaN;
else if (isNaN(a))                        d = b;
else if (isNaN(b))                        d = a;
else                                      d = (a < b) ? a : b;
if (.xorsign && !isNaN(d)) {
    setSignBit(d, xorsign);
}


Notes
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
min.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

min.f64 supports subnormal numbers.
min.f32 flushes subnormal inputs and results to sign-preserving zero.


If values of both inputs are 0.0, then +0.0 > -0.0.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
min.NaNintroduced in PTX ISA version 7.0.
min.xorsign.abs introduced in PTX ISA version 7.2.
Target ISA Notes
min.f32 supported on all target architectures.
min.f64 requires sm_13 or higher.
min.NaNrequires sm_80 or higher.
min.xorsign.abs requires sm_86 or higher.
Examples

@p  min.ftz.f32  z,z,x;
    min.f64      a,b,c;
    // fp32 min with .NaN
    min.NaN.f32  f0,f1,f2;
    // fp32 min with .xorsign.abs
    min.xorsign.abs.f32 Rd, Ra, Rb;





9.7.3.12. Floating Point Instructions: maxï

max
Find the maximum of two values.
Syntax

max{.ftz}{.NaN}{.xorsign.abs}.f32  d, a, b;
max.f64                            d, a, b;


Description
Store the maximum of a and b in d.
If .NaN modifier is specified, the result is canonical NaN if either of the inputs is
NaN.
If .abs modifier is specified, the magnitude of destination operand d is the maximum of
absolute values of both the input arguments.
If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the
sign bits of both the inputs.
Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign
bit of both inputs before applying .abs operation.
If the result of max is NaN then the .xorsign and .abs modifiers will be ignored.
Semantics

if (.xorsign) {
    xorsign = getSignBit(a) ^ getSignBit(b);
    if (.abs) {
        a = |a|;
        b = |b|;
    }
}
if (isNaN(a) && isNaN(b))                 d = NaN;
else if (.NaN && (isNaN(a) || isNaN(b)))  d = NaN;
else if (isNaN(a))                        d = b;
else if (isNaN(b))                        d = a;
else                                      d = (a > b) ? a : b;
if (.xorsign && !isNaN(d)) {
    setSignBit(d, xorsign);
}


Notes
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
max.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

max.f64 supports subnormal numbers.
max.f32 flushes subnormal inputs and results to sign-preserving zero.


If values of both inputs are 0.0, then +0.0 > -0.0.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
max.NaNintroduced in PTX ISA version 7.0.
max.xorsign.abs introduced in PTX ISA version 7.2.
Target ISA Notes
max.f32 supported on all target architectures.
max.f64 requires sm_13 or higher.
max.NaNrequires sm_80 or higher.
max.xorsign.abs requires sm_86 or higher.
Examples

max.ftz.f32  f0,f1,f2;
max.f64      a,b,c;
// fp32 max with .NaN
max.NaN.f32  f0,f1,f2;
// fp32 max with .xorsign.abs
max.xorsign.abs.f32 Rd, Ra, Rb;





9.7.3.13. Floating Point Instructions: rcpï

rcp
Take the reciprocal of a value.
Syntax

rcp.approx{.ftz}.f32  d, a;  // fast, approximate reciprocal
rcp.rnd{.ftz}.f32     d, a;  // IEEE 754 compliant rounding
rcp.rnd.f64           d, a;  // IEEE 754 compliant rounding

.rnd = { .rn, .rz, .rm, .rp };


Description
Compute 1/a, store result in d.
Semantics

d = 1 / a;


Notes
Fast, approximate single-precision reciprocal:
rcp.approx.f32 implements a fast approximation to reciprocal. The maximum absolute error is 2-23.0 over the range 1.0-2.0.







Input
Result




-Inf
-0.0


-subnormal
-Inf


-0.0
-Inf


+0.0
+Inf


+subnormal
+Inf


+Inf
+0.0


NaN
NaN



Reciprocal with IEEE 754 compliant rounding:
Rounding modifiers (no default):

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
rcp.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

rcp.f64 supports subnormal numbers.
rcp.f32 flushes subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
rcp.f32 and rcp.f64 introduced in PTX ISA version 1.0. rcp.rn.f64 and explicit modifiers
.approx and .ftz were introduced in PTX ISA version 1.4. General rounding modifiers were
added in PTX ISA version 2.0.
For PTX ISA version 1.4 and later, one of .approx or .rnd is required.
For PTX ISA versions 1.0 through 1.3, rcp.f32 defaults to rcp.approx.ftz.f32, and
rcp.f64 defaults to rcp.rn.f64.
Target ISA Notes
rcp.approx.f32 supported on all target architectures.
rcp.rnd.f32 requires sm_20 or higher.
rcp.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32.
rcp.{rz,rm,rp}.f64 requires sm_20 or higher.
Examples

rcp.approx.ftz.f32  ri,r;
rcp.rn.ftz.f32      xi,x;
rcp.rn.f64          xi,x;





9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64ï

rcp.approx.ftz.f64
Compute a fast, gross approximation to the reciprocal of a value.
Syntax

rcp.approx.ftz.f64  d, a;


Description
Compute a fast, gross approximation to the reciprocal as follows:

extract the most-significant 32 bits of .f64 operand a in 1.11.20 IEEE floating-point
format (i.e., ignore the least-significant 32 bits of a),
compute an approximate .f64 reciprocal of this value using the most-significant 20 bits of
the mantissa of operand a,
place the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits
of destination d,and
zero the least significant 32 mantissa bits of .f64 destination d.

Semantics

tmp = a[63:32]; // upper word of a, 1.11.20 format
d[63:32] = 1.0 / tmp;
d[31:0] = 0x00000000;


Notes
rcp.approx.ftz.f64 implements a fast, gross approximation to reciprocal.







Input a[63:32]
Result d[63:32]




-Inf
-0.0


-subnormal
-Inf


-0.0
-Inf


+0.0
+Inf


+subnormal
+Inf


+Inf
+0.0


NaN
NaN



Input NaNs map to a canonical NaN with encoding 0x7fffffff00000000.
Subnormal inputs and results are flushed to sign-preserving zero.
PTX ISA Notes
rcp.approx.ftz.f64 introduced in PTX ISA version 2.1.
Target ISA Notes
rcp.approx.ftz.f64 requires sm_20 or higher.
Examples

rcp.ftz.f64  xi,x;





9.7.3.15. Floating Point Instructions: sqrtï

sqrt
Take the square root of a value.
Syntax

sqrt.approx{.ftz}.f32  d, a; // fast, approximate square root
sqrt.rnd{.ftz}.f32     d, a; // IEEE 754 compliant rounding
sqrt.rnd.f64           d, a; // IEEE 754 compliant rounding

.rnd = { .rn, .rz, .rm, .rp };


Description
Compute sqrt(a) and store the result in d.
Semantics

d = sqrt(a);


Notes
sqrt.approx.f32 implements a fast approximation to square root.







Input
Result




-Inf
NaN


-normal
NaN


-subnormal
-0.0


-0.0
-0.0


+0.0
+0.0


+subnormal
+0.0


+Inf
+Inf


NaN
NaN



Square root with IEEE 754 compliant rounding:
Rounding modifiers (no default):

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
sqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

sqrt.f64 supports subnormal numbers.
sqrt.f32 flushes subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
sqrt.f32 and sqrt.f64 introduced in PTX ISA version 1.0. sqrt.rn.f64 and explicit
modifiers .approx and .ftz were introduced in PTX ISA version 1.4. General rounding
modifiers were added in PTX ISA version 2.0.
For PTX ISA version 1.4 and later, one of .approx or .rnd is required.
For PTX ISA versions 1.0 through 1.3, sqrt.f32 defaults to sqrt.approx.ftz.f32, and
sqrt.f64 defaults to sqrt.rn.f64.
Target ISA Notes
sqrt.approx.f32 supported on all target architectures.
sqrt.rnd.f32 requires sm_20 or higher.
sqrt.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32.
sqrt.{rz,rm,rp}.f64 requires sm_20 or higher.
Examples

sqrt.approx.ftz.f32  r,x;
sqrt.rn.ftz.f32      r,x;
sqrt.rn.f64          r,x;





9.7.3.16. Floating Point Instructions: rsqrtï

rsqrt
Take the reciprocal of the square root of a value.
Syntax

rsqrt.approx{.ftz}.f32  d, a;
rsqrt.approx.f64        d, a;


Description
Compute 1/sqrt(a) and store the result in d.
Semantics

d = 1/sqrt(a);


Notes
rsqrt.approx implements an approximation to the reciprocal square root.







Input
Result




-Inf
NaN


-normal
NaN


-subnormal
-Inf


-0.0
-Inf


+0.0
+Inf


+subnormal
+Inf


+Inf
+0.0


NaN
NaN



The maximum absolute error for rsqrt.f32 is 2-22.4 over the range 1.0-4.0.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
rsqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

rsqrt.f64 supports subnormal numbers.
rsqrt.f32 flushes subnormal inputs and results to sign-preserving zero.


Note that rsqrt.approx.f64 is emulated in software and are relatively slow.
PTX ISA Notes
rsqrt.f32 and rsqrt.f64 were introduced in PTX ISA version 1.0. Explicit modifiers
.approx and .ftz were introduced in PTX ISA version 1.4.
For PTX ISA version 1.4 and later, the .approx modifier is required.
For PTX ISA versions 1.0 through 1.3, rsqrt.f32 defaults to rsqrt.approx.ftz.f32, and
rsqrt.f64 defaults to rsqrt.approx.f64.
Target ISA Notes
rsqrt.f32 supported on all target architectures.
rsqrt.f64 requires sm_13 or higher.
Examples

rsqrt.approx.ftz.f32  isr, x;
rsqrt.approx.f64      ISR, X;





9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64ï

rsqrt.approx.ftz.f64
Compute an approximation of the square root reciprocal of a value.
Syntax

rsqrt.approx.ftz.f64 d, a;


Description
Compute a double-precision (.f64) approximation of the square root reciprocal of a value. The
least significant 32 bits of the double-precision (.f64) destination d are all zeros.
Semantics

tmp = a[63:32]; // upper word of a, 1.11.20 format
d[63:32] = 1.0 / sqrt(tmp);
d[31:0] = 0x00000000;


Notes
rsqrt.approx.ftz.f64 implements a fast approximation of the square root reciprocal of a value.







Input
Result




-Inf
NaN


-subnormal
-Inf


-0.0
-Inf


+0.0
+Inf


+subnormal
+Inf


+Inf
+0.0


NaN
NaN



Input NaNs map to a canonical NaN with encoding 0x7fffffff00000000.
Subnormal inputs and results are flushed to sign-preserving zero.
PTX ISA Notes
rsqrt.approx.ftz.f64 introduced in PTX ISA version 4.0.
Target ISA Notes
rsqrt.approx.ftz.f64 requires sm_20 or higher.
Examples

rsqrt.approx.ftz.f64 xi,x;





9.7.3.18. Floating Point Instructions: sinï

sin
Find the sine of a value.
Syntax

sin.approx{.ftz}.f32  d, a;


Description
Find the sine of the angle a (in radians).
Semantics

d = sin(a);


Notes
sin.approx.f32 implements a fast approximation to sine.







Input
Result




-Inf
NaN


-subnormal
-0.0


-0.0
-0.0


+0.0
+0.0


+subnormal
+0.0


+Inf
NaN


NaN
NaN



The maximum absolute error is 2-20.9 in quadrant 00.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
sin.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

Subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
sin.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz
introduced in PTX ISA version 1.4.
For PTX ISA version 1.4 and later, the .approx modifier is required.
For PTX ISA versions 1.0 through 1.3, sin.f32 defaults to sin.approx.ftz.f32.
Target ISA Notes
Supported on all target architectures.
Examples

sin.approx.ftz.f32  sa, a;





9.7.3.19. Floating Point Instructions: cosï

cos
Find the cosine of a value.
Syntax

cos.approx{.ftz}.f32  d, a;


Description
Find the cosine of the angle a (in radians).
Semantics

d = cos(a);


Notes
cos.approx.f32 implements a fast approximation to cosine.







Input
Result




-Inf
NaN


-subnormal
+1.0


-0.0
+1.0


+0.0
+1.0


+subnormal
+1.0


+Inf
NaN


NaN
NaN



The maximum absolute error is 2-20.9 in quadrant 00.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
cos.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

Subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
cos.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz
introduced in PTX ISA version 1.4.
For PTX ISA version 1.4 and later, the .approx modifier is required.
For PTX ISA versions 1.0 through 1.3, cos.f32 defaults to cos.approx.ftz.f32.
Target ISA Notes
Supported on all target architectures.
Examples

cos.approx.ftz.f32  ca, a;





9.7.3.20. Floating Point Instructions: lg2ï

lg2
Find the base-2 logarithm of a value.
Syntax

lg2.approx{.ftz}.f32  d, a;


Description
Determine the log2 of a.
Semantics

d = log(a) / log(2);


Notes
lg2.approx.f32 implements a fast approximation to log2(a).







Input
Result




-Inf
NaN


-subnormal
-Inf


-0.0
-Inf


+0.0
-Inf


+subnormal
-Inf


+Inf
+Inf


NaN
NaN



The maximum absolute error is 2-22.6 for mantissa.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
lg2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

Subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
lg2.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz
introduced in PTX ISA version 1.4.
For PTX ISA version 1.4 and later, the .approx modifier is required.
For PTX ISA versions 1.0 through 1.3, lg2.f32 defaults to lg2.approx.ftz.f32.
Target ISA Notes
Supported on all target architectures.
Examples

lg2.approx.ftz.f32  la, a;





9.7.3.21. Floating Point Instructions: ex2ï

ex2
Find the base-2 exponential of a value.
Syntax

ex2.approx{.ftz}.f32  d, a;


Description
Raise 2 to the power a.
Semantics

d = 2 ^ a;


Notes
ex2.approx.f32 implements a fast approximation to 2a.







Input
Result




-Inf
+0.0


-subnormal
+1.0


-0.0
+1.0


+0.0
+1.0


+subnormal
+1.0


+Inf
+Inf


NaN
NaN



The maximum absolute error is 2-22.5 for fraction in the primary range.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
ex2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero.

sm_1x

Subnormal inputs and results to sign-preserving zero.


PTX ISA Notes
ex2.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz
introduced in PTX ISA version 1.4.
For PTX ISA version 1.4 and later, the .approx modifier is required.
For PTX ISA versions 1.0 through 1.3, ex2.f32 defaults to ex2.approx.ftz.f32.
Target ISA Notes
Supported on all target architectures.
Examples

ex2.approx.ftz.f32  xa, a;





9.7.3.22. Floating Point Instructions: tanhï

tanh
Find the hyperbolic tangent of a value (in radians)
Syntax

tanh.approx.f32 d, a;


Description
Take hyperbolic tangent value of a.
The operands d and a are of type .f32.
Semantics

d = tanh(a);


Notes
tanh.approx.f32 implements a fast approximation to FP32 hyperbolic-tangent.
Results of tanh for various corner-case inputs are as follows:







Input
Result




-Inf
-1.0


-subnormal
Same as input


-0.0
-0.0


+0.0
+0.0


+subnormal
Same as input


+Inf
1.0


NaN
NaN



The subnormal numbers are supported.

Note
The subnormal inputs gets passed through to the output since the value of tanh(x) for small
values of x is approximately the same as x.

PTX ISA Notes
Introduced in PTX ISA version 7.0.
Target ISA Notes
Requires sm_75 or higher.
Examples

tanh.approx.f32 sa, a;






9.7.4. Half Precision Floating-Point Instructionsï

Half precision floating-point instructions operate on .f16 and .f16x2 register operands. The
half precision floating-point instructions are:

add
sub
mul
fma
neg
abs
min
max
tanh
ex2

Half-precision add, sub, mul, and fma support saturation of results to the range
[0.0, 1.0], with NaNs being flushed to positive zero. Half-precision instructions return an
unspecified NaN.


9.7.4.1. Half Precision Floating Point Instructions: addï

add
Add two values.
Syntax

add{.rnd}{.ftz}{.sat}.f16   d, a, b;
add{.rnd}{.ftz}{.sat}.f16x2 d, a, b;

add{.rnd}.bf16   d, a, b;
add{.rnd}.bf16x2 d, a, b;

.rnd = { .rn };


Description
Performs addition and writes the resulting value into a destination register.
For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source
operands. Half-word operands are then added in parallel to produce .f16x2 or .bf16x2 result
in destination.
For .f16 instruction type, operands d, a and b have .f16 or .b16 type. For
.f16x2 instruction type, operands d, a and b have .b32 type. For .bf16
instruction type, operands d, a, b have .b16 type. For .bf16x2 instruction type,
operands d, a, b have .b32 type.
Semantics

if (type == f16 || type == bf16) {
    d = a + b;
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = fA[i] + fB[i];
    }
}


Notes
Rounding modifiers:

.rn

mantissa LSB rounds to nearest even


The default value of rounding modifier is .rn. Note that an add instruction with an explicit
rounding modifier is treated conservatively by the code optimizer. An add instruction with no
rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code
optimizer. In particular, mul/add sequences with no rounding modifiers may be optimized to
use fused-multiply-add instructions on the target device.

Subnormal numbers:

By default, subnormal numbers are supported.
add.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.

Saturation modifier:

add.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.


PTX ISA Notes
Introduced in PTX ISA version 4.2.
add{.rnd}.bf16 and add{.rnd}.bf16x2 introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_53 or higher.
add{.rnd}.bf16 and add{.rnd}.bf16x2 requires sm_90 or higher.
Examples

// scalar f16 additions
add.f16        d0, a0, b0;
add.rn.f16     d1, a1, b1;
add.bf16       bd0, ba0, bb0;
add.rn.bf16    bd1, ba1, bb1;

// SIMD f16 addition
cvt.rn.f16.f32 h0, f0;
cvt.rn.f16.f32 h1, f1;
cvt.rn.f16.f32 h2, f2;
cvt.rn.f16.f32 h3, f3;
mov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2
mov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2
add.f16x2  p3, p1, p2;   // SIMD f16x2 addition

// SIMD bf16 addition
cvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2
cvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2
add.bf16x2  p6, p4, p5;       // SIMD bf16x2 addition

// SIMD fp16 addition
ld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2
ld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2
add.f16x2       f2, f0, f1;     // SIMD f16x2 addition

ld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2
ld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2
add.bf16x2      f5, f3, f4;      // SIMD bf16x2 addition





9.7.4.2. Half Precision Floating Point Instructions: subï

sub
Subtract two values.
Syntax

sub{.rnd}{.ftz}{.sat}.f16   d, a, b;
sub{.rnd}{.ftz}{.sat}.f16x2 d, a, b;

sub{.rnd}.bf16   d, a, b;
sub{.rnd}.bf16x2 d, a, b;

.rnd = { .rn };


Description
Performs subtraction and writes the resulting value into a destination register.
For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source
operands. Half-word operands are then subtracted in parallel to produce .f16x2 or .bf16x2
result in destination.
For .f16 instruction type, operands d, a and b have .f16 or .b16 type. For
.f16x2 instruction type, operands d, a and b have .b32 type. For .bf16
instruction type, operands d, a, b have .b16 type. For .bf16x2 instruction type,
operands d, a, b have .b32 type.
Semantics

if (type == f16 || type == bf16) {
    d = a - b;
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = fA[i] - fB[i];
    }
}


Notes
Rounding modifiers:

.rn

mantissa LSB rounds to nearest even


The default value of rounding modifier is .rn. Note that a sub instruction with an explicit
rounding modifier is treated conservatively by the code optimizer. A sub instruction with no
rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code
optimizer. In particular, mul/sub sequences with no rounding modifiers may be optimized to
use fused-multiply-add instructions on the target device.

Subnormal numbers:

By default, subnormal numbers are supported.
sub.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.

Saturation modifier:

sub.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.


PTX ISA Notes
Introduced in PTX ISA version 4.2.
sub{.rnd}.bf16 and sub{.rnd}.bf16x2 introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_53 or higher.
sub{.rnd}.bf16 and sub{.rnd}.bf16x2 requires sm_90 or higher.
Examples

// scalar f16 subtractions
sub.f16        d0, a0, b0;
sub.rn.f16     d1, a1, b1;
sub.bf16       bd0, ba0, bb0;
sub.rn.bf16    bd1, ba1, bb1;

// SIMD f16 subtraction
cvt.rn.f16.f32 h0, f0;
cvt.rn.f16.f32 h1, f1;
cvt.rn.f16.f32 h2, f2;
cvt.rn.f16.f32 h3, f3;
mov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2
mov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2
sub.f16x2  p3, p1, p2;   // SIMD f16x2 subtraction

// SIMD bf16 subtraction
cvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2
cvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2
sub.bf16x2  p6, p4, p5;       // SIMD bf16x2 subtraction

// SIMD fp16 subtraction
ld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2
ld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2
sub.f16x2       f2, f0, f1;     // SIMD f16x2 subtraction

// SIMD bf16 subtraction
ld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2
ld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2
sub.bf16x2      f5, f3, f4;      // SIMD bf16x2 subtraction





9.7.4.3. Half Precision Floating Point Instructions: mulï

mul
Multiply two values.
Syntax

mul{.rnd}{.ftz}{.sat}.f16   d, a, b;
mul{.rnd}{.ftz}{.sat}.f16x2 d, a, b;

mul{.rnd}.bf16   d, a, b;
mul{.rnd}.bf16x2 d, a, b;

.rnd = { .rn };


Description
Performs multiplication and writes the resulting value into a destination register.
For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source
operands. Half-word operands are then multiplied in parallel to produce .f16x2 or .bf16x2
result in destination.
For .f16 instruction type, operands d, a and b have .f16 or .b16 type. For
.f16x2 instruction type, operands d, a and b have .b32 type. For .bf16
instruction type, operands d, a, b have .b16 type. For .bf16x2 instruction type,
operands d, a, b have .b32 type.
Semantics

if (type == f16 || type == bf16) {
    d = a * b;
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = fA[i] * fB[i];
    }
}


Notes
Rounding modifiers:

.rn

mantissa LSB rounds to nearest even


The default value of rounding modifier is .rn. Note that a mul instruction with an explicit
rounding modifier is treated conservatively by the code optimizer. A mul instruction with no
rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code
optimizer. In particular, mul/add and mul/sub sequences with no rounding modifiers may
be optimized to use fused-multiply-add instructions on the target device.

Subnormal numbers:

By default, subnormal numbers are supported.
mul.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.

Saturation modifier:

mul.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.


PTX ISA Notes
Introduced in PTX ISA version 4.2.
mul{.rnd}.bf16 and mul{.rnd}.bf16x2 introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_53 or higher.
mul{.rnd}.bf16 and mul{.rnd}.bf16x2 requires sm_90 or higher.
Examples

// scalar f16 multiplications
mul.f16        d0, a0, b0;
mul.rn.f16     d1, a1, b1;
mul.bf16       bd0, ba0, bb0;
mul.rn.bf16    bd1, ba1, bb1;

// SIMD f16 multiplication
cvt.rn.f16.f32 h0, f0;
cvt.rn.f16.f32 h1, f1;
cvt.rn.f16.f32 h2, f2;
cvt.rn.f16.f32 h3, f3;
mov.b32  p1, {h0, h1};   // pack two f16 to 32bit f16x2
mov.b32  p2, {h2, h3};   // pack two f16 to 32bit f16x2
mul.f16x2  p3, p1, p2;   // SIMD f16x2 multiplication

// SIMD bf16 multiplication
cvt.rn.bf16x2.f32 p4, f4, f5; // Convert two f32 into packed bf16x2
cvt.rn.bf16x2.f32 p5, f6, f7; // Convert two f32 into packed bf16x2
mul.bf16x2  p6, p4, p5;       // SIMD bf16x2 multiplication

// SIMD fp16 multiplication
ld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2
ld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2
mul.f16x2       f2, f0, f1;     // SIMD f16x2 multiplication

// SIMD bf16 multiplication
ld.global.b32   f3, [addr + 8];  // load 32 bit which hold packed bf16x2
ld.global.b32   f4, [addr + 12]; // load 32 bit which hold packed bf16x2
mul.bf16x2      f5, f3, f4;      // SIMD bf16x2 multiplication





9.7.4.4. Half Precision Floating Point Instructions: fmaï

fma
Fused multiply-add
Syntax

fma.rnd{.ftz}{.sat}.f16     d, a, b, c;
fma.rnd{.ftz}{.sat}.f16x2   d, a, b, c;
fma.rnd{.ftz}.relu.f16      d, a, b, c;
fma.rnd{.ftz}.relu.f16x2    d, a, b, c;
fma.rnd{.relu}.bf16         d, a, b, c;
fma.rnd{.relu}.bf16x2       d, a, b, c;
fma.rnd.oob.{relu}.type     d, a, b, c;

.rnd = { .rn };


Description
Performs a fused multiply-add with no loss of precision in the intermediate product and addition.
For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source
operands. Half-word operands are then operated in parallel to produce .f16x2 or .bf16x2
result in destination.
For .f16 instruction type, operands d, a, b and c have .f16 or .b16
type. For .f16x2 instruction type, operands d, a, b and c have .b32
type. For .bf16 instruction type, operands d, a, b and c have .b16 type. For
.bf16x2 instruction type, operands d, a, b and c have .b32 type.
Semantics

if (type == f16 || type == bf16) {
    d = a * b + c;
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    fC[0] = c[0:15];
    fC[1] = c[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = fA[i] * fB[i] + fC[i];
    }
}


Notes
Rounding modifiers (default is .rn):

.rn

mantissa LSB rounds to nearest even

Subnormal numbers:

By default, subnormal numbers are supported.
fma.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.

Saturation modifier:

fma.sat.{f16, f16x2} clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f.
fma.relu.{f16, f16x2, bf16, bf16x2} clamps the result to 0 if negative. NaN result is
converted to canonical NaN.

Out Of Bounds modifier:

fma.oob.{f16, f16x2, bf16, bf16x2} clamps the result to 0 if either of the operands
is OOB NaN (defined under Tensors) value. The test for the special NaN value
and resultant forcing of the result to +0.0 is performed independently for each of the
two SIMD operations.


PTX ISA Notes
Introduced in PTX ISA version 4.2.
fma.relu.{f16, f16x2} and fma{.relu}.{bf16, bf16x2} introduced in PTX ISA version 7.0.
Support for modifier .oob introduced in PTX ISA version 8.1.
Target ISA Notes
Requires sm_53 or higher.
fma.relu.{f16, f16x2} and fma{.relu}.{bf16, bf16x2} require sm_80 or higher.
fma{.oob}.{f16, f16x2, bf16, bf16x2} requires sm_90 or higher.
Examples

// scalar f16 fused multiply-add
fma.rn.f16         d0, a0, b0, c0;
fma.rn.f16         d1, a1, b1, c1;
fma.rn.relu.f16    d1, a1, b1, c1;
fma.rn.oob.f16      d1, a1, b1, c1;
fma.rn.oob.relu.f16 d1, a1, b1, c1;

// scalar bf16 fused multiply-add
fma.rn.bf16        d1, a1, b1, c1;
fma.rn.relu.bf16   d1, a1, b1, c1;
fma.rn.oob.bf16       d1, a1, b1, c1;
fma.rn.oob.relu.bf16  d1, a1, b1, c1;

// SIMD f16 fused multiply-add
cvt.rn.f16.f32 h0, f0;
cvt.rn.f16.f32 h1, f1;
cvt.rn.f16.f32 h2, f2;
cvt.rn.f16.f32 h3, f3;
mov.b32  p1, {h0, h1}; // pack two f16 to 32bit f16x2
mov.b32  p2, {h2, h3}; // pack two f16 to 32bit f16x2
fma.rn.f16x2  p3, p1, p2, p2;   // SIMD f16x2 fused multiply-add
fma.rn.relu.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with relu saturation mode
fma.rn.oob.f16x2  p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier
fma.rn.oob.relu.f16x2 p3, p1, p2, p2; // SIMD f16x2 fused multiply-add with oob modifier and relu saturation mode

// SIMD fp16 fused multiply-add
ld.global.b32   f0, [addr];     // load 32 bit which hold packed f16x2
ld.global.b32   f1, [addr + 4]; // load 32 bit which hold packed f16x2
fma.rn.f16x2    f2, f0, f1, f1; // SIMD f16x2 fused multiply-add

// SIMD bf16 fused multiply-add
fma.rn.bf16x2       f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add
fma.rn.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with relu saturation mode
fma.rn.oob.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier
fma.rn.oob.relu.bf16x2  f2, f0, f1, f1; // SIMD bf16x2 fused multiply-add with oob modifier and relu saturation mode





9.7.4.5. Half Precision Floating Point Instructions: negï

neg
Arithmetic negate.
Syntax

neg{.ftz}.f16    d, a;
neg{.ftz}.f16x2  d, a;
neg.bf16         d, a;
neg.bf16x2       d, a;


Description
Negate the sign of a and store the result in d.
For .f16x2 and .bf16x2 instruction type, forms input vector by extracting half word values
from the source operand. Half-word operands are then negated in parallel to produce .f16x2 or
.bf16x2 result in destination.
For .f16 instruction type, operands d and a have .f16 or .b16 type. For
.f16x2 instruction type, operands d and a have .b32 type. For .bf16 instruction
type, operands d and a have .b16 type. For .bf16x2 instruction type, operands d
and a have .b32 type.
Semantics

if (type == f16 || type == bf16) {
    d = -a;
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = -fA[i];
    }
}


Notes

Subnormal numbers:

By default, subnormal numbers are supported.
neg.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.


NaN inputs yield an unspecified NaN. Future implementations may comply with the IEEE 754
standard by preserving payload and modifying only the sign bit.
PTX ISA Notes
Introduced in PTX ISA version 6.0.
neg.bf16 and neg.bf16x2 introduced in PTX ISA 7.0.
Target ISA Notes
Requires sm_53 or higher.
neg.bf16 and neg.bf16x2 requires architecture sm_80 or higher.
Examples

neg.ftz.f16  x,f0;
neg.bf16     x,b0;
neg.bf16x2   x1,b1;





9.7.4.6. Half Precision Floating Point Instructions: absï

abs
Absolute value
Syntax

abs{.ftz}.f16    d, a;
abs{.ftz}.f16x2  d, a;
abs.bf16         d, a;
abs.bf16x2       d, a;


Description
Take absolute value of a and store the result in d.
For .f16x2 and .bf16x2 instruction type, forms input vector by extracting half word values
from the source operand. Absolute values of half-word operands are then computed in parallel to
produce .f16x2 or .bf16x2 result in destination.
For .f16 instruction type, operands d and a have .f16 or .b16 type. For
.f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For
.bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction
type, operands d and a have .b32 type.
Semantics

if (type == f16 || type == bf16) {
    d = |a|;
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    for (i = 0; i < 2; i++) {
         d[i] = |fA[i]|;
    }
}


Notes

Subnormal numbers:

By default, subnormal numbers are supported.
abs.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.


NaN inputs yield an unspecified NaN. Future implementations may comply with the IEEE 754
standard by preserving payload and modifying only the sign bit.
PTX ISA Notes
Introduced in PTX ISA version 6.5.
abs.bf16 and abs.bf16x2 introduced in PTX ISA 7.0.
Target ISA Notes
Requires sm_53 or higher.
abs.bf16 and abs.bf16x2 requires architecture sm_80 or higher.
Examples

abs.ftz.f16  x,f0;
abs.bf16     x,b0;
abs.bf16x2   x1,b1;





9.7.4.7. Half Precision Floating Point Instructions: minï

min
Find the minimum of two values.
Syntax

min{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;
min{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;
min{.NaN}{.xorsign.abs}.bf16           d, a, b;
min{.NaN}{.xorsign.abs}.bf16x2         d, a, b;


Description
Store the minimum of a and b in d.
For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values
from source operands. Half-word operands are then processed in parallel to store .f16x2 or
.bf16x2 result in destination.
For .f16 instruction type, operands d and a have .f16 or .b16 type. For
.f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For
.bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction
type, operands d and a have .b32 type.
If .NaN modifier is specified, then the result is canonical NaN if either of the inputs is
NaN.
If .abs modifier is specified, the magnitude of destination operand d is the minimum of
absolute values of both the input arguments.
If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the
sign bits of both the inputs.
Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign
bit of both inputs before applying .abs operation.
If the result of min is NaN then the .xorsign and .abs modifiers will be ignored.
Semantics

if (type == f16 || type == bf16) {
    if (.xorsign) {
        xorsign = getSignBit(a) ^ getSignBit(b);
        if (.abs) {
            a = |a|;
            b = |b|;
        }
    }
    if (isNaN(a) && isNaN(b))              d = NaN;
    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;
    else if (isNaN(a))                     d = b;
    else if (isNaN(b))                     d = a;
    else                                   d = (a < b) ? a : b;
    if (.xorsign && !isNaN(d)) {
         setSignBit(d, xorsign);
    }
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
        if (.xorsign) {
            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);
            if (.abs) {
               fA[i] = |fA[i]|;
               fB[i] = |fB[i]|;
           }
        }
        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;
        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;
        else if (isNaN(fA[i]))                         d[i] = fB[i];
        else if (isNaN(fB[i]))                         d[i] = fA[i];
        else                                           d[i] = (fA[i] < fB[i]) ? fA[i] : fB[i];
        if (.xorsign && !isNaN(d[i])) {
            setSignBit(d[i], xorsign);
        }
    }
}


Notes

Subnormal numbers:

By default, subnormal numbers are supported.
min.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.


If values of both inputs are 0.0, then +0.0 > -0.0.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
min.xorsign introduced in PTX ISA version 7.2.
Target ISA Notes
Requires sm_80 or higher.
min.xorsign.abs support requires sm_86 or higher.
Examples

min.ftz.f16       h0,h1,h2;
min.f16x2         b0,b1,b2;
// SIMD fp16 min with .NaN
min.NaN.f16x2     b0,b1,b2;
min.bf16          h0, h1, h2;
// SIMD bf16 min with NaN
min.NaN.bf16x2    b0, b1, b2;
// scalar bf16 min with xorsign.abs
min.xorsign.abs.bf16 Rd, Ra, Rb





9.7.4.8. Half Precision Floating Point Instructions: maxï

max
Find the maximum of two values.
Syntax

max{.ftz}{.NaN}{.xorsign.abs}.f16      d, a, b;
max{.ftz}{.NaN}{.xorsign.abs}.f16x2    d, a, b;
max{.NaN}{.xorsign.abs}.bf16           d, a, b;
max{.NaN}{.xorsign.abs}.bf16x2         d, a, b;


Description
Store the maximum of a and b in d.
For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values
from source operands. Half-word operands are then processed in parallel to store .f16x2 or
.bf16x2 result in destination.
For .f16 instruction type, operands d and a have .f16 or .b16 type. For
.f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For
.bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction
type, operands d and a have .b32 type.
If .NaN modifier is specified, the result is canonical NaN if either of the inputs is
NaN.
If .abs modifier is specified, the magnitude of destination operand d is the maximum of
absolute values of both the input arguments.
If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the
sign bits of both the inputs.
Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign
bit of both inputs before applying .abs operation.
If the result of max is NaN then the .xorsign and .abs modifiers will be ignored.
Semantics

if (type == f16 || type == bf16) {
    if (.xorsign) {
        xorsign = getSignBit(a) ^ getSignBit(b);
        if (.abs) {
            a = |a|;
            b = |b|;
        }
    }
    if (isNaN(a) && isNaN(b))              d = NaN;
    if (.NaN && (isNaN(a) || isNaN(b)))    d = NaN;
    else if (isNaN(a))                     d = b;
    else if (isNaN(b))                     d = a;
    else                                   d = (a > b) ? a : b;
    if (.xorsign && !isNaN(d)) {
         setSignBit(d, xorsign);
    }
} else if (type == f16x2 || type == bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    for (i = 0; i < 2; i++) {
        if (.xorsign) {
            xorsign = getSignBit(fA[i]) ^ getSignBit(fB[i]);
            if (.abs) {
                fA[i] = |fA[i]|;
                fB[i] = |fB[i]|;
            }
        }
        if (isNaN(fA[i]) && isNaN(fB[i]))              d[i] = NaN;
        if (.NaN && (isNaN(fA[i]) || isNaN(fB[i])))    d[i] = NaN;
        else if (isNaN(fA[i]))                         d[i] = fB[i];
        else if (isNaN(fB[i]))                         d[i] = fA[i];
        else                                           d[i] = (fA[i] > fB[i]) ? fA[i] : fB[i];
        if (.xorsign && !isNaN(fA[i])) {
            setSignBit(d[i], xorsign);
        }
    }
}


Notes

Subnormal numbers:

By default, subnormal numbers are supported.
max.ftz.{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero.


If values of both inputs are 0.0, then +0.0 > -0.0.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
max.xorsign.abs introduced in PTX ISA version 7.2.
Target ISA Notes
Requires sm_80 or higher.
max.xorsign.abs support requires sm_86 or higher.
Examples

max.ftz.f16       h0,h1,h2;
max.f16x2         b0,b1,b2;
// SIMD fp16 max with NaN
max.NaN.f16x2     b0,b1,b2;
// scalar f16 max with xorsign.abs
max.xorsign.abs.f16 Rd, Ra, Rb;
max.bf16          h0, h1, h2;
// scalar bf16 max and NaN
max.NaN.bf16x2    b0, b1, b2;
// SIMD bf16 max with xorsign.abs
max.xorsign.abs.bf16x2 Rd, Ra, Rb;





9.7.4.9. Half Precision Floating Point Instructions: tanhï

tanh
Find the hyperbolic tangent of a value (in radians)
Syntax

tanh.approx.type d, a;

.type = {.f16, .f16x2, .bf16, .bf16x2}


Description
Take hyperbolic tangent value of a.
The type of operands d and a are as specified by .type.
For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in
parallel and the results are packed appropriately into a .f16x2 or .bf16x2.
Semantics

if (.type == .f16 || .type == .bf16) {
  d = tanh(a)
} else if (.type == .f16x2 || .type == .bf16x2) {
  fA[0] = a[0:15];
  fA[1] = a[16:31];
  d[0] = tanh(fA[0])
  d[1] = tanh(fA[1])
}


Notes
tanh.approx.{f16, f16x2, bf16, bf16x2} implements an approximate hyperbolic tangent in the
target format.
Results of tanh for various corner-case inputs are as follows:







Input
Result




-Inf
-1.0


-0.0
-0.0


+0.0
+0.0


+Inf
1.0


NaN
NaN



The maximum absolute error for .f16 type is 2-10.987. The maximum absolute error for .bf16
type is 2-8.
The subnormal numbers are supported.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
tanh.approx.{bf16/bf16x2} introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_75 or higher.
tanh.approx.{bf16/bf16x2} requires sm_90 or higher.
Examples

tanh.approx.f16    h1, h0;
tanh.approx.f16x2  hd1, hd0;
tanh.approx.bf16   b1, b0;
tanh.approx.bf16x2 hb1, hb0;





9.7.4.10. Half Precision Floating Point Instructions: ex2ï

ex2
Find the base-2 exponent of input.
Syntax

ex2.approx.atype     d, a;
ex2.approx.ftz.btype d, a;

.atype = { .f16,  .f16x2}
.btype = { .bf16, .bf16x2}


Description
Raise 2 to the power a.
The type of operands d and a are as specified by .type.
For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in
parallel and the results are packed appropriately into a .f16x2 or .bf16x2.
Semantics

if (.type == .f16 || .type == .bf16) {
  d = 2 ^ a
} else if (.type == .f16x2 || .type == .bf16x2) {
  fA[0] = a[0:15];
  fA[1] = a[16:31];
  d[0] = 2 ^ fA[0]
  d[1] = 2 ^ fA[1]
}


Notes
ex2.approx.{f16, f16x2, bf16, bf16x2} implement a fast approximation to 2a.
For the .f16 type, subnormal inputs are supported. ex2.approx.ftz.bf16 flushes subnormal
inputs and results to sign-preserving zero.
Results of ex2.approx.ftz.bf16 for various corner-case inputs are as follows:







Input
Result




-Inf
+0.0


-subnormal
+1.0


-0.0
+1.0


+0.0
+1.0


+subnormal
+1.0


+Inf
+Inf


NaN
NaN



Results of ex2.approx.f16 for various corner-case inputs are as follows:







Input
Result




-Inf
+0.0


-0.0
+1.0


+0.0
+1.0


+Inf
+Inf


NaN
NaN



The maximum relative error for .f16 type is 2-9.9. The maximum relative error for .bf16 type
is 2-7.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
ex2.approx.ftz.{bf16/bf16x2} introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_75 or higher.
ex2.approx.ftz.{bf16/bf16x2} requires sm_90 or higher.
Examples

ex2.approx.f16         h1, h0;
ex2.approx.f16x2       hd1, hd0;
ex2.approx.ftz.bf16    b1, b2;
ex2.approx.ftz.bf16x2  hb1, hb2;






9.7.5. Comparison and Selection Instructionsï

The comparison select instructions are:

set
setp
selp
slct

As with single-precision floating-point instructions, the set, setp, and slct
instructions support subnormal numbers for sm_20 and higher targets and flush single-precision
subnormal inputs to sign-preserving zero for sm_1x targets. The optional .ftz modifier
provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to
sign-preserving zero regardless of the target architecture.


9.7.5.1. Comparison and Selection Instructions: setï

set
Compare two numeric values with a relational operator, and optionally combine this result with a
predicate value by applying a Boolean operator.
Syntax

set.CmpOp{.ftz}.dtype.stype         d, a, b;
set.CmpOp.BoolOp{.ftz}.dtype.stype  d, a, b, {!}c;

.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,
            equ, neu, ltu, leu, gtu, geu, num, nan };
.BoolOp = { and, or, xor };
.dtype  = { .u32, .s32, .f32 };
.stype  = { .b16, .b32, .b64,
            .u16, .u32, .u64,
            .s16, .s32, .s64,
                  .f32, .f64 };


Description
Compares two numeric values and optionally combines the result with another predicate value by
applying a Boolean operator. If this result is True, 1.0f is written for floating-point
destination types, and 0xffffffff is written for integer destination types. Otherwise,
0x00000000 is written.
Operand dhas type .dtype; operands a and b have type .stype; operand c has
type .pred.
Semantics

t = (a CmpOp b) ? 1 : 0;
if (isFloat(dtype))
    d = BoolOp(t, c) ? 1.0f : 0x00000000;
else
    d = BoolOp(t, c) ? 0xffffffff : 0x00000000;


Integer Notes
The signed and unsigned comparison operators are eq, ne, lt, le, gt, ge.
For unsigned values, the comparison operators lo, ls, hi, and hs for lower,
lower-or-same, higher, and higher-or-same may be used instead of lt, le, gt, ge,
respectively.
The untyped, bit-size comparisons are eq and ne.
Floating Point Notes
The ordered comparisons are eq, ne, lt, le, gt, ge. If either operand is NaN, the result is False.
To aid comparison operations in the presence of NaN values, unordered versions are included:
equ, neu, ltu, leu, gtu, geu. If both operands are numeric values (not
NaN), then these comparisons have the same result as their ordered counterparts. If either
operand is NaN, then the result of these comparisons is True.
num returns True if both operands are numeric values (not NaN), and nan returns
True if either operand is NaN.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
set.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero.

sm_1x

set.dtype.f64 supports subnormal numbers.
set.dtype.f32 flushes subnormal inputs to sign-preserving zero.


Modifier .ftz applies only to .f32 comparisons.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
set with .f64 source type requires sm_13 or higher.
Examples

@p  set.lt.and.f32.s32  d,a,b,r;
    set.eq.u32.u32      d,i,n;





9.7.5.2. Comparison and Selection Instructions: setpï

setp
CompareÂ two numeric values with a relational operator, and (optionally) combine this result with a
predicate value by applying a Boolean operator.
Syntax

setp.CmpOp{.ftz}.type         p[|q], a, b;
setp.CmpOp.BoolOp{.ftz}.type  p[|q], a, b, {!}c;

.CmpOp  = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs,
            equ, neu, ltu, leu, gtu, geu, num, nan };
.BoolOp = { and, or, xor };
.type   = { .b16, .b32, .b64,
            .u16, .u32, .u64,
            .s16, .s32, .s64,
                  .f32, .f64 };


Description
Compares two values and combines the result with another predicate value by applying a Boolean
operator. This result is written to the first destination operand. A related value computed using
the complement of the compare result is written to the second destination operand.
Applies to all numeric types. Operands a and b have type .type; operands p, q,
and c have type .pred. The sink symbol â_â may be used in place of any one of the
destination operands.
Semantics

t = (a CmpOp b) ? 1 : 0;
p = BoolOp(t, c);
q = BoolOp(!t, c);


Integer Notes
The signed and unsigned comparison operators are eq, ne, lt, le, gt, ge.
For unsigned values, the comparison operators lo, ls, hi, and hs for lower,
lower-or-same, higher, and higher-or-same may be used instead of lt, le, gt, ge,
respectively.
The untyped, bit-size comparisons are eq and ne.
Floating Point Notes
The ordered comparisons are eq, ne, lt, le, gt, ge. If either operand is NaN, the result is False.
To aid comparison operations in the presence of NaN values, unordered versions are included:
equ, neu, ltu, leu, gtu, geu. If both operands are numeric values (not
NaN), then these comparisons have the same result as their ordered counterparts. If either
operand is NaN, then the result of these comparisons is True.
num returns True if both operands are numeric values (not NaN), and nan returns
True if either operand is NaN.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
setp.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero.

sm_1x

setp.dtype.f64 supports subnormal numbers.
setp.dtype.f32 flushes subnormal inputs to sign-preserving zero.


Modifier .ftz applies only to .f32 comparisons.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
setp with .f64 source type requires sm_13 or higher.
Examples

    setp.lt.and.s32  p|q,a,b,r;
@q  setp.eq.u32      p,i,n;





9.7.5.3. Comparison and Selection Instructions: selpï

selp
Select between source operands, based on the value of the predicate source operand.
Syntax

selp.type d, a, b, c;

.type = { .b16, .b32, .b64,
          .u16, .u32, .u64,
          .s16, .s32, .s64,
                .f32, .f64 };


Description
Conditional selection. If c is True, a is stored in d, b otherwise. Operands
d, a, and b must be of the same type. Operand c is a predicate.
Semantics

d = (c == 1) ? a : b;


PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
selp.f64 requires sm_13 or higher.
Examples

    selp.s32  r0,r,g,p;
@q  selp.f32  f0,t,x,xp;





9.7.5.4. Comparison and Selection Instructions: slctï

slct
Select one source operand, based on the sign of the third operand.
Syntax

slct.dtype.s32        d, a, b, c;
slct{.ftz}.dtype.f32  d, a, b, c;

.dtype = { .b16, .b32, .b64,
           .u16, .u32, .u64,
           .s16, .s32, .s64,
                 .f32, .f64 };


Description
Conditional selection. If c â¥ 0, a is stored in d, otherwise b is stored in
d. Operands d, a, and b are treated as a bitsize type of the same width as the first
instruction type; operand c must match the second instruction type (.s32 or .f32). The
selected input is copied to the output without modification.
Semantics

d = (c >= 0) ? a : b;


Floating Point Notes
For .f32 comparisons, negative zero equals zero.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
slct.ftz.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and
operand a is selected.

sm_1x

slct.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand
a is selected.


Modifier .ftz applies only to .f32 comparisons.
If operand c is NaN, the comparison is unordered and operand b is selected.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
slct.f64 requires sm_13 or higher.
Examples

slct.u32.s32  x, y, z, val;
slct.ftz.u64.f32  A, B, C, fval;






9.7.6. Half Precision Comparison Instructionsï

The comparison instructions are:

set
setp



9.7.6.1. Half Precision Comparison Instructions: setï

set
Compare two numeric values with a relational operator, and optionally combine this result with a
predicate value by applying a Boolean operator.
Syntax

set.CmpOp{.ftz}.f16.stype            d, a, b;
set.CmpOp.BoolOp{.ftz}.f16.stype     d, a, b, {!}c;

set.CmpOp.bf16.stype                 d, a, b;
set.CmpOp.BoolOp.bf16.stype          d, a, b, {!}c;

set.CmpOp{.ftz}.dtype.f16            d, a, b;
set.CmpOp.BoolOp{.ftz}.dtype.f16     d, a, b, {!}c;
.dtype  = { .u16, .s16, .u32, .s32}

set.CmpOp.dtype.bf16                 d, a, b;
set.CmpOp.BoolOp.dtype.bf16          d, a, b, {!}c;
.dtype  = { .u16, .s16, .u32, .s32}

set.CmpOp{.ftz}.dtype.f16x2          d, a, b;
set.CmpOp.BoolOp{.ftz}.dtype.f16x2   d, a, b, {!}c;
.dtype  = { .f16x2, .u32, .s32}

set.CmpOp.dtype.bf16x2               d, a, b;
set.CmpOp.BoolOp.dtype.bf16x2        d, a, b, {!}c;
.dtype  = { .bf16x2, .u32, .s32}

.CmpOp  = { eq, ne, lt, le, gt, ge,
            equ, neu, ltu, leu, gtu, geu, num, nan };
.BoolOp = { and, or, xor };
.stype  = { .b16, .b32, .b64,
            .u16, .u32, .u64,
            .s16, .s32, .s64,
            .f16, .f32, .f64};


Description
Compares two numeric values and optionally combines the result with another predicate value by
applying a Boolean operator.
Result of this computation is written in destination register in the following way:


If result is True,

0xffffffff is written for destination types .u32/.s32.
0xffff is written for destination types .u16/.s16.
1.0 in target precision floating point format is written for destination type .f16,
.bf16.



If result is False,

0x0 is written for all integer destination types.
0.0 in target precision floating point format is written for destination type .f16,
.bf16.



If the source type is .f16x2 or .bf16x2 then result of individual operations are packed in
the 32-bit destination operand.
Operand c has type .pred.
Semantics

if (stype == .f16x2 || stype == .bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    t[0]   = (fA[0] CmpOp fB[0]) ? 1 : 0;
    t[1]   = (fA[1] CmpOp fB[1]) ? 1 : 0;
    if (dtype == .f16x2 || stype == .bf16x2) {
        for (i = 0; i < 2; i++) {
            d[i] = BoolOp(t[i], c) ? 1.0 : 0.0;
        }
    } else {
        for (i = 0; i < 2; i++) {
            d[i] = BoolOp(t[i], c) ? 0xffff : 0;
        }
    }
} else if (dtype == .f16 || stype == .bf16) {
    t = (a CmpOp b) ? 1 : 0;
    d = BoolOp(t, c) ? 1.0 : 0.0;
} else  { // Integer destination type
    trueVal = (isU16(dtype) || isS16(dtype)) ?  0xffff : 0xffffffff;
    t = (a CmpOp b) ? 1 : 0;
    d = BoolOp(t, c) ? trueVal : 0;
}


Floating Point Notes
The ordered comparisons are eq, ne, lt, le, gt, ge. If either operand is
NaN, the result is False.
To aid comparison operations in the presence of NaN values, unordered versions are included:
equ, neu, ltu, leu, gtu, geu. If both operands are numeric values (not
NaN), then these comparisons have the same result as their ordered counterparts. If either
operand is NaN, then the result of these comparisons is True.
num returns True if both operands are numeric values (not NaN), and nan returns
True if either operand is NaN.

Subnormal numbers:

By default, subnormal numbers are supported.
When .ftz modifier is specified then subnormal inputs and results are flushed to sign
preserving zero.


PTX ISA Notes
Introduced in PTX ISA version 4.2.
set.{u16, u32, s16, s32}.f16 and set.{u32, s32}.f16x2 are introduced in PTX ISA version 6.5.
set.{u16, u32, s16, s32}.bf16, set.{u32, s32, bf16x2}.bf16x2,
set.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64} are introduced in PTX ISA version
7.8.
Target ISA Notes
Requires sm_53 or higher.
set.{u16, u32, s16, s32}.bf16, set.{u32, s32, bf16x2}.bf16x2,
set.bf16.{s16,u16,f16,b16,s32,u32,f32,b32,s64,u64,f64,b64} require sm_90 or higher.
Examples

set.lt.and.f16.f16  d,a,b,r;
set.eq.f16x2.f16x2  d,i,n;
set.eq.u32.f16x2    d,i,n;
set.lt.and.u16.f16  d,a,b,r;
set.ltu.or.bf16.f16    d,u,v,s;
set.equ.bf16x2.bf16x2  d,j,m;
set.geu.s32.bf16x2     d,j,m;
set.num.xor.s32.bf16   d,u,v,s;





9.7.6.2. Half Precision Comparison Instructions: setpï

setp
Compare two numeric values with a relational operator, and optionally combine this result with a
predicate value by applying a Boolean operator.
Syntax

setp.CmpOp{.ftz}.f16           p, a, b;
setp.CmpOp.BoolOp{.ftz}.f16    p, a, b, {!}c;

setp.CmpOp{.ftz}.f16x2         p|q, a, b;
setp.CmpOp.BoolOp{.ftz}.f16x2  p|q, a, b, {!}c;

setp.CmpOp.bf16                p, a, b;
setp.CmpOp.BoolOp.bf16         p, a, b, {!}c;

setp.CmpOp.bf16x2              p|q, a, b;
setp.CmpOp.BoolOp.bf16x2       p|q, a, b, {!}c;

.CmpOp  = { eq, ne, lt, le, gt, ge,
            equ, neu, ltu, leu, gtu, geu, num, nan };
.BoolOp = { and, or, xor };


Description
Compares two values and combines the result with another predicate value by applying a Boolean
operator. This result is written to the destination operand.
Operand c, p and q has type .pred.
For instruction type .f16, operands a and b have type .b16 or .f16.
For instruction type .f16x2, operands a and b have type .b32.
For instruction type .bf16, operands a and b have type .b16.
For instruction type .bf16x2, operands a and b have type .b32.
Semantics

if (type == .f16 || type == .bf16) {
     t = (a CmpOp b) ? 1 : 0;
     p = BoolOp(t, c);
} else if (type == .f16x2 || type == .bf16x2) {
    fA[0] = a[0:15];
    fA[1] = a[16:31];
    fB[0] = b[0:15];
    fB[1] = b[16:31];
    t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0;
    t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0;
    p = BoolOp(t[0], c);
    q = BoolOp(t[1], c);
}


Floating Point Notes
The ordered comparisons are eq, ne, lt, le, gt, ge. If either operand is
NaN, the result is False.
To aid comparison operations in the presence of NaN values, unordered versions are included:
equ, neu, ltu, leu, gtu, geu. If both operands are numeric values (not
NaN), then these comparisons have the same result as their ordered counterparts. If either
operand is NaN, then the result of these comparisons is True.
num returns True if both operands are numeric values (not NaN), and nan returns
True if either operand is NaN.

Subnormal numbers:

By default, subnormal numbers are supported.
setp.ftz.{f16,f16x2} flushes subnormal inputs to sign-preserving zero.


PTX ISA Notes
Introduced in PTX ISA version 4.2.
setp.{bf16/bf16x2} introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_53 or higher.
setp.{bf16/bf16x2} requires sm_90 or higher.
Examples

setp.lt.and.f16x2  p|q,a,b,r;
@q  setp.eq.f16    p,i,n;

setp.gt.or.bf16x2  u|v,c,d,s;
@q  setp.eq.bf16   u,j,m;






9.7.7. Logic and Shift Instructionsï

The logic and shift instructions are fundamentally untyped, performing bit-wise operations on
operands of any type, provided the operands are of the same size. This permits bit-wise operations
on floating point values without having to define a union to access the bits. Instructions and,
or, xor, and not also operate on predicates.
The logical shift instructions are:

and
or
xor
not
cnot
lop3
shf
shl
shr



9.7.7.1. Logic and Shift Instructions: andï

and
Bitwise AND.
Syntax

and.type d, a, b;

.type = { .pred, .b16, .b32, .b64 };


Description
Compute the bit-wise and operation for the bits in a and b.
Semantics

d = a & b;


Notes
The size of the operands must match, but not necessarily the type.
Allowed types include predicate registers.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

and.b32  x,q,r;
and.b32  sign,fpvalue,0x80000000;





9.7.7.2. Logic and Shift Instructions: orï

or
Biwise OR.
Syntax

or.type d, a, b;

.type = { .pred, .b16, .b32, .b64 };


Description
Compute the bit-wise or operation for the bits in a and b.
Semantics

d = a | b;


Notes
The size of the operands must match, but not necessarily the type.
Allowed types include predicate registers.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

or.b32  mask mask,0x00010001
or.pred  p,q,r;





9.7.7.3. Logic and Shift Instructions: xorï

xor
Bitwise exclusive-OR (inequality).
Syntax

xor.type d, a, b;

.type = { .pred, .b16, .b32, .b64 };


Description
Compute the bit-wise exclusive-or operation for the bits in a and b.
Semantics

d = a ^ b;


Notes
The size of the operands must match, but not necessarily the type.
Allowed types include predicate registers.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

xor.b32  d,q,r;
xor.b16  d,x,0x0001;





9.7.7.4. Logic and Shift Instructions: notï

not
Bitwise negation; oneâs complement.
Syntax

not.type d, a;

.type = { .pred, .b16, .b32, .b64 };


Description
Invert the bits in a.
Semantics

d = ~a;


Notes
The size of the operands must match, but not necessarily the type.
Allowed types include predicates.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

not.b32  mask,mask;
not.pred  p,q;





9.7.7.5. Logic and Shift Instructions: cnotï

cnot
C/C++ style logical negation.
Syntax

cnot.type d, a;

.type = { .b16, .b32, .b64 };


Description
Compute the logical negation using C/C++ semantics.
Semantics

d = (a==0) ? 1 : 0;


Notes
The size of the operands must match, but not necessarily the type.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

cnot.b32 d,a;





9.7.7.6. Logic and Shift Instructions: lop3ï

lop3
Arbitrary logical operation on 3 inputs.
Syntax

lop3.b32 d, a, b, c, immLut;
lop3.BoolOp.b32 d|p, a, b, c, immLut, q;

.BoolOp   = { .or , .and };


Description
Compute bitwise logical operation on inputs a, b, c and store the result in destination
d.
Optionally, .BoolOp can be specified to compute the predicate result p by performing a
Boolean operation on the destination operand d with the predicate q in the following manner:

p = (d != 0) BoolOp q;


The sink symbol â_â may be used in place of the destination operand d when .BoolOp qualifier
is specified.
The logical operation is defined by a look-up table which, for 3 inputs, can be represented as an
8-bit value specified by operand immLut as described below. immLut is an integer constant
that can take values from 0 to 255, thereby allowing up to 256 distinct logical operations on inputs
a, b, c.
For a logical operation F(a, b, c) the value of immLut can be computed by applying the same
operation to three predefined constant values as follows:

ta = 0xF0;
tb = 0xCC;
tc = 0xAA;

immLut = F(ta, tb, tc);


Examples:

If F = (a & b & c);
immLut = 0xF0 & 0xCC & 0xAA = 0x80

If F = (a | b | c);
immLut = 0xF0 | 0xCC | 0xAA = 0xFE

If F = (a & b & ~c);
immLut = 0xF0 & 0xCC & (~0xAA) = 0x40

If F = ((a & b | c) ^ a);
immLut = (0xF0 & 0xCC | 0xAA) ^ 0xF0 = 0x1A


The following table illustrates computation of immLut for various logical operations:














ta
tb
tc
Oper 0 (False)
Oper 1 (ta & tb & tc)
Oper 2 (ta & tb & ~tc)
â¦
Oper 254 (ta | tb | tc)
Oper 255 (True)




0
0
0
0
0
0
â¦
0
1


0
0
1
0
0
0
1
1


0
1
0
0
0
0
1
1


0
1
1
0
0
0
1
1


1
0
0
0
0
0
1
1


1
0
1
0
0
0
1
1


1
1
0
0
0
1
1
1


1
1
1
0
1
0
1
1


immLut
0x0
0x80
0x40
â¦
0xFE
0xFF



Semantics

F = GetFunctionFromTable(immLut); // returns the function corresponding to immLut value
d = F(a, b, c);
if (BoolOp specified) {
    p = (d != 0) BoolOp q;
}


PTX ISA Notes
Introduced in PTX ISA version 4.3.
Support for .BoolOp qualifier introduced in PTX ISA version 8.2.
Target ISA Notes
Requires sm_50 or higher.
Qualifier .BoolOp requires sm_70 or higher.
Examples

lop3.b32       d, a, b, c, 0x40;
lop3.or.b32  d|p, a, b, c, 0x3f, q;
lop3.and.b32 _|p, a, b, c, 0x3f, q;





9.7.7.7. Logic and Shift Instructions: shfï

shf
Funnel shift.
Syntax

shf.l.mode.b32  d, a, b, c;  // left shift
shf.r.mode.b32  d, a, b, c;  // right shift

.mode = { .clamp, .wrap };


Description
Shift the 64-bit value formed by concatenating operands a and b left or right by the amount
specified by the unsigned 32-bit value in c. Operand b holds bits 63:32 and operand a
holds bits 31:0 of the 64-bit source value. The source is shifted left or right by the clamped
or wrapped value in c. For shf.l, the most-significant 32-bits of the result are written
into d; for shf.r, the least-significant 32-bits of the result are written into d.
Semantics

u32  n = (.mode == .clamp) ? min(c, 32) : c & 0x1f;
switch (shf.dir) {  // shift concatenation of [b, a]
    case shf.l:     // extract 32 msbs
           u32  d = (b << n)      | (a >> (32-n));
    case shf.r:     // extract 32 lsbs
           u32  d = (b << (32-n)) | (a >> n);
}


Notes
Use funnel shift for multi-word shift operations and for rotate operations. The shift amount is
limited to the range 0..32 in clamp mode and 0..31 in wrap mode, so shifting multi-word
values by distances greater than 32 requires first moving 32-bit words, then using shf to shift
the remaining 0..31 distance.
To shift data sizes greater than 64 bits to the right, use repeated shf.r instructions applied
to adjacent words, operating from least-significant word towards most-significant word. At each
step, a single word of the shifted result is computed. The most-significant word of the result is
computed using a shr.{u32,s32} instruction, which zero or sign fills based on the instruction
type.
To shift data sizes greater than 64 bits to the left, use repeated shf.l instructions applied to
adjacent words, operating from most-significant word towards least-significant word. At each step, a
single word of the shifted result is computed. The least-significant word of the result is computed
using a shl instruction.
Use funnel shift to perform 32-bit left or right rotate by supplying the same value for source
arguments a and b.
PTX ISA Notes
Introduced in PTX ISA version 3.1.
Target ISA Notes
Requires sm_32 or higher.
Example

shf.l.clamp.b32  r3,r1,r0,16;

// 128-bit left shift; n < 32
// [r7,r6,r5,r4] = [r3,r2,r1,r0] << n
shf.l.clamp.b32  r7,r2,r3,n;
shf.l.clamp.b32  r6,r1,r2,n;
shf.l.clamp.b32  r5,r0,r1,n;
shl.b32          r4,r0,n;

// 128-bit right shift, arithmetic; n < 32
// [r7,r6,r5,r4] = [r3,r2,r1,r0] >> n
shf.r.clamp.b32  r4,r0,r1,n;
shf.r.clamp.b32  r5,r1,r2,n;
shf.r.clamp.b32  r6,r2,r3,n;
shr.s32          r7,r3,n;     // result is sign-extended

shf.r.clamp.b32  r1,r0,r0,n;  // rotate right by n; n < 32
shf.l.clamp.b32  r1,r0,r0,n;  // rotate left by n; n < 32

// extract 32-bits from [r1,r0] starting at position n < 32
shf.r.clamp.b32  r0,r0,r1,n;





9.7.7.8. Logic and Shift Instructions: shlï

shl
Shift bits left, zero-fill on right.
Syntax

shl.type d, a, b;

.type = { .b16, .b32, .b64 };


Description
Shift a left by the amount specified by unsigned 32-bit value in b.
Semantics

d = a << b;


Notes
Shift amounts greater than the register width N are clamped to N.
The sizes of the destination and first source operand must match, but not necessarily the type. The
b operand must be a 32-bit value, regardless of the instruction type.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Example

shl.b32  q,a,2;





9.7.7.9. Logic and Shift Instructions: shrï

shr
Shift bits right, sign or zero-fill on left.
Syntax

shr.type d, a, b;

.type = { .b16, .b32, .b64,
          .u16, .u32, .u64,
          .s16, .s32, .s64 };


Description
Shift a right by the amount specified by unsigned 32-bit value in b. Signed shifts fill with
the sign bit, unsigned and untyped shifts fill with 0.
Semantics

d = a >> b;


Notes
Shift amounts greater than the register width N are clamped to N.
The sizes of the destination and first source operand must match, but not necessarily the type. The
b operand must be a 32-bit value, regardless of the instruction type.
Bit-size types are included for symmetry with shl.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Example

shr.u16  c,a,2;
shr.s32  i,i,1;
shr.b16  k,i,j;






9.7.8. Data Movement and Conversion Instructionsï

These instructions copy data from place to place, and from state space to state space, possibly
converting it from one format to another. mov, ld, ldu, and st operate on both
scalar and vector types. The isspacep instruction is provided to query whether a generic address
falls within a particular state space window. The cvta instruction converts addresses between
generic and const, global, local, or shared state spaces.
Instructions ld, st, suld, and sust support optional cache operations.
The Data Movement and Conversion Instructions are:

mov
shfl.sync
prmt
ld
ldu
st
st.async
multimem.ld_reduce, multimem.st, multimem.red
prefetch, prefetchu
isspacep
cvta
cvt
cvt.pack
cp.async
cp.async.commit_group
cp.async.wait_group, cp.async.wait_all
cp.async.bulk
cp.reduce.async.bulk
cp.async.bulk.prefetch
cp.async.bulk.tensor
cp.reduce.async.bulk.tensor
cp.async.bulk.prefetch.tensor
cp.async.bulk.commit_group
cp.async.bulk.wait_group
tensormap.replace



9.7.8.1. Cache Operatorsï

PTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache
operators require a target architecture of sm_20 or higher.
Cache operators on load or store instructions are treated as performance hints only. The use of a
cache operator on an ld or st instruction does not change the memory consistency behavior of
the program.
For sm_20 and higher, the cache operators have the following definitions and behavior.


Table 27 Cache Operators for Memory Load Instructionsï







Operator
Meaning




.ca

Cache at all levels, likely to be accessed again.
The default load instruction cache operation is ld.ca, which allocates cache lines in all
levels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but
multiple L1 caches are not coherent for global data. If one thread stores to global memory
via one L1 cache, and a second thread loads that address via a second L1 cache with ld.ca,
the second thread may get stale L1 cache data, rather than the data stored by the first thread.
The driver must invalidate global L1 cache lines between dependent grids of parallel threads.
Stores by the first grid program are then correctly fetched by the second grid program issuing
default ld.ca loads cached in L1.



.cg

Cache at global level (cache in L2 and below, not L1).
Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2
cache.



.cs

Cache streaming, likely to be accessed once.
The ld.cs load cached streaming operation allocates global lines with evict-first policy in
L1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or
twice. When ld.cs is applied to a Local window address, it performs the ld.lu
operation.



.lu

Last use.
The compiler/programmer may use ld.lu when restoring spilled registers and popping function
stack frames to avoid needless write-backs of lines that will not be used again. The ld.lu
instruction performs a load cached streaming operation (ld.cs) on global addresses.



.cv

Donât cache and fetch again (consider cached system memory lines stale, fetch again).
The ld.cv load operation applied to a global System Memory address invalidates (discards) a
matching L2 line and re-fetches the line on each new load.






Table 28 Cache Operators for Memory Store Instructionsï







Operator
Meaning




.wb

Cache write-back all coherent levels.
The default store instruction cache operation is st.wb, which writes back cache lines of
coherent cache levels with normal eviction policy.
If one thread stores to global memory, bypassing its L1 cache, and a second thread in a
different SM later loads from that address via a different L1 cache with ld.ca, the second
thread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored
by the first thread.
The driver must invalidate global L1 cache lines between dependent grids of thread arrays.
Stores by the first grid program are then correctly missed in L1 and fetched by the second grid
program issuing default ld.ca loads.



.cg

Cache at global level (cache in L2 and below, not L1).
Use st.cg to cache global store data only globally, bypassing the L1 cache, and cache only
in the L2 cache.



.cs

Cache streaming, likely to be accessed once.
The st.cs store cached-streaming operation allocates cache lines with evict-first policy to
limit cache pollution by streaming output data.



.wt

Cache write-through (to system memory).
The st.wt store write-through operation applied to a global System Memory address writes
through the L2 cache.







9.7.8.2. Cache Eviction Priority Hintsï

PTX ISA version 7.4 adds optional cache eviction priority hints on load and store
instructions. Cache eviction priority requires target architecture sm_70 or higher.
Cache eviction priority on load or store instructions is treated as a performance hint. It is
supported for .global state space and generic addresses where the address points to .global
state space.


Table 29 Cache Eviction Priority Hints for Memory Load and Store Instructionsï







Cache Eviction Priority
Meaning




evict_normal
Cache data with normal eviction priority. This is the default eviction priority.


evict_first
Data cached with this priority will be first in the eviction priority order and
will likely be evicted when cache eviction is required. This priority is suitable
for streaming data.


evict_last
Data cached with this priority will be last in the eviction priority order and will
likely be evicted only after other data with evict_normal or evict_first
eviction priotity is already evicted. This priority is suitable for data that
should remain persistent in cache.


evict_unchanged
Do not change eviction priority order as part of this operation.


no_allocate
Do not allocate data to cache. This priority is suitable for streaming data.






9.7.8.3. Data Movement and Conversion Instructions: movï

mov
Set a register variable with the value of a register variable or an immediate value. Take the
non-generic address of a variable in global, local, or shared state space.
Syntax

mov.type  d, a;
mov.type  d, sreg;
mov.type  d, avar;       // get address of variable
mov.type  d, avar+imm;   // get address of variable with offset
mov.u32   d, fname;      // get address of device function
mov.u64   d, fname;      // get address of device function
mov.u32   d, kernel;     // get address of entry function
mov.u64   d, kernel;     // get address of entry function

.type = { .pred,
          .b16, .b32, .b64,
          .u16, .u32, .u64,
          .s16, .s32, .s64,
                .f32, .f64 };


Description
Write register d with the value of a.
Operand a may be a register, special register, variable with optional offset in an addressable
memory space, or function name.
For variables declared in .const, .global, .local, and .shared state spaces, mov
places the non-generic address of the variable (i.e., the address of the variable in its state
space) into the destination register. The generic address of a variable in const, global,
local, or shared state space may be generated by first taking the address within the state
space with mov and then converting it to a generic address using the cvta instruction;
alternately, the generic address of a variable declared in const, global, local, or
shared state space may be taken directly using the cvta instruction.
Note that if the address of a device function parameter is moved to a register, the parameter will
be copied onto the stack and the address will be in the local state space.
Semantics

d = a;
d = sreg;
d = &avar;        // address is non-generic; i.e., within the variable's declared state space
d = &avar+imm;


Notes

Although only predicate and bit-size types are required, we include the arithmetic types for the
programmerâs convenience: their use enhances program readability and allows additional type
checking.
When moving address of a kernel or a device function, only .u32 or .u64 instruction types
are allowed. However, if a signed type is used, it is not treated as a compilation error. The
compiler issues a warning in this case.

PTX ISA Notes
Introduced in PTX ISA version 1.0.
Taking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function
addresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the CUDA
Dynamic Parallelism Programming Guide for details.
Target ISA Notes
mov.f64 requires sm_13 or higher.
Taking the address of kernel entry functions requires sm_35 or higher.
Examples

mov.f32  d,a;
mov.u16  u,v;
mov.f32  k,0.1;
mov.u32  ptr, A;        // move address of A into ptr
mov.u32  ptr, A[5];     // move address of A[5] into ptr
mov.u32  ptr, A+20;     // move address with offset into ptr
mov.u32  addr, myFunc;  // get address of device function 'myFunc'
mov.u64  kptr, main;    // get address of entry function 'main'





9.7.8.4. Data Movement and Conversion Instructions: movï

mov
Move vector-to-scalar (pack) or scalar-to-vector (unpack).
Syntax

mov.type  d, a;

.type = { .b16, .b32, .b64, .b128 };


Description
Write scalar register d with the packed value of vector register a, or write vector register
d with the unpacked values from scalar register a.
When destination operand d is a vector register, the sink symbol '_' may be used for one or
more elements provided that at least one element is a scalar register.
For bit-size types, mov may be used to pack vector elements into a scalar register or unpack
sub-fields of a scalar register into a vector. Both the overall size of the vector and the size of
the scalar must match the size of the instruction type.
Semantics

// pack two 8-bit elements into .b16
d = a.x | (a.y << 8)
// pack four 8-bit elements into .b32
d = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)
// pack two 16-bit elements into .b32
d = a.x | (a.y << 16)
// pack four 16-bit elements into .b64
d = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)
// pack two 32-bit elements into .b64
d = a.x | (a.y << 32)
// pack four 32-bit elements into .b128
d = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)
// pack two 64-bit elements into .b128
d = a.x | (a.y << 64)

// unpack 8-bit elements from .b16
{ d.x, d.y } = { a[0..7], a[8..15] }
// unpack 8-bit elements from .b32
{ d.x, d.y, d.z, d.w }
        { a[0..7], a[8..15], a[16..23], a[24..31] }

// unpack 16-bit elements from .b32
{ d.x, d.y }  = { a[0..15], a[16..31] }
// unpack 16-bit elements from .b64
{ d.x, d.y, d.z, d.w } =
        { a[0..15], a[16..31], a[32..47], a[48..63] }

// unpack 32-bit elements from .b64
{ d.x, d.y } = { a[0..31], a[32..63] }

// unpack 32-bit elements from .b128
{ d.x, d.y, d.z, d.w } =
        { a[0..31], a[32..63], a[64..95], a[96..127] }
// unpack 64-bit elements from .b128
{ d.x, d.y } = { a[0..63], a[64..127] }


PTX ISA Notes
Introduced in PTX ISA version 1.0.
Support for .b128 type introduced in PTX ISA version 8.3.
Target ISA Notes
Supported on all target architectures.
Support for .b128 type requires sm_70 or higher.
Examples

mov.b32 %r1,{a,b};      // a,b have type .u16
mov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32
mov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8
mov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8
mov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32
mov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64
mov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64





9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated)ï

shfl (deprecated)
Register data shuffle within threads of a warp.
Syntax

shfl.mode.b32  d[|p], a, b, c;

.mode = { .up, .down, .bfly, .idx };


Deprecation Note
The shfl instruction without a .sync qualifier is deprecated in PTX ISA version 6.0.

Support for this instruction with .target lower than sm_70 may be removed in a future PTX ISA version.

Removal Note
Support for shfl instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .targetsm_70 or higher.
Description
Exchange register data between threads of a warp.
Each thread in the currently executing warp will compute a source lane index j based on input
operands b and c and the mode. If the computed source lane index j is in range, the
thread will copy the input operand a from lane j into its own destination register d;
otherwise, the thread will simply copy its own input a to destination d. The optional
destination predicate p is set to True if the computed source lane is in range, and
otherwise set to False.
Note that an out of range value of b may still result in a valid computed source lane index
j. In this case, a data transfer occurs and the destination predicate p is True.
Note that results are undefined in divergent control flow within a warp, if an active thread sources
a register from an inactive thread.
Operand b specifies a source lane or source lane offset, depending on the mode.
Operand c contains two packed values specifying a mask for logically splitting warps into
sub-segments and an upper bound for clamping the source lane index.
Semantics

lane[4:0]  = [Thread].laneid;  // position of thread in warp
bval[4:0] = b[4:0];            // source lane or lane offset (0..31)
cval[4:0] = c[4:0];            // clamp value
mask[4:0] = c[12:8];

// get value of source register a if thread is active and
// guard predicate true, else unpredictable
if (isActive(Thread) && isGuardPredicateTrue(Thread)) {
    SourceA[lane] = a;
} else {
    // Value of SourceA[lane] is unpredictable for
    // inactive/predicated-off threads in warp
}
maxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);
minLane = (lane[4:0] & mask[4:0]);

switch (.mode) {
    case .up:    j = lane - bval; pval = (j >= maxLane); break;
    case .down:  j = lane + bval; pval = (j <= maxLane); break;
    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;
    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);
                                 pval = (j <= maxLane); break;
}
if (!pval) j = lane;  // copy from own lane
d = SourceA[j];       // copy input a from lane j
if (dest predicate selected)
    p = pval;


PTX ISA Notes
Introduced in PTX ISA version 3.0.
Deprecated in PTX ISA version 6.0 in favor of shfl.sync.
Not supported in PTX ISA version 6.4 for .target sm_70 or higher.
Target ISA Notes
shfl requires sm_30 or higher.
shfl is not supported on sm_70 or higher starting PTX ISA version 6.4.
Examples

    // Warp-level INCLUSIVE PLUS SCAN:
    //
    // Assumes input in following registers:
    //     - Rx  = sequence value for this thread
    //
    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;
@p  add.f32      Rx, Ry, Rx;
    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;
@p  add.f32      Rx, Ry, Rx;
    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;
@p  add.f32      Rx, Ry, Rx;
    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;
@p  add.f32      Rx, Ry, Rx;
    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;
@p  add.f32      Rx, Ry, Rx;


    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:
    //
    // Assumes input in following registers:
    //     - Rx  = sequence value for this thread
    //
    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;
@p  add.f32        Rx, Ry, Rx;
    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;
@p  add.f32        Rx, Ry, Rx;
    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;
@p  add.f32        Rx, Ry, Rx;
    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;
@p  add.f32        Rx, Ry, Rx;
    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;
@p  add.f32        Rx, Ry, Rx;


    // BUTTERFLY REDUCTION:
    //
    // Assumes input in following registers:
    //     - Rx  = sequence value for this thread
    //
    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest
    add.f32        Rx, Ry, Rx;
    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;
    add.f32        Rx, Ry, Rx;
    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;
    add.f32        Rx, Ry, Rx;
    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;
    add.f32        Rx, Ry, Rx;
    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;
    add.f32        Rx, Ry, Rx;
    //
    // All threads now hold sum in Rx





9.7.8.6. Data Movement and Conversion Instructions: shfl.syncï

shfl.sync
Register data shuffle within threads of a warp.
Syntax

shfl.sync.mode.b32  d[|p], a, b, c, membermask;

.mode = { .up, .down, .bfly, .idx };


Description
Exchange register data between threads of a warp.
shfl.sync will cause executing thread to wait until all non-exited threads corresponding to
membermask have executed shfl.sync with the same qualifiers and same membermask value
before resuming execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating
in barrier where the bit position corresponds to threadâs laneid.
shfl.sync exchanges register data between threads in membermask.
Each thread in the currently executing warp will compute a source lane index j based on input
operands b and c and the mode. If the computed source lane index j is in range, the
thread will copy the input operand a from lane j into its own destination register d;
otherwise, the thread will simply copy its own input a to destination d. The optional
destination predicate p is set to True if the computed source lane is in range, and
otherwise set to False.
Note that an out of range value of b may still result in a valid computed source lane index
j. In this case, a data transfer occurs and the destination predicate p is True.
Note that results are undefined if a thread sources a register from an inactive thread or a thread
that is not in membermask.
Operand b specifies a source lane or source lane offset, depending on the mode.
Operand c contains two packed values specifying a mask for logically splitting warps into
sub-segments and an upper bound for clamping the source lane index.
The behavior of shfl.sync is undefined if the executing thread is not in the membermask.

Note
For .target sm_6x or below, all threads in membermask must execute the same shfl.sync
instruction in convergence, and only threads belonging to some membermask can be active when
the shfl.sync instruction is executed. Otherwise, the behavior is undefined.

Semantics

// wait for all threads in membermask to arrive
wait_for_specified_threads(membermask);

lane[4:0]  = [Thread].laneid;  // position of thread in warp
bval[4:0] = b[4:0];            // source lane or lane offset (0..31)
cval[4:0] = c[4:0];            // clamp value
segmask[4:0] = c[12:8];

// get value of source register a if thread is active and
// guard predicate true, else unpredictable
if (isActive(Thread) && isGuardPredicateTrue(Thread)) {
    SourceA[lane] = a;
} else {
    // Value of SourceA[lane] is unpredictable for
    // inactive/predicated-off threads in warp
}
maxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);
minLane = (lane[4:0] & segmask[4:0]);

switch (.mode) {
    case .up:    j = lane - bval; pval = (j >= maxLane); break;
    case .down:  j = lane + bval; pval = (j <= maxLane); break;
    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;
    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);
                                 pval = (j <= maxLane); break;
}
if (!pval) j = lane;  // copy from own lane
d = SourceA[j];       // copy input a from lane j
if (dest predicate selected)
    p = pval;


PTX ISA Notes
Introduced in PTX ISA version 6.0.
Target ISA Notes
Requires sm_30 or higher.
Examples

shfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff;





9.7.8.7. Data Movement and Conversion Instructions: prmtï

prmt
Permute bytes from register pair.
Syntax

prmt.b32{.mode}  d, a, b, c;

.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 };


Description
Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination
register.
In the generic form (no mode specified), the permute control consists of four 4-bit selection
values. The bytes in the two source registers are numbered from 0 to 7: {b, a} = {{b7, b6, b5,
b4}, {b3, b2, b1, b0}}. For each byte in the target register, a 4-bit selection value is defined.
The 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the
target position. The msb defines if the byte value should be copied, or if the sign (msb of the
byte) should be replicated over all 8 bits of the target position (sign extend of the byte value);
msb=0 means copy the literal value; msb=1 means replicate the sign. Note that the sign
extension is only performed as part of generic form.
Thus, the four 4-bit values fully specify an arbitrary byte permute, as a 16b permute code.










default mode

d.b3
source select


d.b2
source select


d.b1
source select


d.b0
source select





index
c[15:12]
c[11:8]
c[7:4]
c[3:0]



The more specialized form of the permute control uses the two lsbâs of operand c (which is
typically an address pointer) to control the byte extraction.











mode

selector
c[1:0]


d.b3
source


d.b2
source


d.b1
source


d.b0
source





f4e (forward 4 extract)
0
3
2
1
0



1
4
3
2
1



2
5
4
3
2



3
6
5
4
3


b4e (backward 4 extract)
0
5
6
7
0



1
6
7
0
1



2
7
0
1
2



3
0
1
2
3


rc8 (replicate 8)
0
0
0
0
0



1
1
1
1
1



2
2
2
2
2



3
3
3
3
3


ecl (edge clamp left)
0
3
2
1
0



1
3
2
1
1



2
3
2
2
2



3
3
3
3
3


ecr (edge clamp right)
0
0
0
0
0



1
1
1
1
0



2
2
2
1
0



3
3
2
1
0


rc16 (replicate 16)
0
1
0
1
0



1
3
2
3
2



2
1
0
1
0



3
3
2
3
2



Semantics

tmp64 = (b<<32) | a;  // create 8 byte source

if ( ! mode ) {
   ctl[0] = (c >>  0) & 0xf;
   ctl[1] = (c >>  4) & 0xf;
   ctl[2] = (c >>  8) & 0xf;
   ctl[3] = (c >> 12) & 0xf;
} else {
   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;
}

tmp[07:00] = ReadByte( mode, ctl[0], tmp64 );
tmp[15:08] = ReadByte( mode, ctl[1], tmp64 );
tmp[23:16] = ReadByte( mode, ctl[2], tmp64 );
tmp[31:24] = ReadByte( mode, ctl[3], tmp64 );


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
prmt requires sm_20 or higher.
Examples

prmt.b32      r1, r2, r3, r4;
prmt.b32.f4e  r1, r2, r3, r4;





9.7.8.8. Data Movement and Conversion Instructions: ldï

ld
Load a register variable from an addressable state space variable.
Syntax

ld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};

ld{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};

ld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];

ld.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};

ld.acquire.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};

ld.mmio.relaxed.sys{.global}.type  d, [a];

.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };
.cop =                      { .ca, .cg, .cs, .lu, .cv };
.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,
                              .L1::evict_first, .L1::evict_last, .L1::no_allocate };
.level::cache_hint =        { .L2::cache_hint };
.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }
.scope =                    { .cta, .cluster, .gpu, .sys };
.vec =                      { .v2, .v4 };
.type =                     { .b8, .b16, .b32, .b64, .b128,
                              .u8, .u16, .u32, .u64,
                              .s8, .s16, .s32, .s64,
                              .f32, .f64 };


Description
Load register variable d from the location specified by the source address operand a in
specified state space. If no state space is given, perform the load using Generic Addressing.
If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default.
Supported addressing modes for operand a and alignment requirements are described in Addresses
as Operands
If no sub-qualifier is specified with .param state space, thenâ¯:

::func is assumed when access is inside a device function.
::entry is assumed when accessing kernel function parameters from entry function. Otherwise, when
accessing device function parameters or any other .param variables from entry function ::func
is assumed by default.

For ld.param::entry instruction, operand a must be a kernel parameter address, otherwise behavior
is undefined. For ld.param::func instruction, operand a must be a device function parameter address,
otherwise behavior is undefined.
Instruction ld.param{::func} used for reading value returned from device function call cannot be
predicated. See Parameter State Space and
Function Declarations and Definitions for descriptions
of the proper use of ld.param.
The .relaxed and .acquire qualifiers indicate memory synchronization as described in the
Memory Consistency Model. The .scope qualifier
indicates the set of threads with which an ld.relaxed or ld.acquire instruction can directly
synchronize1. The .weak qualifier indicates a memory instruction with no synchronization.
The effects of this instruction become visible to other threads only when synchronization is established
by other means.
The semantic details of .mmio qualifier are described in the Memory Consistency Model. Only .sys thread scope is valid for ld.mmio operation. The
qualifiers .mmio and .relaxed must be specified together.
The .weak, .volatile, .relaxed and .acquire qualifiers are mutually exclusive. When
none of these is specified, the .weak qualifier is assumed by default.
An ld.volatile operation is always performed and it will not be reordered with respect to other
volatile operations to the same memory location. volatile and non-volatile load operations
to the same memory location may be reordered. ld.volatile has the same memory synchronization
semantics as ld.relaxed.sys.
The qualifiers .volatile, .relaxed and .acquire may be used only with .global and
.shared spaces and with generic addressing, where the address points to .global or
.shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio
may be used only with .global space and with generic addressing, where the address points to
.global space.
The optional qualifier .unified must be specified on operand a if a is the address of a
variable declared with .unified attribute as described in Variable and Function Attribute
Directive: .attribute.
The qualifier .level::eviction_priority specifies the eviction policy that will be used during
memory access.
The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size
into the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B,
128B, 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes
respectively.
The qualifier .level::prefetch_size may only be used with .global state space and with
generic addressing where the address points to .global state space. If the generic address does
not fall within the address window of the global memory, then the prefetching behavior is undefined.
The .level::prefetch_size qualifier is treated as a performance hint only.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
The qualifiers .unified and .level::cache_hint are only supported for .global state
space and for generic addressing where the address points to the .global state space.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
1 This synchronization is further extended to other threads through the transitive nature of
causality order, as described in the memory consistency model.
Semantics

d = a;             // named variable a
d = *(&a+immOff)   // variable-plus-offset
d = *a;            // register
d = *(a+immOff);   // register-plus-offset
d = *(immAddr);    // immediate address


Notes
Destination d must be in the .reg state space.
A destination register wider than the specified type may be used. The value loaded is sign-extended
to the destination register width for signed integers, and is zero-extended to the destination
register width for unsigned and bit-size types. See
Table 25
for a description of these relaxed type-checking rules.
.f16 data may be loaded using ld.b16, and then converted to .f32 or .f64 using
cvt or can be used in half precision floating point instructions.
.f16x2 data may be loaded using ld.b32 and then used in half precision floating point
instructions.
PTX ISA Notes
ld introduced in PTX ISA version 1.0. ld.volatile introduced in PTX ISA version 1.1.
Generic addressing and cache operations introduced in PTX ISA version 2.0.
Support for scope qualifier, .relaxed, .acquire, .weak qualifiers introduced in PTX ISA
version 6.0.
Support for generic addressing of .const space added in PTX ISA version 3.1.
Support for .level::eviction_priority, .level::prefetch_size and .level::cache_hint
qualifiers introduced in PTX ISA version 7.4.
Support for .cluster scope qualifier introduced in PTX ISA version 7.8.
Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8.
Support for .unified qualifier introduced in PTX ISA version 8.0.
Support for .mmio qualifier introduced in PTX ISA version 8.2.
Support for ::entry and ::func sub-qualifiers on .param space introduced in PTX ISA
version 8.3.
Support for .b128 type introduced in PTX ISA version 8.3.
Support for .sys scope with .b128 type introduced in PTX ISA version 8.4.
Target ISA Notes
ld.f64 requires sm_13 or higher.
Support for scope qualifier, .relaxed, .acquire, .weak qualifiers require sm_70 or
higher.
Generic addressing requires sm_20 or higher.
Cache operations require sm_20 or higher.
Support for .level::eviction_priority qualifier requires sm_70 or higher.
Support for .level::prefetch_size qualifier requires sm_75 or higher.
Support for .L2::256B and .L2::cache_hint qualifiers requires sm_80 or higher.
Support for .cluster scope qualifier requires sm_90 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Support for .unified qualifier requires sm_90 or higher.
Support for .mmio qualifier requires sm_70 or higher.
Support for .b128 type requires sm_70 or higher.
Examples

ld.global.f32    d,[a];
ld.shared.v4.b32 Q,[p];
ld.const.s32     d,[p+4];
ld.local.b32     x,[p+-8]; // negative offset
ld.local.b64     x,[240];  // immediate address

ld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg
cvt.f32.f16      %r,%r;    // up-convert f16 data to f32

ld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg
ld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg
add.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data
ld.global.relaxed.gpu.u32 %r0, [gbl];
ld.shared.acquire.gpu.u32 %r1, [sh];
ld.global.relaxed.cluster.u32 %r2, [gbl];
ld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];
ld.shared::cluster.u32 %r3, [sh + 8];
ld.global.mmio.relaxed.sys.u32 %r3, [gbl];

ld.global.f32    d,[ugbl].unified;
ld.b32           %r0, [%r1].unified;

ld.global.L1::evict_last.u32  d, [p];

ld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2
ld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2
ld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2

createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;
ld.global.L2::cache_hint.b64  x, [p], cache-policy;
ld.param::entry.b32 %rp1, [kparam1];

ld.global.b128   %r0, [gbl];   // 128-bit load





9.7.8.9. Data Movement and Conversion Instructions: ld.global.ncï

ld.global.nc
Load a register variable from global state space via non-coherent cache.
Syntax

ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};
ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};

ld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};
ld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};

.cop  =                     { .ca, .cg, .cs };     // cache operation
.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,
                              .L1::evict_first, .L1::evict_last, .L1::no_allocate};
.level::cache_hint =        { .L2::cache_hint };
.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }
.vec  =                     { .v2, .v4 };
.type =                     { .b8, .b16, .b32, .b64, .b128,
                              .u8, .u16, .u32, .u64,
                              .s8, .s16, .s32, .s64,
                              .f32, .f64 };


Description
Load register variable d from the location specified by the source address operand a in the
global state space, and optionally cache in non-coherent read-only cache.

Note
On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than
the global memory cache. For applications with sufficient parallelism to cover the longer
latency, ld.global.nc should offer better performance than ld.global on such
architectures.

The address operand a may contain a generic address pointing to the
.global state space. Supported addressing modes for operand a and alignment requirements are
described in Addresses as Operands
The qualifier .level::eviction_priority specifies the eviction policy that will be used during
memory access.
The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size
into the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B,
128B, 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes
respectively.
The .level::prefetch_size qualifier is treated as a performance hint only.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
Semantics

d = a;             // named variable a
d = *(&a+immOff)   // variable-plus-offset
d = *a;            // register
d = *(a+immOff);   // register-plus-offset
d = *(immAddr);    // immediate address


Notes
Destination d must be in the .reg state space.
A destination register wider than the specified type may be used. The value loaded is sign-extended
to the destination register width for signed integers, and is zero-extended to the destination
register width for unsigned and bit-size types.
.f16 data may be loaded using ld.b16, and then converted to .f32 or .f64 using cvt.
PTX ISA Notes
Introduced in PTX ISA version 3.1.
Support for .level::eviction_priority, .level::prefetch_size and .level::cache_hint
qualifiers introduced in PTX ISA version 7.4.
Support for .b128 type introduced in PTX ISA version 8.3.
Target ISA Notes
Requires sm_32 or higher.
Support for .level::eviction_priority qualifier requires sm_70 or higher.
Support for .level::prefetch_size qualifier requires sm_75 or higher.
Support for .level::cache_hint qualifier requires sm_80 or higher.
Support for .b128 type requires sm_70 or higher.
Examples

ld.global.nc.f32           d, [a];
ld.gloal.nc.L1::evict_last.u32 d, [a];

createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;
ld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;

ld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2
ld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2

ld.global.nc.b128             d,  [a];





9.7.8.10. Data Movement and Conversion Instructions: lduï

ldu
Load read-only data from an address that is common across threads in the warp.
Syntax

ldu{.ss}.type      d, [a];       // load from address
ldu{.ss}.vec.type  d, [a];       // vec load from address

.ss   = { .global };             // state space
.vec  = { .v2, .v4 };
.type = { .b8, .b16, .b32, .b64, .b128,
          .u8, .u16, .u32, .u64,
          .s8, .s16, .s32, .s64,
                     .f32, .f64 };


Description
Load read-only data into register variable d from the location specified by the source address
operand a in the global state space, where the address is guaranteed to be the same across all
threads in the warp. If no state space is given, perform the load using Generic Addressing.
Supported addressing modes for operand a and alignment requirements are described in Addresses
as Operands
Semantics

d = a;             // named variable a
d = *(&a+immOff)   // variable-plus-offset
d = *a;            // register
d = *(a+immOff);   // register-plus-offset
d = *(immAddr);    // immediate address


Notes
Destination d must be in the .reg state space.
A destination register wider than the specified type may be used. The value loaded is sign-extended
to the destination register width for signed integers, and is zero-extended to the destination
register width for unsigned and bit-size types. See
Table 25
for a description of these relaxed type-checking rules.
.f16 data may be loaded using ldu.b16, and then converted to .f32 or .f64 using
cvtor can be used in half precision floating point instructions.
.f16x2 data may be loaded using ldu.b32 and then used in half precision floating point
instructions.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Support for .b128 type introduced in PTX ISA version 8.3.
Target ISA Notes
ldu.f64 requires sm_13 or higher.
Support for .b128 type requires sm_70 or higher.
Examples

ldu.global.f32    d,[a];
ldu.global.b32    d,[p+4];
ldu.global.v4.f32 Q,[p];
ldu.global.b128   d,[a];





9.7.8.11. Data Movement and Conversion Instructions: stï

st
Store data to an addressable state space variable.
Syntax

st{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};
st{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type
                                                      [a], b{, cache-policy};
st.volatile{.ss}{.vec}.type                           [a], b;
st.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type
                                                      [a], b{, cache-policy};
st.release.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type
                                                      [a], b{, cache-policy};
st.mmio.relaxed.sys{.global}.type         [a], b;

.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };
.level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,
                              .L1::evict_first, .L1::evict_last, .L1::no_allocate };
.level::cache_hint =        { .L2::cache_hint };
.cop =                      { .wb, .cg, .cs, .wt };
.sem =                      { .relaxed, .release };
.scope =                    { .cta, .cluster, .gpu, .sys };
.vec =                      { .v2, .v4 };
.type =                     { .b8, .b16, .b32, .b64, .b128,
                              .u8, .u16, .u32, .u64,
                              .s8, .s16, .s32, .s64,
                              .f32, .f64 };


Description
Store the value of operand b in the location specified by the destination address
operand a in specified state space. If no state space is given, perform the store using Generic
Addressing. Stores to const memory are illegal.
If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default.
Supported addressing modes for operand a and alignment requirements are described in Addresses
as Operands
If .param is specified without any sub-qualifiers then it defaults to .param::func.
Instruction st.param{::func} used for passing arguments to device function cannot be predicated.
See Parameter State Space and Function Declarations and
Definitions for descriptions of the proper use
of st.param.
The qualifiers .relaxed and .release indicate memory synchronization as described in the
Memory Consistency Model. The .scope qualifier
indicates the set of threads with which an st.relaxed or st.release instruction can directly
synchronize1. The .weak qualifier indicates a memory instruction with no synchronization.
The effects of this instruction become visible to other threads only when synchronization is established
by other means.
The semantic details of .mmio qualifier are described in the Memory Consistency Model. Only .sys thread scope is valid for st.mmio operation. The
qualifiers .mmio and .relaxed must be specified together.
The .weak, .volatile, .relaxed and .release qualifiers are mutually exclusive. When
none of these is specified, the .weak qualifier is assumed by default.
An st.volatile operation is always performed and it will not be reordered with respect to other
volatile operations to the same memory location. st.volatile has the same memory
synchronization semantics as st.relaxed.sys.
The qualifiers .volatile, .relaxed and .release may be used only with .global and
.shared spaces and with generic addressing, where the address points to .global or
.shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio
may be used only with .global space and with generic addressing, where the address points to
.global space.
The qualifier .level::eviction_priority specifies the eviction policy that will be used during
memory access.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
The qualifier .level::cache_hint is only supported for .global state space and for generic
addressing where the address points to the .global state space.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
1 This synchronization is further extended to other threads through the transitive nature of
causality order, as described in the memory consistency model.
Semantics

d = a;                // named variable d
*(&a+immOffset) = b;            // variable-plus-offset
*a = b;               // register
*(a+immOffset) = b;   // register-plus-offset
*(immAddr) = b;       // immediate address


Notes
Operand b must be in the .reg state space.
A source register wider than the specified type may be used. The lower n bits corresponding to
the instruction-type width are stored to memory. See
Table 24
for a description of these relaxed type-checking rules.
.f16 data resulting from a cvt instruction may be stored using st.b16.
.f16x2 data may be stored using st.b32.
PTX ISA Notes
st introduced in PTX ISA version 1.0. st.volatile introduced in PTX ISA version 1.1.
Generic addressing and cache operations introduced in PTX ISA version 2.0.
Support for scope qualifier, .relaxed, .release, .weak qualifiers introduced in PTX ISA
version 6.0.
Support for .level::eviction_priority and .level::cache_hint qualifiers introduced in PTX
ISA version 7.4.
Support for .cluster scope qualifier introduced in PTX ISA version 7.8.
Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8.
Support for .mmio qualifier introduced in PTX ISA version 8.2.
Support for ::func sub-qualifier on .param space introduced in PTX ISA version 8.3.
Support for .b128 type introduced in PTX ISA version 8.3.
Support for .sys scope with .b128 type introduced in PTX ISA version 8.4.
Target ISA Notes
st.f64 requires sm_13 or higher.
Support for scope qualifier, .relaxed, .release, .weak qualifiers require sm_70 or
higher.
Generic addressing requires sm_20 or higher.
Cache operations require sm_20 or higher.
Support for .level::eviction_priority qualifier requires sm_70 or higher.
Support for .level::cache_hint qualifier requires sm_80 or higher.
Support for .cluster scope qualifier requires sm_90 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Support for .mmio qualifier requires sm_70 or higher.
Support for .b128 type requires sm_70 or higher.
Examples

st.global.f32    [a],b;
st.local.b32     [q+4],a;
st.global.v4.s32 [p],Q;
st.local.b32     [q+-8],a; // negative offset
st.local.s32     [100],r7; // immediate address

cvt.f16.f32      %r,%r;    // %r is 32-bit register
st.b16           [fs],%r;  // store lower
st.global.relaxed.sys.u32 [gbl], %r0;
st.shared.release.cta.u32 [sh], %r1;
st.global.relaxed.cluster.u32 [gbl], %r2;
st.shared::cta.release.cta.u32 [sh + 4], %r1;
st.shared::cluster.u32 [sh + 8], %r1;
st.global.mmio.relaxed.sys.u32 [gbl], %r1;

st.global.L1::no_allocate.f32 [p], a;

createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;
st.global.L2::cache_hint.b32  [a], b, cache-policy;

st.param::func.b64 [param1], %rp1;

st.global.b128  [a], b;  // 128-bit store





9.7.8.12. Data Movement and Conversion Instructions: st.asyncï

st.async
Asynchronous store operation on shared memory.
Syntax

st.async{.weak}{.ss}{.completion_mechanism}{.vec}.type [a], b, [mbar];

.ss   =                 { .shared::cluster };
.type =                 { .b32, .b64,
                          .u32, .u64,
                          .s32, .s64,
                          .f32, .f64 };
.vec  =                 { .v2, .v4 };
.completion_mechanism = { .mbarrier::complete_tx::bytes };


Description
st.async is a non-blocking instruction which initiates an asynchronous store operation that
stores the value specified by source operand b to the destination memory location
specified by operand a.
The modifier .completion_mechanism specifies that upon completion of the asynchronous operation,
complete-tx
operation, with completeCount argument equal to amount of data stored in bytes, will be
performed on the mbarrier object specified by the operand mbar.
Operand a represents destination address and must be a register or of the form register +
immOff as described in Addresses as Operands.
The shared memory addresses of destination operand a and the mbarrier object mbar, must
meet all of the following conditions:

They belong to the same CTA.
They are different to the CTA of the executing thread but must be within the same cluster.

Otherwise, the behavior is undefined.
The state space of the address {.ss}, if specified, is applicable to both operands a and
mbar. If not specified, then Generic Addressing is used for
both a and mbar. If the generic addresses specified do not fall within the address window of
.shared::cluster state space, then the behaviour is undefined.
The store operation in st.async is treated as a weak memory operation and the complete_tx
operation on the mbarrier has .release semantics at the .cluster scope as described in the
Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 8.1.
Target ISA Notes
Requires sm_90 or higher.
Examples

st.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr]





9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.redï

The multimem.* operations operate on multimem addresses and accesses all of the multiple memory
locations which the multimem address points to.
Multimem addresses can only be accessed only by multimem.* operations. Accessing a multimem address
with ld, st or any other memory operations results in undefined behavior.
Refer to CUDA programming guide for creation and management of the multimem addresses.
multimem.ld_reduce, multimem.st, multimem.red
Perform memory operations on the multimem address.
Syntax

// Integer type:

multimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];
multimem.st{.stsem}{.scope}{.ss}.type                [a], b;
multimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;

.ss =       { .global }
.ldsem =    { .weak, .relaxed, .acquire }
.stsem =    { .weak, .relaxed, .release }
.redsem =   { .relaxed, .release }
.scope =    { .cta, .cluster, .gpu, .sys }
.op  =      { .min, .max, .add, .and, .or, .xor }
.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }

// Floating point type:

multimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];
multimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;
multimem.red{.redsem}{.scope}{.ss}.redop{.vec}.type                 [a], b;

.ss =       { .global }
.ldsem =    { .weak, .relaxed, .acquire }
.stsem =    { .weak, .relaxed, .release }
.redsem =   { .relaxed, .release }
.scope =    { .cta, .cluster, .gpu, .sys }
.op  =      { .min, .max, .add }
.redop  =   { .add }
.acc_prec = { .acc::f32 }
.vec =      { .v2, .v4, .v8 }
.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 }


Description
Instruction multimem.ld_reduce performs the following operations:

load operation on the multimem address a, which involves loading of data from all of the
multiple memory locations pointed to by the multimem address a,
reduction operation specified by .op on the multiple data loaded from the multimem address
a.

The result of the reduction operation in returned in register d.
Instruction multimem.st performs a store operation of the input operand b to all the memory
locations pointed to by the multimem address a.
Instruction multimem.red performs a reduction operation on all the memory locations pointed to
by the multimem address a, with operand b.
Instruction multimem.ld_reduce performs reduction on the values loaded from all the memory
locations that the multimem address points to. In contrast, the multimem.red perform reduction
on all the memory locations that the multimem address points to.
Address operand a must be a multimem address. Otherwise, the behavior is undefined.  Supported
addressing modes for operand a and alignment requirements are described in Addresses as Operands.
If no state space is specified then Generic Addressing is
used. If the address specified by a does not fall within the address window of .global state
space then the behavior is undefined.
For floating-point type multi- operations, the size of the specified type along with .vec must
equal either 32-bits or 64-bits or 128-bits. No other combinations of .vec and type are
allowed. Type .f64 cannot be used with .vec qualifier.
The following table describes the valid combinations of .op and base type:







op
Base type




.add



.u32, .u64, .s32


.f16, .f16x2, .bf16, .bf16x2


.f32, .f64





.and, .or, .xor
.b32, .b64


.min, .max



.u32, .s32, .u64, .s644


.f16, .f16x2, .bf16, .bf16x2






For multimem.ld_reduce, the default precision of the intermediate accumulation is same as the
specified type. Optionally for .f16, .f16x2, .bf16 and .bf16x2 types, .acc::f32
can be specified to change the precision of the intermediate accumulation to .f32.
Optional qualifiers .ldsem, .stsem and .redsem specify the memory synchronizing effect
of the multimem.ld_reduce, multimem.st and multimem.red respectively, as described in
Memory Consistency Model. If explicit semantics qualifiers
are not specified, then multimem.ld_reduce and multimem.st default to .weak and
multimem.red defaults to .relaxed.
The optional .scope qualifier specifies the set of threads that can directly observe the memory
synchronizing effect of this operation, as described in Memory Consistency Model. If the .scope qualifier is not specified for
multimem.red then .sys scope is assumed by default.
PTX ISA Notes
Introduced in PTX ISA version 8.1.
Support for .acc::f32 qualifier introduced in PTX ISA version 8.2.
Target ISA Notes
Requires sm_90 or higher.
Examples

multimem.ld_reduce.and.b32                    val1_b32, [addr1];
multimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];

multimem.st.relaxed.gpu.b32                [addr3], val3_b32;
multimem.st.release.cta.global.u32         [addr4], val4_u32;

multimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;
multimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};
multimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7];





9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchuï

prefetch, prefetchu
Prefetch line containing a generic address at a specified level of memory hierarchy, in specified
state space.
Syntax

prefetch{.space}.level                    [a];   // prefetch to data cache
prefetch.global.level::eviction_priority  [a];   // prefetch to data cache

prefetchu.L1  [a];             // prefetch to uniform cache

prefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap

.space =                    { .global, .local };
.level =                    { .L1, .L2 };
.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };
.tensormap_space =          { .const, .param };


Description
The prefetch instruction brings the cache line containing the specified address in global or
local memory state space into the specified cache level.
If the .tensormap qualifier is specified then the prefetch instruction brings the cache line
containing the specified address in the .const or .param memory state space for subsequent
use by the cp.async.bulk.tensor instruction.
If no state space is given, the prefetch uses Generic Addressing.
Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the
modifier .level::eviction_priority.
Supported addressing modes for operand a and alignment requirements are described in Addresses
as Operands
The prefetchu instruction brings the cache line containing the specified generic address into
the specified uniform cache level.
A prefetch to a shared memory location performs no operation.
A prefetch into the uniform cache requires a generic address, and no operation occurs if the
address maps to a const, local, or shared memory location.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Support for .level::eviction_priority qualifier introduced in PTX ISA version 7.4.
Support for the .tensormap qualifier is introduced in PTX ISA version 8.0.
Target ISA Notes
prefetch and prefetchu require sm_20 or higher.
Support for .level::eviction_priority qualifier requires sm_80 or higher.
Support for the .tensormap qualifier requires sm_90 or higher.
Examples

prefetch.global.L1             [ptr];
prefetch.global.L2::evict_last [ptr];
prefetchu.L1  [addr];
prefetch.global.tensormap      [ptr];





9.7.8.15. Data Movement and Conversion Instructions: applypriorityï

applypriority
Apply the cache eviction priority to the specified address in the specified cache level.
Syntax

applypriority{.global}.level::eviction_priority  [a], size;

.level::eviction_priority = { .L2::evict_normal };


Description
The applypriority instruction applies the cache eviction priority specified by the
.level::eviction_priority qualifier to the address range [a..a+size) in the specified cache
level.
If no state space is specified then Generic Addressing is
used. If the specified address does not fall within the address window of .global state space
then the behavior is undefined.
The operand size is an integer constant that specifies the amount of data, in bytes, in the
specified cache level on which the priority is to be applied. The only supported value for the
size operand is 128.
Supported addressing modes for operand a are described in Addresses as Operands. a must be aligned to 128 bytes.
If the data pointed to by address a is not already present in the specified cache level, then
the data will be prefetched before applying the specified priority.
PTX ISA Notes
Introduced in PTX ISA version 7.4.
Target ISA Notes
Requires sm_80 or higher.
Examples

applypriority.global.L2::evict_normal [ptr], 128;





9.7.8.16. Data Movement and Conversion Instructions: discardï

discard
Invalidate the data in cache at the specified address and cache level.
Syntax

discard{.global}.level  [a], size;

.level = { .L2 };


Description
The discard instruction invalidates the data at the address range [a .. a + (size - 1)] in
the cache level specified by the .level qualifier without writing back the data in the cache to
the memory. Therefore after the discard operation, the data at the address range [a .. a+ (size -
1)] has undetermined value.
The operand size is an integer constant that specifies the amount of data, in bytes, in the
cache level specified by the .level qualifier to be discarded. The only supported value for the
size operand is 128.
If no state space is specified then Generic Addressing is
used. If the specified address does not fall within the address window of .global state space
then the behavior is undefined.
Supported addressing modes for address operand a are described in Addresses as Operands. a must be aligned to 128 bytes.
PTX ISA Notes
Introduced in PTX ISA version 7.4.
Target ISA Notes
Requires sm_80 or higher.
Examples

discard.global.L2 [ptr], 128;





9.7.8.17. Data Movement and Conversion Instructions: createpolicyï

createpolicy
Create a cache eviction policy for the specified cache level.
Syntax

// Range-based policy
createpolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64
                                   cache-policy, [a], primary-size, total-size;

// Fraction-based policy
createpolicy.fractional.level::primary_priority{.level::secondary_priority}.b64
                                   cache-policy{, fraction};

// Converting the access property from CUDA APIs
createpolicy.cvt.L2.b64            cache-policy, access-property;

.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,
                               .L2::evict_first, .L2::evict_unchanged };
.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged };


Description
The createpolicy instruction creates a cache eviction policy for the specified cache level in an
opaque 64-bit register specified by the destination operand cache-policy. The cache eviction
policy specifies how cache eviction priorities are applied to global memory addresses used in memory
operations with .level::cache_hint qualifier.
There are two types of cache eviction policies:


Range-based policy
The cache eviction policy created using createpolicy.range specifies the cache eviction
behaviors for the following three address ranges:

[a .. a + (primary-size - 1)] referred to as primary range.
[a + primary-size .. a + (total-size - 1)] referred to as trailing secondary range.
[a - (total-size - primary-size) .. (a - 1)] referred to as preceding secondary range.

When a range-based cache eviction policy is used in a memory operation with
.level::cache_hint qualifier, the eviction priorities are applied as follows:

If the memory address falls in the primary range, the eviction priority specified by
.L2::primary_priority is applied.
If the memory address falls in any of the secondary ranges, the eviction priority specified by
.L2::secondary_priority is applied.
If the memory address does not fall in either of the above ranges, then the applied eviction
priority is unspecified.

The 32-bit operand primary-size specifies the size, in bytes, of the primary range. The
32-bit operand total-size specifies the combined size, in bytes, of the address range
including primary and secondary ranges. The value of primary-size must be less than or equal
to the value of total-size. Maximum allowed value of total-size is 4GB.
If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged.
If no state space is specified then Generic Addressing is
used. If the specified address does not fall within the address window of .global state space
then the behavior is undefined.


Fraction-based policy
A memory operation with .level::cache_hint qualifier can use the fraction-based cache
eviction policy to request the cache eviction priority specified by .L2:primary_priority to
be applied to a fraction of cache accesses specified by the 32-bit floating point operand
fraction. The remainder of the cache accesses get the eviction priority specified by
.L2::secondary_priority. This implies that in a memory operation that uses a fraction-based
cache policy, the memory access has a probability specified by the operand fraction of
getting the cache eviction priority specified by .L2::primary_priority.
The valid range of values for the operand fraction is (0.0,.., 1.0]. If the operand
fraction is not specified, it defaults to 1.0.
If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged.


The access property created using the CUDA APIs can be converted into cache eviction policy by the
instruction createpolicy.cvt. The source operand access-property is a 64-bit opaque
register. Refer to CUDA programming guide for more details.
PTX ISA Notes
Introduced in PTX ISA version 7.4.
Target ISA Notes
Requires sm_80 or higher.
Examples

createpolicy.fractional.L2::evict_last.b64                      policy, 1.0;
createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;

createpolicy.range.L2::evict_last.L2::evict_first.b64
                                            policy, [ptr], 0x100000, 0x200000;

// access-prop is created by CUDA APIs.
createpolicy.cvt.L2.b64 policy, access-prop;





9.7.8.18. Data Movement and Conversion Instructions: isspacepï

isspacep
Query whether a generic address falls within a specified state space window.
Syntax

isspacep.space  p, a;    // result is .pred

.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };


Description
Write predicate register p with 1 if generic address a falls within the specified state
space window and with 0 otherwise. Destination p has type .pred; the source address
operand must be of type .u32 or .u64.
isspacep.param{::entry} returns 1 if the generic address falls within the window of
Kernel Function Parameters, otherwise returns 0. If .param
is specified without any sub-qualifiers then it defaults to .param::entry.
isspacep.global returns 1 for Kernel Function Parameters as .param window is contained within the .global
window.
If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default.

Note
ispacep.shared::cluster will return 1 for every shared memory address that is accessible to
the threads in the cluster, whereas ispacep.shared::cta will return 1 only if the address is
of a variable declared in the executing CTA.

PTX ISA Notes
Introduced in PTX ISA version 2.0.
isspacep.const introduced in PTX ISA version 3.1.
isspacep.param introduced in PTX ISA version 7.7.
Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8.
Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3.
Target ISA Notes
isspacep requires sm_20 or higher.
isspacep.param{::entry} requires sm_70 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Examples

isspacep.const           iscnst, cptr;
isspacep.global          isglbl, gptr;
isspacep.local           islcl,  lptr;
isspacep.shared          isshrd, sptr;
isspacep.param::entry    isparam, pptr;
isspacep.shared::cta     isshrdcta, sptr;
isspacep.shared::cluster ishrdany sptr;





9.7.8.19. Data Movement and Conversion Instructions: cvtaï

cvta
Convert address from .const, Kernel Function Parameters (.param), .global, .local, or .shared
state space to generic, or vice-versa. Take the generic address of a variable declared in
.const, Kernel Function Parameters (.param),
.global, .local, or .shared state space.
Syntax

// convert const, global, local, or shared address to generic address
cvta.space.size  p, a;        // source address in register a
cvta.space.size  p, var;      // get generic address of var
cvta.space.size  p, var+imm;  // generic address of var+offset

// convert generic address to const, global, local, or shared address
cvta.to.space.size  p, a;

.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };
.size  = { .u32, .u64 };


Description
Convert a const, Kernel Function Parameters
(.param), global, local, or shared address to a generic address, or vice-versa. The
source and destination addresses must be the same size. Use cvt.u32.u64 or cvt.u64.u32 to
truncate or zero-extend addresses.
For variables declared in .const, Kernel Function Parameters (.param), .global, .local, or .shared
state space, the generic address of the variable may be taken using cvta. The source is either a
register or a variable defined in const, Kernel Function Parameters (.param), global, local, or shared memory
with an optional offset.
When converting a generic address into a const, Kernel Function Parameters (.param), global, local, or shared
address, the resulting address is undefined in cases where the generic address does not fall within
the address window of the specified state space. A program may use isspacep to guard against
such incorrect behavior.
For cvta with .shared state space, the address must belong to the space specified by
::cta or ::cluster sub-qualifier, otherwise the behavior is undefined. If no sub-qualifier
is specified with .shared state space, then ::cta is assumed by default.
If .param is specified without any sub-qualifiers then it defaults to .param::entry.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
cvta.const and cvta.to.const introduced in PTX ISA version 3.1.
cvta.param and cvta.to.param introduced in PTX ISA version 7.7.
Note: The current implementation does not allow generic pointers to const space variables in
programs that contain pointers to constant buffers passed as kernel parameters.
Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8.
Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3.
Target ISA Notes
cvta requires sm_20 or higher.
cvta.param{::entry} and cvta.to.param{::entry} requires sm_70 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Examples

cvta.const.u32   ptr,cvar;
cvta.local.u32   ptr,lptr;
cvta.shared::cta.u32  p,As+4;
cvta.shared::cluster.u32 ptr, As;
cvta.to.global.u32  p,gptr;
cvta.param.u64   ptr,pvar;
cvta.to.param::entry.u64  epptr, ptr;





9.7.8.20. Data Movement and Conversion Instructions: cvtï

cvt
Convert a value from one type to another.
Syntax

cvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding
cvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding
cvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;
cvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;
cvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;
cvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;
cvt.rna{.satfinite}.tf32.f32               d, a;
cvt.frnd2{.relu}.tf32.f32                  d, a;
cvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;
cvt.rn.satfinite{.relu}.f8x2type.f16x2     d, a;
cvt.rn.{.relu}.f16x2.f8x2type              d, a;

.irnd   = { .rni, .rzi, .rmi, .rpi };
.frnd   = { .rn,  .rz,  .rm,  .rp  };
.frnd2  = { .rn,  .rz };
.dtype = .atype = { .u8,   .u16, .u32, .u64,
                    .s8,   .s16, .s32, .s64,
                    .bf16, .f16, .f32, .f64 };
.f8x2type = { .e4m3x2, .e5m2x2 };


Description
Convert between different types and sizes.
For .f16x2 and .bf16x2 instruction type, two inputs a and b of .f32 type are
converted into .f16 or .bf16 type and the converted values are packed in the destination
register d, such that the value converted from input a is stored in the upper half of d
and the value converted from input b is stored in the lower half of d
For .f16x2 instruction type, destination operand d has .f16x2 or .b32 type. For
.bf16 instruction type, operand d has .b16 type. For .bf16x2 instruction type,
operand d has .b32 type. For .tf32 instruction type, operand d has .b32 type.
When converting to .e4m3x2/.e5m2x2 data formats, the destination operand d has .b16
type. When converting two .f32 inputs to .e4m3x2/.e5m2x2, each input is converted to the
specified format, and the converted values are packed in the destination operand d such that the
value converted from input a is stored in the upper 8 bits of d and the value converted from
input b is stored in the lower 8 bits of d. When converting an .f16x2 input to
.e4m3x2/ .e5m2x2, each .f16 input from operand a is converted to the specified
format. The converted values are packed in the destination operand d such that the value
converted from the upper 16 bits of input a is stored in the upper 8 bits of d and the value
converted from the lower 16 bits of input a is stored in the lower 8 bits of d.
When converting from .e4m3x2/.e5m2x2 to .f16x2, source operand a has .b16
type. Each 8-bit input value in operand a is converted to .f16 type. The converted values
are packed in the destination operand d such that the value converted from the upper 8 bits of
a is stored in the upper 16 bits of d and the value converted from the lower 8 bits of a
is stored in the lower 16 bits of d.
Rounding modifier is mandatory in all of the following cases:

float-to-float conversions, when destination type is smaller than source type
All float-to-int conversions
All int-to-float conversions
All conversions involving .f16x2, .e4m3x2, .e5m2x2,.bf16x2 and .tf32 instruction
types.

.satfinite modifier is only supported for conversions involving the following types:

.e4m3x2 and .e5m2x2 destination types. .satfinite modifier is mandatory for such
conversions.
.f16, .bf16, .f16x2, .bf16x2 as destination types.
.tf32 as destination type with rounding mode specified as round to nearest, ties away from
zero.

Semantics

if (/* inst type is .f16x2 or .bf16x2 */) {
    d[31:16] = convert(a);
    d[15:0]  = convert(b);
} else {
    d = convert(a);
}


Integer Notes
Integer rounding is required for float-to-integer conversions, and for same-size float-to-float
conversions where the value is rounded to an integer. Integer rounding is illegal in all other
instances.
Integer rounding modifiers:

.rni

round to nearest integer, choosing even integer if source is equidistant between two integers

.rzi

round to nearest integer in the direction of zero

.rmi

round to nearest integer in direction of negative infinity

.rpi

round to nearest integer in direction of positive infinity


In float-to-integer conversion, NaN inputs are converted to 0.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported.
For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float
conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier
.ftz can only be specified when either .dtype or .atype is .f32 and applies only
to single precision (.f32) inputs and results.

sm_1x

For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32
float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving
zero. The optional .ftz modifier may be specified in these cases for clarity.
Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision
subnormal inputs or results to zero if the destination type size was 64-bits. The compiler will
preserve this behavior for legacy PTX code.


Saturation modifier:

.sat

For integer destination types, .sat limits the result to MININT..MAXINT for the size of
the operation. Note that saturation applies to both signed and unsigned integer types.
The saturation modifier is allowed only in cases where the destination typeâs value range is not
a superset of the source typeâs value range; i.e., the .sat modifier is illegal in cases
where saturation is not possible based on the source and destination types.
For float-to-integer conversions, the result is clamped to the destination range by default; i.e,
.sat is redundant.


Floating Point Notes
Floating-point rounding is required for float-to-float conversions that result in loss of precision,
and for integer-to-float conversions. Floating-point rounding is illegal in all other instances.
Floating-point rounding modifiers:

.rn

mantissa LSB rounds to nearest even

.rna

mantissa LSB rounds to nearest, ties away from zero

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


A floating-point value may be rounded to an integral value using the integer rounding modifiers (see
Integer Notes). The operands must be of the same size. The result is an integral value, stored in
floating-point format.
Subnormal numbers:

sm_20+

By default, subnormal numbers are supported. Modifier .ftz may be specified to flush
single-precision subnormal inputs and results to sign-preserving zero. Modifier .ftz can only
be specified when either .dtype or .atype is .f32 and applies only to single
precision (.f32) inputs and results.

sm_1x

Single-precision subnormal inputs and results are flushed to sign-preserving zero. The optional
.ftz modifier may be specified in these cases for clarity.


Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush
single-precision subnormal inputs or results to zero if either source or destination type was
.f64. The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX
ISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to
sign-preserving zero only for cvt.f32.f16, cvt.f16.f32, and cvt.f32.f32 instructions.
Saturation modifier:


.sat:

For floating-point destination types, .sat limits the result to the range [0.0, 1.0]. NaN
results are flushed to positive zero. Applies to .f16, .f32, and .f64 types.


.relu:

For .f16, .f16x2, .bf16, .bf16x2, .e4m3x2, .e5m2x2 and .tf32
destination types, .relu clamps the result to 0 if negative. NaN results are converted to
canonical NaN.


.satfinite:

For .f16, .f16x2, .bf16, .bf16x2, .e4m3x2, .e5m2x2 and .tf32
destination formats, if the input value is NaN, then the result is NaN in the specified
destination format. If the absolute value of input (ignoring sign) is greater than MAX_NORM of
the specified destination format, then the result is sign-preserved MAX_NORM of the destination
format.


Notes
A source register wider than the specified type may be used, except when the source operand has
.bf16 or .bf16x2 format. The lower n bits corresponding to the instruction-type width
are used in the conversion. See Operand Size Exceeding Instruction-Type Size for a description of these relaxed
type-checking rules.
A destination register wider than the specified type may be used, except when the destination
operand has .bf16, .bf16x2 or .tf32 format. The result of conversion is sign-extended to
the destination register width for signed integers, and is zero-extended to the destination register
width for unsigned, bit-size, and floating-point types. See Operand Size Exceeding Instruction-Type
Size for a description of these relaxed
type-checking rules.
For cvt.f32.bf16, NaN input yields unspecified NaN.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
.relu modifier and {.f16x2, .bf16, .bf16x2, .tf32} destination formats
introduced in PTX ISA version 7.0.
cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16},
cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16, and cvt.tf32.f32.{relu}.{rn/rz} introduced
in PTX ISA 7.8.
cvt with .e4m3x2/.e5m2x2 for sm_90 or higher introduced in PTX ISA version 7.8.
cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} for sm_90 or higher introduced in PTX ISA version 7.8.
cvt with .e4m3x2/.e5m2x2 for sm_89 introduced in PTX ISA version 8.1.
cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} for sm_89 introduced in PTX ISA version 8.1.
cvt.satfinite.{f16, bf16, f16x2, bf16x2, tf32}.f32 introduced in PTX ISA version 8.1.
Target ISA Notes
cvt to or from .f64 requires sm_13 or higher.
.relu modifier and {.f16x2, .bf16, .bf16x2, .tf32} destination formats require
sm_80 or higher.
cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16},
cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16, and cvt.tf32.f32.{relu}.{rn/rz} require
sm_90 or higher.
cvt with .e4m3x2/.e5m2x2 requires sm89 or higher.
cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2} requires sm_89 or higher.
Examples

cvt.f32.s32 f,i;
cvt.s32.f64 j,r;     // float-to-int saturates by default
cvt.rni.f32.f32 x,y; // round to nearest int, result is fp
cvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets
cvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode
cvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs
cvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output
cvt.rn.bf16.f32          b, f;        // convert fp32 to bf16
cvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation
cvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs
cvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output
cvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format
cvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format
cvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format
cvt.bf16.f16.rz          b, f         // convert f16 to bf16 format
cvt.bf16.u64.rz          b, u         // convert u64 to bf16 format
cvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format
cvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format
cvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output
cvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu
                                         // saturation on each output and pack as .e5m2x2
cvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output





9.7.8.21. Data Movement and Conversion Instructions: cvt.packï

cvt.pack
Convert two integer values from one integer type to another and pack the results.
Syntax

cvt.pack.sat.convertType.abType  d, a, b;
    .convertType  = { .u16, .s16 }
    .abType       = { .s32 }

cvt.pack.sat.convertType.abType.cType  d, a, b, c;
    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }
    .abType       = { .s32 }
    .cType        = { .b32 }


Description
Convert two 32-bit integers a and b into specified type and pack the results into d.
Destination d is an unsigned 32-bit integer. Source operands a and b are integers of
type .abType and the source operand c is an integer of type .cType.
The inputs a and b are converted to values of type specified by .convertType with
saturation and the results after conversion are packed into lower bits of d.
If operand c is specified then remaining bits of d are copied from lower bits of c.
Semantics

ta = a < MIN(convertType) ? MIN(convertType) : a;
ta = a > MAX(convertType) ? MAX(convertType) : a;
tb = b < MIN(convertType) ? MIN(convertType) : b;
tb = b > MAX(convertType) ? MAX(convertType) : b;

size = sizeInBits(convertType);
td = tb ;
for (i = size; i <= 2 * size - 1; i++) {
    td[i] = ta[i - size];
}

if (isU16(convertType) || isS16(convertType)) {
    d = td;
} else {
    for (i = 0; i < 2 * size; i++) {
        d[i] = td[i];
    }
    for (i = 2 * size; i <= 31; i++) {
        d[i] = c[i - 2 * size];
    }
}


.sat modifier limits the converted values to MIN(convertType)..MAX(convertedType) (no
overflow) if the corresponding inputs are not in the range of datatype specified as
.convertType.
PTX ISA Notes
Introduced in PTX ISA version 6.5.
Target ISA Notes
Requires sm_72 or higher.
Sub byte types (.u4/.s4 and .u2/.s2) requires sm_75 or higher.
Examples

cvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion
cvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion
cvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }
cvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion
cvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion





9.7.8.22. Data Movement and Conversion Instructions: mapaï

mapa
Map the address of the shared variable in the target CTA.
Syntax

mapa{.space}.type          d, a, b;

// Maps shared memory address in register a into CTA b.
mapa.shared::cluster.type  d, a, b;

// Maps shared memory variable into CTA b.
mapa.shared::cluster.type  d, sh, b;

// Maps shared memory variable into CTA b.
mapa.shared::cluster.type  d, sh + imm, b;

// Maps generic address in register a into CTA b.
mapa.type                  d, a, b;

.space = { .shared::cluster }
.type  = { .u32, .u64 }


Description
Get address in the CTA specified by operand b which corresponds to the address specified by
operand a.
Instruction type .type indicates the type of the destination operand d and the source
operand a.
When space is .shared::cluster, source a is either a shared memory variable or a register
containing a valid shared memory address and register d contains a shared memory address. When
the optional qualifier .space is not specified, both a and d are registers containing
generic addresses pointing to shared memory.
b is a 32-bit integer operand representing the rank of the target CTA.
Destination register d will hold an address in CTA b corresponding to operand a.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

mapa.shared::cluster.u64 d1, %reg1, cta;
mapa.shared::cluster.u32 d2, sh, 3;
mapa.u64                 d3, %reg2, cta;





9.7.8.23. Data Movement and Conversion Instructions: getctarankï

getctarank
Generate the CTA rank of the address.
Syntax

getctarank{.space}.type d, a;

// Get cta rank from source shared memory address in register a.
getctarank.shared::cluster.type d, a;

// Get cta rank from shared memory variable.
getctarank.shared::cluster.type d, var;

// Get cta rank from shared memory variable+offset.
getctarank.shared::cluster.type d, var + imm;

// Get cta rank from generic address of shared memory variable in register a.
getctarank.type d, a;

.space = { .shared::cluster }
.type  = { .u32, .u64 }


Description
Write the destination register d with the rank of the CTA which contains the address specified
in operand a.
Instruction type .type indicates the type of source operand a.
When space is .shared::cluster, source a is either a shared memory variable or a register
containing a valid shared memory address. When the optional qualifier .space is not specified,
a is a register containing a generic addresses pointing to shared memory. Destination d is
always a 32-bit register which holds the rank of the CTA.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

getctarank.shared::cluster.u32 d1, addr;
getctarank.shared::cluster.u64 d2, sh + 4;
getctarank.u64                 d3, src;





9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copyï

An asynchronous copy operation performs the underlying operation asynchronously in the background,
thus allowing the issuing threads to perform subsequent tasks.
An asynchronous copy operation can be a bulk operation that operates on a large amount of data, or
a non-bulk operation that operates on smaller sized data. The amount of data handled by a bulk
asynchronous operation must be a multiple of 16 bytes.


9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operationsï

A thread must explicitly wait for the completion of an asynchronous copy operation in order to
access the result of the operation. Once an asynchronous copy operation is initiated, modifying the
source memory location or reading from the destination memory location before the asynchronous
operation completes, will cause unpredictable results.
This section describes two asynchronous copy operation completion mechanisms supported in PTX:
Async-group mechanism and mbarrier-based mechanism.
Async-group mechanism
When using the async-group completion mechanism, the issuing thread specifies a group of
asynchronous operations, called async-group, using a commit operation and tracks the completion
of this group using a wait operation. The thread issuing the asynchronous operation must create
separate async-groups for bulk and non-bulk asynchronous operations.
A commit operation creates a per-thread async-group containing all prior asynchronous operations
initiated by the executing thread but none of the asynchronous operations following the commit
operation. A committed asynchronous operation belongs to a single async-group.
When an async-group completes, all the asynchronous operations belonging to that group are
complete and the executing thread that initiated the asynchronous operations can read the result of
the asynchronous operations. All async-groups committed by an executing thread always complete in
the order in which they were committed. There is no ordering between asynchronous operations within
an async-group.
A typical pattern of using async-group as the completion mechanism is as follows:

Initiate the asynchronous operations.
Group the asynchronous operations into an async-group using a commit operation.
Wait for the completion of the async-group using the wait operation.
Once the async-group completes, access the results of all asynchronous operations in that
async-group.

Mbarrier-based mechanism
A thread can track the completion of one or more asynchronous operations using the current phase of
an mbarrier object. When the current phase of the mbarrier object is complete, it implies that
all asynchronous operations tracked by this phase are complete, and all threads participating in
that mbarrier object can access the result of the asynchronous operations.
The mbarrier object to be used for tracking the completion of an asynchronous operation can be
either specified along with the asynchronous operation as part of its syntax, or as a separate
operation. For a bulk asynchronous operation, the mbarrier object must be specified in the
asynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous
operation.
A typical pattern of using mbarrier-based completion mechanism is as follows:

Initiate the asynchronous operations.
Set up an mbarrier object to track the asynchronous operations in its current phase, either as
part of the asynchronous operation or as a separate operation.
Wait for the mbarrier object to complete its current phase using mbarrier.test_wait or
mbarrier.try_wait.
Once the mbarrier.test_wait or mbarrier.try_wait operation returns True, access the
results of the asynchronous operations tracked by the mbarrier object.




9.7.8.24.2. Async Proxyï

The cp{.reduce}.async.bulk operations are performed in the asynchronous proxy (or async
proxy).
Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the
async proxy, fence.proxy.async should be used to synchronize memory between generic
proxy and the async proxy.
The completion of a cp{.reduce}.async.bulk operation is followed by an implicit generic-async
proxy fence. So the result of the asynchronous operation is made visible to the generic proxy as
soon as its completion is observed. Async-group OR mbarrier-based completion mechanism must
be used to wait for the completion of the cp{.reduce}.async.bulk instructions.



9.7.8.24.3. Data Movement and Conversion Instructions: cp.asyncï

cp.async
Initiates an asynchronous copy operation from one state space to another.
Syntax

cp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}
                         [dst], [src], cp-size{, src-size}{, cache-policy} ;
cp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}
                         [dst], [src], 16{, src-size}{, cache-policy} ;
cp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}
                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;
cp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}
                         [dst], [src], 16{, ignore-src}{, cache-policy} ;

.level::cache_hint =     { .L2::cache_hint }
.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }
cp-size =                { 4, 8, 16 }


Description
cp.async is a non-blocking instruction which initiates an asynchronous copy operation of data
from the location specified by source address operand src to the location specified by
destination address operand dst. Operand src specifies a location in the global state space
and dst specifies a location in the shared state space.
Operand cp-size is an integer constant which specifies the size of data in bytes to be copied to
the destination dst. cp-size can only be 4, 8 and 16.
Instruction cp.async allows optionally specifying a 32-bit integer operand src-size. Operand
src-size represents the size of the data in bytes to be copied from src to dst and must
be less than cp-size. In such case, remaining bytes in destination dst are filled with
zeros. Specifying src-size larger than cp-size results in undefined behavior.
The optional and non-immediate predicate argument ignore-src specifies whether the data from the
source location src should be ignored completely. If the source data is ignored then zeros will
be copied to destination dst. If the argument ignore-src is not specified then it defaults
to False.
Supported alignment requirements and addressing modes for operand src and dst are described
in Addresses as Operands.
The mandatory .async qualifier indicates that the cp instruction will initiate the memory
copy operation asynchronously and control will return to the executing thread before the copy
operation is complete. The executing thread can then use cp.async.wait_all or
cp.async.wait_group or mbarrier instructions to wait for
completion of the asynchronous copy operation. No other synchronization mechanisms described in
Memory Consistency Model can be used to guarantee the
completion of the asynchronous copy operations.
There is no ordering guarantee between two cp.async operations if they are not explicitly
synchronized using cp.async.wait_all or cp.async.wait_group or mbarrier instructions.
As described in Cache Operators, the .cg qualifier indicates
caching of data only at global level cache L2 and not at L1 whereas .ca qualifier indicates
caching of data at all levels including L1 cache. Cache operator are treated as performance hints
only.
cp.async is treated as a weak memory operation in the Memory Consistency Model.
The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size
into the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B,
128B, 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes
respectively.
The qualifier .level::prefetch_size may only be used with .global state space and with
generic addressing where the address points to .global state space. If the generic address does
not fall within the address window of the global memory, then the prefetching behavior is undefined.
The .level::prefetch_size qualifier is treated as a performance hint only.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
The qualifier .level::cache_hint is only supported for .global state space and for generic
addressing where the address points to the .global state space.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Support for .level::cache_hint and .level::prefetch_size qualifiers introduced in PTX ISA
version 7.4.
Support for ignore-src operand introduced in PTX ISA version 7.5.
Support for sub-qualifier ::cta introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_80 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Examples

cp.async.ca.shared.global  [shrd],    [gbl + 4], 4;
cp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;
cp.async.cg.shared.global  [%r2],     [%r3],     16;

cp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;
cp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;
cp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;

createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;
cp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;

cp.async.ca.shared.global                   [shrd], [gbl], 4, p;
cp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy;





9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_groupï

cp.async.commit_group
Commits all prior initiated but uncommitted cp.async instructions into a cp.async-group.
Syntax

cp.async.commit_group ;


Description
cp.async.commit_group instruction creates a new cp.async-group per thread and batches all
prior cp.async instructions initiated by the executing thread but not committed to any
cp.async-group into the new cp.async-group. If there are no uncommitted cp.async
instructions then cp.async.commit_group results in an empty cp.async-group.
An executing thread can wait for the completion of all cp.async operations in a cp.async-group
using cp.async.wait_group.
There is no memory ordering guarantee provided between any two cp.async operations within the
same cp.async-group. So two or more cp.async operations within a cp.async-group copying data
to the same location results in undefined behavior.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Target ISA Notes
Requires sm_80 or higher.
Examples

// Example 1:
cp.async.ca.shared.global [shrd], [gbl], 4;
cp.async.commit_group ; // Marks the end of a cp.async group

// Example 2:
cp.async.ca.shared.global [shrd1],   [gbl1],   8;
cp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;
cp.async.commit_group ; // Marks the end of cp.async group 1

cp.async.ca.shared.global [shrd2],    [gbl2],    16;
cp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;
cp.async.commit_group ; // Marks the end of cp.async group 2





9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_allï

cp.async.wait_group/cp.async.wait_all
Wait for completion of prior asynchronous copy operations.
Syntax

cp.async.wait_group N;
cp.async.wait_all ;


Description
cp.async.wait_group instruction will cause executing thread to wait till only N or fewer of
the most recent cp.async-groups are pending and all the prior cp.async-groups committed by
the executing threads are complete. For example, when N is 0, the executing thread waits on all
the prior cp.async-groups to complete. Operand N is an integer constant.
cp.async.wait_all is equivalent to :

cp.async.commit_group;
cp.async.wait_group 0;


An empty cp.async-group is considered to be trivially complete.
Writes performed by cp.async operations are made visible to the executing thread only after:

The completion of cp.async.wait_all or
The completion of cp.async.wait_group on the cp.async-group in which the cp.async
belongs to or
mbarrier.test_wait
returns True on an mbarrier object which is tracking the completion of the cp.async
operation.

There is no ordering between two cp.async operations that are not synchronized with
cp.async.wait_all or cp.async.wait_group or mbarrier objects.
cp.async.wait_group and cp.async.wait_all does not provide any ordering and visibility
guarantees for any other memory operation apart from cp.async.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Target ISA Notes
Requires sm_80 or higher.
Examples

// Example of .wait_all:
cp.async.ca.shared.global [shrd1], [gbl1], 4;
cp.async.cg.shared.global [shrd2], [gbl2], 16;
cp.async.wait_all;  // waits for all prior cp.async to complete

// Example of .wait_group :
cp.async.ca.shared.global [shrd3], [gbl3], 8;
cp.async.commit_group;  // End of group 1

cp.async.cg.shared.global [shrd4], [gbl4], 16;
cp.async.commit_group;  // End of group 2

cp.async.cg.shared.global [shrd5], [gbl5], 16;
cp.async.commit_group;  // End of group 3

cp.async.wait_group 1;  // waits for group 1 and group 2 to complete





9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulkï

cp.async.bulk
Initiates an asynchronous copy operation from one state space to another.
Syntax

cp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}
                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}

.dst =                  { .shared::cluster }
.src =                  { .global }
.completion_mechanism = { .mbarrier::complete_tx::bytes }
.level::cache_hint =    { .L2::cache_hint }
.multicast =            { .multicast::cluster  }


cp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]

.dst =                  { .shared::cluster }
.src =                  { .shared::cta }
.completion_mechanism = { .mbarrier::complete_tx::bytes }


cp.async.bulk.dst.src.completion_mechanism{.level::cache_hint} [dstMem], [srcMem], size{, cache-policy}

.dst =                  { .global }
.src =                  { .shared::cta }
.completion_mechanism = { .bulk_group }
.level::cache_hint =    { .L2::cache_hint }


Description
cp.async.bulk is a non-blocking instruction which initiates an asynchronous bulk-copy operation
from the location specified by source address operand srcMem to the location specified by
destination address operand dstMem.
The direction of bulk-copy is from the state space specified by the .src modifier to the state
space specified by the .dst modifiers.
The 32-bit operand size specifies the amount of memory to be copied, in terms of number of
bytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is
undefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory
space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory
space. Otherwise, the behavior is undefined. The addresses dstMem and srcMem must be aligned
to 16 bytes.
When the source of the copy is .shared::cta and the destination is .shared::cluster, the
destination has to be in the shared memory of a different CTA within the cluster.
The modifier .completion_mechanism specifies the completion mechanism that is supported on the
instruction variant. The completion mechanisms that are supported for different variants are
summarized in the following table:









Completion mechanism
.dst
.src
Description




.mbarrier::...
.shared::cluster
.global
mbarrier based completion mechanism


.shared::cluster
.shared::cta


.bulk_group
.global
.shared::cta
Bulk async-group based completion mechanism



The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk variant uses
mbarrier based completion mechanism. The complete-tx
operation, with completeCount argument equal to amount of data copied in bytes, will be
performed on the mbarrier object specified by the operand mbar.
The modifier .bulk_group specifies that the cp.async.bulk variant uses bulk async-group
based completion mechanism.
The optional modifier .multicast::cluster allows copying of data from global memory to shared
memory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the
cluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid
of the destination CTA. The source data is multicast to the same CTA-relative offset as dstMem
in the shared memory of each destination CTA. The mbarrier signal is also multicast to the same
CTA-relative offset as mbar in the shared memory of the destination CTA.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier .level::cache_hint is only supported when at least one of the .src or .dst
statespaces is .global state space.
The copy operation in cp.async.bulk is treated as a weak memory operation and the complete-tx
operation on the mbarrier has .release semantics at the .cluster scope as described in the
Memory Consistency Model.
Notes
.multicast::cluster qualifier is optimized for target architecture sm_90a and may have
substantially reduced performance on other targets and hence .multicast::cluster is advised to
be used with .target sm_90a.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
.multicast::cluster qualifier advised to be used with .target sm_90a.
Examples

// .global -> .shared::cluster:
cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];

cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster
                                             [dstMem], [srcMem], size, [mbar], ctaMask;

cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint
                                             [dstMem], [srcMem], size, [mbar], cache-policy;


// .shared::cta -> .shared::cluster (strictly remote):
cp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];

// .shared::cta -> .global:
cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;

cp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy;





9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulkï

cp.reduce.async.bulk
Initiates an asynchronous reduction operation.
Syntax

cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type
              [dstMem], [srcMem], size, [mbar]

.dst =                  { .shared::cluster }
.src =                  { .shared::cta }
.completion_mechanism = { .mbarrier::complete_tx::bytes }
.redOp=                 { .and, .or, .xor,
                          .add, .inc, .dec,
                          .min, .max }
.type =                 { .b32, .u32, .s32, .b64, .u64 }


cp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type
               [dstMem], [srcMem], size{, cache-policy}

.dst =                  { .global      }
.src =                  { .shared::cta }
.completion_mechanism = { .bulk_group }
.level::cache_hint    = { .L2::cache_hint }
.redOp=                 { .and, .or, .xor,
                          .add, .inc, .dec,
                          .min, .max }
.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }


cp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type
               [dstMem], [srcMem], size{, cache-policy}
.dst  =                 { .global }
.src  =                 { .shared::cta }
.completion_mechanism = { .bulk_group }
.type =                 { .f16, .bf16 }


Description
cp.reduce.async.bulk is a non-blocking instruction which initiates an asynchronous reduction
operation on an array of memory locations specified by the destination address operand dstMem
with the source array whose location is specified by the source address operand srcMem. The size
of the source and the destination array must be the same and is specified by the operand size.
Each data element in the destination array is reduced inline with the corresponding data element in
the source array with the reduction operation specified by the modifier .redOp. The type of each
data element in the source and the destination array is specified by the modifier .type.
The source address operand srcMem is located in the state space specified by .src and the
destination address operand dstMem is located in the state specified by the .dst.
The 32-bit operand size specifies the amount of memory to be copied from the source location and
used in the reduction operation, in terms of number of bytes. size must be a multiple of 16. If
the value is not a multiple of 16, then the behavior is undefined. The memory range [dstMem,
dstMem + size - 1] must not overflow the destination memory space and the memory range [srcMem,
srcMem + size - 1] must not overflow the source memory space. Otherwise, the behavior is
undefined. The addresses dstMem and srcMem must be aligned to 16 bytes.
The operations supported by .redOp are classified as follows:

The bit-size operations are .and, .or, and .xor.
The integer operations are .add, .inc, .dec, .min, and .max. The .inc and
.dec operations return a result in the range [0..x] where x is the value at the source
state space.
The floating point operation .add rounds to the nearest even. The current implementation of
cp.reduce.async.bulk.add.f32 flushes subnormal inputs and results to sign-preserving zero. The
cp.reduce.async.bulk.add.f16 and cp.reduce.async.bulk.add.bf16 operations require
.noftz qualifier. It preserves input and result subnormals, and does not flush them to zero.

The following table describes the valid combinations of .redOp and element type:








.dst
.redOp
Element type




.shared::cluster
.add
.u32, .s32, .u64


.min, .max
.u32, .s32


.inc, .dec
.u32


.and, .or, .xor
.b32


.global
.add
.u32, .s32, .u64, .f32, .f64, .f16, .bf16


.min, .max
.u32, .s32, .u64, .s64, .f16, .bf16


.inc, .dec
.u32


.and, .or, .xor
.b32, .b64



The modifier .completion_mechanism specifies the completion mechanism that is supported on the
instruction variant. The completion mechanisms that are supported for different variants are
summarized in the following table:









Completion mechanism
.dst
.src
Description




.mbarrier::...
.shared::cluster
.global
mbarrier based completion mechanism


.shared::cluster
.shared::cta


.bulk_group
.global
.shared::cta
Bulk async-group based completion mechanism



The modifier .mbarrier::complete_tx::bytes specifies that the cp.reduce.async.bulk variant
uses mbarrier based completion mechanism. The complete-tx
operation, with completeCount argument equal to amount of data copied in bytes, will be
performed on the mbarrier object specified by the operand mbar.
The modifier .bulk_group specifies that the cp.reduce.async.bulk variant uses bulk
async-group based completion mechanism.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier .level::cache_hint is only supported when at least one of the .src or .dst
statespaces is .global state space.
Each reduction operation performed by the cp.reduce.async.bulk has individually .relaxed.gpu
memory ordering semantics. The load operations in cp.reduce.async.bulk are treated as weak
memory operation and the complete-tx
operation on the mbarrier has .release semantics at the .cluster scope as described in the
Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64
                                                                  [dstMem], [srcMem], size, [mbar];

cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32
                                                                  [dstMem], [srcMem], size, [mbar];

cp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;

cp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;

cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;





9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetchï

cp.async.bulk.prefetch
Provides a hint to the system to initiate the asynchronous prefetch of data to the cache.
Syntax

cp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}

.src =                { .global }
.level::cache_hint =  { .L2::cache_hint }


Description
cp.async.bulk.prefetch is a non-blocking instruction which may initiate an asynchronous prefetch
of data from the location specified by source address operand srcMem, in .src statespace, to
the L2 cache.
The 32-bit operand size specifies the amount of memory to be prefetched in terms of number of
bytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is
undefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory
space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory
space. Otherwise, the behavior is undefined. The address srcMem must be aligned to 16 bytes.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

cp.async.bulk.prefetch.L2.global                 [srcMem], size;

cp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy;





9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensorï

cp.async.bulk.tensor
Initiates an asynchronous copy operation on the tensor data from one state space to another.
Syntax

// global -> shared::cluster:
cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.level::cache_hint}
                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colOffsets}
                                   {, ctaMask} {, cache-policy}

.dst =                  { .shared::cluster }
.src =                  { .global }
.dim =                  { .1d, .2d, .3d, .4d, .5d }
.completion_mechanism = { .mbarrier::complete_tx::bytes }
.load_mode =            { .tile, .im2col }
.level::cache_hint =    { .L2::cache_hint }
.multicast =            { .multicast::cluster  }


// shared::cta -> global:
cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}
                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}

.dst =                  { .global }
.src =                  { .shared::cta }
.dim =                  { .1d, .2d, .3d, .4d, .5d }
.completion_mechanism = { .bulk_group }
.load_mode =            { .tile, .im2col_no_offs }
.level::cache_hint =    { .L2::cache_hint }


Description
cp.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous copy
operation of tensor data from the location in .src state space to the location in the .dst
state space.
The operand dstMem specifies the location in the .dst state space into which the tensor data
has to be copied and srcMem specifies the location in the .src state space from which the
tensor data has to be copied.
The operand tensorMap is the generic address of the opaque tensor-map object which resides
either in .param space or .const space or .global space. The operand tensorMap specifies
the properties of the tensor copy operation, as described in Tensor-map.
The tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating
the tensor-map objects on the host side.
The dimension of the tensor data is specified by the .dim modifier.
The vector operand tensorCoords specifies the starting coordinates in the tensor data in the
global memory from or to which the copy operation has to be performed. The number of tensor
coordinates in the vector argument tensorCoords should be equal to the dimension specified by
the modifier .dim. The individual tensor coordinates in tensorCoords are of type .s32.
The modifier .completion_mechanism specifies the completion mechanism that is supported on the
instruction variant. The completion mechanisms that are supported for different variants are
summarized in the following table:









Completion mechanism
.dst
.src
Description




.mbarrier::...
.shared::cluster
.global
mbarrier based completion mechanism


.bulk_group
.global
.shared::cta
Bulk async-group based completion mechanism



The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk.tensor variant
uses mbarrier based completion mechanism. The complete-tx
operation, with completeCount argument equal to amount of data copied in bytes, will be
performed on the mbarrier object specified by the operand mbar.
The modifier .bulk_group specifies that the cp.async.bulk.tensor variant uses bulk
async-group based completion mechanism.
The qualifier .load_mode specifies how the data in the source location is copied into the
destination location. If .load_mode is not specified, it defaults to .tile. In .tile
mode, the multi-dimensional layout of the source tensor is preserved at the destination. In
.im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column
at the destination. Details of the im2col mode are described in Im2col mode. In .im2col mode, the tensor has to be at least
3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is
.im2col. The length of the vector operand im2colOffsets is two less than the number of dimension
.dim of the tensor operation. The modifier .im2col_no_offs is the same as .im2col mode
except there is no im2colOffsets vector involved.
The optional modifier .multicast::cluster allows copying of data from global memory to shared
memory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the
cluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid
of the destination CTA. The source data is multicast to the same offset as dstMem in the shared
memory of each destination CTA. The mbarrier signal is also multicast to the same offset as mbar
in the shared memory of the destination CTA.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier .level::cache_hint is only supported when at least one of the .src or .dst
statespaces is .global state space.
The copy operation in cp.async.bulk.tensor is treated as a weak memory operation and the
complete-tx
operation on the mbarrier has .release semantics at the .cluster scope as described in the
Memory Consistency Model.
Notes
.multicast::cluster qualifier is optimized for target architecture sm_90a and may have
substantially reduced performance on other targets and hence .multicast::cluster is advised to
be used with .target sm_90a.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
.multicast::cluster qualifier advised to be used with .target sm_90a.
Examples

.reg .b16 ctaMask;
.reg .u16 i2cOffW, i2cOffH, i2cOffD;
.reg .b64 l2CachePolicy;

cp.async.bulk.tensor.1d.shared::cluster.global.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];

@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster
                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;

@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes
                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};

@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint
                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;

@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3];





9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensorï

cp.reduce.async.bulk.tensor
Initiates an asynchronous reduction operation on the tensor data.
Syntax

// shared::cta -> global:
cp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}
                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}

.dst =                  { .global }
.src =                  { .shared::cta }
.dim =                  { .1d, .2d, .3d, .4d, .5d }
.completion_mechanism = { .bulk_group }
.load_mode =            { .tile, .im2col_no_offs }
.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor}


Description
cp.reduce.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous
reduction operation of tensor data in the .dst state space with tensor data in the .src
state space.
The operand srcMem specifies the location of the tensor data in the .src state space using
which the reduction operation has to be performed.
The operand tensorMap is the generic address of the opaque tensor-map object which resides
either in .param space or .const space or .global space. The operand tensorMap specifies
the properties of the tensor copy operation, as described in Tensor-map.
The tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating
the tensor-map objects on the host side.
Each element of the tensor data in the .dst state space is reduced inline with the corresponding
element from the tensor data in the .src state space. The modifier .redOp specifies the
reduction operation used for the inline reduction. The type of each tensor data element in the
source and the destination tensor is specified in Tensor-map.
The dimension of the tensor is specified by the .dim modifier.
The vector operand tensorCoords specifies the starting coordinates of the tensor data in the
global memory on which the reduce operation is to be performed. The number of tensor coordinates in
the vector argument tensorCoords should be equal to the dimension specified by the modifier
.dim. The individual tensor coordinates are of the type .s32.
The following table describes the valid combinations of .redOp and element type:







.redOp
Element type




.add
.u32, .s32, .u64, .f32, .f16, .bf16


.min, .max
.u32, .s32, .u64, .s64, .f16, .bf16


.inc, .dec
.u32


.and, .or, .xor
.b32, .b64



The modifier .completion_mechanism specifies the completion mechanism that is supported on the
instruction variant. Value .bulk_group of the modifier .completion_mechanism specifies that
cp.reduce.async.bulk.tensor instruction uses bulk async-group based completion mechanism.
The qualifier .load_mode specifies how the data in the source location is copied into the
destination location. If .load_mode is not specified, it defaults to .tile. In .tile
mode, the multi-dimensional layout of the source tensor is preserved at the destination. In
.im2col_no_offs mode, some dimensions of the source tensors are unrolled in a single dimensional
column at the destination. Details of the im2col mode are described in Im2col mode. In .im2col mode, the tensor has to be at least
3-dimensional.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier .level::cache_hint is only supported when at least one of the .src or .dst
statespaces is .global state space.
Each reduction operation performed by cp.reduce.async.bulk.tensor has individually
.relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk.tensor
are treated as weak memory operations and the complete-tx
operation on the mbarrier has .release semantics at the .cluster scope as described in the
Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group
                                             [tensorMap0, {tc0}], [sMem0];

cp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint
                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;

cp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group
                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2]





9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensorï

cp.async.bulk.prefetch.tensor
Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache.
Syntax

// global -> shared::cluster:
cp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]
                                                             {, im2colOffsets } {, cache-policy}

.src =                { .global }
.dim =                { .1d, .2d, .3d, .4d, .5d }
.load_mode =          { .tile, .im2col }
.level::cache_hint =  { .L2::cache_hint }


Description
cp.async.bulk.prefetch.tensor is a non-blocking instruction which may initiate an asynchronous
prefetch of tensor data from the location in .src statespace to the L2 cache.
The operand tensorMap is the generic address of the opaque tensor-map object which resides
either in .param space or .const space or .global space. The operand tensorMap specifies
the properties of the tensor copy operation, as described in Tensor-map.
The tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating
the tensor-map objects on the host side.
The dimension of the tensor data is specified by the .dim modifier.
The vector operand tensorCoords specifies the starting coordinates in the tensor data in the
global memory from or to which the copy operation has to be performed. The number of tensor
coordinates in the vector argument tensorCoords should be equal to the dimension specified by
the modifier .dim. The individual tensor coordinates in tensorCoords are of type .s32.
The qualifier .load_mode specifies how the data in the source location is copied into the
destination location. If .load_mode is not specified, it defaults to .tile. In .tile
mode, the multi-dimensional layout of the source tensor is preserved at the destination. In
.im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column
at the destination. Details of the im2col mode are described in Im2col mode. In .im2col mode, the tensor has to be at least
3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is
.im2col. The length of the vector operand im2colOffsets is two less than the number of dimension
.dim of the tensor operation.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
cp.async.bulk.prefetch.tensor is treated as a weak memory operation in the Memory Consistency
Model.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b16 ctaMask;
.reg .u16 i2cOffW, i2cOffH, i2cOffD;
.reg .b64 l2CachePolicy;

cp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];

@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];

@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col
                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};

@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint
                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy;





9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_groupï

cp.async.bulk.commit_group
Commits all prior initiated but uncommitted cp.async.bulk instructions into a
cp.async.bulk-group.
Syntax

cp.async.bulk.commit_group;


Description
cp.async.bulk.commit_group instruction creates a new per-thread bulk async-group and batches
all prior cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions satisfying the following
conditions into the new bulk async-group:

The prior cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions use bulk_group based
completion mechanism, and
They are initiated by the executing thread but not committed to any bulk async-group.

If there are no uncommitted cp{.reduce}.async.bulk.{.prefetch}{.tensor} instructions then
cp.async.bulk.commit_group results in an empty bulk async-group.
An executing thread can wait for the completion of all
cp{.reduce}.async.bulk.{.prefetch}{.tensor} operations in a bulk async-group using
cp.async.wait_group.
There is no memory ordering guarantee provided between any two
cp{.reduce}.async.bulk.{.prefetch}{.tensor} operations within the same bulk async-group.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

cp.async.bulk.commit_group;





9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_groupï

cp.async.bulk.wait_group
Wait for completion of bulk async-groups.
Syntax

cp.async.bulk.wait_group{.read} N;


Description
cp.async.bulk.wait_group instruction will cause the executing thread to wait until only N or
fewer of the most recent bulk async-groups are pending and all the prior bulk async-groups
committed by the executing threads are complete. For example, when N is 0, the executing thread
waits on all the prior bulk async-groups to complete. Operand N is an integer constant.
By default, cp.async.bulk.wait_group instruction will cause the executing thread to wait till
all the bulk async operations in the specified bulk async-group have completed all of the
following:

Reading from the source locations.
Writing to their respective destination locations.
Writes being made visible to the executing thread.

The optional .read modifier indicates that the waiting has to be done until all the bulk async
operations in the specified bulk async-group have completed reading from their source locations.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

cp.async.bulk.wait_group.read   0;
cp.async.bulk.wait_group        2;






9.7.8.25. Data Movement and Conversion Instructions: tensormap.replaceï

tensormap.replace
Modifies the field of a tensor-map object.
Syntax

tensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;
tensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;
tensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;

.mode    = { .tile }
.field1  = { .global_address, .rank }
.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }
.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .fill_mode }
.ss      = { .global, .shared::cta }
.type    = { .b32, .b64 }


Description
The tensormap.replace instruction replaces the field, specified by .field qualifier,
of the tensor-map object at the location specified by the address operand addr with a
new value. The new value is specified by the argument new_val.
Qualifier .mode specifies the mode of the tensor-map object
located at the address operand addr.
Instruction type .b1024 indicates the size of the tensor-map
object, which is 1024 bits.
Operand new_val has the type .type. When .field is specified as .global_address
or .global_stride, .type must be .b64. Otherwise, .type must be .b32.
The immediate integer operand ord specifies the ordinal of the field across the rank of the
tensor which needs to be replaced in the tensor-map object.
For field .rank, the operand new_val must be ones less than the desired tensor rank as
this field uses zero-based numbering.
When .field3 is specified, the operand new_val must be an immediate and the
Table 30 shows the mapping of the operand new_val across various fields.


Table 30 Tensormap new_val validityï










new_val
.field3


.elemtype
.interleave_layout
.swizzle_mode
.fill_mode




0
.u8
No interleave
No swizzling
Zero fill


1
.u16
16B interleave
32B swizzling
OOB-NaN fill


2
.u32
32B interleave
64B swizzling
x


3
.s32
x
128B swizzling
x


4
.u64
x
x
x


5
.s64
x
x
x


6
.f16
x
x
x


7
.f32
x
x
x


8
.f32.ftz
x
x
x


9
.f64
x
x
x


10
.bf16
x
x
x


11
.tf32
x
x
x


12
.tf32.ftz
x
x
x



If no state space is specified then Generic Addressing is used.
If the address specified by addr does not fall within the address window of .global
or .shared::cta state space then the behavior is undefined.
tensormap.replace is treated as a weak memory operation, on the entire 1024-bit opaque
tensor-map object, in the Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 8.3.
Target ISA Notes
Requires sm_90a.
Examples

tensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val;






9.7.9. Texture Instructionsï

This section describes PTX instructions for accessing textures and samplers. PTX supports the
following operations on texture and sampler descriptors:

Static initialization of texture and sampler descriptors.
Module-scope and per-entry scope definitions of texture and sampler descriptors.
Ability to query fields within texture and sampler descriptors.



9.7.9.1. Texturing Modesï

For working with textures and samplers, PTX has two modes of operation. In the unified mode,
texture and sampler information is accessed through a single .texref handle. In the independent
mode, texture and sampler information each have their own handle, allowing them to be defined
separately and combined at the site of usage in the program.
The advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior
to sm_3x), with the restriction that they correspond 1-to-1 with the 256 possible textures per
kernel (128 for architectures prior to sm_3x). The advantage of independent mode is that
textures and samplers can be mixed and matched, but the number of samplers is greatly restricted to
32 per kernel (16 for architectures prior to sm_3x).
Table 31 summarizes the number of textures, samplers and
surfaces available in different texturing modes.


Table 31 Texture, sampler and surface limitsï









Texturing mode
Resource
sm_1x, sm_2x
sm_3x+




Unified mode
Textures
128
256



Samplers
128
256



Surfaces
8
16


Independent mode
Textures
128
256



Samplers
16
32



Surfaces
8
16



The texturing mode is selected using .target options texmode_unified and
texmode_independent. A PTX module may declare only one texturing mode. If no texturing mode is
declared, the module is assumed to use unified mode.
Example: calculate an elementâs power contribution as elementâs power/total number of elements.

.target texmode_independent
.global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border,
                               filter_mode = nearest
                             };
...
.entry compute_power
  ( .param .texref tex1 )
{
  txq.width.b32  r6, [tex1]; // get tex1's width
  txq.height.b32 r5, [tex1]; // get tex1's height
  tex.2d.v4.f32.f32  {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}];
  mul.u32 r5, r5, r6;
  add.f32 r1, r1, r2;
  add.f32 r3, r3, r4;
  add.f32 r1, r1, r3;
  cvt.f32.u32 r5, r5;
  div.f32 r1, r1, r5;
}





9.7.9.2. Mipmapsï

A mipmap is a sequence of textures, each of which is a progressively lower resolution
representation of the same image. The height and width of each image, or level of detail (LOD), in
the mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics
applications to improve rendering speed and reduce aliasing artifacts. For example, a
high-resolution mipmap image is used for objects that are close to the user; lower-resolution images
are used as the object appears farther away. Mipmap filtering modes are provided when switching
between two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity.
Example: If the texture has a basic size of 256 by 256 pixels, then the associated mipmap set
may contain a series of eight images, each one-fourth the total area of the previous one: 128Ã128
pixels, 64Ã64, 32Ã32, 16Ã16, 8Ã8, 4Ã4, 2Ã2, 1Ã1 (a single pixel). If, for example, a scene is
rendering this texture in a space of 40Ã40 pixels, then either a scaled up version of the 32Ã32
(without trilinear interpolation) or an interpolation of the 64Ã64 and the 32Ã32 mipmaps (with
trilinear interpolation) would be used.
The total number of LODs in a complete mipmap pyramid is calculated through the following equation:

numLODs = 1 + floor(log2(max(w, h, d)))


The finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st
level, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller
mipmap level has half the {width, height, depth} of the previous level, but if this half value is a
fractional value, itâs rounded down to the next largest integer. Essentially, the size of a mipmap
level can be specified as:

max(1, floor(w_b / 2^i)) x
max(1, floor(h_b / 2^i)) x
max(1, floor(d_b / 2^i))


where i is the ith level beyond the 0th level (the base level). And w_b, h_b and d_b are the
width, height and depth of the base level respectively.
PTX support for mipmaps
The PTX tex instruction supports three modes for specifying the LOD: base, level, and
gradient. In base mode, the instruction always picks level 0. In level mode, an additional
argument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector
arguments provide partials (e.g., {ds/dx, dt/dx} and {ds/dy, dt/dy} for a 2d texture),
which the tex instruction uses to compute the LOD.
These instructions provide access to texture memory.

tex
tld4
txq




9.7.9.3. Texture Instructions: texï

tex
Perform a texture memory lookup.
Syntax

tex.geom.v4.dtype.ctype  d, [a, c] {, e} {, f};
tex.geom.v4.dtype.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler

tex.geom.v2.f16x2.ctype  d[|p], [a, c] {, e} {, f};
tex.geom.v2.f16x2.ctype  d[|p], [a, b, c] {, e} {, f};  // explicit sampler

// mipmaps
tex.base.geom.v4.dtype.ctype   d[|p], [a, {b,} c] {, e} {, f};
tex.level.geom.v4.dtype.ctype  d[|p], [a, {b,} c], lod {, e} {, f};
tex.grad.geom.v4.dtype.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};

tex.base.geom.v2.f16x2.ctype   d[|p], [a, {b,} c] {, e} {, f};
tex.level.geom.v2.f16x2.ctype  d[|p], [a, {b,} c], lod {, e} {, f};
tex.grad.geom.v2.f16x2.ctype   d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f};

.geom  = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms };
.dtype = { .u32, .s32, .f16,  .f32 };
.ctype = {       .s32, .f32 };          // .cube, .acube require .f32
                                        // .2dms, .a2dms require .s32


Description
tex.{1d,2d,3d}
Texture lookup using a texture coordinate vector. The instruction loads data from the texture named
by operand a at coordinates given by operand c into destination d. Operand c is a
scalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a
four-element vector for 3d textures, where the fourth element is ignored. An optional texture
sampler b may be specified. If no sampler is specified, the sampler behavior is a property of
the named texture. The optional destination predicate p is set to True if data from texture
at specified coordinates is resident in memory, False otherwise. When optional destination
predicate p is set to False, data loaded will be all zeros. Memory residency of Texture Data
at specified coordinates is dependent on execution environment setup using Driver API calls, prior
to kernel launch. Refer to Driver API documentation for more details including any
system/implementation specific behavior.
An optional operand e may be specified. Operand e is a vector of .s32 values that
specifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset
value is in the range of -8 to +7. Operand e is a singleton tuple for 1d textures; is a two
element vector 2d textures; and is four-element vector for 3d textures, where the fourth element is
ignored.
An optional operand f may be specified for depth textures. Depth textures are special type
of textures which hold data from the depth buffer. Depth buffer contains depth information of each
pixel. Operand f is .f32 scalar value that specifies depth compare value for depth
textures. Each element fetched from texture is compared against value given in f operand. If
comparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are
used for the filtering. When using depth compare operand, the elements in texture coordinate vector
c have .f32 type.
Depth compare operand is not supported for 3d textures.
The instruction returns a two-element vector for destination type .f16x2. For all other
destination types, the instruction returns a four-element vector. Coordinates may be given in either
signed 32-bit integer or 32-bit floating point form.
A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the
coordinate vector must be naturally aligned to a multiple of the access size. If an address is not
properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently
masking off low-order address bits to achieve proper rounding, or the instruction may fault.
tex.{a1d,a2d}
Texture array selection, followed by texture lookup. The instruction first selects a texture from
the texture array named by operand a using the index given by the first element of the array
coordinate vector c. The instruction then loads data from the selected texture at coordinates
given by the remaining elements of operand c into destination d. Operand c is a bit-size
type vector or tuple containing an index into the array of textures followed by coordinates within
the selected texture, as follows:

For 1d texture arrays, operand c has type .v2.b32. The first element is interpreted as an
unsigned integer index (.u32) into the texture array, and the second element is interpreted as
a 1d texture coordinate of type .ctype.
For 2d texture arrays, operand c has type .v4.b32. The first element is interpreted as an
unsigned integer index (.u32) into the texture array, and the next two elements are
interpreted as 2d texture coordinates of type .ctype. The fourth element is ignored.

An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior
is a property of the named texture.
An optional operand e may be specified. Operand e is a vector of .s32 values that
specifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset
value is in the range of -8 to +7. Operand e is a singleton tuple for 1d texture arrays; and is
a two element vector 2d texture arrays.
An optional operand f may be specified for depth textures arrays. Operand f is .f32
scalar value that specifies depth compare value for depth textures. When using depth compare
operand, the coordinates in texture coordinate vector c have .f32 type.
The instruction returns a two-element vector for destination type .f16x2. For all other
destination types, the instruction returns a four-element vector. The texture array index is a
32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point
values.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
tex.cube
Cubemap texture lookup. The instruction loads data from the cubemap texture named by operand a
at coordinates given by operand c into destination d. Cubemap textures are special
two-dimensional layered textures consisting of six layers that represent the faces of a cube. All
layers in a cubemap are of the same size and are square (i.e., width equals height).
When accessing a cubemap, the texture coordinate vector c has type .v4.f32, and comprises
three floating-point coordinates (s, t, r) and a fourth padding argument which is
ignored. Coordinates (s, t, r) are projected onto one of the six cube faces. The (s,
t, r) coordinates can be thought of as a direction vector emanating from the center of the
cube. Of the three coordinates (s, t, r), the coordinate of the largest magnitude (the
major axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by
the absolute value of the major axis to produce a new (s, t) coordinate pair to lookup into
the selected cube face.
An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior
is a property of the named texture.
Offset vector operand e is not supported for cubemap textures.
an optional operand f may be specified for cubemap depth textures. operand f is .f32
scalar value that specifies depth compare value for cubemap depth textures.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
tex.acube
Cubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture
from the cubemap array named by operand a using the index given by the first element of the
array coordinate vector c. The instruction then loads data from the selected cubemap texture at
coordinates given by the remaining elements of operand c into destination d.
Cubemap array textures consist of an array of cubemaps, i.e., the total number of layers is a
multiple of six. When accessing a cubemap array texture, the coordinate vector c has type
.v4.b32. The first element is interpreted as an unsigned integer index (.u32) into the
cubemap array, and the remaining three elements are interpreted as floating-point cubemap
coordinates (s, t, r), used to lookup in the selected cubemap as described above.
An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior
is a property of the named texture.
Offset vector operand e is not supported for cubemap texture arrays.
An optional operand f may be specified for cubemap depth texture arrays. Operand f is
.f32 scalar value that specifies depth compare value for cubemap depth textures.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
tex.2dms
Multi-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of
multiple samples per data element. The instruction loads data from the texture named by operand
a from sample number given by first element of the operand c, at coordinates given by
remaining elements of operand c into destination d. When accessing a multi-sample texture,
texture coordinate vector c has type .v4.b32. The first element in operand c is
interpreted as unsigned integer sample number (.u32), and the next two elements are interpreted
as signed integer (.s32) 2d texture coordinates. The fourth element is ignored. An optional
texture sampler b may be specified. If no sampler is specified, the sampler behavior is a
property of the named texture.
An optional operand e may be specified. Operand e is a vector of type .v2.s32 that
specifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset
value is in the range of -8 to +7.
Depth compare operand f is not supported for multi-sample textures.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
tex.a2dms
Multi-sample texture array selection, followed by multi-sample texture lookup. The instruction first
selects a multi-sample texture from the multi-sample texture array named by operand a using the
index given by the first element of the array coordinate vector c. The instruction then loads
data from the selected multi-sample texture from sample number given by second element of the
operand c, at coordinates given by remaining elements of operand c into destination
d. When accessing a multi-sample texture array, texture coordinate vector c has type
.v4.b32. The first element in operand c is interpreted as unsigned integer sampler number, the
second element is interpreted as unsigned integer index (.u32) into the multi-sample texture
array and the next two elements are interpreted as signed integer (.s32) 2d texture
coordinates. An optional texture sampler b may be specified. If no sampler is specified, the
sampler behavior is a property of the named texture.
An optional operand e may be specified. Operand e is a vector of type .v2.s32 values
that specifies coordinate offset. Offset is applied to coordinates before doing texture
lookup. Offset value is in the range of -8 to +7.
Depth compare operand f is not supported for multi-sample texture arrays.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
Mipmaps


.base (lod zero)

Pick level 0 (base level). This is the default if no mipmap mode is specified. No additional arguments.


.level (lod explicit)

Requires an additional 32-bit scalar argument, lod, which contains the LOD to fetch from. The
type of lod follows .ctype (either .s32 or .f32). Geometries .2dms and
.a2dms are not supported in this mode.


.grad (lod gradient)

Requires two .f32 vectors, dPdx and dPdy, that specify the partials. The vectors are
singletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are
four-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d
and cube geometries. Geometries .2dms and .a2dms are not supported in this mode.


For mipmap texture lookup, an optional operand e may be specified. Operand e is a vector of
.s32 that specifies coordinate offset. Offset is applied to coordinates before doing texture
lookup. Offset value is in the range of -8 to +7. Offset vector operand is not supported for cube
and cubemap geometries.
An optional operand f may be specified for mipmap textures. Operand f is .f32 scalar
value that specifies depth compare value for depth textures. When using depth compare operand, the
coordinates in texture coordinate vector c have .f32 type.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
Depth compare operand is not supported for 3d textures.
Indirect texture access
Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target
architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding
the address of a .texref variable.
Notes
For compatibility with prior versions of PTX, the square brackets are not required and .v4
coordinate vectors are allowed for any geometry, with the extra elements being ignored.
PTX ISA Notes
Unified mode texturing introduced in PTX ISA version 1.0. Extension using opaque .texref and
.samplerref types and independent mode texturing introduced in PTX ISA version 1.5.
Texture arrays tex.{a1d,a2d} introduced in PTX ISA version 2.3.
Cubemaps and cubemap arrays introduced in PTX ISA version 3.0.
Support for mipmaps introduced in PTX ISA version 3.1.
Indirect texture access introduced in PTX ISA version 3.1.
Multi-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2.
Support for textures returning .f16 and .f16x2 data introduced in PTX ISA version 4.2.
Support for tex.grad.{cube, acube} introduced in PTX ISA version 4.3.
Offset vector operand introduced in PTX ISA version 4.3.
Depth compare operand introduced in PTX ISA version 4.3.
Support for optional destination predicate introduced in PTX ISA version 7.1.
Target ISA Notes
Supported on all target architectures.
The cubemap array geometry (.acube) requires sm_20 or higher.
Mipmaps require sm_20 or higher.
Indirect texture access requires sm_20 or higher.
Multi-sample textures and multi-sample texture arrays require sm_30 or higher.
Texture fetch returning .f16 and .f16x2 data require sm_53 or higher.
tex.grad.{cube, acube} requires sm_20 or higher.
Offset vector operand requires sm_30 or higher.
Depth compare operand requires sm_30 or higher.
Support for optional destination predicate requires sm_60 or higher.
Examples

 // Example of unified mode texturing
 // - f4 is required to pad four-element tuple and is ignored
 tex.3d.v4.s32.s32  {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}];

 // Example of independent mode texturing
 tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}];

 // Example of 1D texture array, independent texturing mode
 tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}];

 // Example of 2D texture array, unified texturing mode
 // - f3 is required to pad four-element tuple and is ignored
 tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}];

 // Example of cubemap array, unified textureing mode
 tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}];

 // Example of multi-sample texture, unified texturing mode
 tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}];

 // Example of multi-sample texture, independent texturing mode
 tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}];

 // Example of multi-sample texture array, unified texturing mode
 tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}];

 // Example of texture returning .f16 data
 tex.1d.v4.f16.f32  {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}];

 // Example of texture returning .f16x2 data
 tex.1d.v2.f16x2.f32  {h1,h2}, [tex_a,smpl_x,{f1}];

 // Example of 3d texture array access with tex.grad,unified texturing mode
 tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}],
                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};

// Example of cube texture array access with tex.grad,unified texturing mode
 tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}],
                 {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3};

 // Example of 1d texture lookup with offset, unified texturing mode
 tex.1d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a, {f1}], {r5};

 // Example of 2d texture array lookup with offset, unified texturing mode
 tex.a2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6};

 // Example of 2d mipmap texture lookup with offset, unified texturing mode
 tex.level.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}],
                          flvl, {r7, r8};

 // Example of 2d depth texture lookup with compare, unified texturing mode
 tex.1d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a, {f1}], f0;

 // Example of depth 2d texture array lookup with offset, compare
 tex.a2d.v4.s32.f32  {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6;

 // Example of destination predicate use
 tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}];





9.7.9.4. Texture Instructions: tld4ï

tld4
Perform a texture fetch of the 4-texel bilerp footprint.
Syntax

tld4.comp.2d.v4.dtype.f32    d[|p], [a, c] {, e} {, f};
tld4.comp.geom.v4.dtype.f32  d[|p], [a, b, c] {, e} {, f};  // explicit sampler

.comp  = { .r, .g, .b, .a };
.geom  = { .2d, .a2d, .cube, .acube };
.dtype = { .u32, .s32, .f32 };


Description
Texture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction
loads the bilerp footprint from the texture named by operand a at coordinates given by operand
c into vector destination d. The texture component fetched for each texel sample is
specified by .comp. The four texel samples are placed into destination vector d in
counter-clockwise order starting at lower left.
An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior
is a property of the named texture.
The optional destination predicate p is set to True if data from texture at specified
coordinates is resident in memory, False otherwise. When optional destination predicate p is
set to False, data loaded will be all zeros. Memory residency of Texture Data at specified
coordinates is dependent on execution environment setup using Driver API calls, prior to kernel
launch. Refer to Driver API documentation for more details including any system/implementation
specific behavior.
An optional operand f may be specified for depth textures. Depth textures are special type of
textures which hold data from the depth buffer. Depth buffer contains depth information of each
pixel. Operand f is .f32 scalar value that specifies depth compare value for depth
textures. Each element fetched from texture is compared against value given in f operand. If
comparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are
used for the filtering.
A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the
coordinate vector must be naturally aligned to a multiple of the access size. If an address is not
properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently
masking off low-order address bits to achieve proper rounding, or the instruction may fault.
tld4.2d
For 2D textures, operand c specifies coordinates as a two-element, 32-bit floating-point vector.
An optional operand e may be specified. Operand e is a vector of type .v2.s32 that
specifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset
value is in the range of -8 to +7.
tld4.a2d
Texture array selection, followed by tld4 texture fetch of 2d texture. For 2d texture arrays
operand c is a four element, 32-bit vector. The first element in operand c is interpreted as an
unsigned integer index (.u32) into the texture array, and the next two elements are interpreted
as 32-bit floating point coordinates of 2d texture. The fourth element is ignored.
An optional operand e may be specified. Operand e is a vector of type .v2.s32 that
specifies coordinate offset. Offset is applied to coordinates before doing texture fetch. Offset
value is in the range of -8 to +7.
tld4.cube
For cubemap textures, operand c specifies four-element vector which comprises three
floating-point coordinates (s, t, r) and a fourth padding argument which is ignored.
Cubemap textures are special two-dimensional layered textures consisting of six layers that
represent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e.,
width equals height).
Coordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be
thought of as a direction vector emanating from the center of the cube. Of the three coordinates (s,
t, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the
other two coordinates (the minor axes) are divided by the absolute value of the major axis to
produce a new (s, t) coordinate pair to lookup into the selected cube face.
Offset vector operand e is not supported for cubemap textures.
tld4.acube
Cubemap array selection, followed by tld4 texture fetch of cubemap texture. The first element in
operand c is interpreted as an unsigned integer index (.u32) into the cubemap texture array,
and the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r),
used to lookup in the selected cubemap.
Offset vector operand e is not supported for cubemap texture arrays.
Indirect texture access
Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target
architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding
the address of a .texref variable.
PTX ISA Notes
Introduced in PTX ISA version 2.2.
Indirect texture access introduced in PTX ISA version 3.1.
tld4.{a2d,cube,acube} introduced in PTX ISA version 4.3.
Offset vector operand introduced in PTX ISA version 4.3.
Depth compare operand introduced in PTX ISA version 4.3.
Support for optional destination predicate introduced in PTX ISA version 7.1.
Target ISA Notes
tld4 requires sm_20 or higher.
Indirect texture access requires sm_20 or higher.
tld4.{a2d,cube,acube} requires sm_30 or higher.
Offset vector operand requires sm_30 or higher.
Depth compare operand requires sm_30 or higher.
Support for optional destination predicate requires sm_60 or higher.
Examples

//Example of unified mode texturing
tld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}];

// Example of independent mode texturing
tld4.r.2d.v4.u32.f32  {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}];

// Example of unified mode texturing using offset
tld4.r.2d.v4.s32.f32  {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};

// Example of unified mode texturing using compare
tld4.r.2d.v4.f32.f32  {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7;

// Example of optional destination predicate
tld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7;





9.7.9.5. Texture Instructions: txqï

txq
Query texture and sampler attributes.
Syntax

txq.tquery.b32         d, [a];       // texture attributes
txq.level.tlquery.b32  d, [a], lod;  // texture attributes
txq.squery.b32         d, [a];       // sampler attributes

.tquery  = { .width, .height, .depth,
             .channel_data_type, .channel_order,
             .normalized_coords, .array_size,
             .num_mipmap_levels, .num_samples};

.tlquery = { .width, .height, .depth };

.squery  = { .force_unnormalized_coords, .filter_mode,
             .addr_mode_0, addr_mode_1, addr_mode_2 };


Description
Query an attribute of a texture or sampler. Operand a is either a .texref or .samplerref variable, or a .u64 register.







Query
Returns





.width
.height
.depth

value in elements


.channel_data_type
Unsigned integer corresponding to source languageâs channel data type
enumeration. If the source language combines channel data type and channel
order into a single enumeration type, that value is returned for both
channel_data_type and channel_order queries.


.channel_order
Unsigned integer corresponding to source languageâs channel order
enumeration. If the source language combines channel data type and channel
order into a single enumeration type, that value is returned for both
channel_data_type and channel_order queries.


.normalized_coords
1 (True) or 0 (False).


.force_unnormalized_coords
1 (True) or 0 (False). Defined only for .samplerref
variables in independent texture mode. Overrides the normalized_coords
field of a .texref variable used with a .samplerref in a tex
instruction.


.filter_mode
Integer from enum { nearest, linear }



.addr_mode_0
.addr_mode_1
.addr_mode_2

Integer from
enum { wrap, mirror, clamp_ogl, clamp_to_edge, clamp_to_border }


.array_size
For a texture array, number of textures in array, 0 otherwise.


.num_mipmap_levels
For a mipmapped texture, number of levels of details (LOD), 0 otherwise.


.num_samples
For a multi-sample texture, number of samples, 0 otherwise.



Texture attributes are queried by supplying a .texref argument to txq. In unified mode,
sampler attributes are also accessed via a .texref argument, and in independent mode sampler
attributes are accessed via a separate .samplerref argument.
txq.level
txq.level requires an additional 32bit integer argument, lod, which specifies LOD and
queries requested attribute for the specified LOD.
Indirect texture access
Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target
architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding
the address of a .texref variable.
PTX ISA Notes
Introduced in PTX ISA version 1.5.
Channel data type and channel order queries were added in PTX ISA version 2.1.
The .force_unnormalized_coords query was added in PTX ISA version 2.2.
Indirect texture access introduced in PTX ISA version 3.1.
.array_size, .num_mipmap_levels, .num_samples samples queries were added in PTX ISA
version 4.1.
txq.level introduced in PTX ISA version 4.3.
Target ISA Notes
Supported on all target architectures.
Indirect texture access requires sm_20 or higher.
Querying the number of mipmap levels requires sm_20 or higher.
Querying the number of samples requires sm_30 or higher.
txq.level requires sm_30 or higher.
Examples

txq.width.b32       %r1, [tex_A];
txq.filter_mode.b32 %r1, [tex_A];   // unified mode
txq.addr_mode_0.b32 %r1, [smpl_B];  // independent mode
txq.level.width.b32 %r1, [tex_A], %r_lod;





9.7.9.6. Texture Instructions: istypepï

istypep
Query whether a register points to an opaque variable of a specified type.
Syntax

istypep.type   p, a;  // result is .pred

.type = { .texref, .samplerref, .surfref };


Description
Write predicate register p with 1 if register a points to an opaque variable of the
specified type, and with 0 otherwise. Destination p has type .pred; the source address
operand must be of type .u64.
PTX ISA Notes
Introduced in PTX ISA version 4.0.
Target ISA Notes
istypep requires sm_30 or higher.
Examples

istypep.texref istex, tptr;
istypep.samplerref issampler, sptr;
istypep.surfref issurface, surfptr;






9.7.10. Surface Instructionsï

This section describes PTX instructions for accessing surfaces. PTX supports the following
operations on surface descriptors:

Static initialization of surface descriptors.
Module-scope and per-entry scope definitions of surface descriptors.
Ability to query fields within surface descriptors.

These instructions provide access to surface memory.

suld
sust
sured
suq



9.7.10.1. Surface Instructions: suldï

suld
Load from surface memory.
Syntax

suld.b.geom{.cop}.vec.dtype.clamp  d, [a, b];  // unformatted

.geom  = { .1d, .2d, .3d, .a1d, .a2d };
.cop   = { .ca, .cg, .cs, .cv };               // cache operation
.vec   = { none, .v2, .v4 };
.dtype = { .b8 , .b16, .b32, .b64 };
.clamp = { .trap, .clamp, .zero };


Description
suld.b.{1d,2d,3d}
Load from surface memory using a surface coordinate vector. The instruction loads data from the
surface named by operand a at coordinates given by operand b into destination d. Operand
a is a .surfref variable or .u64 register. Operand b is a scalar or singleton tuple
for 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d
surfaces, where the fourth element is ignored. Coordinate elements are of type .s32.
suld.b performs an unformatted load of binary data. The lowest dimension coordinate represents a
byte offset into the surface and is not scaled, and the size of the data transfer matches the size
of destination operand d.
suld.b.{a1d,a2d}
Surface layer selection, followed by a load from the selected surface. The instruction first selects
a surface layer from the surface array named by operand a using the index given by the first
element of the array coordinate vector b. The instruction then loads data from the selected
surface at coordinates given by the remaining elements of operand b into destination
d. Operand a is a .surfref variable or .u64 register. Operand b is a bit-size
type vector or tuple containing an index into the array of surfaces followed by coordinates within
the selected surface, as follows:
For 1d surface arrays, operand b has type .v2.b32. The first element is interpreted as an
unsigned integer index (.u32) into the surface array, and the second element is interpreted as a
1d surface coordinate of type .s32.
For 2d surface arrays, operand b has type .v4.b32. The first element is interpreted as an
unsigned integer index (.u32) into the surface array, and the next two elements are interpreted
as 2d surface coordinates of type .s32. The fourth element is ignored.
A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the
coordinate vector must be naturally aligned to a multiple of the access size. If an address is not
properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently
masking off low-order address bits to achieve proper rounding, or the instruction may fault.
The .clamp field specifies how to handle out-of-bounds addresses:

.trap

causes an execution trap on out-of-bounds addresses

.clamp

loads data at the nearest surface location (sized appropriately)

.zero

loads zero for out-of-bounds addresses


Indirect surface access
Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture
sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of
a .surfref variable.
PTX ISA Notes
suld.b.trap introduced in PTX ISA version 1.5.
Additional clamp modifiers and cache operations introduced in PTX ISA version 2.0.
suld.b.3d andsuld.b.{a1d,a2d} introduced in PTX ISA version 3.0.
Indirect surface access introduced in PTX ISA version 3.1.
Target ISA Notes
suld.b supported on all target architectures.
sm_1x targets support only the .trap clamping modifier.
suld.3d andsuld.{a1d,a2d} require sm_20 or higher.
Indirect surface access requires sm_20 or higher.
Cache operations require sm_20 or higher.
Examples

suld.b.1d.v4.b32.trap  {s1,s2,s3,s4}, [surf_B, {x}];
suld.b.3d.v2.b64.trap  {r1,r2}, [surf_A, {x,y,z,w}];
suld.b.a1d.v2.b32      {r0,r1}, [surf_C, {idx,x}];
suld.b.a2d.b32         r0, [surf_D, {idx,x,y,z}];  // z ignored





9.7.10.2. Surface Instructions: sustï

sust
Store to surface memory.
Syntax

sust.b.{1d,2d,3d}{.cop}.vec.ctype.clamp  [a, b], c;  // unformatted
sust.p.{1d,2d,3d}.vec.b32.clamp          [a, b], c;  // formatted

sust.b.{a1d,a2d}{.cop}.vec.ctype.clamp   [a, b], c;  // unformatted

.cop   = { .wb, .cg, .cs, .wt };                     // cache operation
.vec   = { none, .v2, .v4 };
.ctype = { .b8 , .b16, .b32, .b64 };
.clamp = { .trap, .clamp, .zero };


Description
sust.{1d,2d,3d}
Store to surface memory using a surface coordinate vector. The instruction stores data from operand
c to the surface named by operand a at coordinates given by operand b. Operand a is
a .surfref variable or .u64 register. Operand b is a scalar or singleton tuple for 1d
surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces,
where the fourth element is ignored. Coordinate elements are of type .s32.
sust.b performs an unformatted store of binary data. The lowest dimension coordinate represents
a byte offset into the surface and is not scaled. The size of the data transfer matches the size of
source operand c.
sust.p performs a formatted store of a vector of 32-bit data values to a surface sample. The
source vector elements are interpreted left-to-right as R, G, B, and A surface
components. These elements are written to the corresponding surface sample components. Source
elements that do not occur in the surface sample are ignored. Surface sample components that do not
occur in the source vector will be written with an unpredictable value. The lowest dimension
coordinate represents a sample offset rather than a byte offset.
The source data interpretation is based on the surface sample format as follows: If the surface
format contains UNORM, SNORM, or FLOAT data, then .f32 is assumed; if the surface
format contains UINT data, then .u32 is assumed; if the surface format contains SINT
data, then .s32 is assumed. The source data is then converted from this type to the surface
sample format.
sust.b.{a1d,a2d}
Surface layer selection, followed by an unformatted store to the selected surface. The instruction
first selects a surface layer from the surface array named by operand a using the index given by
the first element of the array coordinate vector b. The instruction then stores the data in
operand c to the selected surface at coordinates given by the remaining elements of operand
b. Operand a is a .surfref variable or .u64 register. Operand b is a bit-size type
vector or tuple containing an index into the array of surfaces followed by coordinates within the
selected surface, as follows:

For 1d surface arrays, operand b has type .v2.b32. The first element is interpreted as an
unsigned integer index (.u32) into the surface array, and the second element is interpreted as
a 1d surface coordinate of type .s32.
For 2d surface arrays, operand b has type .v4.b32. The first element is interpreted as an
unsigned integer index (.u32) into the surface array, and the next two elements are
interpreted as 2d surface coordinates of type .s32. The fourth element is ignored.

A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the
coordinate vector must be naturally aligned to a multiple of the access size. If an address is not
properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently
masking off low-order address bits to achieve proper rounding, or the instruction may fault.
The .clamp field specifies how to handle out-of-bounds addresses:

.trap

causes an execution trap on out-of-bounds addresses

.clamp

stores data at the nearest surface location (sized appropriately)

.zero

drops stores to out-of-bounds addresses


Indirect surface access
Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture
sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of
a .surfref variable.
PTX ISA Notes
sust.b.trap introduced in PTX ISA version 1.5.Â  sust.p, additional clamp modifiers, and
cache operations introduced in PTX ISA version 2.0.
sust.b.3d and sust.b.{a1d,a2d} introduced in PTX ISA version 3.0.
Indirect surface access introduced in PTX ISA version 3.1.
Target ISA Notes
sust.b supported on all target architectures.
sm_1x targets support only the .trap clamping modifier.
sust.3d and sust.{a1d,a2d} require sm_20 or higher.
sust.p requires sm_20 or higher.
Indirect surface access requires sm_20 or higher.
Cache operations require sm_20 or higher.
Examples

sust.p.1d.v4.b32.trap  [surf_B, {x}], {f1,f2,f3,f4};
sust.b.3d.v2.b64.trap  [surf_A, {x,y,z,w}], {r1,r2};
sust.b.a1d.v2.b64      [surf_C, {idx,x}], {r1,r2};
sust.b.a2d.b32         [surf_D, {idx,x,y,z}], r0;  // z ignored





9.7.10.3. Surface Instructions: suredï

sured
Reduce surface memory.
Syntax

sured.b.op.geom.ctype.clamp  [a,b],c; // byte addressing
sured.p.op.geom.ctype.clamp  [a,b],c; // sample addressing

.op    = { .add, .min, .max, .and, .or };
.geom  = { .1d, .2d, .3d };
.ctype = { .u32, .u64, .s32, .b32, .s64 };  // for sured.b
.ctype = { .b32, .b64 };                    // for sured.p
.clamp = { .trap, .clamp, .zero };


Description
Reduction to surface memory using a surface coordinate vector. The instruction performs a reduction
operation with data from operand c to the surface named by operand a at coordinates given by
operand b. Operand a is a .surfref variable or .u64 register. Operand b is a
scalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a
four-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of
type .s32.
sured.b performs an unformatted reduction on .u32, .s32, .b32, .u64, or .s64
data. The lowest dimension coordinate represents a byte offset into the surface and is not
scaled. Operation add applies to .u32, .u64, and .s32 types; min and max
apply to .u32, .s32, .u64 and .s64 types; operations and and or apply to
.b32 type.
sured.p performs a reduction on sample-addressed data. The lowest dimension coordinate
represents a sample offset rather than a byte offset. The instruction type .b64 is restricted to
min and max operations. For type .b32, the data is interpreted as .u32 or .s32
based on the surface sample format as follows: if the surface format contains UINT data, then
.u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. For
type .b64, if the surface format contains UINT data, then .u64 is assumed; if the
surface format contains SINT data, then .s64 is assumed.
A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the
coordinate vector must be naturally aligned to a multiple of the access size. If an address is not
properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently
masking off low-order address bits to achieve proper rounding, or the instruction may fault.
The .clamp field specifies how to handle out-of-bounds addresses:

.trap

causes an execution trap on out-of-bounds addresses

.clamp

stores data at the nearest surface location (sized appropriately)

.zero

drops stores to out-of-bounds addresses


Indirect surface access
Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture
sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of
a .surfref variable.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Indirect surface access introduced in PTX ISA version 3.1.
.u64/.s64/.b64 types with .min/.max operations introduced in PTX ISA version
8.1.
Target ISA Notes
sured requires sm_20 or higher.
Indirect surface access requires sm_20 or higher.
.u64/.s64/.b64 types with .min/.max operations requires sm_50 or higher.
Examples

sured.b.add.2d.u32.trap  [surf_A, {x,y}], r1;
sured.p.min.1d.u32.trap  [surf_B, {x}], r1;
sured.b.max.1d.u64.trap  [surf_C, {x}], r1;
sured.p.min.1d.b64.trap  [surf_D, {x}], r1;





9.7.10.4. Surface Instructions: suqï

suq
Query a surface attribute.
Syntax

suq.query.b32   d, [a];

.query = { .width, .height, .depth,
           .channel_data_type, .channel_order,
           .array_size, .memory_layout };


Description
Query an attribute of a surface. Operand a is a .surfref variable or a .u64 register.







Query
Returns





.width
.height
.depth

value in elements


.channel_data_type
Unsigned integer corresponding to source languageâs channel data
type enumeration. If the source language combines channel data
type and channel order into a single enumeration type, that value
is returned for both channel_data_type and channel_order
queries.


.channel_order
Unsigned integer corresponding to source languageâs channel order
enumeration. If the source language combines channel data type and
channel order into a single enumeration type, that value is
returned for both channel_data_type and channel_order
queries.


.array_size
For a surface array, number of surfaces in array, 0 otherwise.


.memory_layout
1 for surface with linear memory layout; 0 otherwise



Indirect surface access
Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture
sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of
a .surfref variable.
PTX ISA Notes
Introduced in PTX ISA version 1.5.
Channel data type and channel order queries added in PTX ISA version 2.1.
Indirect surface access introduced in PTX ISA version 3.1.
The .array_size query was added in PTX ISA version 4.1.
The .memory_layout query was added in PTX ISA version 4.2.
Target ISA Notes
Supported on all target architectures.
Indirect surface access requires sm_20 or higher.
Examples

suq.width.b32       %r1, [surf_A];






9.7.11. Control Flow Instructionsï

The following PTX instructions and syntax are for controlling execution in a PTX program:

{}
@
bra
call
ret
exit



9.7.11.1. Control Flow Instructions: {}ï

{}
Instruction grouping.
Syntax

{ instructionList }


Description
The curly braces create a group of instructions, used primarily for defining a function body. The
curly braces also provide a mechanism for determining the scope of a variable: any variable declared
within a scope is not available outside the scope.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

{ add.s32  a,b,c; mov.s32  d,a; }





9.7.11.2. Control Flow Instructions: @ï

@
Predicated execution.
Syntax

@{!}p    instruction;


Description
Execute an instruction or instruction block for threads that have the guard predicate
True. Threads with a False guard predicate do nothing.
Semantics
If {!}p then instruction
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

    setp.eq.f32  p,y,0;     // is y zero?
@!p div.f32      ratio,x,y  // avoid division by zero

@q  bra L23;                // conditional branch





9.7.11.3. Control Flow Instructions: braï

bra
Branch to a target and continue execution there.
Syntax

@p   bra{.uni}  tgt;           // tgt is a label
     bra{.uni}  tgt;           // unconditional branch


Description
Continue execution at the target. Conditional branches are specified by using a guard predicate. The
branch target must be a label.
bra.uni is guaranteed to be non-divergent, i.e. all active threads in a warp that are currently
executing this instruction have identical values for the guard predicate and branch target.
Semantics

if (p) {
    pc = tgt;
}


PTX ISA Notes
Introduced in PTX ISA version 1.0.
Unimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec.
Target ISA Notes
Supported on all target architectures.
Examples

bra.uni  L_exit;    // uniform unconditional jump
@q  bra      L23;   // conditional branch





9.7.11.4. Control Flow Instructions: brx.idxï

brx.idx
Branch to a label indexed from a list of potential branch targets.
Syntax

@p    brx.idx{.uni} index, tlist;
      brx.idx{.uni} index, tlist;


Description
Index into a list of possible destination labels, and continue execution from the chosen
label. Conditional branches are specified by using a guard predicate.
brx.idx.uni guarantees that the branch is non-divergent, i.e. all active threads in a warp that
are currently executing this instruction have identical values for the guard predicate and the
index argument.
The index operand is a .u32 register. The tlist operand must be the label of a
.branchtargets directive. It is accessed as a zero-based sequence using index. Behaviour is
undefined if the value of index is greater than or equal to the length of tlist.
The .branchtargets directive must be defined in the local function scope before it is used. It
must refer to labels within the current function.
Semantics

if (p) {
    if (index < length(tlist)) {
      pc = tlist[index];
    } else {
      pc = undefined;
    }
}


PTX ISA Notes
Introduced in PTX ISA version 6.0.
Target ISA Notes
Requires sm_30 or higher.
Examples

.function foo () {
    .reg .u32 %r0;
    ...
    L1:
    ...
    L2:
    ...
    L3:
    ...
    ts: .branchtargets L1, L2, L3;
    @p brx.idx %r0, ts;
    ...
}





9.7.11.5. Control Flow Instructions: callï

call
Call a function, recording the return location.
Syntax

// direct call to named function, func is a symbol
call{.uni} (ret-param), func, (param-list);
call{.uni} func, (param-list);
call{.uni} func;

// indirect call via pointer, with full list of call targets
call{.uni} (ret-param), fptr, (param-list), flist;
call{.uni} fptr, (param-list), flist;
call{.uni} fptr, flist;

// indirect call via pointer, with no knowledge of call targets
call{.uni} (ret-param), fptr, (param-list), fproto;
call{.uni} fptr, (param-list), fproto;
call{.uni} fptr, fproto;


Description
The call instruction stores the address of the next instruction, so execution can resume at that
point after executing a ret instruction. A call is assumed to be divergent unless the
.uni suffix is present. The .uni suffix indicates that the call is guaranteed to be
non-divergent, i.e. all active threads in a warp that are currently executing this instruction have
identical values for the guard predicate and call target.
For direct calls, the called location func must be a symbolic function name; for indirect calls,
the called location fptr must be an address of a function held in a register. Input arguments
and return values are optional.Â Arguments may be registers, immediate constants, or variables in
.param space. Arguments are pass-by-value.
Indirect calls require an additional operand, flist or fproto, to communicate the list of
potential call targets or the common function prototype of all call targets,
respectively. In the first case, flist gives a complete list of potential call targets and
the optimizing backend is free to optimize the calling convention. In the second case, where the
complete list of potential call targets may not be known, the common function prototype is given
and the call must obey the ABIâs calling convention.
The flist operand is either the name of an array (call table) initialized to a list of function
names; or a label associated with a .calltargets directive, which declares a list of potential
call targets. In both cases the fptr register holds the address of a function listed in the call
table or .calltargets list, and the call operands are type-checked against the type
signature of the functions indicated by flist.
The fproto operand is the name of a label associated with a .callprototype directive. This
operand is used when a complete list of potential targets is not known. The call operands are
type-checked against the prototype, and code generation will follow the ABI calling convention. If a
function that doesnât match the prototype is called, the behavior is undefined.
Call tables may be declared at module scope or local scope, in either the constant or global state
space. The .calltargets and .callprototype directives must be declared within a function
body. All functions must be declared prior to being referenced in a call table initializer or
.calltargets directive.
PTX ISA Notes
Direct call introduced in PTX ISA version 1.0. Indirect call introduced in PTX ISA version 2.1.
Target ISA Notes
Direct call supported on all target architectures. Indirect call requires sm_20 or higher.
Examples

// examples of direct call
    call     init;    // call function 'init'
    call.uni g, (a);  // call function 'g' with parameter 'a'
@p  call     (d), h, (a, b);  // return value into register d

// call-via-pointer using jump table
.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...
.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...
.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...

.global .u32 jmptbl[5] = { foo, bar, baz };
      ...
@p    ld.global.u32  %r0, [jmptbl+4];
@p    ld.global.u32  %r0, [jmptbl+8];
      call  (retval), %r0, (x, y), jmptbl;

// call-via-pointer using .calltargets directive
.func (.reg .u32 rv) foo (.reg .u32 a, .reg .u32 b) ...
.func (.reg .u32 rv) bar (.reg .u32 a, .reg .u32 b) ...
.func (.reg .u32 rv) baz (.reg .u32 a, .reg .u32 b) ...
      ...
@p    mov.u32  %r0, foo;
@q    mov.u32  %r0, baz;
Ftgt: .calltargets foo, bar, baz;
      call  (retval), %r0, (x, y), Ftgt;

// call-via-pointer using .callprototype directive
.func dispatch (.reg .u32 fptr, .reg .u32 idx)
{
...
Fproto: .callprototype _ (.param .u32 _, .param .u32 _);
      call  %fptr, (x, y), Fproto;
...





9.7.11.6. Control Flow Instructions: retï

ret
Return from function to instruction after call.
Syntax

ret{.uni};


Description
Return execution to callerâs environment. A divergent return suspends threads until all threads are
ready to return to the caller. This allows multiple divergent ret instructions.
A ret is assumed to be divergent unless the .uni suffix is present, indicating that the
return is guaranteed to be non-divergent.
Any values returned from a function should be moved into the return parameter variables prior to
executing the ret instruction.
A return instruction executed in a top-level entry routine will terminate thread execution.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

    ret;
@p  ret;





9.7.11.7. Control Flow Instructions: exitï

exit
Terminate a thread.
Syntax

exit;


Description
Ends execution of a thread.
As threads exit, barriers waiting on all threads are checked to see if the exiting threads are the
only threads that have not yet made it to a barrier{.cta} for all threads in the CTA or to a
barrier.cluster for all threads in the cluster. If the exiting threads are holding up the
barrier, the barrier is released.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

    exit;
@p  exit;






9.7.12. Parallel Synchronization and Communication Instructionsï

These instructions are:

bar{.cta}, barrier{.cta}
barrier.cluster
bar.warp.sync
membar
atom
red
red.async
vote
match.sync
activemask
redux.sync
griddepcontrol
elect.sync
mbarrier.init
mbarrier.inval
mbarrier.arrive
mbarrier.arrive_drop
mbarrier.test_wait
mbarrier.try_wait
mbarrier.pending_count
cp.async.mbarrier.arrive
tensormap.cp_fenceproxy



9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrierï

bar{.cta}, barrier{.cta}
Barrier synchronization.
Syntax

barrier{.cta}.sync{.aligned}      a{, b};
barrier{.cta}.arrive{.aligned}    a, b;

barrier{.cta}.red.popc{.aligned}.u32  d, a{, b}, {!}c;
barrier{.cta}.red.op{.aligned}.pred   p, a{, b}, {!}c;

bar{.cta}.sync      a{, b};
bar{.cta}.arrive    a, b;

bar{.cta}.red.popc.u32  d, a{, b}, {!}c;
bar{.cta}.red.op.pred   p, a{, b}, {!}c;

.op = { .and, .or };


Description
Performs barrier synchronization and communication within a CTA. Each CTA instance has sixteen
barriers numbered 0..15.
barrier{.cta} instructions can be used by the threads within the CTA for synchronization and
communication.
Operands a, b, and d have type .u32; operands p and c are predicates. Source
operand a specifies a logical barrier resource as an immediate constant or register with value
0 through 15. Operand b specifies the number of threads participating in the barrier. If
no thread count is specified, all threads in the CTA participate in the barrier. When specifying a
thread count, the value must be a multiple of the warp size. Note that a non-zero thread count is
required for barrier{.cta}.arrive.
Depending on operand b, either specified number of threads (in multiple of warp size) or all
threads in the CTA participate in barrier{.cta} instruction. The barrier{.cta} instructions
signal the arrival of the executing threads at the named barrier.
barrier{.cta} instruction causes executing thread to wait for all non-exited threads from its
warp and marks warpsâ arrival at barrier. In addition to signaling its arrival at the barrier, the
barrier{.cta}.red and barrier{.cta}.sync instructions causes executing thread to wait for
non-exited threads of all other warps participating in the barrier to
arrive. barrier{.cta}.arrive does not cause executing thread to wait for threads of other
participating warps.
When a barrier completes, the waiting threads are restarted without delay, and the barrier is
reinitialized so that it can be immediately reused.
The barrier{.cta}.sync or barrier{.cta}.red or barrier{.cta}.arrive instruction
guarantees that when the barrier completes, prior memory accesses requested by this thread are
performed relative to all threads participating in the barrier. The barrier{.cta}.sync and
barrier{.cta}.red instruction further guarantees that no new memory access is requested by this
thread before the barrier completes.
A memory read (e.g., by ld or atom) has been performed when the value read has been
transmitted from memory and cannot be modified by another thread participating in the barrier. A
memory write (e.g., by st, red or atom) has been performed when the value written has
become visible to other threads participating in the barrier, that is, when the previous value can
no longer be read.
barrier{.cta}.red performs a reduction operation across threads. The c predicate (or its
complement) from all threads in the CTA are combined using the specified reduction operator. Once
the barrier count is reached, the final value is written to the destination register in all threads
waiting at the barrier.
The reduction operations for barrier{.cta}.red are population-count (.popc),
all-threads-True (.and), and any-thread-True (.or). The result of .popc is the number of
threads with a True predicate, while .and and .or indicate if all the threads had a
True predicate or if any of the threads had a True predicate.
Instruction barrier{.cta} has optional .aligned modifier. When specified, it indicates that
all threads in CTA will execute the same barrier{.cta} instruction. In conditionally executed
code, an aligned barrier{.cta} instruction should only be used if it is known that all threads
in CTA evaluate the condition identically, otherwise behavior is undefined.
Different warps may execute different forms of the barrier{.cta} instruction using the same
barrier name and thread count. One example mixes barrier{.cta}.sync and barrier{.cta}.arrive
to implement producer/consumer models. The producer threads execute barrier{.cta}.arrive to
announce their arrival at the barrier and continue execution without delay to produce the next
value, while the consumer threads execute the barrier{.cta}.sync to wait for a resource to be
produced. The roles are then reversed, using a different barrier, where the producer threads execute
a barrier{.cta}.sync to wait for a resource to consumed, while the consumer threads announce
that the resource has been consumed with barrier{.cta}.arrive. Care must be taken to keep a warp
from executing more barrier{.cta} instructions than intended (barrier{.cta}.arrive followed
by any other barrier{.cta} instruction to the same barrier) prior to the reset of the
barrier. barrier{.cta}.red should not be intermixed with barrier{.cta}.sync or
barrier{.cta}.arrive using the same active barrier. Execution in this case is unpredictable.
The optional .cta qualifier simply indicates CTA-level applicability of the barrier and it
doesnât change the semantics of the instruction.
bar{.cta}.sync is equivalent to barrier{.cta}.sync.aligned. bar{.cta}.arrive is
equivalent to barrier{.cta}.arrive.aligned. bar{.cta}.red is equivalent to
barrier{.cta}.red.aligned.

Note
For .target sm_6x or below,

barrier{.cta} instruction without .aligned modifier is equivalent to .aligned
variant and has the same restrictions as of .aligned variant.
All threads in warp (except for those have exited) must execute barrier{.cta} instruction
in convergence.


PTX ISA Notes
bar.sync without a thread count introduced in PTX ISA version 1.0.
Register operands, thread count, and bar.{arrive,red} introduced in PTX ISA version 2.0.
barrier instruction introduced in PTX ISA version 6.0.
.cta qualifier introduced in PTX ISA version 7.8.
Target ISA Notes
Register operands, thread count, and bar{.cta}.{arrive,red} require sm_20 or higher.
Only bar{.cta}.sync with an immediate barrier number is supported for sm_1x targets.
barrier{.cta} instruction requires sm_30 or higher.
Examples

// Use bar.sync to arrive at a pre-computed barrier number and
// wait for all threads in CTA to also arrive:
    st.shared [r0],r1;  // write my result to shared memory
    bar.cta.sync  1;    // arrive, wait for others to arrive
    ld.shared r2,[r3];  // use shared results from other threads

// Use bar.sync to arrive at a pre-computed barrier number and
// wait for fixed number of cooperating threads to arrive:
    #define CNT1 (8*12) // Number of cooperating threads

    st.shared [r0],r1;     // write my result to shared memory
    bar.cta.sync  1, CNT1; // arrive, wait for others to arrive
    ld.shared r2,[r3];     // use shared results from other threads

// Use bar.red.and to compare results across the entire CTA:
    setp.eq.u32 p,r1,r2;         // p is True if r1==r2
    bar.cta.red.and.pred r3,1,p; // r3=AND(p) forall threads in CTA

// Use bar.red.popc to compute the size of a group of threads
// that have a specific condition True:
    setp.eq.u32 p,r1,r2;         // p is True if r1==r2
    bar.cta.red.popc.u32 r3,1,p; // r3=SUM(p) forall threads in CTA

/* Producer/consumer model. The producer deposits a value in
 * shared memory, signals that it is complete but does not wait
 * using bar.arrive, and begins fetching more data from memory.
 * Once the data returns from memory, the producer must wait
 * until the consumer signals that it has read the value from
 * the shared memory location. In the meantime, a consumer
 * thread waits until the data is stored by the producer, reads
 * it, and then signals that it is done (without waiting).
 */
    // Producer code places produced value in shared memory.
    st.shared   [r0],r1;
    bar.arrive  0,64;
    ld.global   r1,[r2];
    bar.sync    1,64;
    ...

    // Consumer code, reads value from shared memory
    bar.sync   0,64;
    ld.shared  r1,[r0];
    bar.arrive 1,64;
    ...

    // Examples of barrier.cta.sync
    st.shared         [r0],r1;
    barrier.cta.sync  0;
    ld.shared         r1, [r0];





9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.syncï

bar.warp.sync
Barrier synchronization for threads in a warp.
Syntax

bar.warp.sync      membermask;


Description
bar.warp.sync will cause executing thread to wait until all threads corresponding to
membermask have executed a bar.warp.sync with the same membermask value before resuming
execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating
in barrier where the bit position corresponds to threadâs laneid.
The behavior of bar.warp.sync is undefined if the executing thread is not in the membermask.
bar.warp.sync also guarantee memory ordering among threads participating in barrier. Thus,
threads within warp that wish to communicate via memory can store to memory, execute
bar.warp.sync, and then safely read values stored by other threads in warp.

Note
For .target sm_6x or below, all threads in membermask must execute the same
bar.warp.sync instruction in convergence, and only threads belonging to some membermask
can be active when the bar.warp.sync instruction is executed. Otherwise, the behavior is
undefined.

PTX ISA Notes
Introduced in PTX ISA version 6.0.
Target ISA Notes
Requires sm_30 or higher.
Examples

st.shared.u32 [r0],r1;         // write my result to shared memory
bar.warp.sync  0xffffffff;     // arrive, wait for others to arrive
ld.shared.u32 r2,[r3];         // read results written by other threads





9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.clusterï

barrier.cluster
Barrier synchronization within a cluster.
Syntax

barrier.cluster.arrive{.sem}{.aligned};
barrier.cluster.wait{.acquire}{.aligned};

.sem = {.release, .relaxed}


Description
Performs barrier synchronization and communication within a cluster.
barrier.cluster instructions can be used by the threads within the cluster for synchronization
and communication.
barrier.cluster.arrive instruction marks warpsâ arrival at barrier without causing executing
thread to wait for threads of other participating warps.
barrier.cluster.wait instruction causes the executing thread to wait for all non-exited threads
of the cluster to perform barrier.cluster.arrive.
In addition, barrier.cluster instructions cause the executing thread to wait for all non-exited
threads from its warp.
When all non-exited threads that executed barrier.cluster.arrive have executed
barrier.cluster.wait, the barrier completes and is reinitialized so it can be reused
immediately. Each thread must arrive at the barrier only once before the barrier completes.
The barrier.cluster.wait instruction guarantees that when it completes the execution, memory
accesses (except asynchronous operations) requested, in program order, prior to the preceding
barrier.cluster.arrive by all threads in the cluster are complete and visible to the executing
thread.
There is no memory ordering and visibility guarantee for memory accesses requested by the executing
thread, in program order, after barrier.cluster.arrive and prior to barrier.cluster.wait.
The optional .relaxed qualifier on barrier.cluster.arrive specifies that there are no memory
ordering and visibility guarantees provided for the memory accesses performed prior to
barrier.cluster.arrive.
The optional .sem and .acquire qualifiers on instructions barrier.cluster.arrive and
barrier.cluster.wait specify the memory synchronization as described in the Memory Consistency
Model. If the optional .sem qualifier is absent for
barrier.cluster.arrive, .release is assumed by default. If the optional .acquire
qualifier is absent for barrier.cluster.wait, .acquire is assumed by default.
The optional .aligned qualifier indicates that all threads in the warp must execute the same
barrier.cluster instruction. In conditionally executed code, an aligned barrier.cluster
instruction should only be used if it is known that all threads in the warp evaluate the condition
identically, otherwise behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Support for .acquire, .relaxed, .release qualifiers introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

// use of arrive followed by wait
ld.shared::cluster.u32 r0, [addr];
barrier.cluster.arrive.aligned;
...
barrier.cluster.wait.aligned;
st.shared::cluster.u32 [addr], r1;

// use memory fence prior to arrive for relaxed barrier
@cta0 ld.shared::cluster.u32 r0, [addr];
fence.cluster.acq_rel;
barrier.cluster.arrive.relaxed.aligned;
...
barrier.cluster.wait.aligned;
@cta1 st.shared::cluster.u32 [addr], r1;





9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fenceï

membar/fence
Enforce an ordering of memory operations.
Syntax

// Thread fence :
fence{.sem}.scope;

// Operation fence :
fence.op_restrict.release.cluster;

// Proxy fence (bi-directional) :
fence.proxy.proxykind;

// Proxy fence (uni-directional) :
fence.proxy.to_proxykind::from_proxykind.release.scope;
fence.proxy.to_proxykind::from_proxykind.acquire.scope  [addr], size;

// Old style membar :
membar.level;
membar.proxy.proxykind;

.sem       = { .sc, .acq_rel };
.scope     = { .cta, .cluster, .gpu, .sys };
.level     = { .cta, .gl, .sys };
.proxykind = { .alias, .async, async.global, .async.shared::{cta, cluster} };
.op_restrict = { .mbarrier_init };
.to_proxykind::from_proxykind = {.tensormap::generic};


Description
The membar instruction guarantees that prior memory accesses requested by this thread (ld,
st, atom and red instructions) are performed at the specified level, before later
memory operations requested by this thread following the membar instruction. The level
qualifier specifies the set of threads that may observe the ordering effect of this operation.
A memory read (e.g., by ld or atom) has been performed when the value read has been
transmitted from memory and cannot be modified by another thread at the indicated level. A memory
write (e.g., by st, red or atom) has been performed when the value written has become
visible to other threads at the specified level, that is, when the previous value can no longer be
read.
The fence instruction establishes an ordering between memory accesses requested by this thread
(ld, st, atom and red instructions) as described in the Memory Consistency Model. The scope qualifier specifies the set of threads that may
observe the ordering effect of this operation.
fence.acq_rel is a light-weight fence that is sufficient for memory synchronization in most
programs. Instances of fence.acq_rel synchronize when combined with additional memory operations
as described in acquire and release patterns in the Memory Consistency Model. If the optional .sem qualifier is absent, .acq_rel
is assumed by default.
fence.sc is a slower fence that can restore sequential consistency when used in sufficient
places, at the cost of performance. Instances of fence.sc with sufficient scope always
synchronize by forming a total order per scope, determined at runtime. This total order can be
constrained further by other synchronization in the program.
Qualifier .op_restrict restricts the class of prior memory operations for which the fence
instruction provides the memory ordering guarantees. When .op_restrict is .mbarrier_init,
the fence only applies to the prior mbarrier.init operations executed by the same thread on
mbarrier objects in .shared::cta state space.
The address operand addr and the operand size together specifies the memory range
[addr, addr+size-1] on which the ordering guarantees on the memory accesses across the proxies is to be
provided. The only supported value for the size operand is 128. Generic Addressing
is used unconditionally, and the address specified by the operand addr must fall within the .global
state space. Otherwise, the behavior is undefined.
On sm_70 and higher membar is a synonym for fence.sc1, and the membar
levels cta, gl and sys are synonymous with the fence scopes cta, gpu and
sys respectively.
membar.proxy and fence.proxy instructions establish an ordering between memory accesses that
may happen through different proxies.
A uni-directional proxy ordering from the from-proxykind to the to-proxykind establishes
ordering between a prior memory access performed via the from-proxykind and a subsequent memory access
performed via the to-proxykind.
A bi-directional proxy ordering between two proxykinds establishes two uni-directional proxy orderings
: one from the first proxykind to the second proxykind and the other from the second proxykind to the first
proxykind.
The .proxykind qualifier indicates the bi-directional proxy ordering that is established between the memory
accesses done between the generic proxy and the proxy specified by .proxykind.
Value .alias of the .proxykind qualifier refers to memory accesses performed using virtually
aliased addresses to the same memory location. Value .async of the .proxykind qualifier specifies
that the memory ordering is established between the async proxy and the generic proxy. The memory
ordering is limited only to the state space specified. If no state space is specified, then the memory
ordering applies on all state spaces.
A .release proxy fence can form a release sequence that synchronizes with an acquire
sequence that contains a .acquire proxy fence. The .to_proxykind and
.from_proxykind qualifiers indicate the uni-directional proxy ordering that is established.
On sm_70 and higher, membar.proxy is a synonym for fence.proxy.
1 The semantics of fence.sc introduced with sm_70 is a superset of the semantics of
membar and the two are compatible; when executing on sm_70 or later architectures,
membar acquires the full semantics of fence.sc.
PTX ISA Notes
membar.{cta,gl} introduced in PTX ISA version 1.4.
membar.sys introduced in PTX ISA version 2.0.
fence introduced in PTX ISA version 6.0.
membar.proxy and fence.proxy introduced in PTX ISA version 7.5.
.cluster scope qualifier introduced in PTX ISA version 7.8.
.op_restrict qualifier introduced in PTX ISA version 8.0.
fence.proxy.async is introduced in PTX ISA version 8.0.
.to_proxykind::from_proxykind qualifier introduced in PTX ISA version 8.3.
Target ISA Notes
membar.{cta,gl} supported on all target architectures.
membar.sys requires sm_20 or higher.
fence requires sm_70 or higher.
membar.proxy requires sm_60 or higher.
fence.proxy requires sm_70 or higher.
.cluster scope qualifier requires sm_90 or higher.
.op_restrict qualifier requires sm_90 or higher.
fence.proxy.async requires sm_90 or higher.
.to_proxykind::from_proxykind qualifier requires sm_90 or higher.
Examples

membar.gl;
membar.cta;
membar.sys;
fence.sc;
fence.sc.cluster;
fence.proxy.alias;
membar.proxy.alias;
fence.mbarrier_init.release.cluster;
fence.proxy.async;
fence.proxy.async.shared::cta;
fence.proxy.async.shared::cluster;
fence.proxy.async.global;

tensormap.replace.tile.global_address.global.b1024.b64   [gbl], new_addr;
fence.proxy.tensormap::generic.release.gpu;
fence.proxy.tensormap::generic.acquire.gpu [tmap], 128;
cvta.global.u64  tmap, gbl;
cp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [tmap, {tc0}], [mbar0];





9.7.12.5. Parallel Synchronization and Communication Instructions: atomï

atom
Atomic reduction operations for thread-to-thread communication.
Syntax
Atomic operation with scalar type:

atom{.sem}{.scope}{.space}.op{.level::cache_hint}.type d, [a], b{, cache-policy};
atom{.sem}{.scope}{.space}.op.type d, [a], b, c;

atom{.sem}{.scope}{.space}.cas.b16 d, [a], b, c;

atom{.sem}{.scope}{.space}.cas.b128 d, [a], b, c {, cache-policy};
atom{.sem}{.scope}{.space}.exch{.level::cache_hint}.b128 d, [a], b {, cache-policy};

atom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16     d, [a], b{, cache-policy};
atom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2   d, [a], b{, cache-policy};

atom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16    d, [a], b{, cache-policy};
atom{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2  d, [a], b{, cache-policy};

.space =              { .global, .shared{::cta, ::cluster} };
.sem =                { .relaxed, .acquire, .release, .acq_rel };
.scope =              { .cta, .cluster, .gpu, .sys };

.op =                 { .and, .or, .xor,
                        .cas, .exch,
                        .add, .inc, .dec,
                        .min, .max };
.level::cache_hint =  { .L2::cache_hint };
.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };


Atomic operation with vector type:

atom{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32                  d, [a], b{, cache-policy};
atom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_16_bit.half_word_type  d, [a], b{, cache-policy};
atom{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type     d, [a], b{, cache-policy};

.sem =               { .relaxed, .acquire, .release, .acq_rel };
.scope =             { .cta, .cluster, .gpu, .sys };
.op =                { .add, .min, .max };
.half_word_type =    { .f16, .bf16 };
.packed_type =       { .f16x2, .bf16x2 };
.vec_16_bit =        { .v2, .v4, .v8 }
.vec_32_bit =        { .v2, .v4 };
.level::cache_hint = { .L2::cache_hint }


Description
Atomically loads the original value at location a into destination register d, performs a
reduction operation with operand b and the value in location a, and stores the result of the
specified operation at location a, overwriting the original value. Operand a specifies a
location in the specified state space. If no state space is given, perform the memory accesses using
Generic Addressing. atom with scalar type may be used only
with .global and .shared spaces and with generic addressing, where the address points to
.global or .shared space. atom with vector type may be used only with .global space
and with generic addressing where the address points to .global space.
For atom with vector type, operands d and b are brace-enclosed vector expressions, size
of which is equal to the size of vector qualifier.
If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default.
The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory
Consistency Model. If the .sem qualifier is absent,
.relaxed is assumed by default.
The optional .scope qualifier specifies the set of threads that can directly observe the memory
synchronizing effect of this operation, as described in the Memory Consistency Model. If the .scope qualifier is absent, .gpu scope is
assumed by default.
For atom with vector type, the supported combinations of vector qualifier and types, and atomic
operations supported on these combinations are depicted in the following table:









Vector qualifier
Types


.f16/ bf16
.f16x2/ bf16x2
.f32




.v2
.add, .min, .max
.add, .min, .max
.add


.v4
.add, .min, .max
.add, .min, .max
.add


.v8
.add, .min, .max
Not supported
Not Supported



Two atomic operations {atom or red} are performed atomically with respect to each other only
if each operation specifies a scope that includes the other. When this condition is not met, each
operation observes the other operation being performed as if it were split into a read followed by a
dependent write.
atom instruction on packed type or vector type, accesses adjacent scalar elements in memory. In
such cases, the atomicity is guaranteed separately for each of the individual scalar elements; the
entire atom is not guaranteed to be atomic as a single access.
For sm_6x and earlier architectures, atom operations on .shared state space do not
guarantee atomicity with respect to normal store instructions to the same address. It is the
programmerâs responsibility to guarantee correctness of programs that use shared memory atomic
instructions, e.g., by inserting barriers between normal stores and atomic operations to a common
address, or by using atom.exch to store to locations accessed by other atomic operations.
Supported addressing modes for operand a and alignment requirements are described in Addresses
as Operands
The bit-size operations are .and, .or, .xor, .cas (compare-and-swap), and .exch
(exchange).
The integer operations are .add, .inc, .dec, .min, .max. The .inc and
.dec operations return a result in the range [0..b].
The floating-point operation .add operation rounds to nearest even. Current implementation of
atom.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero;
whereas atom.add.f32 on shared memory supports subnormal inputs and results and doesnât flush
them to zero.
atom.add.f16, atom.add.f16x2, atom.add.bf16 and atom.add.bf16x2 operation requires
the .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to
zero.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
The qualifier .level::cache_hint is only supported for .global state space and for generic
addressing where the address points to the .global state space.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
Semantics

atomic {
    d = *a;
    *a = (operation == cas) ? operation(*a, b, c)
                            : operation(*a, b);
}
where
    inc(r, s)  = (r >= s) ? 0 : r+1;
    dec(r, s)  = (r==0 || r > s)  ? s : r-1;
    exch(r, s) =  s;
    cas(r,s,t) = (r == s) ? t : r;


Notes
Simple reductions may be specified by using the bit bucket destination operand _.
PTX ISA Notes
32-bit atom.global introduced in PTX ISA version 1.1.
atom.shared and 64-bitatom.global.{add,cas,exch} introduced in PTX ISA 1.2.
atom.add.f32 and 64-bitatom.shared.{add,cas,exch} introduced in PTX ISA 2.0.
64-bit atom.{and,or,xor,min,max} introduced in PTX ISA 3.1.
atom.add.f64 introduced in PTX ISA 5.0.
.scope qualifier introduced in PTX ISA 5.0.
.sem qualifier introduced in PTX ISA version 6.0.
atom.add.noftz.f16x2 introduced in PTX ISA 6.2.
atom.add.noftz.f16 and atom.cas.b16 introduced in PTX ISA 6.3.
Per-element atomicity of atom.f16x2 clarified in PTX ISA version 6.3, with retrospective effect
from PTX ISA version 6.2.
Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4.
atom.add.noftz.bf16 and atom.add.noftz.bf16x2 introduced in PTX ISA 7.8.
Support for .cluster scope qualifier introduced in PTX ISA version 7.8.
Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8.
Support for vector types introduced in PTX ISA version 8.1.
Support for .b128 type introduced in PTX ISA version 8.3.
Support for .sys scope with .b128 type introduced in PTX ISA version 8.4.
Target ISA Notes
atom.global requires sm_11 or higher.
atom.shared requires sm_12 or higher.
64-bit atom.global.{add,cas,exch} require sm_12 or higher.
64-bit atom.shared.{add,cas,exch} require sm_20 or higher.
64-bit atom.{and,or,xor,min,max} require sm_32 or higher.
atom.add.f32 requires sm_20 or higher.
atom.add.f64 requires sm_60 or higher.
.scope qualifier requires sm_60 or higher.
.sem qualifier requires sm_70 or higher.
Use of generic addressing requires sm_20 or higher.
atom.add.noftz.f16x2 requires sm_60 or higher.
atom.add.noftz.f16 and atom.cas.b16 requires sm_70 or higher.
Support for .level::cache_hint qualifier requires sm_80 or higher.
atom.add.noftz.bf16 and atom.add.noftz.bf16x2 require sm_90 or higher.
Support for .cluster scope qualifier requires sm_90 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Support for vector types requires sm_90 or higher.
Support for .b128 type requires sm_90 or higher.
Examples

atom.global.add.s32  d,[a],1;
atom.shared::cta.max.u32  d,[x+4],0;
@p  atom.global.cas.b32  d,[p],my_val,my_new_val;
atom.global.sys.add.u32 d, [a], 1;
atom.global.acquire.sys.inc.u32 ans, [gbl], %r0;
atom.add.noftz.f16x2 d, [a], b;
atom.add.noftz.f16   hd, [ha], hb;
atom.global.cas.b16  hd, [ha], hb, hc;
atom.add.noftz.bf16   hd, [a], hb;
atom.add.noftz.bf16x2 bd, [b], bb;
atom.add.shared::cluster.noftz.f16   hd, [ha], hb;
atom.shared.b128.cas d, a, b, c; // 128-bit atom
atom.global.b128.exch d, a, b;   // 128-bit atom

atom.global.cluster.relaxed.add.u32 d, [a], 1;

createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;
atom.global.add.L2::cache_hint.s32  d, [a], 1, cache-policy;

atom.global.v8.f16.max.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],
                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};
atom.global.v8.bf16.add.noftz  {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl],
                                              {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};
atom.global.v2.f16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};
atom.global.v2.bf16.add.noftz  {%hd0, %hd1}, [gbl], {%h0, %h1};
atom.global.v4.b16x2.min.noftz  {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3};
atom.global.v4.f32.add  {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3};
atom.global.v2.f16x2.min.noftz  {%bd0, %bd1}, [g], {%b0, %b1};
atom.global.v2.bf16x2.max.noftz  {%bd0, %bd1}, [g], {%b0, %b1};
atom.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};





9.7.12.6. Parallel Synchronization and Communication Instructions: redï

red
Reduction operations on global and shared memory.
Syntax
Reduction operation with scalar type:

red{.sem}{.scope}{.space}.op{.level::cache_hint}.type          [a], b{, cache-policy};

red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16    [a], b{, cache-policy};

red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2  [a], b{, cache-policy};

red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16
                                                      [a], b {, cache-policy};

red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2
                                                      [a], b {, cache-policy};

.space =              { .global, .shared{::cta, ::cluster} };
.sem =                {.relaxed, .release};
.scope =              {.cta, .cluster, .gpu, .sys};

.op =                 { .and, .or, .xor,
                        .add, .inc, .dec,
                        .min, .max };
.level::cache_hint =  { .L2::cache_hint };
.type =               { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 };


Reduction operation with vector type:

red{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy};
red{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy};
red{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy};

.sem =                { .relaxed, .release };
.scope =              { .cta, .cluster, .gpu, .sys };
.op =                 { .add, .min, .max };
.half_word_type =     { .f16, .bf16 };
.packed_type =        { .f16x2,.bf16x2 };
.vec_16_bit =         { .v2, .v4, .v8 }
.vec_32_bit =         { .v2, .v4 };
.level::cache_hint =  { .L2::cache_hint }


Description
Performs a reduction operation with operand b and the value in location a, and stores the
result of the specified operation at location a, overwriting the original value. Operand a
specifies a location in the specified state space. If no state space is given, perform the memory
accesses using Generic Addressing. red with scalar type may
be used only with .global and .shared spaces and with generic addressing, where the address
points to .global or .shared space. red with vector type may be used only with
.global space and with generic addressing where the address points to .global space.
For red with vector type, operand b is brace-enclosed vector expressions, size of which is
equal to the size of vector qualifier.
If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default.
The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory
Consistency Model. If the .sem qualifier is absent,
.relaxed is assumed by default.
The optional .scope qualifier specifies the set of threads that can directly observe the memory
synchronizing effect of this operation, as described in the Memory Consistency Model. If the .scope qualifier is absent, .gpu scope is
assumed by default.
For red with vector type, the supported combinations of vector qualifier, types and reduction
operations supported on these combinations are depicted in following table:









Vector qualifier
Types


.f16/ bf16
.f16x2/ bf16x2
.f32




.v2
.add, .min, .max
.add, .min, .max
.add


.v4
.add, .min, .max
.add, .min, .max
.add


.v8
.add, .min, .max
Not supported
Not Supported



Two atomic operations {atom or red} are performed atomically with respect to each other only
if each operation specifies a scope that includes the other. When this condition is not met, each
operation observes the other operation being performed as if it were split into a read followed by a
dependent write.
red instruction on packed type or vector type, accesses adjacent scalar elements in memory. In
such case, the atomicity is guaranteed separately for each of the individual scalar elements; the
entire red is not guaranteed to be atomic as a single access.
For sm_6x and earlier architectures, red operations on .shared state space do not
guarantee atomicity with respect to normal store instructions to the same address. It is the
programmerâs responsibility to guarantee correctness of programs that use shared memory reduction
instructions, e.g., by inserting barriers between normal stores and reduction operations to a common
address, or by using atom.exch to store to locations accessed by other reduction operations.
Supported addressing modes for operand a and alignment requirements are described in Addresses
as Operands
The bit-size operations are .and, .or, and .xor.
The integer operations are .add, .inc, .dec, .min, .max. The .inc and
.dec operations return a result in the range [0..b].
The floating-point operation .add operation rounds to nearest even. Current implementation of
red.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero;
whereas red.add.f32 on shared memory supports subnormal inputs and results and doesnât flush
them to zero.
red.add.f16, red.add.f16x2, red.add.bf16 and red.add.bf16x2 operation requires the
.noftz qualifier; it preserves subnormal inputs and results, and does not flush them to zero.
When the optional argument cache-policy is specified, the qualifier .level::cache_hint is
required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used
during the memory access.
The qualifier .level::cache_hint is only supported for .global state space and for generic
addressing where the address points to the .global state space.
cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.
Semantics

*a = operation(*a, b);

where
    inc(r, s) = (r >= s) ? 0 : r+1;
    dec(r, s) = (r==0 || r > s)  ? s : r-1;


PTX ISA Notes
Introduced in PTX ISA version 1.2.
red.add.f32 and red.shared.add.u64 introduced in PTX ISA 2.0.
64-bit red.{and,or,xor,min,max} introduced in PTX ISA 3.1.
red.add.f64 introduced in PTX ISA 5.0.
.scope qualifier introduced in PTX ISA 5.0.
.sem qualifier introduced in PTX ISA version 6.0.
red.add.noftz.f16x2 introduced in PTX ISA 6.2.
red.add.noftz.f16 introduced in PTX ISA 6.3.
Per-element atomicity of red.f16x2 clarified in PTX ISA version 6.3, with retrospective effect
from PTX ISA version 6.2
Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4.
red.add.noftz.bf16 and red.add.noftz.bf16x2 introduced in PTX ISA 7.8.
Support for .cluster scope qualifier introduced in PTX ISA version 7.8.
Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8.
Support for vector types introduced in PTX ISA version 8.1.
Target ISA Notes
red.global requires sm_11 or higher
red.shared requires sm_12 or higher.
red.global.add.u64 requires sm_12 or higher.
red.shared.add.u64 requires sm_20 or higher.
64-bit red.{and,or,xor,min,max} require sm_32 or higher.
red.add.f32 requires sm_20 or higher.
red.add.f64 requires sm_60 or higher.
.scope qualifier requires sm_60 or higher.
.sem qualifier requires sm_70 or higher.
Use of generic addressing requires sm_20 or higher.
red.add.noftz.f16x2 requires sm_60 or higher.
red.add.noftz.f16 requires sm_70 or higher.
Support for .level::cache_hint qualifier requires sm_80 or higher.
red.add.noftz.bf16 and red.add.noftz.bf16x2 require sm_90 or higher.
Support for .cluster scope qualifier requires sm_90 or higher.
Sub-qualifier ::cta requires sm_30 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Support for vector types requires sm_90 or higher.
Examples

red.global.add.s32  [a],1;
red.shared::cluster.max.u32  [x+4],0;
@p  red.global.and.b32  [p],my_val;
red.global.sys.add.u32 [a], 1;
red.global.acquire.sys.add.u32 [gbl], 1;
red.add.noftz.f16x2 [a], b;
red.add.noftz.bf16   [a], hb;
red.add.noftz.bf16x2 [b], bb;
red.global.cluster.relaxed.add.u32 [a], 1;
red.shared::cta.min.u32  [x+4],0;

createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;
red.global.and.L2::cache_hint.b32 [a], 1, cache-policy;

red.global.v8.f16.add.noftz  [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};
red.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7};
red.global.v2.f16.add.noftz [gbl], {%h0, %h1};
red.global.v2.bf16.add.noftz [gbl], {%h0, %h1};
red.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3};
red.global.v4.f32.add  [gbl], {%f0, %f1, %f2, %f3};
red.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1};
red.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1};
red.global.v2.f32.add  {%f0, %f1}, [g], {%f0, %f1};





9.7.12.7. Parallel Synchronization and Communication Instructions: red.asyncï

red.async
Asynchronous reduction operation on shared memory.
Syntax

// Increment and Decrement reductions
red.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];

.ss   =                 { .shared::cluster };
.op   =                 { .inc, .dec };
.type =                 { .u32 };
.completion_mechanism = { .mbarrier::complete_tx::bytes };


// MIN and MAX reductions
red.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];

.ss   = { .shared::cluster };
.op   = { .min, .max };
.type = { .u32, .s32 };
.completion_mechanism = { .mbarrier::complete_tx::bytes };

// Bitwise AND, OR and XOR reductions
red.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar];

.ss   = { .shared::cluster };
.op   = { .and, .or, .xor };
.type = { .b32 };
.completion_mechanism = { .mbarrier::complete_tx::bytes };

// ADD reductions
red.async.relaxed.cluster{.ss}.completion_mechanism.add.type [a], b, [mbar];

.ss   = { .shared::cluster };
.type = { .u32, .s32, .u64 };
.completion_mechanism = { .mbarrier::complete_tx::bytes };


Description
red.async is a non-blocking instruction which initiates an asynchronous reduction operation
specified by .op, with the operand b and the value at destination shared memory location
specified by operand a.
The .inc and .dec operations return a result in the range [0..b].
The modifier .completion_mechanism specifies that upon completion of the asynchronous operation,
complete-tx
operation, with completeCount argument equal to amount of data stored in bytes, will be
performed on the mbarrier object specified by the operand mbar.
Operand a represents destination address and must be a register or of the form register +
immOff as described in Addresses as Operands.
The shared memory addresses of destination operand a and the mbarrier object mbar, must
meet all of the following conditions:

They Belong to the same CTA.
They are different to the CTA of the executing thread but must be within the same cluster.

Otherwise, the behavior is undefined.
The state space of the address {.ss}, if specified, is applicable to both operands a and
mbar. If not specified, then Generic Addressing is used for
both a and mbar.
With .shared::cluster, if the addresses specified do not fall within the address window of
.shared::cluster state space, then the behaviour is undefined.
The reduce operation in red.async is treated as a relaxed memory operation and the complete_tx
operation on the mbarrier has .release semantics at the .cluster scope as described in the
Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 8.1.
Target ISA Notes
Requires sm_90 or higher.
Examples

red.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr];





9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated)ï

vote (deprecated)
Vote across thread group.
Syntax

vote.mode.pred  d, {!}a;
vote.ballot.b32 d, {!}a;  // 'ballot' form, returns bitmask

.mode = { .all, .any, .uni };


Deprecation Note
The vote instruction without a .sync qualifier is deprecated in PTX ISA version 6.0.

Support for this instruction with .target lower than sm_70 may be removed in a future PTX
ISA version.

Removal Note
Support for vote instruction without a .sync qualifier is removed in PTX ISA version 6.4 for
.targetsm_70 or higher.
Description
Performs a reduction of the source predicate across all active threads in a warp. The destination
predicate value is the same across all threads in the warp.
The reduction modes are:

.all

True if source predicate is True for all active threads in warp. Negate the source
predicate to compute .none.

.any

True if source predicate is True for some active thread in warp. Negate the source
predicate to compute .not_all.

.uni

True if source predicate has the same value in all active threads in warp. Negating the
source predicate also computes .uni.


In the ballot form, vote.ballot.b32 simply copies the predicate from each thread in a warp
into the corresponding bit position of destination register d, where the bit position
corresponds to the threadâs lane id.
An inactive thread in warp will contribute a 0 for its entry when participating in
vote.ballot.b32.
PTX ISA Notes
Introduced in PTX ISA version 1.2.
Deprecated in PTX ISA version 6.0 in favor of vote.sync.
Not supported in PTX ISA version 6.4 for .target sm_70 or higher.
Target ISA Notes
vote requires sm_12 or higher.
vote.ballot.b32 requires sm_20 or higher.
vote is not supported on sm_70 or higher starting PTX ISA version 6.4.
Release Notes
Note that vote applies to threads in a single warp, not across an entire CTA.
Examples

vote.all.pred    p,q;
vote.uni.pred    p,q;
vote.ballot.b32  r1,p;  // get 'ballot' across warp





9.7.12.9. Parallel Synchronization and Communication Instructions: vote.syncï

vote.sync
Vote across thread group.
Syntax

vote.sync.mode.pred  d, {!}a, membermask;
vote.sync.ballot.b32 d, {!}a, membermask;  // 'ballot' form, returns bitmask

.mode = { .all, .any, .uni };


Description
vote.sync will cause executing thread to wait until all non-exited threads corresponding to
membermask have executed vote.sync with the same qualifiers and same membermask value
before resuming execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating
in this instruction where the bit position corresponds to threadâs laneid. Operand a is a
predicate register.
In the mode form, vote.sync performs a reduction of the source predicate across all non-exited
threads in membermask. The destination operand d is a predicate register and its value is
the same across all threads in membermask.
The reduction modes are:

.all

True if source predicate is True for all non-exited threads in membermask. Negate the
source predicate to compute .none.

.any

True if source predicate is True for some thread in membermask. Negate the source
predicate to compute .not_all.

.uni

True if source predicate has the same value in all non-exited threads in
membermask. Negating the source predicate also computes .uni.


In the ballot form, the destination operand d is a .b32 register. In this form,
vote.sync.ballot.b32 simply copies the predicate from each thread in membermask into the
corresponding bit position of destination register d, where the bit position corresponds to the
threadâs lane id.
A thread not specified in membermask will contribute a 0 for its entry in
vote.sync.ballot.b32.
The behavior of vote.sync is undefined if the executing thread is not in the membermask.

Note
For .target sm_6x or below, all threads in membermask must execute the same vote.sync
instruction in convergence, and only threads belonging to some membermask can be active when
the vote.sync instruction is executed. Otherwise, the behavior is undefined.

PTX ISA Notes
Introduced in PTX ISA version 6.0.
Target ISA Notes
Requires sm_30 or higher.
Examples

vote.sync.all.pred    p,q,0xffffffff;
vote.sync.ballot.b32  r1,p,0xffffffff;  // get 'ballot' across warp





9.7.12.10. Parallel Synchronization and Communication Instructions: match.syncï

match.sync
Broadcast and compare a value across threads in warp.
Syntax

match.any.sync.type  d, a, membermask;
match.all.sync.type  d[|p], a, membermask;

.type = { .b32, .b64 };


Description
match.sync will cause executing thread to wait until all non-exited threads from membermask
have executed match.sync with the same qualifiers and same membermask value before resuming
execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating
in this instruction where the bit position corresponds to threadâs laneid.
match.sync performs broadcast and compare of operand a across all non-exited threads in
membermask and sets destination d and optional predicate p based on mode.
Operand a has instruction type and d has .b32 type.
Destination d is a 32-bit mask where bit position in mask corresponds to threadâs laneid.
The matching operation modes are:

.all

d is set to mask corresponding to non-exited threads in membermask if all non-exited
threads in membermask have same value of operand a; otherwise d is set
to 0. Optionally predicate p is set to true if all non-exited threads in membermask have
same value of operand a; otherwise p is set to false. The sink symbol â_â may be used in
place of any one of the destination operands.

.any

d is set to mask of non-exited threads in membermask that have same value of operand
a.


The behavior of match.sync is undefined if the executing thread is not in the membermask.
PTX ISA Notes
Introduced in PTX ISA version 6.0.
Target ISA Notes
Requires sm_70 or higher.
Release Notes
Note that match.sync applies to threads in a single warp, not across an entire CTA.
Examples

match.any.sync.b32    d, a, 0xffffffff;
match.all.sync.b64    d|p, a, mask;





9.7.12.11. Parallel Synchronization and Communication Instructions: activemaskï

activemask
Queries the active threads within a warp.
Syntax

activemask.b32 d;


Description
activemask queries predicated-on active threads from the executing warp and sets the destination
d with 32-bit integer mask where bit position in the mask corresponds to the threadâs
laneid.
Destination d is a 32-bit destination register.
An active thread will contribute 1 for its entry in the result and exited or inactive or
predicated-off thread will contribute 0 for its entry in the result.
PTX ISA Notes
Introduced in PTX ISA version 6.2.
Target ISA Notes
Requires sm_30 or higher.
Examples

activemask.b32  %r1;





9.7.12.12. Parallel Synchronization and Communication Instructions: redux.syncï

redux.sync
Perform reduction operation on the data from each predicated active thread in the thread group.
Syntax

redux.sync.op.type dst, src, membermask;
.op   = {.add, .min, .max}
.type = {.u32, .s32}

redux.sync.op.b32 dst, src, membermask;
.op   = {.and, .or, .xor}


Description
redux.sync will cause the executing thread to wait until all non-exited threads corresponding to
membermask have executed redux.sync with the same qualifiers and same membermask value
before resuming execution.
Operand membermask specifies a 32-bit integer which is a mask indicating threads participating
in this instruction where the bit position corresponds to threadâs laneid.
redux.sync performs a reduction operation .op of the 32 bit source register src across
all non-exited threads in the membermask. The result of the reduction operation is written to
the 32 bit destination register dst.
Reduction operation can be one of the bitwise operation in .and, .or, .xor or arithmetic
operation in .add, .min , .max.
For the .add operation result is truncated to 32 bits.
The behavior of redux.sync is undefined if the executing thread is not in the membermask.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Target ISA Notes
Requires sm_80 or higher.
Release Notes
Note that redux.sync applies to threads in a single warp, not across an entire CTA.
Examples

.reg .b32 dst, src, init, mask;
redux.sync.add.s32 dst, src, 0xff;
redux.sync.xor.b32 dst, src, mask;





9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrolï

griddepcontrol
Control execution of dependent grids.
Syntax

griddepcontrol.action;

.action   = { .launch_dependents, .wait }


Description
The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by
the runtime, to control execution in the following way:
.launch_dependents modifier signals that specific dependents the runtime system designated to
react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same
instruction or have completed. The dependent may launch before the completion of the current
grid. There is no guarantee that the dependent will launch before the completion of the current
grid. Repeated invocations of this instruction by threads in the current CTA will have no additional
side effects past that of the first invocation.
.wait modifier causes the executing thread to wait until all prerequisite grids in flight have
completed and all the memory operations from the prerequisite grids are performed and made visible
to the current grid.

Note
If the prerequisite grid is using griddepcontrol.launch_dependents, then the dependent grid
must use griddepcontrol.wait to ensure correct functional execution.

PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

griddepcontrol.launch_dependents;
griddepcontrol.wait;





9.7.12.14. Parallel Synchronization and Communication Instructions: elect.syncï

elect.sync
Elect a leader thread from a set of threads.
Syntax

elect.sync d|p, membermask;


Description
elect.sync elects one predicated active leader thread from among a set of threads specified by
membermask. laneid of the elected thread is returned in the 32-bit destination operand
d. The sink symbol â_â can be used for destination operand d. The predicate destination
p is set to True for the leader thread, and False for all other threads.
Operand membermask specifies a 32-bit integer indicating the set of threads from which a leader
is to be elected. The behavior is undefined if the executing thread is not in membermask.
Election of a leader thread happens deterministically, i.e. the same leader thread is elected for
the same membermask every time.
The mandatory .sync qualifier indicates that elect causes the executing thread to wait until
all threads in the membermask execute the elect instruction before resuming execution.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

elect.sync    %r0|%p0, 0xffffffff;





9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrierï

mbarrier is a barrier created in shared memory that supports :

Synchronizing any subset of threads within a CTA
One-way synchronization of threads across CTAs of a cluster. As noted in mbarrier support with
shared memory, threads can
perform only arrive operations but not *_wait on an mbarrier located in shared::cluster
space.
Waiting for completion of asynchronous memory operations initiated by a thread and making them
visible to other threads.

An mbarrier object is an opaque object in memory which can be initialized and invalidated using :

mbarrier.init
mbarrier.inval

Operations supported on mbarrier objects are :

mbarrier.expect_tx
mbarrier.complete_tx
mbarrier.arrive
mbarrier.arrive_drop
mbarrier.test_wait
mbarrier.try_wait
mbarrier.pending_count
cp.async.mbarrier.arrive

Performing any mbarrier operation except mbarrier.init on an uninitialized mbarrier object
results in undefined behavior.
Unlike bar{.cta}/barrier{.cta} instructions which can access a limited number of barriers
per CTA, mbarrier objects are used defined and are only limited by the total shared memory size
available.
mbarrier operations enable threads to perform useful work after the arrival at the mbarrier and
before waiting for the mbarrier to complete.


9.7.12.15.1. Size and alignment of mbarrier objectï

An mbarrier object is an opaque object with the following type and alignment requirements :








Type
Alignment (bytes)
Memory space




.b64
8
.shared






9.7.12.15.2. Contents of the mbarrier objectï

An opaque mbarrier object keeps track of the following information :

Current phase of the mbarrier object
Count of pending arrivals for the current phase of the mbarrier object
Count of expected arrivals for the next phase of the mbarrier object
Count of pending asynchronous memory operations (or transactions) tracked by the current phase of
the mbarrier object. This is also referred to as tx-count.

An mbarrier object progresses through a sequence of phases where each phase is defined by threads
performing an expected number of arrive-on
operations.
The valid range of each of the counts is as shown below:








Count name
Minimum value
Maximum value




Expected arrival count
1
220 - 1


Pending arrival count
0
220 - 1


tx-count
-(220 - 1)
220 - 1






9.7.12.15.3. Lifecycle of the mbarrier objectï

The mbarrier object must be initialized prior to use.
An mbarrier object is used to synchronize threads and asynchronous memory operations.
An mbarrier object may be used to perform a sequence of such synchronizations.
An mbarrier object must be invalidated to repurpose its memory.



9.7.12.15.4. Phase of the mbarrier objectï

The phase of an mbarrier object is the number of times the mbarrier object has been used to
synchronize threads and cp.async
operations. In each phase {0, 1, 2, â¦}, threads perform in program order :

arrive-on
operations to complete the current phase and
test_wait / try_wait operations to check for the completion of the current phase.

An mbarrier object is automatically reinitialized upon completion of the current phase for
immediate use in the next phase. The current phase is incomplete and all prior phases are complete.
For each phase of the mbarrier object, at least one test_wait or try_wait operation must be
performed which returns True for waitComplete before an arrive-on operation
in the subsequent phase.



9.7.12.15.5. Tracking asynchronous operations by the mbarrier objectï

Starting with the Hopper architecture (sm_9x), mbarrier object supports a new count, called
tx-count, which is used for tracking the completion of asynchronous memory operations or
transactions. tx-count tracks the number of asynchronous transactions, in units specified by the
asynchronous memory operation, that are outstanding and yet to be complete.
The tx-count of an mbarrier object must be set to the total amount of asynchronous memory
operations, in units as specified by the asynchronous operations, to be tracked by the current
phase. Upon completion of each of the asynchronous operations, the complete-tx
operation will be performed on the mbarrier object and thus progress the mbarrier towards the
completion of the current phase.


9.7.12.15.5.1. expect-tx operationï

The expect-tx operation, with an expectCount argument, increases the tx-count of an
mbarrier object by the value specified by expectCount. This makes the current phase of the
mbarrier object to expect and track the completion of additional asynchronous transactions.



9.7.12.15.5.2. complete-tx operationï

The complete-tx operation, with an completeCount argument, on an mbarrier object consists of the following:

mbarrier signaling

Signals the completion of asynchronous transactions that were tracked by the current phase. As a
result of this, tx-count is decremented by completeCount.

mbarrier potentially completing the current phase

If the current phase has been completed then the mbarrier transitions to the next phase. Refer to
Phase Completion of the mbarrier object
for details on phase completion requirements and phase transition process.






9.7.12.15.6. Phase Completion of the mbarrier objectï

The requirements for completion of the current phase are described below. Upon completion of the
current phase, the phase transitions to the subsequent phase as described below.

Current phase completion requirements

An mbarrier object completes the current phase when all of the following conditions are met:

The count of the pending arrivals has reached zero.
The tx-count has reached zero.


Phase transition

When an mbarrier object completes the current phase, the following actions are performed
atomically:

The mbarrier object transitions to the next phase.
The pending arrival count is reinitialized to the expected arrival count.






9.7.12.15.7. Arrive-on operation on mbarrier objectï

An arrive-on operation, with an optional count argument, on an mbarrier object consists of the
following 2 steps :


mbarrier signalling:
Signals the arrival of the executing thread OR completion of the cp.async instruction which
signals the arrive-on operation initiated by the executing thread on the mbarrier object. As a
result of this, the pending arrival count is decremented by count. If the count argument is
not specified, then it defaults to 1.


mbarrier potentially completing the current phase:
If the current phase has been completed then the mbarrier transitions to the next phase. Refer to
Phase Completion of the mbarrier object
for details on phase completion requirements and phase transition process.





9.7.12.15.8. mbarrier support with shared memoryï

The following table summarizes the support of various mbarrier operations on mbarrier objects
located at different shared memory locations:








mbarrier operations
.shared::cta
.shared::cluster




mbarrier.arrive
Supported
Supported, cannot return result


mbarrier.expect_tx
Supported
Supported


mbarrier.complete_tx
Supported
Supported


Other mbarrier operations
Supported
Not supported






9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.initï

mbarrier.init
Initialize the mbarrier object.
Syntax

mbarrier.init{.shared{::cta}}.b64 [addr], count;


Description
mbarrier.init initializes the mbarrier object at the location specified by the address operand
addr with the unsigned 32-bit integer count. The value of operand count must be in the range
as specified in Contents of the mbarrier object.
Initialization of the mbarrier object involves :

Initializing the current phase to 0.
Initializing the expected arrival count to count.
Initializing the pending arrival count to count.
Initializing the tx-count to 0.

If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta state space then the behavior is undefined.
Supported addressing modes for operand addr is as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_80 or higher.
Examples

.shared .b64 shMem, shMem2;
.reg    .b64 addr;
.reg    .b32 %r1;

cvta.shared.u64          addr, shMem2;
mbarrier.init.b64        [addr],   %r1;
bar.cta.sync             0;
// ... other mbarrier operations on addr

mbarrier.init.shared::cta.b64 [shMem], 12;
bar.sync                 0;
// ... other mbarrier operations on shMem





9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.invalï

mbarrier.inval
Invalidates the mbarrier object.
Syntax

mbarrier.inval{.shared{::cta}}.b64 [addr];


Description
mbarrier.inval invalidates the mbarrier object at the location specified by the address
operand addr.
An mbarrier object must be invalidated before using its memory location for any other purpose.
Performing any mbarrier operation except mbarrier.init on an invalidated mbarrier object
results in undefined behaviour.
If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta state space then the behavior is undefined.
Supported addressing modes for operand addr is as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_80 or higher.
Examples

.shared .b64 shmem;
.reg    .b64 addr;
.reg    .b32 %r1;
.reg    .pred t0;

// Example 1 :
bar.sync                      0;
@t0 mbarrier.init.b64     [addr], %r1;
// ... other mbarrier operations on addr
bar.sync                      0;
@t0 mbarrier.inval.b64    [addr];


// Example 2 :
bar.cta.sync                  0;
mbarrier.init.shared.b64           [shmem], 12;
// ... other mbarrier operations on shmem
bar.cta.sync                  0;
@t0 mbarrier.inval.shared.b64      [shmem];

// shmem can be reused here for unrelated use :
bar.cta.sync                  0;
st.shared.b64                      [shmem], ...;

// shmem can be re-initialized as mbarrier object :
bar.cta.sync                  0;
@t0 mbarrier.init.shared.b64       [shmem], 24;
// ... other mbarrier operations on shmem
bar.cta.sync                  0;
@t0 mbarrier.inval.shared::cta.b64 [shmem];





9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_txï

mbarrier.expect_tx
Perfoms expect-tx operation on the mbarrier object.
Syntax

mbarrier.expect_tx{.sem}{.scope}{.space}.b64 [addr], txCount;

.sem   = { .relaxed }
.scope = { .cta, .cluster }
.space = { .shared{::cta}, .shared::cluster }


Description
A thread executing mbarrier.expect_tx performs an expect-tx
operation on the mbarrier object at the location specified by the address operand addr. The
32-bit unsigned integer operand txCount specifies the expectCount argument to the
expect-tx operation.
If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta or .shared::cluster state space then the behavior is undefined.
Supported addressing modes for operand addr are as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
This operation does not provide any memory ordering semantics and thus is a relaxed operation.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

mbarrier.expect_tx.b64                       [addr], 32;
mbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj1], 512;
mbarrier.expect_tx.relaxed.cta.shared.b64    [mbarObj2], 512;





9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_txï

mbarrier.complete_tx
Perfoms complete-tx
operation on the mbarrier object.
Syntax

mbarrier.complete_tx{.sem}{.scope}{.space}.b64 [addr], txCount;

.sem   = { .relaxed }
.scope = { .cta, .cluster }
.space = { .shared{::cta}, .shared::cluster }


Description
A thread executing mbarrier.complete_tx performs a complete-tx
operation on the mbarrier object at the location specified by the address operand addr. The
32-bit unsigned integer operand txCount specifies the completeCount argument to the
complete-tx operation.
mbarrier.complete_tx does not involve any asynchronous memory operations and only simulates the
completion of an asynchronous memory operation and its side effect of signaling to the mbarrier
object.
If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta or .shared::cluster state space then the behavior is undefined.
Supported addressing modes for operand addr are as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
This operation does not provide any memory ordering semantics and thus is a relaxed operation.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90 or higher.
Examples

mbarrier.complete_tx.b64             [addr],     32;
mbarrier.complete_tx.shared.b64      [mbarObj1], 512;
mbarrier.complete_tx.relaxed.cta.b64 [addr2],    32;





9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arriveï

mbarrier.arrive
Performs arrive-on operation on the
mbarrier object.
Syntax

mbarrier.arrive{.sem}{.scope}{.shared{::cta}}.b64           state, [addr]{, count};
mbarrier.arrive{.sem}{.scope}{.shared::cluster}.b64         _, [addr] {,count}
mbarrier.arrive.expect_tx{.sem}{.scope}{.shared{::cta}}.b64 state, [addr], txCount;
mbarrier.arrive.expect_tx{.sem}{.scope}{.shared::cluster}.b64   _, [addr], txCount;
mbarrier.arrive.noComplete{.sem}{.cta}{.shared{::cta}}.b64  state, [addr], count;

.sem   = { .release }
.scope = { .cta, .cluster }


Description
A thread executing mbarrier.arrive performs an arrive-on operation
on the mbarrier object at the location specified by the address operand addr. The 32-bit
unsigned integer operand count specifies the count argument to the arrive-on
operation.
If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta state space then the behavior is undefined.
Supported addressing modes for operand addr is as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
The optional qualifier .expect_tx specifies that an expect-tx
operation is performed prior to the arrive-on
operation. The 32-bit unsigned integer operand txCount specifies the expectCount argument to
the expect-tx operation. When both qualifiers .arrive and .expect_tx are specified, then
the count argument of the arrive-on operation is assumed to be 1.
A mbarrier.arrive operation with .noComplete qualifier must not cause the mbarrier to
complete its current phase, otherwise the behavior is undefined.
The value of the operand count must be in the range as specified in Contents of the mbarrier
object.
Note: for sm_8x, when the argument count is specified, the modifier .noComplete is
required.
mbarrier.arrive operation on an mbarrier object located in .shared::cta returns an opaque
64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the
destination operand state. Contents of the state operand are implementation
specific. Optionally, sink symbol '_' can be used for the state argument.
mbarrier.arrive operation on an mbarrier object located in .shared::cluster but not in
.shared::cta cannot return a value. Sink symbol â_â is mandatory for the destination operand for
such cases.
The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory
Consistency Model. If the .sem qualifier is absent,
.release is assumed by default.
The optional .scope qualifier indicates the set of threads that directly observe the memory
synchronizing effect of this operation, as described in the Memory Consistency Model. If the .scope qualifier is not specified then it
defaults to .cta. In contrast, the .shared::<scope> indicates the state space where the
mbarrier resides.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Support for sink symbol â_â as the destination operand is introduced in PTX ISA version 7.1.
Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8.
Support for count argument without the modifier .noComplete introduced in PTX ISA version
7.8.
Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0.
Support for qualifier .expect_tx is introduced in PTX ISA version 8.0.
Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0
Target ISA Notes
Requires sm_80 or higher.
Support for count argument without the modifier .noComplete requires sm_90 or higher.
Qualifier .expect_tx requires sm_90 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Support for .cluster scope requires sm_90 or higher.
Examples

.reg .b32 cnt, remoteAddr32, remoteCTAId, addr32;
.reg .b64 %r<3>, addr, remoteAddr64;
.shared .b64 shMem, shMem2;

cvta.shared.u64            addr, shMem2;
mov.b32                    addr32, shMem2;
mapa.shared::cluster.u32   remoteAddr32, addr32, remoteCTAId;
mapa.u64                   remoteAddr64, addr,   remoteCTAId;

cvta.shared.u64          addr, shMem2;

mbarrier.arrive.shared.b64                       %r0, [shMem];
mbarrier.arrive.shared::cta.b64                  %r0, [shMem2];
mbarrier.arrive.release.cta.shared::cluster.b64  _, [remoteAddr32];
mbarrier.arrive.release.cluster.b64              _, [remoteAddr64], cnt;
mbarrier.arrive.expect_tx.release.cluster.b64    _, [remoteAddr64], tx_count;
mbarrier.arrive.noComplete.b64                   %r1, [addr], 2;
mbarrier.arrive.b64                              %r2, [addr], cnt;





9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_dropï

mbarrier.arrive_drop
Decrements the expected count of the mbarrier object and performs arrive-on operation.
Syntax

mbarrier.arrive_drop{.sem}{.scope}{.shared{::cta}}.b64 state,           [addr]{, count};
mbarrier.arrive_drop{.sem}{.scope}{.shared::cluster}.b64           _,   [addr] {,count};
mbarrier.arrive_drop.expect_tx{.shared{::cta}}{.sem}{.scope}.b64 state, [addr], tx_count;
mbarrier.arrive_drop.expect_tx{.shared::cluster}{.sem}{.scope}.b64   _, [addr], tx_count;
mbarrier.arrive_drop.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state,  [addr], count;

.sem   = { .release }
.scope = { .cta, .cluster }


Description
A thread executing mbarrier.arrive_drop on the mbarrier object at the location specified by
the address operand addr performs the following steps:

Decrements the expected arrival count of the mbarrier object by the value specified by the
32-bit integer operand count. If count operand is not specified, it defaults to 1.
Performs an arrive-on operation on the
mbarrier object. The operand count specifies the count argument to the arrive-on
operation.

The decrement done in the expected arrivals count of the mbarrier object will be for all the
subsequent phases of the mbarrier object.
If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta or .shared::cluster state space then the behavior is undefined.
Supported addressing modes for operand addr is as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
The optional qualifier .expect_tx specifies that an expect-tx
operation is performed prior to the arrive-on
operation. The 32-bit unsigned integer operand txCount specifies the expectCount argument to
the expect-tx operation. When both qualifiers .arrive and .expect_tx are specified, then
the count argument of the arrive-on operation is assumed to be 1.
mbarrier.arrive_drop operation forms the release pattern as described in the Memory
Consistency Model and synchronizes with the acquire patterns.
The optional .scope qualifier indicates the set of threads that an mbarrier.arrive_drop
instruction can directly synchronize. If the .scope qualifier is not specified then it defaults
to .cta. In contrast, the .shared::<scope> indicates the state space where the mbarrier
resides.
A mbarrier.arrive_drop with .noComplete qualifier must not complete the mbarrier,
otherwise the behavior is undefined.
The value of the operand count must be in the range as specified in Contents of the mbarrier
object.
Note: for sm_8x, when the argument count is specified, the modifier .noComplete is
required.
A thread that wants to either exit or opt out of participating in the arrive-on operation can use
mbarrier.arrive_drop to drop itself from the mbarrier.
mbarrier.arrive_drop operation on an mbarrier object located in .shared::cta returns an
opaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on
operation
in the destination operand state. Contents of the returned state are implementation
specific. Optionally, sink symbol '_' can be used for the state argument.
mbarrier.arrive_drop operation on an mbarrier object located in .shared::cluster but not
in .shared::cta cannot return a value. Sink symbol â_â is mandatory for the destination operand
for such cases.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8.
Support for count argument without the modifier .noComplete introduced in PTX ISA version
7.8.
Support for qualifier .expect_tx is introduced in PTX ISA version 8.0.
Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0.
Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0
Target ISA Notes
Requires sm_80 or higher.
Support for count argument without the modifier .noComplete requires sm_90 or higher.
Qualifier .expect_tx requires sm_90 or higher.
Sub-qualifier ::cluster requires sm_90 or higher.
Support for .cluster scope requires sm_90 or higher.
Examples

.reg .b32 cnt;
.reg .b64 %r1;
.shared .b64 shMem;

// Example 1
@p mbarrier.arrive_drop.shared.b64 _, [shMem];
@p exit;
@p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a;
@p2 exit;
..
@!p mbarrier.arrive.shared.b64   %r1, [shMem];
@!p mbarrier.test_wait.shared.b64  q, [shMem], %r1;

// Example 2
mbarrier.arrive_drop.shared::cluster.b64 _, [addr];
mbarrier.arrive_drop.shared::cta.release.cluster.b64     _, [addr], cnt;

// Example 3
mbarrier.arrive_drop.expect_tx.shared::cta.release.cta.b64 state, [addr], tx_count;





9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arriveï

cp.async.mbarrier.arrive
Makes the mbarrier object track all prior cp.async operations initiated by the
executing thread.
Syntax

cp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr];


Description
Causes an arrive-on operation to be
triggered by the system on the mbarrier object upon the completion of all prior cp.async operations initiated by the
executing thread. The mbarrier object is at the location specified by the operand addr. The
arrive-on operation is
asynchronous to execution of cp.async.mbarrier.arrive.
When .noinc modifier is not specified, the pending count of the mbarrier object is incremented
by 1 prior to the asynchronous arrive-on operation. This
results in a zero-net change for the pending count from the asynchronous arrive-on operation
during the current phase. The pending count of the mbarrier object after the increment should not
exceed the limit as mentioned in Contents of the mbarrier object. Otherwise,
the behavior is undefined.
When the .noinc modifier is specified, the increment to the pending count of the mbarrier
object is not performed. Hence the decrement of the pending count done by the asynchronous
arrive-on operation must be
accounted for in the initialization of the mbarrier object.
If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta state space then the behavior is undefined.
Supported addressing modes for operand addr is as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_80 or higher.
Examples

// Example 1: no .noinc
mbarrier.init.shared.b64 [shMem], threadCount;
....
cp.async.ca.shared.global [shard1], [gbl1], 4;
cp.async.cg.shared.global [shard2], [gbl2], 16;
....
// Absence of .noinc accounts for arrive-on from completion of prior cp.async operations.
// So mbarrier.init must only account for arrive-on from mbarrier.arrive.
cp.async.mbarrier.arrive.shared.b64 [shMem];
....
mbarrier.arrive.shared.b64 state, [shMem];

waitLoop:
mbarrier.test_wait.shared.b64 p, [shMem], state;
@!p bra waitLoop;



// Example 2: with .noinc

// Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive.

// All threads participating in the mbarrier perform cp.async
mov.b32 copyOperationCnt, threadCount;

// 3 arrive-on operations will be triggered per-thread
mul.lo.u32 copyArrivalCnt, copyOperationCnt, 3;

add.u32 totalCount, threadCount, copyArrivalCnt;

mbarrier.init.shared.b64 [shMem], totalCount;
....
cp.async.ca.shared.global [shard1], [gbl1], 4;
cp.async.cg.shared.global [shard2], [gbl2], 16;
...
// Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async
cp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance
....
cp.async.ca.shared.global [shard3], [gbl3], 4;
cp.async.ca.shared.global [shard4], [gbl4], 16;
cp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance
....
cp.async.ca.shared.global [shard5], [gbl5], 4;
cp.async.cg.shared.global [shard6], [gbl6], 16;
cp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance
....
mbarrier.arrive.shared.b64 state, [shMem];

waitLoop:
mbarrier.test_wait.shared.b64 p, [shMem], state;
@!p bra waitLoop;





9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_waitï

mbarrier.test_wait/mbarrier.try_wait
Checks whether the mbarrier object has completed the phase.
Syntax

mbarrier.test_wait{.sem}{.scope}{.shared{::cta}}.b64        waitComplete, [addr], state;
mbarrier.test_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity;

mbarrier.try_wait{.sem}{.scope}{.shared{::cta}}.b64         waitComplete, [addr], state
                                                               {, suspendTimeHint};

mbarrier.try_wait.parity{.sem}{.scope}{.shared{::cta}}.b64  waitComplete, [addr], phaseParity
                                                               {, suspendTimeHint};

.sem   = { .acquire }
.scope = { .cta, .cluster }


Description
The test_wait and try_wait operations test for the completion of the current or the immediately
preceding phase of an mbarrier object at the location specified by the operand addr.
mbarrier.test_wait is a non-blocking instruction which tests for the completion of the phase.
mbarrier.try_wait is a potentially blocking instruction which tests for the completion of the
phase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes
execution when the specified phase completes OR before the phase completes following a
system-dependent time limit. The optional 32-bit unsigned integer operand suspendTimeHint
specifies the time limit, in nanoseconds, that may be used for the time limit instead of the
system-dependent limit.
mbarrier.test_wait and mbarrier.try_wait test for completion of the phase :

Specified by the operand state, which was returned by an mbarrier.arrive instruction on
the same mbarrier object during the current or the immediately preceding phase. Or
Indicated by the operand phaseParity, which is the integer parity of either the current phase
or the immediately preceding phase of the mbarrier object.

The .parity variant of the instructions test for the completion of the phase indicated by the
operand phaseParity, which is the integer parity of either the current phase or the immediately
preceding phase of the mbarrier object. An even phase has integer parity 0 and an odd phase has
integer parity of 1. So the valid values of phaseParity operand are 0 and 1.
Note: the use of the .parity variants of the instructions requires tracking the phase of an
mbarrier object throughout its lifetime.
The test_wait and try_wait operations are valid only for :

the current incomplete phase, for which waitComplete returns False.
the immediately preceding phase, for which waitComplete returns True.

If no state space is specified then Generic Addressing is
used. If the address specified by addr does not fall within the address window of
.shared::cta state space then the behavior is undefined.
Supported addressing modes for operand addr is as described in Addresses as Operands. Alignment for operand addr is as described in the Size
and alignment of mbarrier object.
When mbarrier.test_wait and mbarrier.try_wait operations return True, they form the
acquire pattern as described in the Memory Consistency Model.
The optional .scope qualifier indicates the set of threads that the mbarrier.test_wait and
mbarrier.try_wait instructions can directly synchronize. If the .scope qualifier is not
specified then it defaults to .cta. In contrast, the .shared::<scope> indicates the state
space where the mbarrier resides.
The following ordering of memory operations hold for the executing thread when
mbarrier.test_wait or mbarrier.try_wait returns True :

All memory accesses (except async operations ) requested prior, in program
order, to mbarrier.arrive during the completed phase by the participating threads of the CTA
are performed and are visible to the executing thread.
All cp.async operations
requested prior, in program order, to cp.async.mbarrier.arrive during the completed phase by
the participating threads of the CTA are performed and made visible to the executing thread.
All cp.async.bulk asynchronous operations using the same mbarrier object requested prior,
in program order, to mbarrier.arrive during the completed phase by the participating threads
of the CTA are performed and made visible to the executing thread.
All memory accesses requested after the mbarrier.test_wait or mbarrier.try_wait, in
program order, are not performed and not visible to memory accesses performed prior to
mbarrier.arrive, in program order, by other threads participating in the mbarrier.
There is no ordering and visibility guarantee for memory accesses requested by the thread after
mbarrier.arrive and prior to mbarrier.test_wait, in program order.

PTX ISA Notes
mbarrier.test_wait introduced in PTX ISA version 7.0.
Modifier .parity is introduced in PTX ISA version 7.1.
mbarrier.try_wait introduced in PTX ISA version 7.8.
Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8.
Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0
Target ISA Notes
mbarrier.test_wait requires sm_80 or higher.
mbarrier.try_wait requires sm_90 or higher.
Support for .cluster scope requires sm_90 or higher.
Examples

// Example 1a, thread synchronization with test_wait:

.reg .b64 %r1;
.shared .b64 shMem;

mbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.
...
mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive

// computation not requiring mbarrier synchronization...

waitLoop:
mbarrier.test_wait.shared.b64    complete, [shMem], %r1;
@!complete nanosleep.u32 20;
@!complete bra waitLoop;

// Example 1b, thread synchronization with try_wait :

.reg .b64 %r1;
.shared .b64 shMem;

mbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.
...
mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads executing mbarrier.arrive

// computation not requiring mbarrier synchronization...

waitLoop:
mbarrier.try_wait.shared.b64    complete, [shMem], %r1;
@!complete bra waitLoop;


// Example 2, thread synchronization using phase parity :

.reg .b32 i, parArg;
.reg .b64 %r1;
.shared .b64 shMem;

mov.b32 i, 0;
mbarrier.init.shared.b64 [shMem], N;  // N threads participating in the mbarrier.
...
loopStart :                           // One phase per loop iteration
    ...
    mbarrier.arrive.shared.b64  %r1, [shMem]; // N threads
    ...
    and.b32 parArg, i, 1;
    waitLoop:
    mbarrier.test_wait.parity.shared.b64  complete, [shMem], parArg;
    @!complete nanosleep.u32 20;
    @!complete bra waitLoop;
    ...
    add.u32 i, i, 1;
    setp.lt.u32 p, i, IterMax;
@p bra loopStart;


// Example 3, Asynchronous copy completion waiting :

.reg .b64 state;
.shared .b64 shMem2;
.shared .b64 shard1, shard2;
.global .b64 gbl1, gbl2;

mbarrier.init.shared.b64 [shMem2], threadCount;
...
cp.async.ca.shared.global [shard1], [gbl1], 4;
cp.async.cg.shared.global [shard2], [gbl2], 16;

// Absence of .noinc accounts for arrive-on from prior cp.async operation
cp.async.mbarrier.arrive.shared.b64 [shMem2];
...
mbarrier.arrive.shared.b64 state, [shMem2];

waitLoop:
mbarrier.test_wait.shared::cta.b64 p, [shMem2], state;
@!p bra waitLoop;

// Example 4, Synchronizing the CTA0 threads with cluster threads
.reg .b64 %r1, addr, remAddr;
.shared .b64 shMem;

cvta.shared.u64          addr, shMem;
mapa.u64                 remAddr, addr, 0;     // CTA0âs shMem instance

// One thread from CTA0 executing the below initialization operation
@p0 mbarrier.init.shared::cta.b64 [shMem], N;  // N = no of cluster threads

barrier.cluster.arrive;
barrier.cluster.wait;

// Entire cluster executing the below arrive operation
mbarrier.arrive.release.cluster.b64              _, [remAddr];

// computation not requiring mbarrier synchronization ...

// Only CTA0 threads executing the below wait operation
waitLoop:
mbarrier.try_wait.parity.acquire.cluser.shared::cta.b64  complete, [shMem], 0;
@!complete bra waitLoop;





9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_countï

mbarrier.pending_count
Query the pending arrival count from the opaque mbarrier state.
Syntax

mbarrier.pending_count.b64 count, state;


Description
The pending count can be queried from the opaque mbarrier state using mbarrier.pending_count.
The state operand is a 64-bit register that must be the result of a prior
mbarrier.arrive.noComplete or mbarrier.arrive_drop.noComplete instruction. Otherwise, the
behavior is undefined.
The destination register count is a 32-bit unsigned integer representing the pending count of
the mbarrier object prior to the arrive-on operation from
which the state register was obtained.
PTX ISA Notes
Introduced in PTX ISA version 7.0.
Target ISA Notes
Requires sm_80 or higher.
Examples

.reg .b32 %r1;
.reg .b64 state;
.shared .b64 shMem;

mbarrier.arrive.noComplete.b64 state, [shMem], 1;
mbarrier.pending_count.b64 %r1, state;





9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxyï

tensormap.cp_fenceproxy
A fused copy and fence operation.
Syntax

tensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned  [dst], [src], size;

.cp_qualifiers    = { .global.shared::cta }
.fence_qualifiers = { .to_proxy::from_proxy.release.scope }
.to_proxy::from_proxy  = { .tensormap::generic }
.scope            = { .cta, .cluster, .gpu , .sys }


Description
The tensormap.cp_fence instructions perform the following operations in order :

Copies data of size specified by the size argument, in bytes, from the location specified
by the address operand src in shared memory to the location specified by the address operand
dst in the global memory, in the generic proxy.
Establishes a uni-directional proxy release pattern on the ordering from the copy operation
to the subsequent access performed in the tensormap proxy on the address dst.

The valid value of size operand is 128.
The operands src and dst specify non-generic addresses in shared::cta and global
state space respectively.
The optional .scope qualifier specifies the set of threads that can directly observe the proxy
synchronizing effect of this operation, as described in Memory Consistency Model.
The mandatory .sync qualifier indicates that tensormap.cp_fenceproxy causes the executing
thread to wait until all threads in the warp execute the same tensormap.cp_fenceproxy
instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
tensormap.cp_fenceproxy instruction. In conditionally executed code, an aligned tensormap.cp_fenceproxy
instruction should only be used if it is known that all threads in the warp evaluate the condition
identically, otherwise behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.3.
Target ISA Notes
Requires sm_90 or higher.
Examples

// Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor

.reg .b64 new_addr;
.global .align 128 .b8 gbl[128];
.shared .align 128 .b8 sMem[128];

cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar];
...
try_wait_loop:
mbarrier.try_wait.shared.b64 p, [mbar], state;
@!p bra try_wait loop;

tensormap.replace.tile.global_address.shared.b1024.b64   [sMem], new_addr;
tensormap.cp_fenceproxy.global.shared::cta.proxy.tensormap::generic.release.gpu
                                    .sync.aligned        [gbl], [sMem], 128;
fence.proxy.tensormap::generic.acquire.gpu [gbl], 128;
cp.async.bulk.tensor.1d.shared::cluster.global.tile  [addr0], [gbl, {tc0}], [mbar0];







9.7.13. Warp Level Matrix Multiply-Accumulate Instructionsï

The matrix multiply and accumulate operation has the following form:

D = A * B + C


where D and C are called accumulators and may refer to the same matrix.
PTX provides two ways to perform matrix multiply-and-accumulate computation:


Using wmma instructions:


This warp-level computation is performed collectively by all threads in the warp as follows:

Load matrices A, B and C from memory into registers using the wmma.load operation. When
the operation completes, the destination registers in each thread hold a fragment of the
loaded matrix.
Perform the matrix multiply and accumulate operation using the wmma.mma operation on the
loaded matrices. When the operation completes, the destination registers in each thread hold
a fragment of the result matrix returned by the wmma.mma operation.
Store result Matrix D back to memory using the wmma.store operation. Alternately, result
matrix D can also be used as argument C for a subsequent wmma.mma operation.

The wmma.load and wmma.store instructions implicitly handle the organization of matrix
elements when loading the input matrices from memory for the wmma.mma operation and when
storing the result back to memory.




Using mma instruction:

Similar to wmma, mma also requires computation to be performed collectively by all
threads in the warp however distribution of matrix elements across different threads in warp
needs to be done explicitly before invoking the mma operation. The mma instruction
supports both dense as well as sparse matrix A. The sparse variant can be used when A is a
structured sparse matrix as described in Sparse matrix storage.





9.7.13.1. Matrix Shapeï

The matrix multiply and accumulate operations support a limited set of shapes for the operand
matrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple
MxNxK, where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices.
The following matrix shapes are supported for the specified types:










Instruction
Sparsity
Multiplicand Data-type
Shape
PTX ISA version




wmma
Dense
Floating-point - .f16
.m16n16k16, .m8n32k16,
and .m32n8k16
PTX ISA version 6.0


wmma
Dense
Alternate floating-point format - .bf16
.m16n16k16, .m8n32k16,
and .m32n8k16
PTX ISA version 7.0


wmma
Dense
Alternate floating-point format - .tf32
.m16n16k8
PTX ISA version 7.0


wmma
Dense
Integer - .u8/.s8
.m16n16k16, .m8n32k16,
and .m32n8k16
PTX ISA version 6.3


wmma
Dense
Sub-byte integer - .u4/.s4
.m8n8k32
PTX ISA version 6.3
(preview feature)


wmma
Dense
Single-bit - .b1
.m8n8k128
PTX ISA version 6.3
(preview feature)


mma
Dense
Floating-point - .f64
.m8n8k4
PTX ISA version 7.0


.m16n8k4, .m16n8k8,
and .m16n8k16
PTX ISA version 7.8


mma
Dense
Floating-point - .f16
.m8n8k4
PTX ISA version 6.4


.m16n8k8
PTX ISA version 6.5


.m16n8k16
PTX ISA version 7.0


mma
Dense
Alternate floating-point format - .bf16
.m16n8k8 and .m16n8k16
PTX ISA version 7.0


mma
Dense
Alternate floating-point format - .tf32
.m16n8k4 and .m16n8k8
PTX ISA version 7.0


mma
Dense
Integer - .u8/.s8
.m8n8k16
PTX ISA version 6.5


.m16n8k16 and .m16n8k32
PTX ISA version 7.0


mma
Dense
Sub-byte integer - .u4/.s4
.m8n8k32
PTX ISA version 6.5


.m16n8k32 and .m16n8k64
PTX ISA version 7.0


mma
Dense
Single-bit - .b1
.m8n8k128, .m16n8k128,
and .m16n8k256
PTX ISA version 7.0


mma
Dense
Alternate floating-point format - .e4m3
/ .e5m2
.m16n8k32
PTX ISA version 8.4


mma
Sparse
Floating-point - .f16
.m16n8k16 and .m16n8k32
PTX ISA version 7.1


mma
Sparse
Alternate floating-point format - .bf16
.m16n8k16 and .m16n8k32
PTX ISA version 7.1


mma
Sparse
Alternate floating-point format - .tf32
.m16n8k8 and .m16n8k16
PTX ISA version 7.1


mma
Sparse
Integer - .u8/.s8
.m16n8k32 and .m16n8k64
PTX ISA version 7.1


mma
Sparse
Sub-byte integer - .u4/.s4
.m16n8k64 and
.m16n8k128
PTX ISA version 7.1


mma
Sparse
Alternate floating-point format - .e4m3
/ .e5m2
.m16n8k64
PTX ISA version 8.4


mma
Sparse
with
ordered
metadata
Floating-point - .f16
.m16n8k16 and .m16n8k32
PTX ISA version 8.5


mma
Sparse
with
ordered
metadata
Alternate floating-point format - .bf16
.m16n8k16 and .m16n8k32
PTX ISA version 8.5


mma
Sparse
with
ordered
metadata
Alternate floating-point format - .tf32
.m16n8k8 and .m16n8k16
PTX ISA version 8.5


mma
Sparse
with
ordered
metadata
Integer - .u8/.s8
.m16n8k32 and .m16n8k64
PTX ISA version 8.5


mma
Sparse
with
ordered
metadata
Sub-byte integer - .u4/.s4
.m16n8k64 and
.m16n8k128
PTX ISA version 8.5


mma
Sparse
with
ordered
metadata
Alternate floating-point format - .e4m3
/ .e5m2
.m16n8k64
PTX ISA version 8.5






9.7.13.2. Matrix Data-typesï

The matrix multiply and accumulate operation is supported separately on integer, floating-point,
sub-byte integer and single bit data-types. All operands must contain the same basic type kind,
i.e., integer or floating-point.
For floating-point matrix multiply and accumulate operation, different matrix operands may have
different precision, as described later.








Data-type
Multiplicands (A or B)
Accumulators (C or D)




Integer
.u8, .s8
.s32


Floating Point
.f16
.f16,.f32


Alternate floating Point
.bf16
.f32


Alternate floating Point
.tf32
.f32


Alternate floating Point
.e4m3 or .e5m2
.f32


Floating Point
.f64
.f64


Sub-byte integer
both .u4 or both .s4
.s32


Single-bit integer
.b1
.s32






9.7.13.3. Matrix multiply-accumulate operation using wmma instructionsï

This section describes warp level wmma.load, wmma.mma and wmma.store instructions and the
organization of various matrices invovled in these instruction.


9.7.13.3.1. Matrix Fragments for WMMAï

Each thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the
threads in a warp is unspecified and is target architecture dependent, and hence the identity of the
fragment within the matrix is also unspecified and is target architecture dependent. The fragment
returned by a wmma operation can be used as an operand for another wmma operation if the
shape, layout and element type of the underlying matrix matches. Since fragment layout is
architecture dependent, using the fragment returned by a wmma operation in one function as an
operand for a wmma operation in a different function may not work as expected if the two
functions are linked together but were compiled for different link-compatible SM architectures. Note
passing wmma fragment to a function having .weak linkage is unsafe since at link time
references to such function may get resolved to a function in different compilation module.
Each fragment is a vector expression whose contents are determined as follows. The identity of
individual matrix elements in the fragment is unspecified.

Integer fragments

Multiplicands (A or B):









Data-type
Shape
Matrix
Fragment




.u8 or .s8
.m16n16k16
A
A vector expression of two .b32 registers, with each
register containing four elements from the matrix.


B
A vector expression of two .b32 registers, with each
register containing four elements from the matrix.



.m8n32k16
A
A vector expression containing a single .b32 register
containing four elements from the matrix.


B
A vector expression of four .b32 registers, with each
register containing four elements from the matrix.



.m32n8k16
A
A vector expression of four .b32 registers, with each
register containing four elements from the matrix.


B
A vector expression containing single .b32 register,
with each containing four elements from the matrix.



Accumulators (C or D):








Data-type
Shape
Fragment




.s32
.m16n16k16
A vector expression of eight .s32 registers.


.m8n32k16


.m32n8k16





Floating point fragments










Data-type
Matrix
Fragment




.f16
A or B
A vector expression of eight .f16x2 registers.


.f16
C or D
A vector expression of four .f16x2 registers.


.f32
A vector expression of eight .f32 registers.





Floating point fragments for .bf16 data format


Multiplicands (A or B):









Data-type
Shape
Matrix
Fragment




.bf16
.m16n16k16
A
A vector expression of four .b32 registers, with each
register containing two elements from the matrix.


B


.m8n32k16
A
A vector expression containing a two .b32 registers,
with containing two elements from the matrix.


B
A vector expression of eight .b32 registers, with
each register containing two elements from the matrix.


.m32n8k16
A
A vector expression of eight .b32 registers, with
each register containing two elements from the matrix.


B
A vector expression containing two .b32 registers,
with each containing two elements from the matrix.



Accumulators (C or D):








Data-type
Matrix
Fragment




.f32
C or D
A vector expression containing eight .f32 registers.





Floating point fragments for .tf32 data format


Multiplicands (A or B):









Data-type
Shape
Matrix
Fragment




.tf32
.m16n16k8
A
A vector expression of four .b32 registers.


B
A vector expression of four .b32 registers.



Accumulators (C or D):









Data-type
Shape
Matrix
Fragment




.f32
.m16n16k8
C or D
A vector expression containing eight .f32 registers.





Double precision floating point fragments


Multiplicands (A or B):









Data-type
Shape
Matrix
Fragment




.f64
.m8n8k4
A or B
A vector expression of single .f64 register.



Accumulators (C or D):









Data-type
Shape
Matrix
Fragment




.f64
.m8n8k4
C or D
A vector expression containing single .f64 register.





Sub-byte integer and single-bit fragments


Multiplicands (A or B):








Data-type
Shape
Fragment




.u4 or .s4
.m8n8k32
A vector expression containing a single .b32 register, containing eight elements from the matrix.


.b1
.m8n8k128
A vector expression containing a single .b32 register, containing 32 elements from the matrix.



Accumulators (C or D):








Data-type
Shape
Fragment




.s32
.m8n8k32
A vector expression of two .s32 registers.


.m8n8k128
A vector expression of two .s32 registers.





Manipulating fragment contents


The contents of a matrix fragment can be manipulated by reading and writing to individual
registers in the fragment, provided the following conditions are satisfied:

All matrix element in the fragment are operated on uniformly across threads, using the same
parameters.
The order of the matrix elements is not changed.

For example, if each register corresponding to a given matrix is multiplied by a uniform constant
value, then the resulting matrix is simply the scaled version of the original matrix.
Note that type conversion between .f16 and .f32 accumulator fragments is not supported in
either direction. The result is undefined even if the order of elements in the fragment remains
unchanged.





9.7.13.3.2. Matrix Storage for WMMAï

Each matrix can be stored in memory with a row-major or column-major layout. In a row-major
format, consecutive elements of each row are stored in contiguous memory locations, and the row is
called the leading dimension of the matrix. In a column-major format, consecutive elements of
each column are stored in contiguous memory locations and the column is called the leading
dimension of the matrix.
Consecutive instances of the leading dimension (rows or columns) need not be stored contiguously
in memory. The wmma.load and wmma.store operations accept an optional argument stride
that specifies the offset from the beginning of each row (or column) to the next, in terms of matrix
elements (and not bytes). For example, the matrix being accessed by a wmma operation may be a
submatrix from a larger matrix stored in memory. This allows the programmer to compose a
multiply-and-accumulate operation on matrices that are larger than the shapes supported by the
wmma operation.
Address Alignment:


The starting address of each instance of the leading dimension (row or column) must be aligned
with the size of the corresponding fragment in bytes. Note that the starting address is
determined by the base pointer and the optional stride.


Consider the following instruction as an example:

wmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s;



Fragment size in bytes = 32 (eight elements of type .f16x2)
Actual stride in bytes = 2 * s (since stride is specified in terms of .f16
elements, not bytes)

For each row of this matrix to be aligned at fragment size the following must be true:

p is a multiple of 32.
2*s is a multiple of 32.



Default value for stride:


The default value of the stride is the size of the leading dimension of the matrix. For
example, for an MxK matrix, the stride is K for a row-major layout and M for a
column-major layout. In particular, the default strides for the supported matrix shapes are as
follows:












Shape
A (row)
A (column)
B (row)
B (column)
Accumulator (row)
Accumulator (column)




16x16x16
16
16
16
16
16
16


8x32x16
16
8
32
16
32
8


32x8x16
16
32
8
16
8
32


8x8x32
32
8
8
32
8
8


8x8x128
128
8
8
128
8
8


16x16x8
8
16
16
8
16
16


8x8x4
4
8
8
4
8
8








9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.loadï

wmma.load
Collectively load a matrix from memory for WMMA
Syntax
Floating point format .f16 loads:

wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride};
wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride};
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride};

.layout = {.row, .col};
.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};
.ss     = {.global, .shared{::cta}};
.atype  = {.f16, .s8, .u8};
.btype  = {.f16, .s8, .u8};
.ctype  = {.f16, .f32, .s32};


Alternate floating point format .bf16 loads:

wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}
wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}
.layout = {.row, .col};
.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};
.ss     = {.global, .shared{::cta}};
.atype  = {.bf16 };
.btype  = {.bf16 };
.ctype  = {.f32 };


Alternate floating point format .tf32 loads:

wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}
wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}
.layout = {.row, .col};
.shape  = {.m16n16k8 };
.ss     = {.global, .shared{::cta}};
.atype  = {.tf32 };
.btype  = {.tf32 };
.ctype  = {.f32 };


Double precision Floating point .f64 loads:

wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}
wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}
.layout = {.row, .col};
.shape  = {.m8n8k4 };
.ss     = {.global, .shared{::cta}};
.atype  = {.f64 };
.btype  = {.f64 };
.ctype  = {.f64 };


Sub-byte loads:

wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}
wmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}
.layout = {.row, .col};
.shape  = {.m8n8k32};
.ss     = {.global, .shared{::cta}};
.atype  = {.s4, .u4};
.btype  = {.s4, .u4};
.ctype  = {.s32};


Single-bit loads:

wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride}
wmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride}
wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}
.layout = {.row, .col};
.shape  = {.m8n8k128};
.ss     = {.global, .shared{::cta}};
.atype  = {.b1};
.btype  = {.b1};
.ctype  = {.s32};


Description
Collectively load a matrix across all threads in a warp from the location indicated by address
operand p in the specified state space into destination register r.
If no state space is given, perform the memory accesses using Generic Addressing. wmma.load operation may be used only with .global and
.shared spaces and with generic addressing, where the address points to .global or
.shared space.
The mutually exclusive qualifiers .a, .b and .c indicate whether matrix A, B or C is
being loaded respectively for the wmma computation.
The destination operand r is a brace-enclosed vector expression that can hold the fragment
returned by the load operation, as described in Matrix Fragments for WMMA.
The .shape qualifier indicates the dimensions of all the matrix arguments involved in the
intended wmma computation.
The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or
column-major format.
stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements
between the start of consecutive instances of the leading dimension (rows or columns). The default
value of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than
the default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride
is the leading dimension of the larger matrix. Specifying a value lower than the default value
results in undefined behavior.
The required alignment for address p and stride is described in the Matrix Storage for WMMA.
The mandatory .sync qualifier indicates that wmma.load causes the executing thread to wait
until all threads in the warp execute the same wmma.load instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
wmma.load instruction. In conditionally executed code, a wmma.load instruction should only
be used if it is known that all threads in the warp evaluate the condition identically, otherwise
behavior is undefined.
The behavior of wmma.load is undefined if all threads do not use the same qualifiers and the
same values of p and stride, or if any thread in the warp has exited.
wmma.load is treated as a weak memory operation in the Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 6.0.
.m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1.
Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3.
.m8n8k4 and .m16n16k8 on wmma introduced in PTX ISA version 7.0.
Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0.
Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX
ISA versions less than 6.3.
Support for ::cta sub-qualifier introduced in PTX ISA version 7.8.

Preview Feature:

Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. All
details are subject to change with no guarantees of backward compatibility on future PTX ISA
versions or SM architectures.


Target ISA Notes
Floating point wmma requires sm_70 or higher.
Integer wmma requires sm_72 or higher.
Sub-byte and single-bit wmma requires sm_75 or higher.
Double precision and alternate floating point precision wmma requires sm_80 or higher.
Examples

// Load elements from f16 row-major matrix B
.reg .b32 x<8>;

wmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr];
// Now use {x0, ..., x7} for the actual wmma.mma

// Load elements from f32 column-major matrix C and scale the values:
.reg .b32 x<8>;

wmma.load.c.sync.aligned.m16n16k16.col.f32
                 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr];

mul.f32 x0, x0, 0.1;
// repeat for all registers x<8>;
...
mul.f32 x7, x7, 0.1;
// Now use {x0, ..., x7} for the actual wmma.mma

// Load elements from integer matrix A:
.reg .b32 x<4>
// destination registers x<4> contain four packed .u8 values each
wmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr];

// Load elements from sub-byte integer matrix A:
.reg .b32 x0;
// destination register x0 contains eight packed .s4 values
wmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr];

// Load elements from .bf16 matrix A:
.reg .b32 x<4>;
wmma.load.a.sync.aligned.m16n16k16.row.bf16
                {x0,x1,x2,x3}, [ptr];

// Load elements from .tf32 matrix A:
.reg .b32 x<4>;
wmma.load.a.sync.aligned.m16n16k8.row.tf32
                {x0,x1,x2,x3}, [ptr];

// Load elements from .f64 matrix A:
.reg .b32 x<4>;
wmma.load.a.sync.aligned.m8n8k4.row.f64
                {x0}, [ptr];





9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.storeï

wmma.store
Collectively store a matrix into memory for WMMA
Syntax

wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride};

.layout = {.row, .col};
.shape  = {.m16n16k16, .m8n32k16, .m32n8k16};
.ss     = {.global, .shared{::cta}};
.type   = {.f16, .f32, .s32};

wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}
.layout = {.row, .col};
.shape  = {.m8n8k32, .m8n8k128};
.ss     = {.global, .shared{::cta}};
.type   = {.s32};

wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}
.layout = {.row, .col};
.shape  = {.m16n16k8};
.ss     = {.global, .shared{::cta}};
.type   = {.f32};

wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}
.layout = {.row, .col};
.shape  = {.m8n8k4 };
.ss     = {.global, .shared{::cta}};
.type   = {.f64};


Description
Collectively store a matrix across all threads in a warp at the location indicated by address
operand p in the specified state space from source register r.
If no state space is given, perform the memory accesses using Generic Addressing. wmma.load operation may be used only with .global and
.shared spaces and with generic addressing, where the address points to .global or
.shared space.
The source operand r is a brace-enclosed vector expression that matches the shape of the
fragment expected by the store operation, as described in Matrix Fragments for WMMA.
The .shape qualifier indicates the dimensions of all the matrix arguments involved in the
intended wmma computation. It must match the .shape qualifier specified on the wmma.mma
instruction that produced the D matrix being stored.
The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or
column-major format.
stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements
between the start of consecutive instances of the leading dimension (rows or columns). The default
value of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than
the default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride
is the leading dimension of the larger matrix. Specifying a value lower than the default value
results in undefined behavior.
The required alignment for address p and stride is described in the Matrix Storage for WMMA.
The mandatory .sync qualifier indicates that wmma.store causes the executing thread to wait
until all threads in the warp execute the same wmma.store instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
wmma.store instruction. In conditionally executed code, a wmma.store instruction should only
be used if it is known that all threads in the warp evaluate the condition identically, otherwise
behavior is undefined.
The behavior of wmma.store is undefined if all threads do not use the same qualifiers and the
same values of p and stride, or if any thread in the warp has exited.
wmma.store is treated as a weak memory operation in the Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 6.0.
.m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1.
Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3.
.m16n16k8 introduced in PTX ISA version 7.0.
Double precision wmma introduced in PTX ISA version 7.0.
Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX
ISA versions less than 6.3.
Support for ::cta sub-qualifier introduced in PTX ISA version 7.8.

Preview Feature:

Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. All
details are subject to change with no guarantees of backward compatibility on future PTX ISA
versions or SM architectures.


Target ISA Notes
Floating point wmma requires sm_70 or higher.
Integer wmma requires sm_72 or higher.
Sub-byte and single-bit wmma requires sm_75 or higher.
Double precision wmma and shape .m16n16k8 requires sm_80 or higher.
Examples

// Storing f32 elements computed by a wmma.mma
.reg .b32 x<8>;

wmma.mma.sync.m16n16k16.row.col.f32.f32
              {d0, d1, d2, d3, d4, d5, d6, d7}, ...;
wmma.store.d.sync.m16n16k16.row.f32
              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};

// Store s32 accumulator for m16n16k16 shape:
.reg .b32 d<8>;
wmma.store.d.sync.aligned.m16n16k16.row.s32
              [ptr], {d0, d1, d2, d3, d4, d5, d6, d7};

// Store s32 accumulator for m8n8k128 shape:
.reg .b32 d<2>
wmma.store.d.sync.aligned.m8n8k128.row.s32
[ptr], {d0, d1};

// Store f64 accumulator for m8n8k4 shape:
.reg .f64 d<2>;
wmma.store.d.sync.aligned.m8n8k4.row.f64
              [ptr], {d0, d1};





9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mmaï

wmma.mma
Perform a single matrix multiply-and-accumulate operation across a warp
Syntax

// Floating point (.f16 multiplicands) wmma.mma
wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c;

// Integer (.u8/.s8 multiplicands) wmma.mma
wmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;

.alayout = {.row, .col};
.blayout = {.row, .col};
.shape  =  {.m16n16k16, .m8n32k16, .m32n8k16};
.dtype   = {.f16, .f32};
.atype   = {.s8, .u8};
.btype   = {.s8, .u8};
.ctype   = {.f16, .f32};


Floating point format .bf16wmma.mma:

wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;
.alayout = {.row, .col};
.blayout = {.row, .col};
.shape   = {.m16n16k16, .m8n32k16, .m32n8k16};
.atype   = {.bf16 };
.btype   = {.bf16};


Floating point format .tf32wmma.mma:

wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c;
.alayout = {.row, .col};
.blayout = {.row, .col};
.shape   = {.m16n16k8 };
.atype   = {.tf32 };
.btype   = {.tf32};


Floating point Double precision wmma.mma:

wmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c;
.alayout = {.row, .col};
.blayout = {.row, .col};
.shape   = {.m8n8k4 };
.rnd = { .rn, .rz, .rm, .rp };


Sub-byte (.u4/.s4 multiplicands) wmma.mma:

wmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c;
.shape  = {.m8n8k32};
.atype  = {.s4, .u4};
.btype  = {.s4, .u4};


Single-bit (.b1 multiplicands) wmma.mma:

wmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c;
.shape  = {.m8n8k128};
.atype  = {.b1};
.btype  = {.b1};
.op     = {.xor, .and}


Description
Perform a warp-level matrix multiply-and-accumulate computation D = A * B + C using matrices A,
B and C loaded in registers a, b and c respectively, and store the result matrix in
register d. The register arguments a, b, c and d hold unspecified fragments of
the corresponding matrices as described in Matrix Fragments for WMMA
The qualifiers .dtype, .atype, .btype and .ctype indicate the data-type of the
elements in the matrices D, A, B and C respectively.
For wmma.mma without explicit .atype and .btype: .atype and .btype are
implicitly set to .f16.
For integer wmma, .ctype and .dtype must be specified as .s32. Also, the values for
.atype and .btype must be the same, i.e., either both are .s8 or both are .u8.
For sub-byte single-bit wmma, .ctype and .dtype must be specified as .s32. Also, the
values for .atype and .btype must be the same; i.e., either both are .s4, both are
.u4, or both are .b1.
For single-bit wmma, multiplication is replaced by a sequence of logical operations;
specifically, wmma.xor.popc and wmma.and.popc computes the XOR, AND respectively of a
128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result
(popc). This result is added to the corresponding element of C and written into D.
The qualifiers .alayout and .blayout must match the layout specified on the wmma.load
instructions that produce the contents of operands a and b respectively. Similarly, the
qualifiers .atype, .btype and .ctype must match the corresponding qualifiers on the
wmma.load instructions that produce the contents of operands a, b and c
respectively.
The .shape qualifier must match the .shape qualifier used on the wmma.load instructions
that produce the contents of all three input operands a, b and c respectively.
The destination operand d is a brace-enclosed vector expression that matches the .shape of
the fragment computed by the wmma.mma instruction.

Saturation at the output:

The optional qualifier .satfinite indicates that the final values in the destination register
are saturated as follows:

The output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the
accumulation would overflow, the value wraps.


Precision and rounding for .f16 floating point operations:

Element-wise multiplication of matrix A and B is performed with at least single precision. When
.ctype or .dtype is .f32, accumulation of the intermediate values is performed with
at least single precision. When both .ctype and .dtype are specified as .f16, the
accumulation is performed with at least half precision.
The accumulation order, rounding and handling of subnormal inputs is unspecified.

Precision and rounding for .bf16, .tf32 floating point operations:

Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation
of the intermediate values is performed with at least single precision.
The accumulation order, rounding and handling of subnormal inputs is unspecified.


Rounding modifiers on double precision wmma.mma (default is .rn):

.rn

mantissa LSB rounds to nearest even

.rz

mantissa LSB rounds towards zero

.rm

mantissa LSB rounds towards negative infinity

.rp

mantissa LSB rounds towards positive infinity


The mandatory .sync qualifier indicates that wmma.mma causes the executing thread to wait
until all threads in the warp execute the same wmma.mma instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
wmma.mma instruction. In conditionally executed code, a wmma.mma instruction should only be
used if it is known that all threads in the warp evaluate the condition identically, otherwise
behavior is undefined.
The behavior of wmma.mma is undefined if all threads in the same warp do not use the same
qualifiers, or if any thread in the warp has exited.
PTX ISA Notes
Introduced in PTX ISA version 6.0.
.m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1.
Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3.
Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0.
Support for .and operation in single-bit wmma introduced in PTX ISA version 7.1.
Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX
ISA versions less than 6.3.
Support for .satfinite on floating point wmma.mma is deprecated in PTX ISA version 6.4 and
is removed from PTX ISA version 6.5.

Preview Feature:

Sub-byte wmma and single-bit wmma are preview features in PTX ISA. All details are
subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM
architectures.


Target ISA Notes
Floating point wmma requires sm_70 or higher.
Integer wmma requires sm_72 or higher.
Sub-byte and single-bit wmma requires sm_75 or higher.
Double precision, alternate floating point precision wmma require sm_80 or higher.
.and operation in single-bit wmma requires sm_80 or higher.
Examples

.global .align 32 .f16 A[256], B[256];
.global .align 32 .f32 C[256], D[256];
.reg .b32 a<8> b<8> c<8> d<8>;

wmma.load.a.sync.aligned.m16n16k16.global.row.f16
        {a0, a1, a2, a3, a4, a5, a6, a7}, [A];
wmma.load.b.sync.aligned.m16n16k16.global.col.f16
        {b0, b1, b2, b3, b4, b5, b6, b7}, [B];

wmma.load.c.sync.aligned.m16n16k16.global.row.f32
        {c0, c1, c2, c3, c4, c5, c6, c7}, [C];

wmma.mma.sync.aligned.m16n16k16.row.col.f32.f32
        {d0, d1, d2, d3, d4, d5, d6, d7},
        {a0, a1, a2, a3, a4, a5, a6, a7},
        {b0, b1, b2, b3, b4, b5, b6, b7},
        {c0, c1, c2, c3, c4, c5, c6, c7};

wmma.store.d.sync.aligned.m16n16k16.global.col.f32
        [D], {d0, d1, d2, d3, d4, d5, d6, d7};

// Compute an integer WMMA:
.reg .b32  a, b<4>;
.reg .b32 c<8>, d<8>;
wmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32
        {d0, d1, d2, d3, d4, d5, d6, d7},
        {a}, {b0, b1, b2,  b3},
        {c0, c1, c2, c3, c4, c5, c6, c7};

// Compute sub-byte WMMA:
.reg .b32 a, b, c<2> d<2>
wmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32
        {d0, d1}, {a}, {b}, {c0, c1};

// Compute single-bit type WMMA:
.reg .b32 a, b, c<2> d<2>
wmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32
        {d0, d1}, {a}, {b}, {c0, c1};

// Compute double precision wmma
.reg .f64 a, b, c<2>, d<2>;
wmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64
        {d0, d1}, {a}, {b}, {c0, c1};

// Compute alternate floating point precision wmma
.reg .b32 a<2>, b<2>, c<8>, d<8>;
wmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32
        {d0, d1, d2, d3, d4, d5, d6, d7},
        {a0, a1, a2, a3}, {b0, b1, b2, b3},
        {c0, c1, c2, c3, c4, c5, c6, c7};






9.7.13.4. Matrix multiply-accumulate operation using mma instructionï

This section describes warp-level mma, ldmatrix, stmatrix, and movmatrix
instructions and the organization of various matrices involved in these instructions.


9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point typeï

A warp executing mma.m8n8k4 with .f16 floating point type will compute 4 MMA operations of shape
.m8n8k4.
Elements of 4 matrices need to be distributed across the threads in a warp. The following table
shows distribution of matrices for MMA operations.







MMA Computation
Threads participating in MMA computation




MMA computation 1
Threads with %laneid 0-3 (low group) and 16-19 (high group)


MMA computation 2
Threads with %laneid 4-7 (low group) and 20-23 (high group)


MMA computation 3
Threads with %laneid 8-11 (low group) and 24-27 (high group)


MMA computation 4
Threads with %laneid 12-15 (low group) and 28-31 (high group)



For each of the individual MMA computation shown above, each of the required thread holds a fragment
of the matrix for performing mma operation as follows:


Multiplicand A:








.atype
Fragment
Elements (low to high)




.f16
A vector expression containing two .f16x2 registers,
with each register containing two .f16 elements from
the matrix A.
a0, a1, a2, a3



The layout of the fragments held by different threads is shown below:


Fragment layout for Row Major matrix A is shown in Figure 21.



Figure 21 MMA .m8n8k4 fragment layout for row-major matrix A with .f16 typeï


The row and column of a matrix fragment can be computed as:

row =            %laneid % 4          if %laneid < 16
                (%laneid % 4) + 4     otherwise

col =            i                    for ai where i = {0,..,3}




Fragment layout for Column Major matrix A is shown in Figure 22.
The layout of the fragments held by different threads is shown below:



Figure 22 MMA .m8n8k4 fragment layout for column-major matrix A with .f16 typeï


The row and column of a matrix fragment can be computed as:

row =        i % 4            for ai  where i = {0,..,3}   if %laneid < 16
            (i % 4) + 4       for ai  where i = {0,..,3}   otherwise

col =        %laneid % 4






Multiplicand B:








.btype
Fragment
Elements (low to high)




.f16
A vector expression containing two .f16x2 registers, with each
register containing two .f16 elements from the matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown below:


Fragment layout for Row Major matrix B is shown in Figure 23.



Figure 23 MMA .m8n8k4 fragment layout for row-major matrix B with .f16 typeï


The row and column of a matrix fragment can be computed as:

row =        %laneid % 4

col =         i      for bi   where i = {0,..,3}   if %laneid < 16
             i+4     for bi   where i = {0,..,3}   otherwise




Fragment layout for Column Major matrix B is shown in Figure 24.



Figure 24 MMA .m8n8k4 fragment layout for column-major matrix B with .f16 typeï


The row and column of a matrix fragment can be computed as:

row =       i                 for bi   where i = {0,..,3}

col =      %laneid % 4        if %laneid < 16
          (%laneid % 4) + 4   otherwise






Accumulators C (or D):








.ctype / .dtype
Fragment
Elements (low to high)




.f16
A vector expression containing four .f16x2 registers, with
each register containing two .f16 elements from the matrix
C (or D).
c0, c1, c2, c3, c4, c5, c6, c7


.f32
A vector expression of eight .f32 registers.



The layout of the fragments held by different threads is shown below:


Fragment layout for accumulator matrix when .ctype is .f16 is shown in Figure 25.



Figure 25 MMA .m8n8k4 fragment layout for matrix C/D with .ctype = .f16ï


The row and column of a matrix fragment can be computed as:

row =       %laneid % 4         if %laneid < 16
           (%laneid % 4) + 4    otherwise

col =          i                for ci   where i = {0,..,7}




Fragment layout for accumulator matrix when .ctype is .f32 is shown in
Figure 26 and Figure 27.



Figure 26 MMA .m8n8k4 computation 1 and 2 fragment layout for matrix C/D with .ctype = .f32ï





Figure 27 MMA .m8n8k4 computation 3 and 4 fragment layout for matrix C/D with .ctype = .f32ï


The row and column of a matrix fragment can be computed as:

row =     X           if %laneid < 16
        X + 4         otherwise

          where X = (%laneid & 0b1) + (i & 0b10)  for ci where i = {0,..,7}

col = (i & 0b100) + (%laneid & 0b10) + (i & 0b1)  for ci where i = {0,..,7}









9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point typeï

A warp executing mma.m8n8k4 with .f64 floating point type will compute an MMA operation of
shape .m8n8k4.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.f64
A vector expression containing a single .f64 register, containing
single .f64 element from the matrix A.
a0



The layout of the fragments held by different threads is shown in Figure 28.



Figure 28 MMA .m8n8k4 fragment layout for matrix A  with .f64 typeï


The row and column of a matrix fragment can be computed as:

row =        %laneid >> 2

col =        %laneid % 4




Multiplicand B:








.btype
Fragment
Elements (low to high)




.f64
A vector expression containing a single .f64 register, containing a single
.f64 element from the matrix B.
b0



The layout of the fragments held by different threads is shown in Figure 29.



Figure 29 MMA .m8n8k4 fragment layout for matrix B  with .f64 typeï


The row and column of a matrix fragment can be computed as:

row =        %laneid % 4

col =        %laneid >> 2




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.f64
A vector expression containing of two .f64 registers containing two
.f64 elements from the matrix C.
c0, c1



The layout of the fragments held by different threads is shown in Figure 30.



Figure 30 MMA .m8n8k4 fragment layout for accumulator matrix C/D  with .f64 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID

col =      (threadID_in_group * 2) + (i & 0x1)       for ci   where i = {0, 1}







9.7.13.4.3. Matrix Fragments for mma.m8n8k16ï

A warp executing mma.m8n8k16 will compute an MMA operation of shape .m8n8k16.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.s8 / .u8
A vector expression containing a single .b32 register, containing
four .s8 or .u8 elements from the matrix A.
a0, a1, a2, a3



The layout of the fragments held by different threads is shown in Figure 31.



Figure 31 MMA .m8n8k16 fragment layout for matrix A with .u8/.s8 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row = groupID

col =  (threadID_in_group * 4) + i       for ai    where i = {0,..,3}




Multiplicand B:








.btype
Fragment
Elements (low to high)




.s8 / .u8
A vector expression containing a single .b32 register, containing
four .s8 or .u8 elements from the matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown in Figure 32.



Figure 32 MMA .m8n8k16 fragment layout for matrix B with .u8/.s8 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  (threadID_in_group * 4) + i         for bi    where i = {0,..,3}

col =    groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing of two .s32 registers.
c0, c1



The layout of the fragments held by different threads is shown in Figure 33.



Figure 33 MMA .m8n8k16 fragment layout for accumulator matrix C/D with .s32 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row = groupID

col = (threadID_in_group * 2) + i         for ci    where i = {0, 1}







9.7.13.4.4. Matrix Fragments for mma.m8n8k32ï

A warp executing mma.m8n8k32 will compute an MMA operation of shape .m8n8k32.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.s4 / .u4
A vector expression containing a single .b32 register, containing
eight .s4 or .u4 elements from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7



The layout of the fragments held by different threads is shown in Figure 34.



Figure 34 MMA .m8n8k32 fragment layout for matrix A with .u4/.s4 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID

col = (threadID_in_group * 8) + i         for ai    where i = {0,..,7}




Multiplicand B:








.btype
Fragment
Elements (low to high)




.s4 / .u4
A vector expression containing a single .b32 register, containing
eight .s4 or .u4 elements from the matrix B.
b0, b1, b2, b3, b4, b5, b6, b7



The layout of the fragments held by different threads is shown in Figure 35.



Figure 35 MMA .m8n8k32 fragment layout for matrix B with .u4/.s4 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row = (threadID_in_group * 8) + i         for bi   where i = {0,..,7}

col = groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression of two .s32 registers.
c0, c1



The layout of the fragments held by different threads is shown in Figure 36:



Figure 36 MMA .m8n8k32 fragment layout for accumulator matrix C/D with .s32 typeï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =   groupID
col = (threadID_in_group * 2) + i         for ci   where i = {0, 1}







9.7.13.4.5. Matrix Fragments for mma.m8n8k128ï

A warp executing mma.m8n8k128 will compute an MMA operation of shape .m8n8k128.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.b1
A vector expression containing a single .b32 register, containing thirty
two .b1 elements from the matrix A.
a0, a1, â¦ a30, a31



The layout of the fragments held by different threads is shown in Figure 37.



Figure 37 MMA .m8n8k128 fragment layout for matrix A with .b1 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  groupID

col =  (threadID_in_group * 32) + i       for ai where i = {0,..,31}




Multiplicand B:








.btype
Fragment
Elements (low to high)




.b1
A vector expression containing a single .b32 register, containing thirty
two .b1 elements from the matrix B.
b0, b1, â¦, b30, b31



The layout of the fragments held by different threads is shown in Figure 38.



Figure 38 MMA .m8n8k128 fragment layout for matrix B with .b1 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row = (threadID_in_group * 32) + i         for bi where i = {0,..,31}

col = groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing two .s32 registers, containing two
.s32 elements from the matrix C (or D).
c0, c1



The layout of the fragments held by different threads is shown in Figure 39.



Figure 39 MMA .m8n8k128 fragment layout for accumulator matrix C/D with .s32 typeï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID

col =  (threadID_in_group * 2) + i    for ci where i = {0, 1}







9.7.13.4.6. Matrix Fragments for mma.m16n8k4ï

A warp executing mma.m16n8k4 will compute an MMA operation of shape .m16n8k4.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:


.tf32:








.atype
Fragment
Elements (low to high)




.tf32
A vector expression containing two .b32 registers, containing two
.tf32 elements from the matrix A.
a0, a1



The layout of the fragments held by different threads is shown in Figure 40.



Figure 40 MMA .m16n8k4 fragment layout for matrix A with .tf32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0
           groupID + 8        for a1

col =  threadID_in_group




.f64:










.atype
Fragment
Elements (low to high)




.f64
A vector expression containing two .f64 registers, containing two
.f64 elements from the matrix A.
a0, a1



The layout of the fragments held by different threads is shown in Figure 41.



Figure 41 MMA .m16n8k4 fragment layout for matrix A with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0
           groupID + 8        for a1

col =  threadID_in_group








Multiplicand B:


.tf32:










.btype
Fragment
Elements (low to high)




.tf32
A vector expression of a single .b32 register, containing a single
.tf32 element from the matrix B.
b0



The layout of the fragments held by different threads is shown in Figure 42.



Figure 42 MMA .m16n8k4 fragment layout for matrix B with .tf32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  threadID_in_group

col =  groupID






.f64:










.btype
Fragment
Elements (low to high)




.f64
A vector expression of a single .f64 register, containing a single
.f64 element from the matrix B.
b0



The layout of the fragments held by different threads is shown in Figure 43.



Figure 43 MMA .m16n8k4 fragment layout for matrix B with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  threadID_in_group

col =  groupID








Accumulators (C or D):


.tf32:










.ctype / .dtype
Fragment
Elements (low to high)




.f32
A vector expression containing four .f32 registers, containing four
.f32 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 44.



Figure 44 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                            for c0 and c1
         groupID + 8                          for c2 and c3

col =  (threadID_in_group * 2) + (i & 0x1)    for ci   where i = {0,..,3}






.f64:










.ctype / .dtype
Fragment
Elements (low to high)




.f64
A vector expression containing four .f64 registers, containing four
.f64 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 45.



Figure 45 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                            for c0 and c1
         groupID + 8                          for c2 and c3

col =  (threadID_in_group * 2) + (i & 0x1)    for ci   where i = {0,..,3}











9.7.13.4.7. Matrix Fragments for mma.m16n8k8ï

A warp executing mma.m16n8k8 will compute an MMA operation of shape .m16n8k8.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:


.f16 and .bf16 :










.atype
Fragment
Elements (low to high)




.f16 / .bf16
A vector expression containing two .f16x2 registers, with each
register containing two .f16 / .bf16 elements from the
matrix A.
a0, a1, a2, a3



The layout of the fragments held by different threads is shown in Figure 46.



Figure 46 MMA .m16n8k8 fragment layout for matrix A with .f16 / .bf16 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0 and a1
           groupID + 8        for a2 and a3

col =  threadID_in_group * 2 + (i & 0x1)    for ai     where i = {0,..,3}






.tf32 :










.atype
Fragment
Elements (low to high)




.tf32
A vector expression containing four .b32 registers, containing four
.tf32 elements from the matrix A.
a0, a1, a2, a3



The layout of the fragments held by different threads is shown in Figure 47.



Figure 47 MMA .m16n8k8 fragment layout for matrix A with .tf32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0 and a2
           groupID + 8        for a1 and a3

col =  threadID_in_group       for a0 and a1
       threadID_in_group + 4   for a2 and a3






.f64 :








.atype
Fragment
Elements (low to high)




.f64
A vector expression containing four .f64 registers, containing four
.f64 elements from the matrix A.
a0, a1, a2, a3



The layout of the fragments held by different threads is shown in Figure 48.



Figure 48 MMA .m16n8k8 fragment layout for matrix A with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0 and a2
           groupID + 8        for a1 and a3

col =  threadID_in_group       for a0 and a1
       threadID_in_group + 4   for a2 and a3






Multiplicand B:


.f16 and .bf16 :










.btype
Fragment
Elements (low to high)




.f16 / .bf16
A vector expression containing a single .f16x2 register, containing
two .f16 / .bf16 elements from the matrix B.
b0, b1



The layout of the fragments held by different threads is shown in Figure 49.



Figure 49 MMA .m16n8k8 fragment layout for matrix B with .f16 / .bf16 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row = (threadID_in_group * 2) + i       for bi    where i = {0, 1}

col =  groupID






.tf32 :










.btype
Fragment
Elements (low to high)




.tf32
A vector expression containing two .b32 registers, containing two
.tf32 elements from the matrix B.
b0, b1



The layout of the fragments held by different threads is shown in Figure 50.



Figure 50 MMA .m16n8k8 fragment layout for matrix B with .tf32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =    threadID_in_group         for b0
       threadID_in_group + 4       for b1

col =  groupID






.f64 :










.btype
Fragment
Elements (low to high)




.f64
A vector expression containing two .f64 registers, containing two
.f64 elements from the matrix B.
b0, b1



The layout of the fragments held by different threads is shown in Figure 51.



Figure 51 MMA .m16n8k8 fragment layout for matrix B with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =    threadID_in_group         for b0
       threadID_in_group + 4       for b1

col =  groupID








Accumulators (C or D):


.f16, .bf16 and .tf32:










.ctype / .dtype
Fragment
Elements (low to high)




.f16
A vector expression containing two .f16x2 registers, with each
register containing two .f16 elements from the matrix C (or D).
c0, c1, c2, c3


.f32
A vector expression of four .f32 registers.




The layout of the fragments held by different threads is shown in Figure 52.



Figure 52 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f16x2/.f32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                            for c0 and c1
         groupID + 8                          for c2 and c3

col =  (threadID_in_group * 2) + (i & 0x1)    for ci   where i = {0,..,3}






.f64 :










.ctype / .dtype
Fragment
Elements (low to high)




.f64
A vector expression of four .f64 registers containing four
.f64 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 53.



Figure 53 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                            for c0 and c1
         groupID + 8                          for c2 and c3

col =  (threadID_in_group * 2) + (i & 0x1)    for ci   where i = {0,..,3}











9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point typeï

A warp executing mma.m16n8k16 floating point types will compute an MMA operation of shape
.m16n8k16.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:


.f16 and .bf16 :










.atype
Fragment
Elements (low to high)




.f16 / .bf16
A vector expression containing four .f16x2 registers, with
each register containing two .f16 / .bf16 elements
from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7



The layout of the fragments held by different threads is shown in Figure 54.



Figure 54 MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for ai where  0 <= i < 2 || 4 <= i < 6
          groupID + 8         Otherwise

col =  (threadID_in_group * 2) + (i & 0x1)          for ai where i <  4
(threadID_in_group * 2) + (i & 0x1) + 8      for ai where i >= 4






.f64 :










.atype
Fragment
Elements (low to high)




.f64
A vector expression containing eight .f64 registers, with each
register containing one .f64 element from the matrix A.
a0, a1, a2, a3, a4, a5, a6, a7



The layout of the fragments held by different threads is shown in Figure 55.



Figure 55 MMA .m16n8k16 fragment layout for matrix A with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  groupID                               for ai where  i % 2 = 0
       groupID + 8                           Otherwise

col =  (i * 2) + threadID_in_group           for ai where i % 2 = 0
       (i * 2) - 2 + (threadID_in_group      Otherwise








Multiplicand B:


.f16 and .bf16 :










.btype
Fragment
Elements (low to high)




.f16 / .bf16
A vector expression containing two .f16x2 registers, with
each register containing two .f16 / .bf16 elements
from the matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown in Figure 56.



Figure 56 MMA .m16n8k16 fragment layout for matrix B with .f16 / .bf16 type.ï


where the row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  (threadID_in_group * 2) + (i & 0x1)           for bi where i <  2
       (threadID_in_group * 2) + (i & 0x1) + 8       for bi where i >= 2

col = groupID






.f64 :










.atype
Fragment
Elements (low to high)




.f64
A vector expression containing four .f64 registers, with each
register containing one .f64 element from the matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown in Figure 57.



Figure 57 MMA .m16n8k16 fragment layout for matrix B with .f64 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  threadID_in_group + (i * 4)           for bi where  i < 4

col =  groupID








Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.f64
A vector expression containing four .f64 registers containing
.f64 elements from the matrix C (or D).
c0, c1, c2, c3


.f32
A vector expression containing four .f32 registers containing
four .f32 elements from the matrix C (or D).


.f16
A vector expression containing two .f16x2 registers, with each
register containing two .f16 elements from the matrix C (or D).



The layout of the fragments held by different threads is shown in Figure 58.



Figure 58 MMA .m16n8k16 fragment layout for accumulator matrix matrix C/D.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                               for ci where i <  2
         groupID + 8                             for ci where i >= 2

col =  (threadID_in_group * 2) + (i & 0x1)        for ci where i = {0,..,3}







9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer typeï

A warp executing mma.m16n8k16 with .u8 or .s8 integer type will compute an MMA operation
of shape .m16n8k16.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.u8 / .s8
A vector expression containing two .b32 registers, with each
register containing four .u8 / .s8 elements from the
matrix A.
a0, a1, a2, a3, a4, a5, a6, a7



The layout of the fragments held by different threads is shown in Figure 59.



Figure 59 MMA .m16n8k16 fragment layout for matrix A with .u8 / .s8 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                            for ai where i < 4
         groupID + 8                          for ai where i >= 4

col =  (threadID_in_group * 4) + (i & 0x3)    for ai where i = {0,..,7}




Multiplicand B:








.btype
Fragment
Elements (low to high)




.u8 / .s8
A vector expression containing a single .b32 register, containing
four .u8 / .s8 elements from the matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown in Figure 60.



Figure 60 MMA .m16n8k16 fragment layout for matrix B with .u8 / .s8 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  (threadID_in_group * 4) + i         for bi where i = {0,..,3}

col = groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing four .s32 registers, containing
four .s32 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 61.



Figure 61 MMA .m16n8k16 fragment layout for accumulator matrix C/D with .s32  type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                           for ci where i <  2
         groupID + 8                         for ci where i >= 2

col =  (threadID_in_group * 2) + (i & 0x1)    for ci where i = {0,..,3}







9.7.13.4.10. Matrix Fragments for mma.m16n8k32ï

A warp executing mma.m16n8k32 will compute an MMA operation of shape .m16n8k32.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:


.s4 or .u4 :










.atype
Fragment
Elements (low to high)




.s4 / .u4
A vector expression containing two .b32 registers, with each
register containing eight .u4 / .s4 elements from the
matrix A.
a0, a1, â¦, a14, a15



The layout of the fragments held by different threads is shown in Figure 62.



Figure 62 MMA .m16n8k32 fragment layout for matrix A with .u4 / .s4 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                           for ai where i < 8
         groupID + 8                         for ai where i >= 8

col =  (threadID_in_group * 8) + (i & 0x7)    for ai where i = {0,..,15}






.s8 or .u8 or .e4m3 or .e5m2 :










.atype
Fragment
Elements (low to high)




.s8 / .u8
A vector expression containing four .b32 registers, with each
register containing four .s8 / .u8 elements from the
matrix A.
a0, a1, .., a14, a15


.e4m3 /
.e5m2
A vector expression containing four .b32 registers, with each
register containing four .e4m3 / .e5m2 elements from
the matrix A.
a0, a1, â¦, a14, a15



The layout of the fragments held by different threads is shown in Figure 63.



Figure 63 MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =   groupID                                        for ai where 0 <= i < 4 || 8 <= i < 12
       groupID + 8                                     otherwise

col =    (threadID_in_group * 4) + (i & 0x3)           for ai where i < 8
         (threadID_in_group * 4) + (i & 0x3) + 16      for ai where i >= 8








Multiplicand B:


.s4 or .u4 :










.btype
Fragment
Elements (low to high)




.s4 / .u4
A vector expression containing a single .b32 register,
containing eight .s4 / .u4 elements from the matrix B.
b0, b1, b2, b3, b4, b5, b6, b7



The layout of the fragments held by different threads is shown in Figure 64.



Figure 64 MMA .m16n8k32 fragment layout for matrix B with .u4 / .s4 type.ï


The row and column of a matrix fragment can be computed as:





groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =    (threadID_in_group * 8) + (i & 0x7)      for bi where i = {0,..,7}
col =     groupID




.s8 or .u8 or .e4m3 or .e5m2 :










.btype
Fragment
Elements (low to high)




.s8 / .u8
A vector expression containing two .b32 registers, with each
register containing four .s8 / .u8 elements from the
matrix B.
b0, b1, b2, b3, b4, b5, b6, b7


.e4m3 /
.e5m2
A vector expression containing two .b32 registers, with each
register containing four .e4m3 / .e5m2 elements from the
matrix B.
b0, b1, b2, b3, b4, b5, b6, b7



The layout of the fragments held by different threads is shown in Figure 65 and
Figure 66.



Figure 65 MMA .m16n8k32 fragment layout for rows 0â15 of matrix B with .u8 / .s8 type.ï





Figure 66 MMA .m16n8k32 fragment layout for rows 16â31 of matrix B with .u8 / .s8 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      (threadID_in_group * 4) + (i & 0x3)           for bi where i < 4
           (threadID_in_group * 4) + (i & 0x3) + 16      for bi where i >= 4

col =   groupID








Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing four .s32 registers, containing
four .s32 elements from the matrix C (or D).
c0, c1, c2, c3


.f32
A vector expression containing four .f32 registers, containing
four .f32 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 67.



Figure 67 MMA .m16n8k32 fragment layout for accumulator matrix C/D with .s32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                           for ci where i <  2
         groupID + 8                         for ci where i >= 2

col =  (threadID_in_group * 2) + (i & 0x1)    for ci where i = {0,..,3}







9.7.13.4.11. Matrix Fragments for mma.m16n8k64ï

A warp executing mma.m16n8k64 will compute an MMA operation of shape .m16n8k64.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.s4 / .u4
A vector expression containing four .b32 registers, with each
register containing eight .s4 / .u4 elements from the
matrix A.
a0, a1, â¦, a30, a31



The layout of the fragments held by different threads is shown in Figure 68.



Figure 68 MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =     groupID                                     for ai where 0 <= i < 8 || 16 <= i < 24
        groupID + 8                                   otherwise

col =      (threadID_in_group * 8) + (i & 0x7)        for ai where i < 16
           (threadID_in_group * 8) + (i & 0x7) + 32   for ai where i >= 16




Multiplicand B:








.btype
Fragment
Elements (low to high)




.s4 / .u4
A vector expression containing two .b32 registers, with each
register containing eight .s4 / .u4 elements from the
matrix B.
b0, b1, â¦, b14, b15



The layout of the fragments held by different threads is shown in Figure 69
and Figure 70.



Figure 69 MMA .m16n8k64 fragment layout for rows 0â31 of matrix B with .u4 / .s4 type.ï





Figure 70 MMA .m16n8k64 fragment layout for rows 32â63 of matrix B with .u4 / .s4 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      (threadID_in_group * 8) + (i & 0x7)          for bi where i < 8
           (threadID_in_group * 8) + (i & 0x7) + 32     for bi where i >= 8

col =   groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing four .s32 registers, containing four
.s32 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 71.



Figure 71 MMA .m16n8k64 fragment layout for accumulator matrix C/D with .s32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                           for ci where i <  2
           groupID + 8                       for ci where i >= 2

col =  (threadID_in_group * 2) + (i & 0x1)    for ci  where i = {0,..,3}







9.7.13.4.12. Matrix Fragments for mma.m16n8k128ï

A warp executing mma.m16n8k128 will compute an MMA operation of shape .m16n8k128.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.b1
A vector expression containing two .b32 registers, with each register containing
thirty two .b1 elements from the matrix A.
a0, a1, â¦, a62, a63



The layout of the fragments held by different threads is shown in Figure 72.



Figure 72 MMA .m16n8k128 fragment layout for matrix A with .b1 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                              for ai where i < 32
          groupID + 8                           for ai where i >= 32

col =  (threadID_in_group * 32) + (i & 0x1F)     for ai where i = {0, ...,63}




Multiplicand B:








.btype
Fragment
Elements (low to high)




.b1
A vector expression containing a single .b32 register containing thirty
two .b1 elements from the matrix B.
b0, b1, â¦ , b30, b31



The layout of the fragments held by different threads is shown in Figure 73.



Figure 73 MMA .m16n8k128 fragment layout for matrix B with .b1 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =  (threadID_in_group * 32) + i         for bi where i = {0,...,31}
col = groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing four .s32 registers, containing
four .s32 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 74.



Figure 74 MMA .m16n8k128 fragment layout for accumulator matrix C/D with .s32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID                           for ci where i <  2
          groupID + 8                        for ci where i >= 2

col =  (threadID_in_group * 2) + (i & 0x1)    for ci where i = {0, 1, 2, 3}







9.7.13.4.13. Matrix Fragments for mma.m16n8k256ï

A warp executing mma.m16n8k256 will compute an MMA operation of shape .m16n8k256.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements (low to high)




.b1
A vector expression containing four .b32 registers, with each register
containing thirty two .b1 elements from the matrix A.
a0, a1, â¦, a126, a127



The layout of the fragments held by different threads is shown in Figure 75.



Figure 75 MMA .m16n8k256 fragment layout for matrix A with .b1 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =   groupID                                            for ai where 0 <= i < 32 || 64 <= i < 96
        groupID + 8                                        otherwise

col =      (threadID_in_group * 32) + i                    for ai where i < 64
           (threadID_in_group * 32) + (i & 0x1F) + 128     for ai where i >= 64




Multiplicand B:








.btype
Fragment
Elements (low to high)




.b1
A vector expression containing two .b32 registers, with each register
containing thirty two .b1 elements from the matrix B.
b0, b1, â¦, b62, b63



The layout of the fragments held by different threads is shown in Figure 76 and
Figure 77.



Figure 76 MMA .m16n8k256 fragment layout for rows 0â127 of matrix B with .b1 type.ï





Figure 77 MMA .m16n8k256 fragment layout for rows 128â255 of matrix B with .b1 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =      (threadID_in_group * 32) + (i & 0x1F)             for bi where i < 32
           (threadID_in_group * 32) + (i & 0x1F) + 128       for bi where i >= 32

col =      groupID




Accumulators (C or D):








.ctype / .dtype
Fragment
Elements (low to high)




.s32
A vector expression containing four .s32 registers, containing
four .s32 elements from the matrix C (or D).
c0, c1, c2, c3



The layout of the fragments held by different threads is shown in Figure 78.



Figure 78 MMA .m16n8k256 fragment layout for accumulator matrix C/D with .s32 type.ï


The row and column of a matrix fragment can be computed as:

groupID           = %laneid >> 2
threadID_in_group = %laneid % 4

row =        groupID                         for ci where i < 2
           groupID + 8                       for ci where i >= 2

col =  (threadID_in_group * 2) + (i & 0x1)    for ci where i = {0, 1, 2, 3}







9.7.13.4.14. Multiply-and-Accumulate Instruction: mmaï

mma
Perform matrix multiply-and-accumulate operation
Syntax
Half precision floating point type:

mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype  d, a, b, c;
mma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype  d, a, b, c;
mma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c;

.alayout = {.row, .col};
.blayout = {.row, .col};
.ctype   = {.f16, .f32};
.dtype   = {.f16, .f32};


Alternate floating point type :

mma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32        d, a, b, c;
mma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32      d, a, b, c;
mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32       d, a, b, c;
mma.sync.aligned.m16n8k32.row.col.f32.f8type.f8type.f32   d, a, b, c;

.atype  = {.bf16, .tf32};
.btype  = {.bf16, .tf32};
.f8type = {.e4m3, .e5m2};


Double precision floating point type:

mma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c;

.shape   = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16};


Integer type:

mma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;

.shape   = {.m8n8k16, .m16n8k16, .m16n8k32}
.atype   = {.u8, .s8};
.btype   = {.u8, .s8};

mma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c;

.shape   = {.m8n8k32, .m16n8k32, .m16n8k64}
.atype   = {.u4, .s4};
.btype   = {.u4, .s4};


Single bit:

mma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c;

.bitOp = {.xor, .and}
.shape = {.m8n8k128, .m16n8k128, .m16n8k256}


Description
Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C, where the A matrix is
MxK, the B matrix is KxN, and the C and D matrices are MxN.
A warp executing mma.sync.m8n8k4 instruction computes 4 matrix multiply and accumulate
operations. Rest of the mma.sync operations compute a single matrix mutliply and accumulate
operation per warp.
For single-bit mma.sync, multiplication is replaced by a sequence of logical operations;
specifically, mma.xor.popcand mma.and.popc computes the XOR, AND respectively of a k-bit
row of A with a k-bit column of B, then counts the number of set bits in the result (popc). This
result is added to the corresponding element of C and written into D.
Operands a and b represent two multiplicand matrices A and B, while c and d
represent the accumulator and destination matrices, distributed across the threads in warp.
The registers in each thread hold a fragment of matrix as described in Matrix multiply-accumulate
operation using mma instruction.
The qualifiers .dtype, .atype, .btype and .ctype indicate the data-type of the
elements in the matrices D, A, B and C respectively. Specific shapes have type restrictions :

.m8n8k4 : When .ctype is .f32, .dtype must also be .f32.

.m16n8k8 :

.dtype must be the same as .ctype.
.atype must be the same as .btype.



The qualifiers .alayout and .blayout indicate the row-major or column-major layouts of
matrices A and B respectively.

Precision and rounding :



.f16 floating point operations:
Element-wise multiplication of matrix A and B is performed with at least single
precision. When .ctype or .dtype is .f32, accumulation of the intermediate values
is performed with at least single precision. When both .ctype and .dtype are specified
as .f16, the accumulation is performed with at least half precision.
The accumulation order, rounding and handling of subnormal inputs are unspecified.


.e4m3 and .e5m2 floating point operations :
Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation
of the intermediate values is performed with at least single precision.
The accumulation order, rounding, and handling of subnormal inputs are unspecified.


.bf16 and .tf32 floating point operations :
Element-wise multiplication of matrix A and B is performed with specified
precision. Accumulation of the intermediate values is performed with at least single
precision.
The accumulation order, rounding, and handling of subnormal inputs are unspecified.

.f64 floating point operations :



Precision of the element-wise multiplication and addition operation is identical to that of .f64
precision fused multiply-add. Supported rounding modifiers are :

.rn : mantissa LSB rounds to nearest even. This is the default.
.rz : mantissa LSB rounds towards zero.
.rm : mantissa LSB rounds towards negative infinity.

.rp : mantissa LSB rounds towards positive infinity.


Integer operations :
The integer mma operation is performed with .s32 accumulators. The .satfinite
qualifier indicates that on overflow, the accumulated value is limited to the range
MIN_INT32..MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit
integer and the maximum positive signed 32-bit integer respectively).
If .satfinite is not specified, the accumulated value is wrapped instead.




The mandatory .sync qualifier indicates that mma instruction causes the executing thread to
wait until all threads in the warp execute the same mma instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
mma instruction. In conditionally executed code, a mma instruction should only be used if it
is known that all threads in the warp evaluate the condition identically, otherwise behavior is
undefined.
The behavior of mma instruction is undefined if all threads in the same warp do not use the same
qualifiers, or if any thread in the warp has exited.
Notes
Programs using double precision floating point mma instruction with shapes .m16n8k4,
.m16n8k8, and .m16n8k16 require at least 64 registers for compilation.
PTX ISA Notes
Introduced in PTX ISA version 6.4.
.f16 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version
6.4.
.f16 floating point type mma operation with .m16n8k8 shape introduced in PTX ISA version
6.5.
.u8/.s8 integer type mma operation with .m8n8k16 shape introduced in PTX ISA version
6.5.
.u4/.s4 integer type mma operation with .m8n8k32 shape introduced in PTX ISA version
6.5.
.f64 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version
7.0.
.f16 floating point type mma operation with .m16n8k16 shape introduced in PTX ISA
version 7.0.
.bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes
introduced in PTX ISA version 7.0.
.tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes
introduced in PTX ISA version 7.0.
.u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes introduced in
PTX ISA version 7.0.
.u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes introduced in
PTX ISA version 7.0.
.b1 single-bit integer type mma operation with .m8n8k128, .m16n8k128 and
.m16n8k256 shapes introduced in PTX ISA version 7.0.
Support for .and operation in single-bit mma introduced in PTX ISA version 7.1.
.f64 floating point typemma operation with .m16n8k4, .m16n8k8, and .m16n8k16
shapes introduced in PTX ISA version 7.8.
Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in
PTX ISA version 8.4.
Target ISA Notes
Requires sm_70 or higher.
.f16 floating point type mma operation with .m8n8k4 shape requires sm_70 or higher.

Note
mma.sync.m8n8k4 is optimized for target architecture sm_70 and may have substantially
reduced performance on other target architectures.

.f16 floating point type mma operation with .m16n8k8 shape requires sm_75 or higher.
.u8/.s8 integer type mma operation with .m8n8k16 shape requires sm_75 or higher.
.u4/.s4 integer type mma operation with .m8n8k32 shape sm_75 or higher.
.b1 single-bit integer type mma operation with .m8n8k128 shape sm_75 or higher.
.f64 floating point type mma operation with .m8n8k4 shape requires sm_80 or higher.
.f16 floating point type mma operation with .m16n8k16 shape requires sm_80 or
higher.
.bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes
requires sm_80 or higher.
.tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes
requires sm_80 or higher.
.u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes requires
sm_80 or higher.
.u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes requires
sm_80 or higher.
.b1 single-bit integer type mma operation with .m16n8k128 and .m16n8k256 shapes
requires sm_80 or higher.
.and operation in single-bit mma requires sm_80 or higher.
.f64 floating point type mma operation with .m16n8k4, .m16n8k8, and .m16n8k16
shapes require sm_90 or higher.
.e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher.
Examples of half precision floating point type

// f16 elements in C and D matrix
.reg .f16x2 %Ra<2> %Rb<2> %Rc<4> %Rd<4>
mma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16
{%Rd0, %Rd1, %Rd2, %Rd3},
{%Ra0, %Ra1},
{%Rb0, %Rb1},
{%Rc0, %Rc1, %Rc2, %Rc3};


// f16 elements in C and f32 elements in D
.reg .f16x2 %Ra<2> %Rb<2> %Rc<4>
.reg .f32 %Rd<8>
mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16
{%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},
{%Ra0, %Ra1},
{%Rb0, %Rb1},
{%Rc0, %Rc1, %Rc2, %Rc3};

 // f32 elements in C and D
.reg .f16x2 %Ra<2>, %Rb<1>;
.reg .f32 %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .f16x2 %Ra<4>, %Rb<2>, %Rc<2>, %Rd<2>;
mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16
  {%Rd0, %Rd1},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1};

.reg .f16 %Ra<4>, %Rb<2>;
.reg .f32 %Rc<2>, %Rd<2>;
mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};


Examples of alternate floating point type

.reg .b32 %Ra<2>, %Rb<1>;
.reg .f32 %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .f16x2 %Ra<2>, %Rb<1>;
.reg .f32 %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .b32 %Ra<2>, %Rb<1>;
.reg .f32 %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Rb2, %Rb3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .f16x2 %Ra<2>, %Rb<1>;
.reg .f32 %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .b32 %Ra<4>, %Rb<4>;
.reg .f32 %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};


Examples of integer type

.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;

// s8 elements in A and u8 elements in B
mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32
  {%Rd0, %Rd1},
  {%Ra},
  {%Rb},
  {%Rc0, %Rc1};

// u4 elements in A and B matrix
mma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32
  {%Rd0, %Rd1},
  {%Ra},
  {%Rb},
  {%Rc0, %Rc1};

// s8 elements in A and u8 elements in B
.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb},
  {%Rc0, %Rc1, %Rc2, %Rc3};

// u4 elements in A and s4 elements in B
.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb},
  {%Rc0, %Rc1, %Rc2, %Rc3};

// s8 elements in A and s8 elements in B
.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};

// u8 elements in A and u8 elements in B
.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1 },
  {%Rc0, %Rc1, %Rc2, %Rc3};


Examples of single bit type

// b1 elements in A and B
.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;
mma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc
  {%Rd0, %Rd1},
  {%Ra},
  {%Rb},
  {%Rc0, %Rc1};

// b1 elements in A and B
.reg .b32 %Ra, %Rb, %Rc<2>, %Rd<2>;
mma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc
  {%Rd0, %Rd1},
  {%Ra},
  {%Rb},
  {%Rc0, %Rc1};

.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .b32 %Ra<2>, %Rb, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};

.reg .b32 %Ra<4>, %Rb<2>, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};


Examples of .f64 floating point type

.reg .f64 %Ra, %Rb, %Rc<2>, %Rd<2>;
mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64
  {%Rd0, %Rd1},
  {%Ra},
  {%Rb},
  {%Rc0, %Rc1};

.reg .f64 %Ra<8>, %Rb<4>, %Rc<4>, %Rd<4>;
mma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0},
  {%Rc0, %Rc1, %Rc2, %Rc3};

mma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3};

mma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7},
  {%Rb0, %Rb1, %Rb2, %Rb3},
  {%Rc0, %Rc1, %Rc2, %Rc3};





9.7.13.4.15. Warp-level matrix load instruction: ldmatrixï

ldmatrix
Collectively load one or more matrices from shared memory for mma instruction
Syntax

ldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p];

.shape  = {.m8n8};
.num    = {.x1, .x2, .x4};
.ss     = {.shared{::cta}};
.type   = {.b16};


Description
Collectively load one or more matrices across all threads in a warp from the location indicated by
the address operand p, from .shared state space into destination register r. If no state
space is provided, generic addressing is used, such that the address in p points into
.shared space. If the generic address doesnât fall in .shared state space, then the behavior
is undefined.
The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element
holds 16-bit data as indicated by the .type qualifier.
The values .x1, .x2 and .x4 for .num indicate one, two or four matrices
respectively.
The mandatory .sync qualifier indicates that ldmatrix causes the executing thread to wait
until all threads in the warp execute the same ldmatrix instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
ldmatrix instruction. In conditionally executed code, an ldmatrix instruction should only be
used if it is known that all threads in the warp evaluate the condition identically, otherwise the
behavior is undefined.
The behavior of ldmatrix is undefined if all threads do not use the same qualifiers, or if any
thread in the warp has exited.
The destination operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit
registers as per the value of .num. Each component of the vector expression holds a fragment
from the corresponding matrix.
Supported addressing modes for p are described in Addresses as Operands.
Consecutive instances of row need not be stored contiguously in memory. The eight addresses required
for each matrix are provided by eight threads, depending upon the value of .num as shown in the
following table. Each address corresponds to the start of a matrix row. Addresses addr0âaddr7
correspond to the rows of the first matrix, addresses addr8âaddr15 correspond to the rows of the
second matrix, and so on.










.num
Threads 0â7
Threads 8â15
Threads 16â23
Threads 24â31




.x1
addr0âaddr7
â
â
â


.x2
addr0âaddr7
addr8âaddr15
â
â


.x4
addr0âaddr7
addr8âaddr15
addr16âaddr23
addr24âaddr31




Note
For .target sm_75 or below, all threads must contain valid addresses. Otherwise, the behavior
is undefined. For .num = .x1 and .num = .x2, addresses contained in lower threads can be
copied to higher threads to achieve the expected behavior.

When reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses
must be naturally aligned accordingly.
Each thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its
register r, and so on. A group of four threads loads an entire row of the matrix as shown in
Figure 79.



Figure 79 ldmatrix fragment layoutï


When .num = .x2, the elements of the second matrix are loaded in the next destination
register in each thread as per the layout in above table. Similarly, when .num = .x4,
elements of the third and fourth matrices are loaded in the subsequent destination registers in each
thread.
Optional qualifier .trans indicates that the matrix is loaded in column-major format.
The ldmatrix instruction is treated as a weak memory operation in the Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 6.5.
Support for ::cta sub-qualifier introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_75 or higher.
Examples

// Load a single 8x8 matrix using 64-bit addressing
.reg .b64 addr;
.reg .b32 d;
ldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr];

// Load two 8x8 matrices in column-major format
.reg .b64 addr;
.reg .b32 d<2>;
ldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr];

// Load four 8x8 matrices
.reg .b64 addr;
.reg .b32 d<4>;
ldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr];





9.7.13.4.16. Warp-level matrix store instruction: stmatrixï

stmatrix
Collectively store one or more matrices to shared memory.
Syntax

stmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r;

.shape  = {.m8n8};
.num    = {.x1, .x2, .x4};
.ss     = {.shared{::cta}};
.type   = {.b16};


Description
Collectively store one or more matrices across all threads in a warp to the location indicated by
the address operand p, in .shared state space. If no state space is provided, generic
addressing is used, such that the address in p points into .shared space. If the generic
address doesnât fall in .shared state space, then the behavior is undefined.
The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element
holds 16-bit data as indicated by the .type qualifier.
The values .x1, .x2 and .x4 for .num indicate one, two or four matrices
respectively.
The mandatory .sync qualifier indicates that stmatrix causes the executing thread to wait
until all threads in the warp execute the same stmatrix instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
stmatrix instruction. In conditionally executed code, an stmatrix instruction should only be
used if it is known that all threads in the warp evaluate the condition identically, otherwise the
behavior is undefined.
The behavior of stmatrix is undefined if all threads do not use the same qualifiers, or if any
thread in the warp has exited.
The source operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit
registers as per the value of .num. Each component of the vector expression holds a fragment
from the corresponding matrix.
Supported addressing modes for p are described in Addresses as Operands.
Consecutive instances of row need not be stored contiguously in memory. The eight addresses required
for each matrix are provided by eight threads, depending upon the value of .num as shown in the
following table. Each address corresponds to the start of a matrix row. Addresses addr0âaddr7
correspond to the rows of the first matrix, addresses addr8âaddr15 correspond to the rows of the
second matrix, and so on.










.num
Threads 0â7
Threads 8â15
Threads 16â23
Threads 24â31




.x1
addr0âaddr7
â
â
â


.x2
addr0âaddr7
addr8âaddr15
â
â


.x4
addr0âaddr7
addr8âaddr15
addr16âaddr23
addr24âaddr31



When storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. The matrix addresses
must be naturally aligned accordingly.
Each thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its
register r, and so on. A group of four threads stores an entire row of the matrix as shown in
Figure 80.



Figure 80 stmatrix fragment layoutï


When .num = .x2, the elements of the second matrix are storedd from the next source register
in each thread as per the layout in above table. Similarly, when .num = .x4, elements of the
third and fourth matrices are stored from the subsequent source registers in each thread.
Optional qualifier .trans indicates that the matrix is stored in column-major format.
The stmatrix instruction is treated as a weak memory operation in the Memory Consistency Model.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

// Store a single 8x8 matrix using 64-bit addressing
.reg .b64 addr;
.reg .b32 r;
stmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r};

// Store two 8x8 matrices in column-major format
.reg .b64 addr;
.reg .b32 r<2>;
stmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1};

// Store four 8x8 matrices
.reg .b64 addr;
.reg .b32 r<4>;
stmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3};





9.7.13.4.17. Warp-level matrix transpose instruction: movmatrixï

movmatrix
Transpose a matrix in registers across the warp.
Syntax

movmatrix.sync.aligned.shape.trans.type d, a;

.shape  = {.m8n8};
.type   = {.b16};


Description
Move a row-major matrix across all threads in a warp, reading elements from source a, and
writing the transposed elements to destination d.
The .shape qualifier indicates the dimensions of the matrix being transposed. Each matrix
element holds 16-bit data as indicated by the .type qualifier.
The mandatory .sync qualifier indicates that movmatrix causes the executing thread to wait
until all threads in the warp execute the same movmatrix instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
movmatrix instruction. In conditionally executed code, a movmatrix instruction should only
be used if it is known that all threads in the warp evaluate the condition identically, otherwise
the behavior is undefined.
Operands a and d are 32-bit registers containing fragments of the input matrix and the
resulting matrix respectively. The mandatory qualifier .trans indicates that the resulting
matrix in d is a transpose of the input matrix specified by a.
Each thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first
fragment in register a, and so on. A group of four threads holds an entire row of the input
matrix as shown in Figure 81.



Figure 81 movmatrix source matrix fragment layoutï


Each thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the
first fragment in register d, and so on. A group of four threads holds an entire column of the
result matrix as shown in Figure 82.



Figure 82 movmatrix result matrix fragment layoutï


PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_75 or higher.
Examples

.reg .b32 d, a;
movmatrix.sync.aligned.m8n8.trans.b16 d, a;






9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix Aï

This section describes warp-level mma.sp{::ordered_metadata} instruction with sparse matrix A.
This variant of the mma operation can be used when A is a structured sparse matrix with 50%
zeros in each row distributed in a shape-specific granularity. For an MxNxK sparse
mma.sp{::ordered_metadata} operation, the MxK matrix A is packed into MxK/2 elements.
For each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements
are packed in the operand representing matrix A. The mapping of these K/2 elements to the
corresponding K-wide row is provided explicitly as metadata.


9.7.13.5.1. Sparse matrix storageï

Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a
sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the
sub-chunk is shape-specific. For example, in a 16x16 matrix A, sparsity is expected to be at 2:4
granularity, i.e. each 4-element vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row
contains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata
operand. Values 0b0000, 0b0101, 0b1010, 0b1111 are invalid values for metadata and
will result in undefined behavior. In a group of four consecutive threads, one or more threads store
the metadata for the whole group depending upon the matrix shape. These threads are specified using
an additional sparsity selector operand.
Figure 83 shows an example of a 16x16 matrix A represented in sparse format and sparsity
selector indicating which thread in a group of four consecutive threads stores the metadata.



Figure 83 Sparse MMA storage exampleï


Granularities for different matrix shapes and data types are described below.
Sparse mma.sp{::ordered_metadata} with half-precision and .bf16 type
For the .m16n8k16 and .m16n8k32mma.sp{::ordered_metadata} operations, matrix A is
structured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements
in a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are
stored in the operand representing matrix A and their positions in the four-wide chunk in matrix
A are indicated by two 2-bit indices in the metadata operand. For mma.sp::ordered_metadata,
0b0100, 0b1000, 0b1001, 0b1100, 0b1101, 0b1110 are the meaningful values
of indices; any other values result in an undefined behavior.



Figure 84 Sparse MMA metadata example for .f16/.bf16 type.ï


The sparsity selector indicates the threads which contribute metadata as listed below:

m16n8k16: One thread within a group of four consecutive threads contributes the metadata for
the entire group. This thread is indicated by a value in {0, 1, 2, 3}.
m16n8k32: A thread-pair within a group of four consecutive threads contributes the sparsity
metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);
any other value results in an undefined behavior.

Sparse mma.sp{::ordered_metadata} with .tf32 type
When matrix A has .tf32 elements, matrix A is structured sparse at a granularity of 1:2. In
other words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero
element. Only the non-zero elements are stored in the operand for matrix A and their positions in a
two-wide chunk in matrix A are indicated by the 4-bit index in the metadata. 0b1110 and
0b0100 are the only meaningful index values; any other values result in an undefined behavior.



Figure 85 Sparse MMA metadata example for .tf32 type.ï


The sparsity selector indicates the threads which contribute metadata as listed below:

m16n8k8: One thread within a group of four consecutive threads contributes the metadata for
the entire group. This thread is indicated by a value in {0, 1, 2, 3}.
m16n8k16: A thread-pair within a group of four consecutive threads contributes the sparsity
metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3);
any other value results in an undefined behavior.

Sparse mma.sp{::ordered_metadata} with integer type
When matrices A and B have .u8/.s8 elements, matrix A is structured sparse at a granularity
of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes
and two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their
positions in the four-wide chunk are indicated by two 2-bit indices in the metadata. For
mma.sp::ordered_metadata, 0b0100, 0b1000, 0b1001, 0b1100, 0b1101, 0b1110
are the meaningful values of indices; any other values result in an undefined behavior.



Figure 86 Sparse MMA metadata example for .u8/.s8 type.ï


when matrices A and B have .u4/.s4 elements, matrix A is pair-wise structured sparse at a
granularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has
four zeroes and four non-zero values. Further, the zero and non-zero values are clustered in
sub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within
the eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored
in sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the
eight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. For
mma.sp::ordered_metadata, 0b0100, 0b1000, 0b1001, 0b1100, 0b1101, 0b1110
are the meaningful values of indices; any other values result in an undefined behavior.



Figure 87 Sparse MMA metadata example for .u4/.s4 type.ï


The sparsity selector indicates the threads which contribute metadata as listed below:

m16n8k32 with .u8/.s8 type and m16n8k64 with .u4/.s4 type: A thread-pair
within a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity
selector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an
undefined behavior.
m16n8k64 with .u8/.s8 type and m16n8k128 with .u4/.s4 type: All threads
within a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity
selector in this case must be 0. Any other value of sparsity selector results in an undefined
behavior.

Sparse mma.sp{::ordered_metadata} with .e4m3/.e5m2 type
When matrices A and B have .e4m3/.e5m2 elements, matrix A is structured sparse at a granularity
of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes and
two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their positions
in the four-wide chunk are indicated by two 2-bit indices in the metadata. 0b0100, 0b1000,
0b1001, 0b1100, 0b1101, 0b1110 are the meaningful values of indices; any other values
result in an undefined behavior.



Figure 88 Sparse MMA metadata example for .e4m3/.e5m2 type.ï


The sparsity selector indicates the threads which contribute metadata as listed below:

m16n8k64: All threads within a group of four consecutive threads contribute the sparsity metadata.
Hence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in
an undefined behavior.




9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix Aï

In this section we describe how the contents of thread registers are associated with fragments of
various matrices and the sparsity metadata. The following conventions are used throughout this
section:

For matrix A, only the layout of a fragment is described in terms of register vector sizes and
their association with the matrix data.
For matrix B, when the combination of matrix dimension and the supported data type is not already
covered in Matrix multiply-accumulate operation using mma instruction, a pictorial representation of matrix
fragments is provided.
For matrices C and D, since the matrix dimension - data type combination is the same for all
supported shapes, and is already covered in Matrix multiply-accumulate operation using mma
instruction, the pictorial representations
of matrix fragments are not included in this section.
For the metadata operand, pictorial representations of the association between indices of the
elements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present
in cell [x][y..z] indicates that bits m through n (with m being higher) in the
metadata operand of thread with %laneid=k contains the indices of the non-zero elements from
the chunk [x][y]..[x][z] of matrix A.



9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 typesï

A warp executing sparse mma.m16n8k16 with .f16 / .bf16 floating point type will compute
an MMA operation of shape .m16n8k16.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.f16 / .bf16
A vector expression containing two .b32 registers,
with each register containing two non-zero .f16 /
.bf16 elements out of 4 consecutive elements from
matrix A.
Mapping of the non-zero elements is as
described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 89.



Figure 89 Sparse MMA .m16n8k16 fragment layout for matrix A with .f16/.bf16 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0 and a1
           groupID + 8        for a2 and a3

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 4
lastcol  = firstcol + 3



Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix
Fragments for mma.m16n8k16 with floating point type for .f16/.b16 formats.

Metadata: A .b32 register containing 16 2-bit vectors each storing the index of a non-zero
element of a 4-wide chunk of matrix A as shown in Figure 90.





Figure 90 Sparse MMA .m16n8k16 metadata layout for .f16/.bf16 type.ï









9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 typesï

A warp executing sparse mma.m16n8k32 with .f16 / .bf16 floating point type will compute
an MMA operation of shape .m16n8k32.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.f16 / .bf16
A vector expression containing four .b32 registers,
with each register containing two non-zero .f16 /
.bf16 elements out of 4 consecutive elements from
matrix A.
Mapping of the non-zero elements is as
described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 91.



Figure 91 Sparse MMA .m16n8k32 fragment layout for matrix A with .f16/.bf16 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for ai where  0 <= i < 2 || 4 <= i < 6
           groupID + 8        Otherwise

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 4          For ai where i <  4
           (threadID_in_group * 4) + 16   for ai where i >= 4
lastcol  = firstcol + 3




Multiplicand B:








.atype
Fragment
Elements (low to high)




.f16 / .bf16
A vector expression containing four .b32 registers, each
containing two .f16 / .bf16 elements from matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown in Figure 92.



Figure 92 Sparse MMA .m16n8k32 fragment layout for matrix B with .f16/.bf16 type.ï



Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for
mma.m16n8k16 with floating point type
for .f16/.b16 formats.

Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing
the indices of two non-zero element from a 4-wide chunk of matrix A as shown in
Figure 93.





Figure 93 Sparse MMA .m16n8k32 metadata layout for .f16/.bf16 type.ï









9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point typeï

A warp executing sparse mma.m16n8k16 with .tf32 floating point type will compute an MMA
operation of shape .m16n8k16.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.tf32
A vector expression containing four .b32 registers, with each
register containing one non-zero .tf32 element out of 2
consecutive elements from matrix A.
Mapping of the non-zero elements is
as described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 94.



Figure 94 Sparse MMA .m16n8k16 fragment layout for matrix A with .tf32 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0 and a2
           groupID + 8        for a1 and a3

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 2          for a0 and a1
           (threadID_in_group * 2) + 8    for a2 and a3
lastcol  = firstcol + 1




Multiplicand B:








.atype
Fragment
Elements (low to high)




.tf32
A vector expression containing four .b32 registers, each
containing four .tf32 elements from matrix B.
b0, b1, b2, b3



The layout of the fragments held by different threads is shown in Figure 95.



Figure 95 Sparse MMA .m16n8k16 fragment layout for matrix B with .tf32 type.ï



Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for
mma.m16n8k16 with floating point type.

Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero
element of a 2-wide chunk of matrix A as shown in Figure 96.





Figure 96 Sparse MMA .m16n8k16 metadata layout for .tf32 type.ï









9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point typeï

A warp executing sparse mma.m16n8k8 with .tf32 floating point type will compute an MMA
operation of shape .m16n8k8.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.tf32
A vector expression containing two .b32 registers, each
containing one non-zero .tf32 element out of 2
consecutive elements from matrix A.
Mapping of the non-zero elements is
as described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 97.



Figure 97 Sparse MMA .m16n8k8 fragment layout for matrix A with .tf32 type.ï


The row and column of a matrix fragment can be computed as:

groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for a0
           groupID + 8        for a1

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 2
lastcol  = firstcol + 1



Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix
Fragments for mma.m16n8k8 for .tf32
format.

Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero
element of a 2-wide chunk of matrix A as shown in Figure 98.





Figure 98 Sparse MMA .m16n8k8 metadata layout for .tf32 type.ï









9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer typeï

A warp executing sparse mma.m16n8k32 with .u8 / .s8 integer type will compute an MMA
operation of shape .m16n8k32.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.u8 / .s8
A vector expression containing two .b32 registers, with each
register containing four non-zero .u8 / .s8 elements out
of 8 consecutive elements from matrix A.
Mapping of the non-zero elements is
as described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 99.



Figure 99 Sparse MMA .m16n8k32 fragment layout for matrix A with .u8/.s8 type.ï



groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for ai where  0 <= i < 4
           groupID + 8        Otherwise

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 8
lastcol  = firstcol + 7



Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix
Fragments for mma.m16n8k32.

Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing
the indices of two non-zero elements from a 4-wide chunk of matrix A as shown in
Figure 100.





Figure 100 Sparse MMA .m16n8k32 metadata layout for .u8/.s8 type.ï









9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 typeï

A warp executing sparse mma.m16n8k64 with .u8 / .s8/ .e4m3/ .e5m2 type will compute an MMA
operation of shape .m16n8k64.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.u8 / .s8
A vector expression containing four .b32 registers, with each
register containing four non-zero .u8 / .s8 elements out
of 8 consecutive elements from matrix A.
Mapping of the non-zero elements is
as described in Sparse matrix storage.


.e4m3 /
.e5m2
A vector expression containing four .b32 registers, with each
register containing four non-zero .e4m3 / .e5m2 elements
out of 8 consecutive elements from matrix A.



The layout of the fragments held by different threads is shown in Figure 101
and Figure 102.



Figure 101 Sparse MMA .m16n8k64 fragment layout for columns 0â31 of matrix A with .u8/.s8/.e4m3/.e5m2 type.ï





Figure 102 Sparse MMA .m16n8k64 fragment layout for columns 32â63 of matrix A with .u8/.s8/.e4m3/.e5m2 type.ï



groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for ai where  0 <= i < 4 || 8 <= i < 12
           groupID + 8        Otherwise

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 8           For ai where i <  8
           (threadID_in_group * 8) + 32    For ai where i >= 8
lastcol  = firstcol + 7




Multiplicand B:








.btype
Fragment
Elements (low to high)




.u8 / .s8
A vector expression containing four .b32 registers,
each containing four .u8 / .s8 elements from
matrix B.
b0, b1, b2, b3, â¦, b15


.e4m3 /
.e5m2
A vector expression containing four .b32 registers,
each containing four .e4m3 / .e5m2 elements from
matrix B.



The layout of the fragments held by different threads is shown in Figure 103,
Figure 104, Figure 105 and Figure 106.



Figure 103 Sparse MMA .m16n8k64 fragment layout for rows 0â15 of matrix B with .u8/.s8/.e4m3/.e5m2 type.ï





Figure 104 Sparse MMA .m16n8k64 fragment layout for rows 16â31 of matrix B with .u8/.s8/.e4m3/.e5m2 type.ï





Figure 105 Sparse MMA .m16n8k64 fragment layout for rows 32â47 of matrix B with .u8/.s8/.e4m3/.e5m2 type.ï





Figure 106 Sparse MMA .m16n8k64 fragment layout for rows 48â63 of matrix B with .u8/.s8/.e4m3/.e5m2 type.ï



Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for
mma.m16n8k16 with integer type.

Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing
the indices of two non-zero elements from a 4-wide chunk of matrix A as shown in
Figure 107 and Figure 108.





Figure 107 Sparse MMA .m16n8k64 metadata layout for columns 0â31 for .u8/.s8/.e4m3/.e5m2 type.ï





Figure 108 Sparse MMA .m16n8k64 metadata layout for columns 32â63 for .u8/.s8/.e4m3/.e5m2 type.ï









9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer typeï

A warp executing sparse mma.m16n8k64 with .u4 / .s4 integer type will compute an MMA
operation of shape .m16n8k64.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.u4 / .s4
A vector expression containing two .b32 registers, with each
register containing eight non-zero .u4 / .s4 elements
out of 16 consecutive elements from matrix A.
Mapping of the non-zero elements is
as described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 109.



Figure 109 Sparse MMA .m16n8k64 fragment layout for matrix A with .u4/.s4 type.ï



groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for ai where  0 <= i < 8
           groupID + 8        Otherwise

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 16
lastcol  = firstcol + 15



Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix
Fragments for mma.m16n8k64.

Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing
the indices of four non-zero elements from a 8-wide chunk of matrix A as shown in
Figure 110.





Figure 110 Sparse MMA .m16n8k64 metadata layout for  .u4/.s4 type.ï









9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer typeï

A warp executing sparse mma.m16n8k128 with .u4 / .s4 integer type will compute an MMA
operation of shape .m16n8k128.
Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds
a fragment of the matrix.


Multiplicand A:








.atype
Fragment
Elements




.u4 / .s4
A vector expression containing four .b32 registers, with each
register containing eight non-zero .u4 / .s4 elements out
of 16 consecutive elements from matrix A.
Mapping of the non-zero elements is
as described in Sparse matrix storage.



The layout of the fragments held by different threads is shown in Figure 111
and Figure 112.



Figure 111 Sparse MMA .m16n8k128 fragment layout for columns 0â63 of matrix A with .u4/.s4 type.ï





Figure 112 Sparse MMA .m16n8k128 fragment layout for columns 64â127 of matrix A with .u4/.s4 type.ï



groupID = %laneid >> 2
threadID_in_group = %laneid % 4

row =      groupID            for ai where  0 <= i < 8 || 16 <= i < 24
           groupID + 8        Otherwise

col = [firstcol ... lastcol]  // As per the mapping of non-zero elements
                              // as described in Sparse matrix storage

Where
firstcol = threadID_in_group * 16           For ai where i <  16
           (threadID_in_group * 16) + 64    For ai where i >= 16
lastcol  = firstcol + 15




Multiplicand B:








.atype
Fragment
Elements (low to high)




.u4 / .s4
A vector expression containing four .b32 registers, each containing
eight .u4 / .s4 elements from matrix B.
b0, b1, b2, b3, â¦, b31



The layout of the fragments held by different threads is shown in Figure 113,
Figure 114, Figure 115, Figure 116.



Figure 113 Sparse MMA .m16n8k128 fragment layout for rows 0â31 of matrix B with .u4/.s4 type.ï





Figure 114 Sparse MMA .m16n8k128 fragment layout for rows 31â63 of matrix B with .u4/.s4 type.ï





Figure 115 Sparse MMA .m16n8k128 fragment layout for rows 64â95 of matrix B with .u4/.s4 type.ï





Figure 116 Sparse MMA .m16n8k128 fragment layout for rows 96â127 of matrix B with .u4/.s4 type.ï



Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for
mma.m16n8k64.

Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing
the indices of four non-zero elements from a 8-wide chunk of matrix A as shown in
Figure 117 and Figure 118.





Figure 117 Sparse MMA .m16n8k128 metadata layout for  columns 0â63 for .u4/.s4 type.ï





Figure 118 Sparse MMA .m16n8k128 metadata layout for  columns 64â127 for .u4/.s4 type.ï










9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadataï

mma.sp/mma.sp::ordered_metadata
Perform matrix multiply-and-accumulate operation with sparse matrix A
Syntax
Half precision floating point type:

mma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;
mma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype  d, a, b, c, e, f;

.ctype     = {.f16, .f32};
.dtype     = {.f16, .f32};
.spvariant = {.sp, .sp::ordered_metadata};


Alternate floating point type :

mma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;
mma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32     d, a, b, c, e, f;
mma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32      d, a, b, c, e, f;
mma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32     d, a, b, c, e, f;
mma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f;

.f8type    = {.e4m3, .e5m2};
.spvariant = {.sp, .sp::ordered_metadata};


Integer type:

mma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;

.shape     = {.m16n8k32, .m16n8k64}
.atype     = {.u8, .s8};
.btype     = {.u8, .s8};
.spvariant = {.sp, .sp::ordered_metadata};

mma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f;

.shape     = {.m16n8k64, .m16n8k128}
.atype     = {.u4, .s4};
.btype     = {.u4, .s4};
.spvariant = {.sp, .sp::ordered_metadata};


Description
Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C, where the A matrix is
MxK, the B matrix is KxN, and the C and D matrices are MxN.
A warp executing mma.sp.sync/mma.sp::ordered_metadata.sync instruction compute a single matrix
mutliply and accumulate operation.
Operands a and b represent two multiplicand matrices A and B, while c and d
represent the accumulator and destination matrices, distributed across the threads in warp. Matrix A
is structured sparse as described in Sparse matrix storage Operands e and f represent sparsity
metadata and sparsity selector respectively. Operand e is a 32-bit integer and operand f is
a 32-bit integer constant with values in the range 0..3
Instruction mma.sp::ordered_metadata requires the indices in the sparsity metadata to be sorted
in an increasing order starting from LSB, otherwise behavior is undefined.
The registers in each thread hold a fragment of matrix as described in Matrix fragments for
multiply-accumulate operation with sparse matrix A.
The qualifiers .dtype, .atype, .btype and .ctype indicate the data-type of the
elements in the matrices D, A, B and C respectively. In case of shapes .m16n8k16 and
.m16n8k32, .dtype must be the same as .ctype

Precision and rounding :



.f16 floating point operations :
Element-wise multiplication of matrix A and B is performed with at least single
precision. When .ctype or .dtype is .f32, accumulation of the intermediate values
is performed with at least single precision. When both .ctype and .dtype are specified
as .f16, the accumulation is performed with at least half precision.
The accumulation order, rounding and handling of subnormal inputs are unspecified.


.e4m3 and .e5m2 floating point operations :
Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation
of the intermediate values is performed with at least single precision.
The accumulation order, rounding, and handling of subnormal inputs are unspecified.


.bf16 and .tf32 floating point operations :
Element-wise multiplication of matrix A and B is performed with specified
precision. Accumulation of the intermediate values is performed with at least single
precision.
The accumulation order, rounding, and handling of subnormal inputs are unspecified.


Integer operations :
The integer mma.sp/mma.sp::ordered_metadata operation is performed with .s32 accumulators.
The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range
MIN_INT32..MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit
integer and the maximum positive signed 32-bit integer respectively).
If .satfinite is not specified, the accumulated value is wrapped instead.




The mandatory .sync qualifier indicates that mma.sp/mma.sp::ordered_metadata instruction causes
the executing thread to wait until all threads in the warp execute the same mma.sp/mma.sp::ordered_metadata
instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warp must execute the same
mma.sp/mma.sp::ordered_metadata instruction. In conditionally executed code, a mma.sp/mma.sp::ordered_metadata
instruction should only be used if it is known that all threads in the warp evaluate the condition identically,
otherwise behavior is undefined.
The behavior of mma.sp/mma.sp::ordered_metadata instruction is undefined if all threads in the same warp
do not use the same qualifiers, or if any thread in the warp has exited.
Notes
mma.sp instruction may have substantially reduced performance on some target architectures.
Hence, it is advised to use mma.sp::ordered_metadata instruction.
PTX ISA Notes
Introduced in PTX ISA version 7.1.
Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in
PTX ISA version 8.4.
mma.sp::ordered_metadata introduced in PTX ISA version 8.5.
Target ISA Notes
Requires sm_80 or higher.
.e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher.
mma.sp::ordered_metadata requires sm_80 or higher.
Examples of half precision floating point type

// f16 elements in C and D matrix
.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>
.reg .b32 %Re;
mma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16
  {%Rd0, %Rd1},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1}, %Re, 0x1;

.reg .f16x2 %Ra<2> %Rb<2> %Rc<2> %Rd<2>
.reg .b32 %Re;

mma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16
  {%Rd0, %Rd1},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1}, %Re, 0x1;


Examples of alternate floating point type

.reg .b32 %Ra<2>, %Rb<2>;
.reg .f32 %Rc<4>, %Rd<4>;
.reg .b32 %Re;
mma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;

.reg .b32 %Ra<2>, %Rb<2>;
.reg .f32 %Rc<4>, %Rd<4>;
.reg .b32 %Re;
mma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;

.reg .b32 %Ra<4>, %Rb<4>;
.reg .f32 %Rc<4>, %Rd<4>;
.reg .b32 %Re;
mma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1, %Rb2, %Rb3},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;

.reg .b32 %Ra<4>, %Rb<4>;
.reg .f32 %Rc<4>, %Rd<4>;
.reg .b32 %Re;
mma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1, %Rb2, %Rb3},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0;

.reg .b32 %Ra<2>, %Rb<2>;
.reg .f32 %Rc<4>, %Rd<4>;
.reg .b32 %Re;
mma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;


Examples of integer type

.reg .b32 %Ra<4>, %Rb<4>, %Rc<4>, %Rd<4>;
.reg .u32 %Re;

// u8 elements in A and B matrix
mma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;

// s8 elements in A and B matrix
mma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1, %Rb2, %Rb3},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;

// s8 elements in A and B matrix with ordered metadata
mma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1, %Rb2, %Rb3},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;

// u4 elements in A and B matrix
mma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1},
  {%Rb0, %Rb1},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1;

// u4 elements in A and B matrix
mma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32
  {%Rd0, %Rd1, %Rd2, %Rd3},
  {%Ra0, %Ra1, %Ra2, %Ra3},
  {%Rb0, %Rb1, %Rb2, %Rb3},
  {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0;







9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructionsï

The warpgroup level matrix multiply and accumulate operation has either of the following forms,
where matrix D is called accumulator:

D = A * B + D
D = A * B, where the input from accumulator D is disabled.

The wgmma instructions perform warpgroup level matrix multiply-and-accumulate operation by
having all threads in a warpgroup collectively perform the following actions:

Load matrices A, B and D into registers or into shared memory.

Perform the following fence operations:

wgmma.fence operations to indicate that the register/shared-memory across the warpgroup
have been written into.
fence.proxy.async operation to make the generic proxy operations visible to the async
proxy.


Issue the asynchronous matrix multiply and accumulate operations using the wgmma.mma_async
operation on the input matrices. The wgmma.mma_async operation is performed in the async
proxy.
Create a wgmma-group and commit all the prior outstanding wgmma.mma_async operations into the
group, by using wgmma.commit_group operation.
Wait for the completion of the required wgmma-group.
Once the wgmma-group completes, all the wgmma.mma_async operations have been performed and
completed.



9.7.14.1. Warpgroupï

A warpgroup is a set of four contiguous warps such that the warp-rank of the first warp is a
multiple of 4.
warp-rank of a warp is defined as:

(%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32





9.7.14.2. Matrix Shapeï

The matrix multiply and accumulate operations support a limited set of shapes for the operand
matrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple
MxNxK, where A is an MxK matrix, B is a KxN matrix, while D is a MxN matrix.
The following matrix shapes are supported for the specified types for the wgmma.mma_async
operation:








Multiplicand Data type
Sparsity
Shape




Floating-point - .f16
Dense
.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16, .m64n40k16,
.m64n48k16, .m64n56k16, .m64n64k16, .m64n72k16, .m64n80k16,
.m64n88k16, .m64n96k16, .m64n104k16, .m64n112k16, .m64n120k16,
.m64n128k16, .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,
.m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16, .m64n200k16,
.m64n208k16, .m64n216k16, .m64n224k16, .m64n232k16, .m64n240k16,
.m64n248k16, .m64n256k16


Alternate floating-point
format - .bf16


Alternate floating-point
format - .tf32
Sparse


Alternate floating-point
format - .tf32
Dense
.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8, .m64n40k8,
.m64n48k8, .m64n56k8, .m64n64k8, .m64n72k8, .m64n80k8,
.m64n88k8, .m64n96k8, .m64n104k8, .m64n112k8, .m64n120k8,
.m64n128k8, .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,
.m64n168k8, .m64n176k8, .m64n184k8, .m64n192k8, .m64n200k8,
.m64n208k8, .m64n216k8, .m64n224k8, .m64n232k8, .m64n240k8,
.m64n248k8, .m64n256k8


Alternate floating-point
format - .e4m3/
.e5m2
Dense
.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32, .m64n40k32,
.m64n48k32, .m64n56k32, .m64n64k32, .m64n72k32, .m64n80k32,
.m64n88k32, .m64n96k32, .m64n104k32, .m64n112k32, .m64n120k32,
.m64n128k32, .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,
.m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32, .m64n200k32,
.m64n208k32, .m64n216k32, .m64n224k32, .m64n232k32, .m64n240k32,
.m64n248k32, .m64n256k32


Floating point - .f16
Sparse


Altername floating-point
format - .bf16


Integer - .u8/.s8
Dense
.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32, .m64n48k32,
.m64n64k32, .m64n80k32, .m64n96k32, .m64n112k32, .m64n128k32,
.m64n144k32, .m64n160k32, .m64n176k32, .m64n192k32, .m64n208k32,
.m64n224k32, .m64n240k32, .m64n256k32


Alternate floating-point
format - .e4m3/
.e5m2
Sparse
.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64, .m64n40k64,
.m64n48k64, .m64n56k64, .m64n64k64, .m64n72k64, .m64n80k64,
.m64n88k64, .m64n96k64, .m64n104k64, .m64n112k64, .m64n120k64,
.m64n128k64, .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,
.m64n168k64, .m64n176k64, .m64n184k64, .m64n192k64, .m64n200k64,
.m64n208k64, .m64n216k64, .m64n224k64, .m64n232k64, .m64n240k64,
.m64n248k64, .m64n256k64


Integer - .u8/.s8
Sparse
.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64, .m64n48k64,
.m64n64k64, .m64n80k64, .m64n96k64, .m64n112k64, .m64n128k64,
.m64n144k64, .m64n160k64, .m64n176k64, .m64n192k64, .m64n208k64,
.m64n224k64, .m64n240k64, .m64n256k64


Single-bit - .b1
Dense
.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256, .m64n48k256,
.m64n64k256, .m64n80k256, .m64n96k256, .m64n112k256,
.m64n128k256, .m64n144k256, .m64n160k256, .m64n176k256,
.m64n192k256, .m64n208k256, .m64n224k256, .m64n240k256,
.m64n256k256






9.7.14.3. Matrix Data-typesï

The matrix multiply and accumulate operation is supported separately on integer, floating-point,
sub-byte integer and single bit data-types. All operands must contain the same basic type kind,
i.e., integer or floating-point.
For floating-point matrix multiply and accumulate operation, different matrix operands may have
different precision, as described later.
For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have
elements of the same data-type, e.g. both signed integer or both unsigned integer.








Data-type
Multiplicands (A or B)
Accumulator (D)




Integer
both .u8 or both .s8
.s32


Floating Point
.f16
.f16,.f32


Alternate floating Point
.bf16
.f32


Alternate floating Point
.tf32
.f32


Alternate floating Point
.e4m3,.e5m2
.f16,.f32


Single-bit integer
.b1
.s32






9.7.14.4. Async Proxyï

The wgmma.mma_async operations are performed in the asynchronous proxy (or async proxy).
Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async
proxy, fence.proxy.async should be used to synchronize memory between generic proxy and the
async proxy.
The completion of a wgmma.mma_async operation is followed by an implicit generic-async proxy
fence. So the result of the asynchronous operation is made visible to the generic proxy as soon as
its completion is observed. wgmma.commit_group and wgmma.wait_group operations must be used
to wait for the completion of the wgmma.mma_async instructions.



9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instructionï

This section describes warpgroup level wgmma.mma_async instruction and the organization of
various matrices involved in this instruction.


9.7.14.5.1. Register Fragments and Shared Memory Matrix Layoutsï

The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared
memory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This
section describes the layouts of register fragments and shared memory expected by the warpgroup MMA
instructions.
When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes.


9.7.14.5.1.1. Register Fragmentsï

This section describes the organization of various matrices located in register operands of the
wgmma.mma_async instruction.

9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16ï
A warpgroup executing wgmma.mma_async.m64nNk16 will compute an MMA operation of shape
.m64nNk16 where N is a valid n dimension as listed in Matrix Shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.


Multiplicand A in registers:








.atype
Fragment
Elements (low to high)




.f16/.bf16
A vector expression containing four .f16x2 registers, with each
register containing two .f16/ .bf16 elements from matrix A.
a0, a1, a2, a3, a4, a5, a6, a7



The layout of the fragments held by different threads is shown in Figure 119.



Figure 119 WGMMA .m64nNk16 register fragment layout for matrix A.ï




Accumulator D:








.dtype
Fragment
Elements (low to high)




.f16
A vector expression containing N/4 number of .f16x2
registers, with each register containing two .f16
elements from matrix D.

d0, d1, d2, d3, â¦, dX, dY, dZ, dW
where
X = N/2Â  -Â  4
Y = N/2Â  -Â  3
Z = N/2Â  -Â  2
W = N/2Â  -Â  1
N = 8*i where i = {1, 2, ... , 32}



.f32
A vector expression containing N/2 number of .f32
registers.



The layout of the fragments held by different threads is shown in Figure 120.



Figure 120 WGMMA .m64nNk16 register fragment layout for accumulator matrix D.ï






9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8ï
A warpgroup executing wgmma.mma_async.m64nNk8 will compute an MMA operation of shape
.m64nNk8 where N is a valid n dimension as listed in Matrix Shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.


Multiplicand A in registers:








.atype
Fragment
Elements (low to high)




.tf32
A vector expression containing four .b32 registers containing
four .tf32 elements from matrix A.
a0, a1, a2, a3



The layout of the fragments held by different threads is shown in Figure 121.



Figure 121 WGMMA .m64nNk8 register fragment layout for matrix A.ï




Accumulator D:








.dtype
Fragment
Elements (low to high)




.f32
A vector expression containing N/2 number of .f32 registers.

d0, d1, d2, d3, â¦, dX, dY, dZ, dW
where
X = N/2Â  -Â  4
Y = N/2Â  -Â  3
Z = N/2Â  -Â  2
W = N/2Â  -Â  1
N = 8*i where i = {1, 2, ... , 32}




The layout of the fragments held by different threads is shown in Figure 122.



Figure 122 WGMMA .m64nNk8 register fragment layout for accumulator matrix D.ï






9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32ï
A warpgroup executing wgmma.mma_async.m64nNk32 will compute an MMA operation of shape
.m64nNk32 where N is a valid n dimension as listed in Matrix Shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.


Multiplicand A in registers:








.atype
Fragment
Elements (low to high)




.s8/.u8
A vector expression containing four .b32 registers, with each
register containing four .u8/ .s8 elements from matrix A.
a0, a1, a2, a3, â¦ , a14, a15


.e4m3/ .e5m2
A vector expression containing four .b32 registers, with each
register containing four .e4m3/ .e5m2 elements from
matrix A.



The layout of the fragments held by different threads is shown in Figure 123.



Figure 123 WGMMA .m64nNk32 register fragment layout for matrix A.ï




Accumulator D:









.dtype
Fragment
Elements (low to high)
Miscellaneous Information




.s32
A vector expression containing
N/2 number of .s32
registers.

d0, d1, d2, d3, â¦, dX, dY, dZ, dW
where
X = N/2Â  -Â  4
Y = N/2Â  -Â  3
Z = N/2Â  -Â  2
W = N/2Â  -Â  1
N depends on .dtype, as
described in the next column.


N = 8*i where i = {1, 2, 3, 4}


= 16*i where i = {3, 4, ..., 15, 16}





.f32
A vector expression containing
N/2 number of .f32
registers.
N = 8*i where i = {1, 2, ... , 32}


.f16
A vector expression containing
N/4 number of .f16x2
registers, with each register
containing two .f16
elements from matrix D.



The layout of the fragments held by different threads is shown in Figure 124.



Figure 124 WGMMA .m64nNk32 register fragment layout for accumulator matrix D.ï






9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256ï
A warpgroup executing wgmma.mma_async.m64nNk256 will compute an MMA operation of shape
.m64nNk256 where N is a valid n dimension as listed in Matrix Shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.


Multiplicand A in registers:








.atype
Fragment
Elements (low to high)




.b1
A vector expression containing four .b32 registers, with each
register containing thirty two .b1 element from matrix A.
a0, a1, a2, â¦, a127



The layout of the fragments held by different threads is shown in Figure 125.



Figure 125 WGMMA .m64nNk256 register fragment layout for matrix A.ï




Accumulator D:








.dtype
Fragment
Elements (low to high)




.s32
A vector expression containing N/2 number of .s32 registers.

d0, d1, d2, d3, â¦, dX, dY, dZ, dW
where
X = N/2Â  -Â  4
Y = N/2Â  -Â  3
Z = N/2Â  -Â  2
W = N/2Â  -Â  1
N = 8*i where i = {1, 2, 3, 4}
= 16*i where i = {3, 4, ..., 15, 16}




The layout of the fragments held by different threads is shown in Figure 126.



Figure 126 WGMMA .m64nNk256 register fragment layout for accumulator matrix D.ï








9.7.14.5.1.2. Shared Memory Matrix Layoutï

Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each
core matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy
contiguous space in shared memory.
Matrix A is made up of 8x2 core matrices and Matrix B is made up of 2x(N/8) core matrices. This
section describes the layout of the core matrices for each shape.

9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16ï
Core matrices of A and B are as follows:








Core matrix
Matrix description
Matrix size




A
Each row is made up of eight .f16/ .bf16 elements.
8x8


B
Each column is made up of eight .f16/ .bf16 elements.
8x8



Matrices A and B consist of core matrices as shown in Figure 127.
Each colored cell represents a core matrix.



Figure 127 WGMMA .m64nNk16 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 128. Each numbered
cell represents an individual element of the core matrix.



Figure 128 WGMMA .m64nNk16 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 129. Each numbered cell
represents an individual element of the core matrix.



Figure 129 WGMMA .m64nNk16 core matrix layout for Bï




9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8ï
Core matrices of A and B are as follows:








Core matrix
Matrix description
Matrix size




A
Each row is made up of four .tf32 elements.
8x4


B
Each row is made up of four .tf32 elements.
4x8



Matrices A and B consist of core matrices as shown in Figure 130. Each
colored cell represents a core matrix.



Figure 130 WGMMA .m64nNk8 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 131. Each numbered cell
represents an individual element of the core matrix.



Figure 131 WGMMA .m64nNk8 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 132. Each numbered cell
represents an individual element of the core matrix.



Figure 132 WGMMA .m64nNk8 core matrix layout for Bï




9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32ï
Core matrices of A and B are as follows:









.atype/ .btype
Core matrix
Matrix description
Matrix size




.s8/.u8
A
Each row is made up of sixteen .s8/ .u8 elements.
8x4


.e4m3/ .e5m2

Each row is made up of sixteen .e4m3/ .e5m2 elements.



.s8/.u8
B
Each column is made up of sixteen .s8/ .u8 elements.
4x8


.e4m3/ .e5m2

Each column is made up of sixteen .e4m3/ .e5m2 elements.




Matrices A and B consist of core matrices as shown in Figure 133. Each
colored cell represents a core matrix.



Figure 133 WGMMA .m64nNk32 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 134. Each numbered cell
represents an individual element of the core matrix.



Figure 134 WGMMA .m64nNk32 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 135. Each numbered cell
represents an individual element of the core matrix.



Figure 135 WGMMA .m64nNk32 core matrix layout for Bï




9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256ï
Core matrices of A and B are as follows:








Core matrix
Matrix description
Matrix size




A
Each row is made up of 256 .b1 elements.
8x128


B
Each column is made up of 256 .b1 elements.
128x8



Matrices A and B consist of core matrices as shown in Figure 136. Each
colored cell represents a core matrix.



Figure 136 WGMMA .m64nNk256 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 137. Each numbered cell
represents an individual element of the core matrix.



Figure 137 WGMMA .m64nNk256 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 138. Each numbered cell
represents an individual element of the core matrix.



Figure 138 WGMMA .m64nNk256 core matrix layout for Bï




9.7.14.5.1.2.5. Stridesï
Leading dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent
core matrices in the K dimension.
Stride dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core
matrices in the M or N dimension.
Figure 139 and Figure 140
show the leading dimension byte offset and the stride dimension byte offsets for A and B matrices.


Matrix A:



Figure 139 WGMMA stride and leading dimension byte offset for matrix Aï




Matrix B:



Figure 140 WGMMA stride and leading dimension byte offset for matrix Bï




Leading dimension byte offset and stride dimension byte offset must be specified in the matrix
descriptor as described in Matrix Descriptor Format.


9.7.14.5.1.2.6. Swizzling Modesï
The core matrices can be swizzled in the shared memory by specifying one of the following swizzling
modes:


No swizzling: All the elements of the entire core matrix are adjacent to each other and there is
no swizzling. Figure 141 illustrates this:



Figure 141 WGMMA core matrices with no swizzlingï




32-Byte swizzling: A group of two adjacent core matrices are swizzled as shown in
Figure 142. The
swizzling pattern repeats for the remaining core matrices.



Figure 142 WGMMA core matrices with 32-byte swizzlingï




64-Byte swizzling: A group of four adjacent core matrices are swizzled as shown in
Figure 143. The
swizzling pattern repeats for the remaining core matrices.



Figure 143 WGMMA core matrices with 64-byte swizzlingï




128-Byte swizzling: A group of eight adjacent core matrices are swizzled as shown in
Figure 144. The
swizzling pattern repeats for the remaining core matrices.



Figure 144 WGMMA core matrices with 128-byte swizzlingï






9.7.14.5.1.2.7. Matrix Descriptor Formatï
Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in
the matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the
following layout:








Bit-field
Size in bits
Description




13â0
14
matrix-descriptor-encode(Matrix start address)


29â16
14
matrix-descriptor-encode(Leading dimension byte offset)


45â32
14
matrix-descriptor-encode(Stride dimension byte offset)


51â49
3
Matrix base offset. This is valid for all swizzling modes except the no-swizzle mode.


63â62
2

Specifies the swizzling mode to be used:

0: No swizzle
1: 128-Byte swizzle
2: 64-Byte swizzle
3: 32-Byte swizzle





where

matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 0x4


The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as
per the below table:









Swizzling mode
Starting address of the repeating pattern




128-Byte swizzle
1024-Byte boundary


64-Byte swizzle
512-Byte boundary


32-Byte swizzle
256-Byte boundary





Otherwise, the base offset must be a non-zero value, computed using the following formula:

base offset = (pattern start addr >> 0x7) & 0x7







9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_asyncï

wgmma.mma_async
Perform matrix multiply-and-accumulate operation across warpgroup
Syntax
Half precision floating point type:

wgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b;

wgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;

.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,
            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,
            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,
            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,
            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,
            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,
            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,
            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};
.dtype   = {.f16, .f32};


Alternate floating point type :

.bf16 floating point type:

wgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b;

wgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;

.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,
            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,
            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,
            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,
            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,
            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,
            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,
            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};
.dtype  = {.f32};

.tf32 floating point type:

wgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b;

wgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b;

.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,
            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,
            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,
            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,
            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,
            .m64n168k8, .m648176k8, .m64n184k8, .m64n192k8,
            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,
            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};
.dtype  = {.f32};

FP8 floating point type

wgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b;

wgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imme-scale-b;

.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,
            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,
            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,
            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,
            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,
            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,
            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,
            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};
.atype  = {.e4m3, .e5m2};
.btype  = {.e4m3, .e5m2};
.dtype  = {.f16, .f32};


Integer type:

wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;

wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;

.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,
            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,
            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,
            .m648176k32, .m64n192k32, .m64n208k32, .m64n224k32};
.atype  = {.s8, .u8};
.btype  = {.s8, .u8};


Single bit:

wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;

wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;

.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,
            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,
            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,
            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,
            .m64n240k256, .m64n256k256};
.op  = {.and};


Description
Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D =
A*B+D, where the A matrix is MxK, the B matrix is KxN, and the D matrix is MxN.
The operation of the form D = A*B is issued when the input predicate argument scale-d is
false.
wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async
instruction from their prior accesses. Otherwise, the behavior is undefined.
wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion
of the asynchronous matrix multiply and accumulate operations before the results are accessed.
Register operand d represents the accumulator matrix as well as the destination matrix,
distributed across the participating threads. Register operand a represents the multiplicand
matrix A in register distributed across the participating threads. The 64-bit register operands
a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and
B in shared memory respectively. The contents of a matrix descriptor must be same across all the warps
in the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format.
Matrices A and B are stored in row-major and column-major format respectively. For certain floating
point variants, the input matrices A and B can be transposed by specifying the value 1 for the
immediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be
used to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0
and 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16/
.bf16 types on matrices accessed from shared memory using matrix descriptors.
For the floating point variants of the wgmma.mma_async operation, each element of the input
matrices A and B can be negated by specifying the value -1 for operands imm-scale-a and
imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid
values of imm-scale-a and imm-scale-b are -1 and 1.
The qualifiers .dtype, .atype and .btype indicate the data type of the elements in
matrices D, A and B respectively. .atype and .btype must be the same for all floating point
wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual
data elements of matrices A and B in alternate floating point variants of the wgmma.mma_async
operation are as follows:

Matrices A and B have 8-bit data elements when .atype/ .btype is .e4m3/.e5m2.
Matrices A and B have 16-bit data elements when .atype/ .btype is .bf16.
Matrices A and B have 32-bit data elements when .atype/ .btype is .tf32.

Precision and rounding:


Floating point operations:
Element-wise multiplication of matrix A and B is performed with at least single precision. When
.dtype is .f32, accumulation of the intermediate values is performed with at least single
precision. When .dtype is .f16, the accumulation is performed with at least half
precision.
The accumulation order, rounding and handling of subnormal inputs are unspecified.


.bf16 and .tf32 floating point operations:
Element-wise multiplication of matrix A and B is performed with specified
precision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of
the 32-bit input data before multiplication is issued. Accumulation of the intermediate values is
performed with at least single precision.
The accumulation order, rounding, and handling of subnormal inputs are unspecified.


Integer operations:
The integer wgmma.mma_async operation is performed with .s32 accumulators. The
.satfinite qualifier indicates that on overflow, the accumulated value is limited to the
range MIN_INT32..MAX_INT32 (where the bounds are defined as the minimum negative signed
32-bit integer and the maximum positive signed 32-bit integer respectively).
If .satfinite is not specified, the accumulated value is wrapped instead.


The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the
executing thread to wait until all threads in the warp execute the same wgmma.mma_async
instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the
same wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4.
Target ISA Notes
Requires sm_90a.
Examples of half precision floating point type

.reg .f16x2 f16a<40>, f16d<40>;
.reg .f32   f32d<40>;
.reg .b64   descA, descB;
.reg .pred  scaleD;
wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16
  {f32d0, f32d1, f32d2, f32d3},
  {f16a0, f16a1, f16a2, f16a3},
  descB,
  1, -1, -1, 1;

wgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16
  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,
   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},
  descA,
  descB,
  scaleD, -1, 1, 1, 0;


Examples of alternate floating point type

.reg .f32   f32d<40>;
.reg .b32   bf16a<40>
.reg .b64   descA, descB;

wgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16
  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,
   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,
   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,
   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,
   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,
   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},
  {bf16a0, bf16a1, bf16a2, bf16a3},
  descB,
  scaleD, -1, -1, 0;

.reg .f32   f32d<40>;
.reg .b64   descA, descB;

wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32
  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},
  descA,
  descB,
  0, -1, -1;

.reg .b32 f16d<8>, f16a<8>;
.reg .f32 f32d<8>;
.reg .b64   descA, descB;

wgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2
  {f16d0, f16d1},
  descA,
  descB,
  scaleD, -1, 1;

wgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3
  {f32d0, f32d1, f32d2, f32d3},
  {f16a0, f16a1, f16a2, f16a3},
  descB,
  1, -1, -1;


Examples of integer type

.reg .s32 s32d<8>, s32a<8>;
.reg .u32 u32a<8>;
.reg .pred scaleD;
.reg .b64   descA, descB;

wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite
  {s32d0, s32d1, s32d2, s32d3},
  {s32a0, s32a1, s32a2, s32a3},
  descB,
  1;

wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8
  {s32d0, s32d1, s32d2, s32d3},
  descA,
  descB,
  scaleD;

wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite
  {s32d0, s32d1, s32d2, s32d3},
  {s32a0, s32a1, s32a2, s32a3},
  descB,
  scaleD;

wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8
  {s32d0, s32d1, s32d2, s32d3},
  descA,
  descB,
  scaleD;


Examples of single bit type

.reg .s32 s32d<4>;
.reg .b32 b32a<4>;
.reg .pred scaleD;
.reg .b64   descA, descB;


wgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc
  {s32d0, s32d1, s32d2, s32d3},
  {b32a0, b32a1, b32a2, b32a3},
  descB,
  scaleD;






9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instructionï

This section describes warp-level wgmma.mma_async.sp instruction with sparse matrix A. This
variant of the wgmma.mma_async operation can be used when A is a structured sparse matrix with
50% zeros in each row distributed in a shape-specific granularity. For an MxNxK sparse
wgmma.mma_async.sp operation, the MxK matrix A is packed into MxK/2 elements. For each
K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements are
packed in the operand representing matrix A. The mapping of these K/2 elements to the
corresponding K-wide row is provided explicitly as metadata.


9.7.14.6.1. Sparse matrix storageï

Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a
sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the
sub-chunk is shape-specific. For example, in a 64x32 matrix A used in floating point
wgmma.mma_async operations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element
vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each
non-zero element in a sub-chunk is stored in the metadata operand. Values 0b0000, 0b0101,
0b1010, 0b1111 are invalid values for metadata and will result in undefined behavior. In a
group of four consecutive threads, one or more threads store the metadata for the whole group
depending upon the matrix shape. These threads are specified using an additional sparsity selector operand.
Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in
Figure 83, with an appropriate matrix size.
Granularities for different matrix shapes and data types are described below.
Sparse wgmma.mma_async.sp with half-precision and .bf16 type
For .f16 and .bf16 types, for all supported 64xNx32 shapes, matrix A is structured
sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of
matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in
matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices
in the metadata operand.



Figure 145 Sparse WGMMA metadata example for .f16/.bf16 type.ï


The sparsity selector indicates a thread-pair within a group of four consecutive threads which
contributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or
1 (threads T2, T3); any other value results in an undefined behavior.
Sparse wgmma.mma_async.sp with .tf32 type
For .tf32 type, for all supported 64xNx16 shapes, matrix A is structured sparse at a
granularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have
one zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and
the 4-bit index in the metadata indicates the position of the non-zero element in the two-wide
chunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in
an undefined behavior.



Figure 146 Sparse WGMMA metadata example for .tf32 type.ï


The sparsity selector indicates a thread-pair within a group of four consecutive threads which
contributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or
1 (threads T2, T3); any other value results in an undefined behavior.
Sparse wgmma.mma_async.sp with .e4m3 and .e5m2 floating point type
For .e4m3 and .e5m2 types, for all supported 64xNx64 shapes, matrix A is structured
sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of
matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in
matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices
in the metadata operand.



Figure 147 Sparse WGMMA metadata example for .e4m3/.e5m2 type.ï


All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value
results in an undefined behavior.
Sparse wgmma.mma_async.sp with integer type
For the integer type, for all supported 64xNx64 shapes, matrix A is structured sparse at a
granularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have
two zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two
2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide
chunk.



Figure 148 Sparse WGMMA metadata example for .u8/.s8 type.ï


All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value
results in an undefined behavior.



9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix Aï

In this section we describe how the contents of thread registers are associated with fragments of A
matrix and the sparsity metadata.
Each warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following
table shows the assignment of warps to rows of matrix A:







Warp
Sparsity information for rows of matrix A




%warpid % 4 = 3
48-63


%warpid % 4 = 2
32-47


%warpid % 4 = 1
16-31


%warpid % 4 = 0
0-15



The following conventions are used throughout this section:

For matrix A, only the layout of a fragment is described in terms of register vector sizes and
their association with the matrix data.
For matrix D, since the matrix dimension - data type combination is the same for all supported
shapes, and is already covered in Matrix multiply-accumulate operation using wgmma instruction, the pictorial
representations of matrix fragments are not included in this section.
For the metadata operand, pictorial representations of the association between indices of the
elements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present
in cell [x][y..z] indicates that bits m through n (with m being higher) in the
metadata operand of thread with %laneid=k contains the indices of the non-zero elements from
the chunk [x][y]..[x][z] of matrix A.



9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32ï

A warpgroup executing sparse wgmma.mma_async.m64nNk32 will compute an MMA operation of shape
.m64nNk32 where N is a valid n dimension as listed in Matrix shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

Multiplicand A, from shared memory is documented in Shared Memory Layout for
wgmma.mma_async.m64nNk32.

Multiplicand A, from registers:










.atype
Fragments
Elements







.f16 /
.bf16




A vector expression containing four .b32

registers, with each register containing two
non-zero .f16 /.bf16 elements out of 4
consecutive elements from matrix A.




Non-zero elements:
a0, a1, a2, a3, a4, a5, a6, a7


Mapping of the non-zero
elements is as described in
Sparse matrix storage





The layout of the fragments held by different threads is shown in Figure 149.



Figure 149 Sparse WGMMA .m64nNk32 fragment layout for matrix A with .f16/.bf16 type.ï






Accumulator D:
Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32
with floating point type
for the same .dtype format.


Multiplicand B:
Shared memory layout for Matrix B is documented in Shared Memory Layout for
wgmma.mma_async.m64nNk32.


Metadata operand is a .b32 register containing 16 2-bit vectors each storing the index of a
non-zero element of a 4-wide chunk of matrix A.
Figure 150 shows the mapping of the metadata bits to the elements
of matrix A for a warp. In this figure, variable i represents the value of the sparsity
selector operand.





Figure 150 Sparse WGMMA .m64nNk32 metadata layout for .f16/.bf16 type.ï









9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16ï

A warpgroup executing sparse wgmma.mma_async.m64nNk16 will compute an MMA operation of shape
.m64nNk16 where N is a valid n dimension as listed in Matrix shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

Multiplicand A, from shared memory is documented in Shared Memory Layout for
wgmma.mma_async.m64nNk16.

Multiplicand A, from registers:










.atype
Fragments
Elements






.tf32




A vector expression containing four .b32

registers, containing four non-zero .tf32

elements out of eight consecutive elements
from matrix A.




Non-zero elements:
a0, a1, a2, a3

Mapping of the non-zero
elements is as described in
Sparse matrix storage





The layout of the fragments held by different threads is shown in Figure 151.



Figure 151 Sparse WGMMA .m64nNk16 fragment layout for matrix A with .tf32 type.ï






Accumulator D:
Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk8
with floating point type
for the same .dtype format.


Multiplicand B:
Shared memory layout for Matrix B is documented in Shared Memory Layout for
wgmma.mma_async.m64nNk16.


Metadata operand is a .b32 register containing eight 4-bit vectors each storing the index of a
non-zero element of a 2-wide chunk of matrix A.
Figure 152 shows the mapping of the metadata bits to the elements
of matrix A for a warp. In this figure, variable i represents the value of the sparsity
selector operand.





Figure 152 Sparse WGMMA .m64nNk16 metadata layout for .tf32 type.ï









9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64ï

A warpgroup executing sparse wgmma.mma_async.m64nNk64 will compute an MMA operation of shape
.m64nNk64 where N is a valid n dimension as listed in Matrix shape.
Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

Multiplicand A, from shared memory is documented in Shared Memory Layout for
wgmma.mma_async.m64nNk64.

Multiplicand A, from registers:










.atype
Fragments
Elements







.e4m3 /
.e5m2




A vector expression containing four .b32

registers, with each register containing four
non-zero .e4m3 /.e5m2 elements out of
eight consecutive elements from matrix A.





Non-zero elements:
a0, a1, a2, â¦ , a15

Mapping of the non-zero
elements is as described in
Sparse matrix storage







.s8 /
.u8




A vector expression containing four .b32

registers, with each register containing four
non-zero .s8 /.u8 elements out of
eight consecutive elements from matrix A.





The layout of the fragments held by different threads is shown in Figure 153.



Figure 153 Sparse WGMMA .m64nNk64 fragment layout for matrix A with .e4m3/ .e5m2/ .s8/ .u8 type.ï






Accumulator D:
Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32
with floating point type
for the same .dtype format.


Multiplicand B:
Shared memory layout for Matrix B is documented in Shared Memory Layout for
wgmma.mma_async.m64nNk64.


Metadata operand is a .b32 register containing 16 4-bit vectors each storing the indices of
two non-zero elements of a 4-wide chunk of matrix A.
Figure 154 shows the mapping of the metadata
bits to the elements of columns 0â31 of matrix A.





Figure 154 Sparse WGMMA .m64nNk64 metadata layout for .e4m3/ .e5m2/ .s8/ .u8 type for columns 0â31ï




Figure 155 shows the mapping of the metadata
bits to the elements of columns 32â63 of matrix A.





Figure 155 Sparse WGMMA .m64nNk64 metadata layout for .e4m3/ .e5m2/ .s8/ .u8 type for columns 32â63ï










9.7.14.6.3. Shared Memory Matrix Layoutï

Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each
core matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy
contiguous space in shared memory.
Matrix A is made up of 8x2 packed core matrices and Matrix B is made up of 4x (N/8) core
matrices. This section describes the layout of the core matrices for each shape.


9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32ï

Core matrices of A and B are as follows:








Core matrix
Matrix Description
Matrix size




A
Each row is made up of sixteen .f16/ .bf16 elements,
with two non-zero elements out of four consecutive elements.
8x16


B
Each column is made up of eight .f16/ .bf16 elements.
8x8



Matrices A and B consist of core matrices as shown in Figure 156.
Each colored cell represents a core matrix.



Figure 156 Sparse WGMMA .m64nNk32 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 157.



Figure 157 Sparse WGMMA .m64nNk32 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 158.



Figure 158 Sparse WGMMA .m64nNk32 core matrix layout for Bï





9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16ï

Core matrices of A and B are as follows:








Core matrix
Matrix Description
Matrix size




A
Each row is made up of eight .tf32 elements
with a non-zero element out of two consecutive elements.
8x8


B
Each column is made up of four .tf32 elements.
4x8



Matrices A and B consist of core matrices as shown in Figure 159.
Each colored cell represents a core matrix.



Figure 159 Sparse WGMMA .m64nNk16 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 160.



Figure 160 Sparse WGMMA .m64nNk16 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 161.



Figure 161 Sparse WGMMA .m64nNk16 core matrix layout for Bï





9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64ï

Core matrices of A and B are as follows:








Core matrix
Matrix Description
Matrix size




A
Each row is made up of thirty-two .e4m3/ .e5m2 elements,
with two non-zero elements out of four consecutive elements.
8x32


B
Each column is made up of eight .f16/ .bf16 elements.
16x8



Matrices A and B consist of core matrices as shown in Figure 162.
Each colored cell represents a core matrix.



Figure 162 Sparse WGMMA .m64nNk64 core matrices for A and Bï


Layout of core matrices of A is shown in Figure 163.



Figure 163 Sparse WGMMA .m64nNk64 core matrix layout for Aï


Layout of core matrices of B is shown in Figure 164.



Figure 164 Sparse WGMMA .m64nNk64 core matrix layout for Bï






9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.spï

wgmma.mma_async.sp
Perform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup
Syntax
Half precision floating point type:

wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;

wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;

.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,
            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,
            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,
            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,
            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,
            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,
            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,
            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};
.dtype   = {.f16, .f32};


Alternate floating point type :

.bf16 floating point type:

wgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;

wgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imme-scale-b, imm-trans-b;

.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,
            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,
            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,
            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,
            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,
            .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32,
            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,
            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};
.dtype  = {.f32};

.tf32 floating point type:

wgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;

wgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;

.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,
            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,
            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,
            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,
            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,
            .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16,
            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,
            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};
.dtype  = {.f32};

FP8 floating point type

wgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;

wgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;

.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,
            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,
            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,
            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,
            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,
            .m64n168k64, .m648176k64, .m64n184k64, .m64n192k64,
            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,
            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};
.atype  = {.e4m3, .e5m2};
.btype  = {.e4m3, .e5m2};
.dtype  = {.f16, .f32};


Integer type:

wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;

wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;

.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,
            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,
            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,
            .m648176k64, .m64n192k64, .m64n208k64, .m64n224k64,
            .m64n240k64, .m64n256k64};
.atype  = {.s8, .u8};
.btype  = {.s8, .u8};


Description
Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D =
A*B+D, where the A matrix is MxK, the B matrix is KxN, and the D matrix is MxN.
The matrix A is stored in the packed format Mx(K/2) as described in Matrix multiply-accumulate
operation using wgmma.mma_async.sp instruction with sparse matrix A.
The operation of the form D = A*B is issued when the input predicate argument scale-d is
false.
wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async
instruction from their prior accesses. Otherwise, the behavior is undefined.
wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion
of the asynchronous matrix multiply and accumulate operations before the results are accessed.
Register operand d represents the accumulator matrix as well as the destination matrix,
distributed across the participating threads. Register operand a represents the multiplicand
matrix A in register distributed across the participating threads. The 64-bit register operands
a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and
B in shared memory respectively. The contents of a matrix descriptor must be same across all the
warps in the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format. Matrix A is
structured sparse as described in Sparse matrix storage.  Operands sp-meta and sp-sel
represent sparsity metadata and sparsity selector respectively. Operand sp-meta is a 32-bit
integer and operand sp-sel is a 32-bit integer constant with values in the range 0..3.
The valid values of sp-meta and sp-sel for each shape is specified in Matrix
multiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A and are summarized here :









Matrix shape
.atype
Valid values of sp-meta
Valid values of sp-sel




.m64nNk16
.tf32
0b1110 , 0b0100
0 (threads T0, T1) or 1 (threads T2, T3)


.m64nNk32
.f16/
.bf16
0b00, 0b01, 0b10, 0b11
0 (threads T0, T1) or 1 (threads T2, T3)


.m64nNk64
.e4m3 /
.e5m2 /
.s8  /
.u8
0b00, 0b01, 0b10, 0b11
0 (all threads contribute)



Matrices A and B are stored in row-major and column-major format respectively. For certain floating
point variants, the input matrices A and B can be transposed by specifying the value 1 for the
immediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be
used to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0
and 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16/
.bf16 types on matrices accessed from shared memory using matrix descriptors.
For the floating point variants of the wgmma.mma_async operation, each element of the input
matrices A and B can be negated by specifying the value -1 for operands imm-scale-a and
imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid
values of imm-scale-a and imm-scale-b are -1 and 1.
The qualifiers .dtype, .atype and .btype indicate the data type of the elements in
matrices D, A and B respectively. .atype and .btype must be the same for all floating point
wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual
data elements of matrices A and B in alternate floating point variants of the wgmma.mma_async
operation are as follows:

Matrices A and B have 8-bit data elements when .atype/ .btype is .e4m3/.e5m2.
Matrices A and B have 16-bit data elements when .atype/ .btype is .bf16.
Matrices A and B have 32-bit data elements when .atype/ .btype is .tf32.

Precision and rounding:


Floating point operations:
Element-wise multiplication of matrix A and B is performed with at least single precision. When
.dtype is .f32, accumulation of the intermediate values is performed with at least single
precision. When .dtype is .f16, the accumulation is performed with at least half
precision.
The accumulation order, rounding and handling of subnormal inputs are unspecified.


.bf16 and .tf32 floating point operations:
Element-wise multiplication of matrix A and B is performed with specified
precision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of
the 32-bit input data before multiplication is issued. Accumulation of the intermediate values is
performed with at least single precision.
The accumulation order, rounding, and handling of subnormal inputs are unspecified.


Integer operations:
The integer wgmma.mma_async operation is performed with .s32 accumulators. The
.satfinite qualifier indicates that on overflow, the accumulated value is limited to the
range MIN_INT32..MAX_INT32 (where the bounds are defined as the minimum negative signed
32-bit integer and the maximum positive signed 32-bit integer respectively).
If .satfinite is not specified, the accumulated value is wrapped instead.


The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the
executing thread to wait until all threads in the warp execute the same wgmma.mma_async
instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the
same wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.2.
Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4.
Target ISA Notes
Requires sm_90a.
Examples of integer type

wgmma.fence.sync.aligned;
wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},
                                                    descA, descB, spMeta, 0, scaleD;
wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},
                                                    descA, descB, spMeta, 0, scaleD;
wgmma.commit_group.sync.aligned;
wgmma.wait_group.sync.aligned 0;






9.7.14.7. Asynchronous wgmma Proxy Operationsï

This section describes warpgroup level wgmma.fence, wgmma.commit_group and wgmma.wait_group instructions.


9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fenceï

wgmma.fence
Enforce an ordering of register accesses between wgmma.mma_async and other operations.
Syntax

wgmma.fence.sync.aligned;


Description
wgmma.fence instruction establishes an ordering between prior accesses to any warpgroup
registers and subsequent accesses to the same registers by a wgmma.mma_async instruction. Only
the accumulator register and the input registers containing the fragments of matrix A require this
ordering.
The wgmma.fence instruction must be issued by all warps of the warpgroup at the following
locations:

Before the first wgmma.mma_async operation in a warpgroup.
Between a register access by a thread in the warpgroup and any wgmma.mma_async instruction
that accesses the same registers, either as accumulator or input register containing fragments of
matrix A, except when these are accumulator register accesses across multiple wgmma.mma_async
instructions of the same shape. In the latter case, an ordering guarantee is provided by default.

Otherwise, the behavior is undefined.
An async proxy fence must be used to establish an ordering between prior writes to shared memory
matrices and subsequent reads of the same matrices in a wgmma.mma_async instruction.
The mandatory .sync qualifier indicates that wgmma.fence instruction causes the executing
thread to wait until all threads in the warp execute the same wgmma.fence instruction before
resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the
same wgmma.fence instruction. In conditionally executed code, an wgmma.fence instruction
should only be used if it is known that all threads in the warpgroup evaluate the condition
identically, otherwise the behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90a.
Examples

// Example 1, first use example:
wgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>
wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},
                                                  descA, descB, scaleD;
wgmma.commit_group.sync.aligned;
wgmma.wait_group.sync.aligned 0;

// Example 2, use-case with the input value updated in between:
wgmma.fence.sync.aligned;
wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},
                                                  descA, descB, scaleD;
...
mov.b32 s32d0, new_val;
wgmma.fence.sync.aligned;
wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},
                                                 {s32d0, s32d1, s32d2, s32d3},
                                                  descB, scaleD;
wgmma.commit_group.sync.aligned;
wgmma.wait_group.sync.aligned 0;





9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_groupï

wgmma.commit_group
Commits all prior uncommitted wgmma.mma_async operations into a wgmma-group.
Syntax

wgmma.commit_group.sync.aligned;


Description
wgmma.commit_group instruction creates a new wgmma-group per warpgroup and batches all prior
wgmma.mma_async instructions initiated by the executing warp but not committed to any
wgmma-group into the new wgmma-group. If there are no uncommitted wgmma.mma_async instructions
then wgmma.commit_group results in an empty wgmma-group.
An executing thread can wait for the completion of all wgmma.mma_async operations in a
wgmma-group by using wgmma.wait_group.
The mandatory .sync qualifier indicates that wgmma.commit_group instruction causes the
executing thread to wait until all threads in the warp execute the same wgmma.commit_group
instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the
same wgmma.commit_group instruction. In conditionally executed code, an wgmma.commit_group
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise the behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90a.
Examples

wgmma.commit_group.sync.aligned;





9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_groupï

wgmma.wait_group
Signal the completion of a preceding warpgroup operation.
Syntax

wgmma.wait_group.sync.aligned N;


Description
wgmma.wait_group instruction will cause the executing thread to wait until only N or fewer of
the most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing
threads are complete. For example, when N is 0, the executing thread waits on all the prior
wgmma-groups to complete. Operand N is an integer constant.
Accessing the accumulator register or the input register containing the fragments of matrix A of a
wgmma.mma_async instruction without first performing a wgmma.wait_group instruction that
waits on a wgmma-group including that wgmma.mma_async instruction is undefined behavior.
The mandatory .sync qualifier indicates that wgmma.wait_group instruction causes the
executing thread to wait until all threads in the warp execute the same wgmma.wait_group
instruction before resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the
same wgmma.wait_group instruction. In conditionally executed code, an wgmma.wait_group
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise the behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90a.
Examples

wgmma.fence.sync.aligned;

wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},
                                                  descA, descB, scaleD;
wgmma.commit_group.sync.aligned;

wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},
                                                  {f16a0, f16a1, f16a2, f16a3},
                                                   descB, 1, -1, -1, 1;
wgmma.commit_group.sync.aligned;

wgmma.wait_group.sync.aligned 0;







9.7.15. Stack Manipulation Instructionsï

The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the
stack frame of the current function.
The stack manipulation instrucitons are:

stacksave
stackrestore
alloca



9.7.15.1. Stack Manipulation Instructions: stacksaveï

stacksave
Save the value of stack pointer into a register.
Syntax

stacksave.type  d;

.type = { .u32, .u64 };


Description
Copies the current value of stack pointer into the destination register d. Pointer returned by
stacksave can be used in a subsequent stackrestore instruction to restore the stack
pointer. If d is modified prior to use in stackrestore instruction, it may corrupt data in
the stack.
Destination operand d has the same type as the instruction type.
Semantics

d = stackptr;


PTX ISA Notes
Introduced in PTX ISA version 7.3.

Preview Feature:

stacksave is a preview feature in PTX ISA version 7.3. All details are subject to change with
no guarantees of backward compatibility on future PTX ISA versions or SM architectures.


Target ISA Notes
stacksave requires sm_52 or higher.
Examples

.reg .u32 rd;
stacksave.u32 rd;

.reg .u64 rd1;
stacksave.u64 rd1;





9.7.15.2. Stack Manipulation Instructions: stackrestoreï

stackrestore
Update the stack pointer with a new value.
Syntax

stackrestore.type  a;

.type = { .u32, .u64 };


Description
Sets the current stack pointer to source register a.
When stackrestore is used with operand a written by a prior stacksave instruction, it
will effectively restore the state of stack as it was before stacksave was executed. Note that
if stackrestore is used with an arbitrary value of a, it may cause corruption of stack
pointer. This implies that the correct use of this feature requires that stackrestore.type a is
used after stacksave.type a without redefining the value of a between them.
Operand a has the same type as the instruction type.
Semantics

stackptr = a;


PTX ISA Notes
Introduced in PTX ISA version 7.3.

Preview Feature:

stackrestore is a preview feature in PTX ISA version 7.3. All details are subject to change
with no guarantees of backward compatibility on future PTX ISA versions or SM architectures.


Target ISA Notes
stackrestore requires sm_52 or higher.
Examples

.reg .u32 ra;
stacksave.u32 ra;
// Code that may modify stack pointer
...
stackrestore.u32 ra;





9.7.15.3. Stack Manipulation Instructions: allocaï

alloca
Dynamically allocate memory on stack.
Syntax

alloca.type  ptr, size{, immAlign};

.type = { .u32, .u64 };


Description
The alloca instruction dynamically allocates memory on the stack frame of the current function
and updates the stack pointer accordingly. The returned pointer ptr points to local memory and
can be used in the address operand of ld.local and st.local instructions.
If sufficient memory is unavailable for allocation on the stack, then execution of alloca may
result in stack overflow. In such cases, attempting to access the allocated memory with ptr will
result in undefined program behavior.
The memory allocated by alloca is deallocated in the following ways:

It is automatically deallocated when the function exits.
It can be explicitly deallocated using stacksave and stackrestore instructions:
stacksave can be used to save the value of stack pointer before executing alloca, and
stackrestore can be used after alloca to restore stack pointer to the original value which
was previously saved with stacksave. Note that accessing deallocated memory after executing
stackrestore results in undefined behavior.

size is an unsigned value which specifies the amount of memory in number of bytes to be
allocated on stack. size = 0 may not lead to a valid memory allocation.
Both ptr and size have the same type as the instruction type.
immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the
memory allocated by alloca. It is an integer constant, must be a power of 2 and must not exceed
2^23. immAlign is an optional argument with default value being 8 which is the minimum
guaranteed alignment.
Semantics

alloca.type ptr, size, immAlign:

a = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment

// Allocate size bytes of stack memory with alignment a and update the stack pointer.
// Since the stack grows down, the updated stack pointer contains a lower address.
stackptr = alloc_stack_mem(size, a);

// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory
// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).
stacksave ptr;


PTX ISA Notes
Introduced in PTX ISA version 7.3.

Preview Feature:

alloca is a preview feature in PTX ISA version 7.3. All details are subject to change with no
guarantees of backward compatibility on future PTX ISA versions or SM architectures.


Target ISA Notes
alloca requires sm_52 or higher.
Examples

.reg .u32 ra, stackptr, ptr, size;

stacksave.u32 stackptr;     // Save the current stack pointer
alloca ptr, size, 8;        // Allocate stack memory
st.local.u32 [ptr], ra;     // Use the allocated stack memory
stackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer






9.7.16. Video Instructionsï

All video instructions operate on 32-bit register operands. However, the video instructions may be
classified as either scalar or SIMD based on whether their core operation applies to one or multiple
values.
The video instructions are:

vadd, vadd2, vadd4
vsub, vsub2, vsub4
vmad
vavrg2, vavrg4
vabsdiff, vabsdiff2, vabsdiff4
vmin, vmin2, vmin4
vmax, vmax2, vmax4
vshl
vshr
vset, vset2, vset4



9.7.16.1. Scalar Video Instructionsï

All scalar video instructions operate on 32-bit register operands. The scalar video instructions
are:

vadd
vsub
vabsdiff
vmin
vmax
vshl
vshr
vmad
vset

The scalar video instructions execute the following stages:

Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to
produce signed 33-bit input values.
Perform a scalar arithmetic operation to produce a signed 34-bit result.
Optionally clamp the result to the range of the destination type.

Optionally perform one of the following:

apply a second operation to the intermediate result and a third operand, or
truncate the intermediate result to a byte or half-word value and merge into a specified
position in the third operand to produce the final result.



The general format of scalar video instructions is as follows:

// 32-bit scalar operation, with optional secondary operation
vop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};
vop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;


.dtype = .atype = .btype = { .u32, .s32 };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.secop = { .add, .min, .max };


The source and destination operands are all 32-bit registers. The type of each operand (.u32 or
.s32) is specified in the instruction type; all combinations of dtype, atype, and
btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are
extracted and sign- or zero-extended internally to .s33 values. The primary operation is then
performed to produce an .s34 intermediate result. The sign of the intermediate result depends on
dtype.
The intermediate result is optionally clamped to the range of the destination type (signed or
unsigned), taking into account the subword destination size in the case of optional data merging.

.s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {
    if ( !sat )  return tmp;

    switch ( dsel ) {
        case .b0, .b1, .b2, .b3:
            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );
            else         return CLAMP( tmp, U8_MAX, U8_MIN );
        case .h0, .h1:
            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );
            else         return CLAMP( tmp, U16_MAX, U16_MIN );
        default:
            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );
            else         return CLAMP( tmp, U32_MAX, U32_MIN );
    }
}


This intermediate result is then optionally combined with the third source operand using a secondary
arithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the
third operand is based on dtype.

.s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {
    switch ( secop ) {
        .add:     return tmp + c;
        .min:     return MIN(tmp, c);
        .max      return MAX(tmp, c);
        default:  return tmp;
    }
}



.s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {
    switch ( dsel ) {
        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);
        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);
        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);
        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);
        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);
        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);
        default:   return tmp;
    }
}


The lower 32-bits are then written to the destination operand.


9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmaxï

vadd, vsub
Integer byte/half-word/word addition/subtraction.
vabsdiff
Integer byte/half-word/word absolute value of difference.
vmin, vmax
Integer byte/half-word/word minimum/maximum.
Syntax

// 32-bit scalar operation, with optional secondary operation
vop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};
vop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;

 vop   = { vadd, vsub, vabsdiff, vmin, vmax };
.dtype = .atype = .btype = { .u32, .s32 };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.op2   = { .add, .min, .max };


Description
Perform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge.
Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a, atype, asel );
tb = partSelectSignExtend( b, btype, bsel );

switch ( vop ) {
    case vadd:     tmp = ta + tb;
    case vsub:     tmp = ta - tb;
    case vabsdiff: tmp = | ta - tb |;
    case vmin:     tmp = MIN( ta, tb );
    case vmax:     tmp = MAX( ta, tb );
}
// saturate, taking into account destination type and merge operations
tmp = optSaturate( tmp, sat, isSigned(dtype), dsel );
d = optSecondaryOp( op2, tmp, c );  // optional secondary operation
d = optMerge( dsel, tmp, c );       // optional merge with c operand


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
vadd, vsub, vabsdiff, vmin, vmax require sm_20 or higher.
Examples

vadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;
vsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;
vabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;
vmin.s32.s32.s32.sat.add  r1, r2, r3, c;





9.7.16.1.2. Scalar Video Instructions: vshl, vshrï

vshl, vshr
Integer byte/half-word/word left/right shift.
Syntax

// 32-bit scalar operation, with optional secondary operation
vop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};
vop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;

 vop   = { vshl, vshr };
.dtype = .atype = { .u32, .s32 };
.mode  = { .clamp, .wrap };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.op2   = { .add, .min, .max };


Description

vshl

Shift a left by unsigned amount in b with optional saturate, and optional secondary
arithmetic operation or subword data merge. Left shift fills with zero.

vshr

Shift a right by unsigned amount in b with optional saturate, and optional secondary
arithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift
fills with zero.


Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a,atype, asel );
tb = partSelectSignExtend( b, .u32, bsel );
if ( mode == .clamp  && tb > 32 )  tb = 32;
if ( mode == .wrap )                       tb = tb & 0x1f;
switch ( vop ){
   case vshl:  tmp = ta << tb;
   case vshr:  tmp = ta >> tb;
}
// saturate, taking into account destination type and merge operations
tmp = optSaturate( tmp, sat, isSigned(dtype), dsel );
d = optSecondaryOp( op2, tmp, c );  // optional secondary operation
d = optMerge( dsel, tmp, c );       // optional merge with c operand


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
vshl, vshr require sm_20 or higher.
Examples

vshl.s32.u32.u32.clamp  r1, r2, r3;
vshr.u32.u32.u32.wrap   r1, r2, r3.h1;





9.7.16.1.3. Scalar Video Instructions: vmadï

vmad
Integer byte/half-word/word multiply-accumulate.
Syntax

// 32-bit scalar operation
vmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},
                                         {-}c;
vmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;

.dtype = .atype = .btype = { .u32, .s32 };
.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.scale = { .shr7, .shr15 };


Description
Calculate (a*b) + c, with optional operand negates, plus one mode, and scaling.
The source operands support optional negation with some restrictions. Although PTX syntax allows
separate negation of the a and b operands, internally this is represented as negation of the
product (a*b). That is, (a*b) is negated if and only if exactly one of a or b is
negated. PTX allows negation of either (a*b) or c.
The plus one mode (.po) computes (a*b) + c + 1, which is used in computing averages. Source
operands may not be negated in .po mode.
The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product
(a*b) is not negated; otherwise, the intermediate result is signed. Input c has the same
sign as the intermediate result.
The final result is unsigned if the intermediate result is unsigned and c is not negated.
Depending on the sign of the a and b operands, and the operand negates, the following
combinations of operands are supported for VMAD:

 (u32 * u32) + u32  // intermediate unsigned; final unsigned
-(u32 * u32) + s32  // intermediate   signed; final   signed
 (u32 * u32) - u32  // intermediate unsigned; final   signed
 (u32 * s32) + s32  // intermediate   signed; final   signed
-(u32 * s32) + s32  // intermediate   signed; final   signed
 (u32 * s32) - s32  // intermediate   signed; final   signed
 (s32 * u32) + s32  // intermediate   signed; final   signed
-(s32 * u32) + s32  // intermediate   signed; final   signed
 (s32 * u32) - s32  // intermediate   signed; final   signed
 (s32 * s32) + s32  // intermediate   signed; final   signed
-(s32 * s32) + s32  // intermediate   signed; final   signed
 (s32 * s32) - s32  // intermediate   signed; final   signed


The intermediate result is optionally scaled via right-shift; this result is sign-extended if the
final result is signed, and zero-extended otherwise.
The final result is optionally saturated to the appropriate 32-bit range based on the type (signed
or unsigned) of the final result.
Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a, atype, asel );
tb = partSelectSignExtend( b, btype, bsel );
signedFinal = isSigned(atype) || isSigned(btype) ||
                                 (a.negate ^ b.negate) || c.negate;
tmp[127:0] = ta * tb;

lsb = 0;
if ( .po )                  {              lsb = 1; } else
if ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else
if ( c.negate )             { c   = ~c;    lsb = 1; }

c128[127:0] = (signedFinal) sext32( c ) : zext ( c );
tmp = tmp + c128 + lsb;
switch( scale ) {
   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;
   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;
}
if ( .sat ) {
     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);
     else             result = CLAMP(result, U32_MAX, U32_MIN);
}


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
vmad requires sm_20 or higher.
Examples

vmad.s32.s32.u32.sat    r0, r1, r2, -r3;
vmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3;





9.7.16.1.4. Scalar Video Instructions: vsetï

vset
Integer byte/half-word/word comparison.
Syntax

// 32-bit scalar operation, with optional secondary operation
vset.atype.btype.cmp       d, a{.asel}, b{.bsel};
vset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;

.atype = .btype = { .u32, .s32 };
.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.op2   = { .add, .min, .max };


Description
Compare input values using specified comparison, with optional secondary arithmetic operation or
subword data merge.
The intermediate result of the comparison is always unsigned, and therefore destination d and
operand c are also unsigned.
Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a, atype, asel );
tb = partSelectSignExtend( b, btype, bsel );
tmp = compare( ta, tb, cmp ) ? 1 : 0;
d = optSecondaryOp( op2, tmp, c );    // optional secondary operation
d = optMerge( dsel, tmp, c );         // optional merge with c operand


PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
vset requires sm_20 or higher.
Examples

vset.s32.u32.lt    r1, r2, r3;
vset.u32.u32.ne    r1, r2, r3.h1;






9.7.16.2. SIMD Video Instructionsï

The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values.
The SIMD video instructions are:

vadd2, vadd4
vsub2, vsub4
vavrg2, vavrg4
vabsdiff2, vabsdiff4
vmin2, vmin4
vmax2, vmax4
vset2, vset4

PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit
values. The SIMD video instructions execute the following stages:

Form input vectors by extracting and sign- or zero-extending byte or half-word values from the
source operands, to form pairs of signed 17-bit values.
Perform a SIMD arithmetic operation on the input pairs.
Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the
destination type.

Optionally perform one of the following:

perform a second SIMD merge operation, or
apply a scalar accumulate operation to reduce the intermediate SIMD results to a single
scalar.



The general format of dual half-word SIMD video instructions is as follows:

// 2-way SIMD operation, with second SIMD merge or accumulate
vop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;

.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .h0, .h1, .h10 };
.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } };


The general format of quad byte SIMD video instructions is as follows:

// 4-way SIMD operation, with second SIMD merge or accumulate
vop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;

.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .b0,
           .b1, .b10
           .b2, .b20, .b21, .b210,
           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };
.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };


The source and destination operands are all 32-bit registers. The type of each operand (.u32 or
.s32) is specified in the instruction type; all combinations of dtype, atype, and
btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are
extracted and sign- or zero-extended internally to .s33 values. The primary operation is then
performed to produce an .s34 intermediate result. The sign of the intermediate result depends on
dtype.
The intermediate result is optionally clamped to the range of the destination type (signed or
unsigned), taking into account the subword destination size in the case of optional data merging.


9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2ï

vadd2, vsub2
Integer dual half-word SIMD addition/subtraction.
vavrg2
Integer dual half-word SIMD average.
vabsdiff2
Integer dual half-word SIMD absolute value of difference.
vmin2, vmax2
Integer dual half-word SIMD minimum/maximum.
Syntax

// SIMD instruction with secondary SIMD merge operation
vop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;

 vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };
.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .h0, .h1, .h10 };  // defaults to .h10
.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };
   .asel defaults to .h10
   .bsel defaults to .h32


Description
Two-way SIMD parallel arithmetic operation with secondary operation.
Elements of each dual half-word source to the operation are selected from any of the four half-words
in the two source operands a and b using the asel and bsel modifiers.
The selected half-words are then operated on in parallel.
The results are optionally clamped to the appropriate range determined by the destination type
(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.
For instructions with a secondary SIMD merge operation:

For half-word positions indicated in mask, the selected half-word results are copied into
destination d. For all other positions, the corresponding half-word from source operand c
is copied to d.

For instructions with a secondary accumulate operation:

For half-word positions indicated in mask, the selected half-word results are added to operand
c, producing a result in d.

Semantics

// extract pairs of half-words and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_2( a, b, .asel, .atype );
Vb = extractAndSignExt_2( a, b, .bsel, .btype );
Vc = extractAndSignExt_2( c );

for (i=0; i<2; i++) {
    switch ( vop2 ) {
       case vadd2:             t[i] = Va[i] + Vb[i];
       case vsub2:             t[i] = Va[i] - Vb[i];
       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {
                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;
                               } else {
                                   t[i] = ( Va[i] + Vb[i] ) >> 1;
                               }
       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;
       case vmin2:             t[i] = MIN( Va[i], Vb[i] );
       case vmax2:             t[i] = MAX( Va[i], Vb[i] );
    }
    if (.sat) {
        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );
        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );
    }
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}


PTX ISA Notes
Introduced in PTX ISA version 3.0.
Target ISA Notes
vadd2, vsub2, varvg2, vabsdiff2, vmin2, vmax2 require sm_30 or higher.
Examples

vadd2.s32.s32.u32.sat  r1, r2, r3, r1;
vsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;
vmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1;





9.7.16.2.2. SIMD Video Instructions: vset2ï

vset2
Integer dual half-word SIMD comparison.
Syntax

// SIMD instruction with secondary SIMD merge operation
vset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;

.atype = .btype = { .u32, .s32 };
.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };
.mask  = { .h0, .h1, .h10 };  // defaults to .h10
.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };
   .asel defaults to .h10
   .bsel defaults to .h32


Description
Two-way SIMD parallel comparison with secondary operation.
Elements of each dual half-word source to the operation are selected from any of the four half-words
in the two source operands a and b using the asel and bsel modifiers.
The selected half-words are then compared in parallel.
The intermediate result of the comparison is always unsigned, and therefore the half-words of
destination d and operand c are also unsigned.
For instructions with a secondary SIMD merge operation:

For half-word positions indicated in mask, the selected half-word results are copied into
destination d. For all other positions, the corresponding half-word from source operand b
is copied to d.

For instructions with a secondary accumulate operation:

For half-word positions indicated in mask, the selected half-word results are added to operand
c, producing a result in d.

Semantics

// extract pairs of half-words and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_2( a, b, .asel, .atype );
Vb = extractAndSignExt_2( a, b, .bsel, .btype );
Vc = extractAndSignExt_2( c );
for (i=0; i<2; i++) {
    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}


PTX ISA Notes
Introduced in PTX ISA version 3.0.
Target ISA Notes
vset2 requires sm_30 or higher.
Examples

vset2.s32.u32.lt      r1, r2, r3, r0;
vset2.u32.u32.ne.add  r1, r2, r3, r0;





9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4ï

vadd4, vsub4
Integer quad byte SIMD addition/subtraction.
vavrg4
Integer quad byte SIMD average.
vabsdiff4
Integer quad byte SIMD absolute value of difference.
vmin4, vmax4
Integer quad byte SIMD minimum/maximum.
Syntax

// SIMD instruction with secondary SIMD merge operation
vop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;
vop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };

.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .b0,
           .b1, .b10
           .b2, .b20, .b21, .b210,
           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };
    defaults to .b3210
.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };
   .asel defaults to .b3210
   .bsel defaults to .b7654


Description
Four-way SIMD parallel arithmetic operation with secondary operation.
Elements of each quad byte source to the operation are selected from any of the eight bytes in the
two source operands a and b using the asel and bsel modifiers.
The selected bytes are then operated on in parallel.
The results are optionally clamped to the appropriate range determined by the destination type
(signed or unsigned). Saturation cannot be used with the secondary accumulate operation.
For instructions with a secondary SIMD merge operation:

For byte positions indicated in mask, the selected byte results are copied into destination
d. For all other positions, the corresponding byte from source operand c is copied to
d.

For instructions with a secondary accumulate operation:

For byte positions indicated in mask, the selected byte results are added to operand c,
producing a result in d.

Semantics

// extract quads of bytes and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_4( a, b, .asel, .atype );
Vb = extractAndSignExt_4( a, b, .bsel, .btype );
Vc = extractAndSignExt_4( c );
for (i=0; i<4; i++) {
    switch ( vop4 ) {
        case vadd4:            t[i] = Va[i] + Vb[i];
        case vsub4:            t[i] = Va[i] - Vb[i];
        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {
                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;
                               } else {
                                   t[i] = ( Va[i] + Vb[i] ) >> 1;
                               }
        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;
        case vmin4:            t[i] = MIN( Va[i], Vb[i] );
        case vmax4:            t[i] = MAX( Va[i], Vb[i] );
    }
    if (.sat) {
        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );
        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );
    }
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}


PTX ISA Notes
Introduced in PTX ISA version 3.0.
Target ISA Notes
vadd4, vsub4, varvg4, vabsdiff4, vmin4, vmax4 require sm_30 or higher.
Examples

vadd4.s32.s32.u32.sat  r1, r2, r3, r1;
vsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;
vmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1;





9.7.16.2.4. SIMD Video Instructions: vset4ï

vset4
Integer quad byte SIMD comparison.
Syntax

// SIMD instruction with secondary SIMD merge operation
vset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;

.atype = .btype = { .u32, .s32 };
.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };
.mask  = { .b0,
           .b1, .b10
           .b2, .b20, .b21, .b210,
           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };
    defaults to .b3210
.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };
   .asel defaults to .b3210
   .bsel defaults to .b7654


Description
Four-way SIMD parallel comparison with secondary operation.
Elements of each quad byte source to the operation are selected from any of the eight bytes in the
two source operands a and b using the asel and bsel modifiers.
The selected bytes are then compared in parallel.
The intermediate result of the comparison is always unsigned, and therefore the bytes of destination
d and operand c are also unsigned.
For instructions with a secondary SIMD merge operation:

For byte positions indicated in mask, the selected byte results are copied into destination
d. For all other positions, the corresponding byte from source operand b is copied to
d.

For instructions with a secondary accumulate operation:

For byte positions indicated in mask, the selected byte results are added to operand c,
producing a result in d.

Semantics

// extract quads of bytes and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_4( a, b, .asel, .atype );
Vb = extractAndSignExt_4( a, b, .bsel, .btype );
Vc = extractAndSignExt_4( c );
for (i=0; i<4; i++) {
    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}


PTX ISA Notes
Introduced in PTX ISA version 3.0.
Target ISA Notes
vset4 requires sm_30 or higher.
Examples

vset4.s32.u32.lt      r1, r2, r3, r0;
vset4.u32.u32.ne.max  r1, r2, r3, r0;







9.7.17. Miscellaneous Instructionsï

The Miscellaneous instructions are:

brkpt
nanosleep
pmevent
trap
setmaxnreg



9.7.17.1. Miscellaneous Instructions: brkptï

brkpt
Breakpoint.
Syntax

brkpt;


Description
Suspends execution.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
brkpt requires sm_11 or higher.
Examples

    brkpt;
@p  brkpt;





9.7.17.2. Miscellaneous Instructions: nanosleepï

nanosleep
Suspend the thread for an approximate delay given in nanoseconds.
Syntax

nanosleep.u32 t;


Description
Suspends the thread for a sleep duration approximately close to the delay t, specified in
nanoseconds. t may be a register or an immediate value.
The sleep duration is approximated, but guaranteed to be in the interval [0, 2*t]. The maximum
sleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual
threads within a warp such that all sleeping threads in the warp wake up together.
PTX ISA Notes
nanosleep introduced in PTX ISA 6.3.
Target ISA Notes
nanosleep requires sm_70 or higher.
Examples

.reg .b32 r;
.reg .pred p;

nanosleep.u32 r;
nanosleep.u32 42;
@p nanosleep.u32 r;





9.7.17.3. Miscellaneous Instructions: pmeventï

pmevent
Trigger one or more Performance Monitor events.
Syntax

pmevent       a;    // trigger a single performance monitor event
pmevent.mask  a;    // trigger one or more performance monitor events


Description
Triggers one or more of a fixed number of performance monitor events, with event index or mask
specified by immediate operand a.
pmevent (without modifier .mask) triggers a single performance monitor event indexed by
immediate operand a, in the range 0..15.
pmevent.mask triggers one or more of the performance monitor events. Each bit in the 16-bit
immediate operand a controls an event.
Programmatic performance moniter events may be combined with other hardware events using Boolean
functions to increment one of the four performance counters. The relationship between events and
counters is programmed via API calls from the host.
Notes
Currently, there are sixteen performance monitor events, numbered 0 through 15.
PTX ISA Notes
pmevent introduced in PTX ISA version 1.4.
pmevent.mask introduced in PTX ISA version 3.0.
Target ISA Notes
pmevent supported on all target architectures.
pmevent.mask requires sm_20 or higher.
Examples

    pmevent      1;
@p  pmevent      7;
@q  pmevent.mask 0xff;





9.7.17.4. Miscellaneous Instructions: trapï

trap
Perform trap operation.
Syntax

trap;


Description
Abort execution and generate an interrupt to the host CPU.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

    trap;
@p  trap;





9.7.17.5. Miscellaneous Instructions: setmaxnregï

setmaxnreg
Hint to change the number of registers owned by the warp.
Syntax

setmaxnreg.action.sync.aligned.u32 imm-reg-count;

.action = { .inc, .dec };


Description
setmaxnreg provides a hint to the system to update the maximum number of per-thread registers
owned by the executing warp to the value specified by the imm-reg-count operand.
Qualifier .dec is used to release extra registers such that the absolute per-thread maximum
register count is reduced from its current value to imm-reg-count. Qualifier .inc is used to
request additional registers such that the absolute per-thread maximum register count is increased
from its current value to imm-reg-count.
A pool of available registers is maintained per-CTA. Register adjustments requested by the
setmaxnreg instructions are handled by supplying extra registers from this pool to the
requesting warp or by releasing extra registers from the requesting warp to this pool, depending
upon the value of the .action qualifier.
The setmaxnreg.inc instruction blocks the execution until enough registers are available in the
CTAâs register pool. After the instruction setmaxnreg.inc obtains new registers from the CTA
pool, the initial contents of the new registers are undefined. The new registers must be initialized
before they are used.
The same setmaxnreg instruction must be executed by all warps in a warpgroup. After executing a
setmaxnreg instruction, all warps in the warpgroup must synchronize explicitly before
executing subsequent setmaxnreg instructions. If a setmaxnreg instruction is not executed by all
warps in the warpgroup, then the behavior is undefined.
Operand imm-reg-count is an integer constant. The value of imm-reg-count must be in the
range 24 to 256 (both inclusive) and must be a multiple of 8.
Changes to the register file of the warp always happen at the tail-end of the register file.
The setmaxnreg instruction requires that the kernel has been launched with a valid value of
maximum number of per-thread registers specified via the appropriate compilation via the appropriate
compile-time option or the appropriate performance tuning directive. Otherwise, the setmaxnreg
instruction may have no effect.
When qualifier .dec is specified, the maximum number of per-thread registers owned by the warp
prior to the execution of setmaxnreg instruction should be greater than or equal to the
imm-reg-count. Otherwise, the behaviour is undefined.
When qualifier .inc is specified, the maximum number of per-thread registers owned by the warp
prior to the execution of setmaxnreg instruction should be less than or equal to the
imm-reg-count. Otherwise, the behaviour is undefined.
The mandatory .sync qualifier indicates that setmaxnreg instruction causes the executing
thread to wait until all threads in the warp execute the same setmaxnreg instruction before
resuming execution.
The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the
same setmaxnreg instruction. In conditionally executed code, setmaxnreg instruction should
only be used if it is known that all threads in warpgroup evaluate the condition identically,
otherwise the behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_90a.
Examples

setmaxnreg.dec.sync.aligned.u32 64;
setmaxnreg.inc.sync.aligned.u32 192;








10. Special Registersï

PTX includes a number of predefined, read-only variables, which are
visible as special registers and accessed through mov or cvt
instructions.
The special registers are:

%tid
%ntid
%laneid
%warpid
%nwarpid
%ctaid
%nctaid
%smid
%nsmid
%gridid
%is_explicit_cluster
%clusterid
%nclusterid
%cluster_ctaid
%cluster_nctaid
%cluster_ctarank
%cluster_nctarank
%lanemask_eq, %lanemask_le, %lanemask_lt, %lanemask_ge, %lanemask_gt
%clock, %clock_hi, %clock64
%pm0, ..., %pm7
%pm0_64, ..., %pm7_64
%envreg0, ..., %envreg31
%globaltimer, %globaltimer_lo, %globaltimer_hi
%reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap,
%reserved_smem_offset<2>
%total_smem_size
%aggr_smem_size
%dynamic_smem_size
%current_graph_exec



10.1. Special Registers: %tidï

%tid
Thread identifier within a CTA.
Syntax (predefined)

.sreg .v4 .u32 %tid;                  // thread id vector
.sreg .u32 %tid.x, %tid.y, %tid.z;    // thread id components


Description
A predefined, read-only, per-thread special register initialized with the thread identifier within
the CTA. The %tid special register contains a 1D, 2D, or 3D vector to match the CTA shape; the
%tid value in unused dimensions is 0. The fourth element is unused and always returns
zero. The number of threads in each dimension are specified by the predefined special register
%ntid.
Every thread in the CTA has a unique %tid.
%tid component values range from 0 through %ntid-1 in each CTA dimension.
%tid.y == %tid.z == 0 in 1D CTAs. %tid.z == 0 in 2D CTAs.
It is guaranteed that:

0  <=  %tid.x <  %ntid.x
0  <=  %tid.y <  %ntid.y
0  <=  %tid.z <  %ntid.z


PTX ISA Notes
Introduced in PTX ISA version 1.0 with type .v4.u16.
Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit
mov and cvt instructions may be used to read the lower 16-bits of each component of
%tid.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u32      %r1,%tid.x;  // move tid.x to %rh

// legacy code accessing 16-bit components of %tid
mov.u16      %rh,%tid.x;
cvt.u32.u16  %r2,%tid.z;  // zero-extend tid.z to %r2





10.2. Special Registers: %ntidï

%ntid
Number of thread IDs per CTA.
Syntax (predefined)

.sreg .v4 .u32 %ntid;                   // CTA shape vector
.sreg .u32 %ntid.x, %ntid.y, %ntid.z;   // CTA dimensions


Description
A predefined, read-only special register initialized with the number of thread ids in each CTA
dimension. The %ntid special register contains a 3D CTA shape vector that holds the CTA
dimensions. CTA dimensions are non-zero; the fourth element is unused and always returns zero. The
total number of threads in a CTA is (%ntid.x * %ntid.y * %ntid.z).

%ntid.y == %ntid.z == 1 in 1D CTAs.
%ntid.z ==1 in 2D CTAs.


Maximum values of %ntid.{x,y,z} are as follows:









.target architecture
%ntid.x
%ntid.y
%ntid.z




sm_1x
512
512
64


sm_20, sm_3x, sm_5x, sm_6x,
sm_7x, sm_8x, sm_9x
1024
1024
64



PTX ISA Notes
Introduced in PTX ISA version 1.0 with type .v4.u16.
Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit
mov and cvt instructions may be used to read the lower 16-bits of each component of
%ntid.
Target ISA Notes
Supported on all target architectures.
Examples

// compute unified thread id for 2D CTA
mov.u32  %r0,%tid.x;
mov.u32  %h1,%tid.y;
mov.u32  %h2,%ntid.x;
mad.u32  %r0,%h1,%h2,%r0;

mov.u16  %rh,%ntid.x;      // legacy code





10.3. Special Registers: %laneidï

%laneid
Lane Identifier.
Syntax (predefined)

.sreg .u32 %laneid;


Description
A predefined, read-only special register that returns the threadâs lane within the warp. The lane
identifier ranges from zero to WARP_SZ-1.
PTX ISA Notes
Introduced in PTX ISA version 1.3.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u32  %r, %laneid;





10.4. Special Registers: %warpidï

%warpid
Warp identifier.
Syntax (predefined)

.sreg .u32 %warpid;


Description
A predefined, read-only special register that returns the threadâs warp identifier. The warp
identifier provides a unique warp number within a CTA but not across CTAs within a grid. The warp
identifier will be the same for all threads within a single warp.
Note that %warpid is volatile and returns the location of a thread at the moment when read, but
its value may change during execution, e.g., due to rescheduling of threads following
preemption. For this reason, %ctaid and %tid should be used to compute a virtual warp index
if such a value is needed in kernel code; %warpid is intended mainly to enable profiling and
diagnostic code to sample and log information such as work place mapping and load distribution.
PTX ISA Notes
Introduced in PTX ISA version 1.3.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u32  %r, %warpid;





10.5. Special Registers: %nwarpidï

%nwarpid
Number of warp identifiers.
Syntax (predefined)

.sreg .u32 %nwarpid;


Description
A predefined, read-only special register that returns the maximum number of warp identifiers.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%nwarpid requires sm_20 or higher.
Examples

mov.u32  %r, %nwarpid;





10.6. Special Registers: %ctaidï

%ctaid
CTA identifier within a grid.
Syntax (predefined)

.sreg .v4 .u32 %ctaid;                      // CTA id vector
.sreg .u32 %ctaid.x, %ctaid.y, %ctaid.z;    // CTA id components


Description
A predefined, read-only special register initialized with the CTA identifier within the CTA
grid. The %ctaid special register contains a 1D, 2D, or 3D vector, depending on the shape and
rank of the CTA grid. The fourth element is unused and always returns zero.
It is guaranteed that:

0  <=  %ctaid.x <  %nctaid.x
0  <=  %ctaid.y <  %nctaid.y
0  <=  %ctaid.z <  %nctaid.z


PTX ISA Notes
Introduced in PTX ISA version 1.0 with type .v4.u16.
Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit
mov and cvt instructions may be used to read the lower 16-bits of each component of
%ctaid.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u32  %r0,%ctaid.x;
mov.u16  %rh,%ctaid.y;   // legacy code





10.7. Special Registers: %nctaidï

%nctaid
Number of CTA ids per grid.
Syntax (predefined)

.sreg .v4 .u32 %nctaid                      // Grid shape vector
.sreg .u32 %nctaid.x,%nctaid.y,%nctaid.z;   // Grid dimensions


Description
A predefined, read-only special register initialized with the number of CTAs in each grid
dimension. The %nctaid special register contains a 3D grid shape vector, with each element
having a value of at least 1. The fourth element is unused and always returns zero.
Maximum values of %nctaid.{x,y,z} are as follows:









.target architecture
%nctaid.x
%nctaid.y
%nctaid.z




sm_1x, sm_20
65535
65535
65535


sm_3x, sm_5x, sm_6x, sm_7x,
sm_8x, sm_9x
231 -1
65535
65535



PTX ISA Notes
Introduced in PTX ISA version 1.0 with type .v4.u16.
Redefined as type .v4.u32 in PTX ISA version 2.0. For compatibility with legacy PTX code, 16-bit
mov and cvt instructions may be used to read the lower 16-bits of each component of
%nctaid.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u32  %r0,%nctaid.x;
mov.u16  %rh,%nctaid.x;     // legacy code





10.8. Special Registers: %smidï

%smid
SM identifier.
Syntax (predefined)

.sreg .u32 %smid;


Description
A predefined, read-only special register that returns the processor (SM) identifier on which a
particular thread is executing. The SM identifier ranges from 0 to %nsmid-1. The SM
identifier numbering is not guaranteed to be contiguous.
Notes
Note that %smid is volatile and returns the location of a thread at the moment when read, but
its value may change during execution, e.g. due to rescheduling of threads following
preemption. %smid is intended mainly to enable profiling and diagnostic code to sample and log
information such as work place mapping and load distribution.
PTX ISA Notes
Introduced in PTX ISA version 1.3.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u32  %r, %smid;





10.9. Special Registers: %nsmidï

%nsmid
Number of SM identifiers.
Syntax (predefined)

.sreg .u32 %nsmid;


Description
A predefined, read-only special register that returns the maximum number of SM identifiers. The SM
identifier numbering is not guaranteed to be contiguous, so %nsmid may be larger than the
physical number of SMs in the device.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%nsmid requires sm_20 or higher.
Examples

mov.u32  %r, %nsmid;





10.10. Special Registers: %grididï

%gridid
Grid identifier.
Syntax (predefined)

.sreg .u64 %gridid;


Description
A predefined, read-only special register initialized with the per-grid temporal grid identifier. The
%gridid is used by debuggers to distinguish CTAs and clusters within concurrent (small) grids.
During execution, repeated launches of programs may occur, where each launch starts a
grid-of-CTAs. This variable provides the temporal grid launch number for this context.
For sm_1x targets, %gridid is limited to the range [0..216-1]. For sm_20,
%gridid is limited to the range [0..232-1]. sm_30 supports the entire 64-bit range.
PTX ISA Notes
Introduced in PTX ISA version 1.0 as type .u16.
Redefined as type .u32 in PTX ISA version 1.3.
Redefined as type .u64 in PTX ISA version 3.0.
For compatibility with legacy PTX code, 16-bit and 32-bit mov and cvt instructions may be
used to read the lower 16-bits or 32-bits of each component of %gridid.
Target ISA Notes
Supported on all target architectures.
Examples

mov.u64  %s, %gridid;  // 64-bit read of %gridid
mov.u32  %r, %gridid;  // legacy code with 32-bit %gridid





10.11. Special Registers: %is_explicit_clusterï

%is_explicit_cluster
Checks if user has explicitly specified cluster launch.
Syntax (predefined)

.sreg .pred %is_explicit_cluster;


Description
A predefined, read-only special register initialized with the predicate value of whether the cluster
launch is explicitly specified by user.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .pred p;

mov.pred  p, %is_explicit_cluster;





10.12. Special Registers: %clusteridï

%clusterid
Cluster identifier within a grid.
Syntax (predefined)

.sreg .v4 .u32 %clusterid;
.sreg .u32 %clusterid.x, %clusterid.y, %clusterid.z;


Description
A predefined, read-only special register initialized with the cluster identifier in a grid in each
dimension. Each cluster in a grid has a unique identifier.
The %clusterid special register contains a 1D, 2D, or 3D vector, depending upon the shape and
rank of the cluster. The fourth element is unused and always returns zero.
It is guaranteed that:

0  <=  %clusterid.x <  %nclusterid.x
0  <=  %clusterid.y <  %nclusterid.y
0  <=  %clusterid.z <  %nclusterid.z


PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b32 %r<2>;
.reg .v4 .b32 %rx;

mov.u32     %r0, %clusterid.x;
mov.u32     %r1, %clusterid.z;
mov.v4.u32  %rx, %clusterid;





10.13. Special Registers: %nclusteridï

%nclusterid
Number of cluster identifiers per grid.
Syntax (predefined)

.sreg .v4 .u32 %nclusterid;
.sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z;


Description
A predefined, read-only special register initialized with the number of clusters in each grid
dimension.
The %nclusterid special register contains a 3D grid shape vector that holds the grid dimensions
in terms of clusters. The fourth element is unused and always returns zero.
Refer to the Cuda Programming Guide for details on the maximum values of %nclusterid.{x,y,z}.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b32 %r<2>;
.reg .v4 .b32 %rx;

mov.u32     %r0, %nclusterid.x;
mov.u32     %r1, %nclusterid.z;
mov.v4.u32  %rx, %nclusterid;





10.14. Special Registers: %cluster_ctaidï

%cluster_ctaid
CTA identifier within a cluster.
Syntax (predefined)

.sreg .v4 .u32 %cluster_ctaid;
.sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z;


Description
A predefined, read-only special register initialized with the CTA identifier in a cluster in each
dimension. Each CTA in a cluster has a unique CTA identifier.
The %cluster_ctaid special register contains a 1D, 2D, or 3D vector, depending upon the shape of
the cluster. The fourth element is unused and always returns zero.
It is guaranteed that:

0  <=  %cluster_ctaid.x <  %cluster_nctaid.x
0  <=  %cluster_ctaid.y <  %cluster_nctaid.y
0  <=  %cluster_ctaid.z <  %cluster_nctaid.z


PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b32 %r<2>;
.reg .v4 .b32 %rx;

mov.u32     %r0, %cluster_ctaid.x;
mov.u32     %r1, %cluster_ctaid.z;
mov.v4.u32  %rx, %cluster_ctaid;





10.15. Special Registers: %cluster_nctaidï

%cluster_nctaid
Number of CTA identifiers per cluster.
Syntax (predefined)

.sreg .v4 .u32 %cluster_nctaid;
.sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z;


Description
A predefined, read-only special register initialized with the number of CTAs in a cluster in each
dimension.
The %cluster_nctaid special register contains a 3D grid shape vector that holds the cluster
dimensions in terms of CTAs. The fourth element is unused and always returns zero.
Refer to the Cuda Programming Guide for details on the maximum values of
%cluster_nctaid.{x,y,z}.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b32 %r<2>;
.reg .v4 .b32 %rx;

mov.u32     %r0, %cluster_nctaid.x;
mov.u32     %r1, %cluster_nctaid.z;
mov.v4.u32  %rx, %cluster_nctaid;





10.16. Special Registers: %cluster_ctarankï

%cluster_ctarank
CTA identifier in a cluster across all dimensions.
Syntax (predefined)

.sreg .u32 %cluster_ctarank;


Description
A predefined, read-only special register initialized with the CTA rank within a cluster across all
dimensions.
It is guaranteed that:

0  <=  %cluster_ctarank <  %cluster_nctarank


PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b32 %r;

mov.u32  %r, %cluster_ctarank;





10.17. Special Registers: %cluster_nctarankï

%cluster_nctarank
Number of CTA identifiers in a cluster across all dimensions.
Syntax (predefined)

.sreg .u32 %cluster_nctarank;


Description
A predefined, read-only special register initialized with the nunber of CTAs within a cluster across
all dimensions.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.reg .b32 %r;

mov.u32  %r, %cluster_nctarank;





10.18. Special Registers: %lanemask_eqï

%lanemask_eq
32-bit mask with bit set in position equal to the threadâs lane number in the warp.
Syntax (predefined)

.sreg .u32 %lanemask_eq;


Description
A predefined, read-only special register initialized with a 32-bit mask with a bit set in the
position equal to the threadâs lane number in the warp.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%lanemask_eq requires sm_20 or higher.
Examples

mov.u32     %r, %lanemask_eq;





10.19. Special Registers: %lanemask_leï

%lanemask_le
32-bit mask with bits set in positions less than or equal to the threadâs lane number in the warp.
Syntax (predefined)

.sreg .u32 %lanemask_le;


Description
A predefined, read-only special register initialized with a 32-bit mask with bits set in positions
less than or equal to the threadâs lane number in the warp.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%lanemask_le requires sm_20 or higher.
Examples

mov.u32     %r, %lanemask_le





10.20. Special Registers: %lanemask_ltï

%lanemask_lt
32-bit mask with bits set in positions less than the threadâs lane number in the warp.
Syntax (predefined)

.sreg .u32 %lanemask_lt;


Description
A predefined, read-only special register initialized with a 32-bit mask with bits set in positions
less than the threadâs lane number in the warp.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%lanemask_lt requires sm_20 or higher.
Examples

mov.u32     %r, %lanemask_lt;





10.21. Special Registers: %lanemask_geï

%lanemask_ge
32-bit mask with bits set in positions greater than or equal to the threadâs lane number in the warp.
Syntax (predefined)

.sreg .u32 %lanemask_ge;


Description
A predefined, read-only special register initialized with a 32-bit mask with bits set in positions
greater than or equal to the threadâs lane number in the warp.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%lanemask_ge requires sm_20 or higher.
Examples

mov.u32     %r, %lanemask_ge;





10.22. Special Registers: %lanemask_gtï

%lanemask_gt
32-bit mask with bits set in positions greater than the threadâs lane number in the warp.
Syntax (predefined)

.sreg .u32 %lanemask_gt;


Description
A predefined, read-only special register initialized with a 32-bit mask with bits set in positions
greater than the threadâs lane number in the warp.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%lanemask_gt requires sm_20 or higher.
Examples

mov.u32     %r, %lanemask_gt;





10.23. Special Registers: %clock, %clock_hiï

%clock, %clock_hi

%clock

A predefined, read-only 32-bit unsigned cycle counter.

%clock_hi

The upper 32-bits of %clock64 special register.


Syntax (predefined)

.sreg .u32 %clock;
.sreg .u32 %clock_hi;


Description
Special register %clock and %clock_hi are unsigned 32-bit read-only cycle counters that wrap
silently.
PTX ISA Notes
%clock introduced in PTX ISA version 1.0.
%clock_hi introduced in PTX ISA version 5.0.
Target ISA Notes
%clock supported on all target architectures.
%clock_hi requires sm_20 or higher.
Examples

mov.u32 r1,%clock;
mov.u32 r2, %clock_hi;





10.24. Special Registers: %clock64ï

%clock64
A predefined, read-only 64-bit unsigned cycle counter.
Syntax (predefined)

.sreg .u64 %clock64;


Description
Special register %clock64 is an unsigned 64-bit read-only cycle counter that wraps silently.
Notes
The lower 32-bits of %clock64 are identical to %clock.
The upper 32-bits of %clock64 are identical to %clock_hi.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
%clock64 requires sm_20 or higher.
Examples

mov.u64  r1,%clock64;





10.25. Special Registers: %pm0..%pm7ï

%pm0..%pm7
Performance monitoring counters.
Syntax (predefined)

.sreg .u32 %pm<8>;


Description
Special registers %pm0..%pm7 are unsigned 32-bit read-only performance monitor counters. Their
behavior is currently undefined.
PTX ISA Notes
%pm0..%pm3 introduced in PTX ISA version 1.3.
%pm4..%pm7 introduced in PTX ISA version 3.0.
Target ISA Notes
%pm0..%pm3 supported on all target architectures.
%pm4..%pm7 require sm_20 or higher.
Examples

mov.u32  r1,%pm0;
mov.u32  r1,%pm7;





10.26. Special Registers: %pm0_64..%pm7_64ï

%pm0_64..%pm7_64
64 bit Performance monitoring counters.
Syntax (predefined)

.sreg .u64 %pm0_64;
.sreg .u64 %pm1_64;
.sreg .u64 %pm2_64;
.sreg .u64 %pm3_64;
.sreg .u64 %pm4_64;
.sreg .u64 %pm5_64;
.sreg .u64 %pm6_64;
.sreg .u64 %pm7_64;


Description
Special registers %pm0_64..%pm7_64 are unsigned 64-bit read-only performance monitor
counters. Their behavior is currently undefined.
Notes
The lower 32bits of %pm0_64..%pm7_64 are identical to %pm0..%pm7.
PTX ISA Notes
%pm0_64..%pm7_64 introduced in PTX ISA version 4.0.
Target ISA Notes
%pm0_64..%pm7_64 require sm_50 or higher.
Examples

mov.u32  r1,%pm0_64;
mov.u32  r1,%pm7_64;





10.27. Special Registers: %envreg<32>ï

%envreg<32>
Driver-defined read-only registers.
Syntax (predefined)

.sreg .b32 %envreg<32>;


Description
A set of 32 pre-defined read-only registers used to capture execution environment of PTX program
outside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch
and can contain cta-wide or grid-wide values.
Precise semantics of these registers is defined in the driver documentation.
PTX ISA Notes
Introduced in PTX ISA version 2.1.
Target ISA Notes
Supported on all target architectures.
Examples

mov.b32      %r1,%envreg0;  // move envreg0 to %r1





10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hiï

%globaltimer, %globaltimer_lo, %globaltimer_hi

%globaltimer

A predefined, 64-bit global nanosecond timer.

%globaltimer_lo

The lower 32-bits of %globaltimer.

%globaltimer_hi

The upper 32-bits of %globaltimer.


Syntax (predefined)

.sreg .u64 %globaltimer;
.sreg .u32 %globaltimer_lo, %globaltimer_hi;


Description
Special registers intended for use by NVIDIA tools. The behavior is target-specific and may change
or be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is
unspecified.
PTX ISA Notes
Introduced in PTX ISA version 3.1.
Target ISA Notes
Requires target sm_30 or higher.
Examples

mov.u64  r1,%globaltimer;





10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2>ï

%reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_<2>

%reserved_smem_offset_begin

Start of the reserved shared memory region.

%reserved_smem_offset_end

End of the reserved shared memory region.

%reserved_smem_offset_cap

Total size of the reserved shared memory region.

%reserved_smem_offset_<2>

Offsets in the reserved shared memory region.


Syntax (predefined)

.sreg .b32 %reserved_smem_offset_begin;
.sreg .b32 %reserved_smem_offset_end;
.sreg .b32 %reserved_smem_offset_cap;
.sreg .b32 %reserved_smem_offset_<2>;


Description
These are predefined, read-only special registers containing information about the shared memory
region which is reserved for the NVIDIA system software use. This region of shared memory is not
available to users, and accessing this region from user code results in undefined behavior. Refer to
CUDA Programming Guide for details.
PTX ISA Notes
Introduced in PTX ISA version 7.6.
Target ISA Notes
Require sm_80 or higher.
Examples

.reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1;

mov.b32 %reg_begin,   %reserved_smem_offset_begin;
mov.b32 %reg_end,     %reserved_smem_offset_end;
mov.b32 %reg_cap,     %reserved_smem_offset_cap;
mov.b32 %reg_offset0, %reserved_smem_offset_0;
mov.b32 %reg_offset1, %reserved_smem_offset_1;





10.30. Special Registers: %total_smem_sizeï

%total_smem_size
Total size of shared memory used by a CTA of a kernel.
Syntax (predefined)

.sreg .u32 %total_smem_size;


Description
A predefined, read-only special register initialized with total size of shared memory allocated
(statically and dynamically, excluding the shared memory reserved for the NVIDIA system software
use) for the CTA of a kernel at launch time.
Size is returned in multiples of shared memory allocation unit size supported by target
architecture.
Allocation unit values are as follows:







Target architecture
Shared memory allocation unit size




sm_2x
128 bytes


sm_3x, sm_5x, sm_6x, sm_7x
256 bytes


sm_8x, sm_9x
128 bytes



PTX ISA Notes
Introduced in PTX ISA version 4.1.
Target ISA Notes
Requires sm_20 or higher.
Examples

mov.u32  %r, %total_smem_size;





10.31. Special Registers: %aggr_smem_sizeï

%aggr_smem_size
Total size of shared memory used by a CTA of a kernel.
Syntax (predefined)

.sreg .u32 %aggr_smem_size;


Description
A predefined, read-only special register initialized with total aggregated size of shared memory
consisting of the size of user shared memory allocated (statically and dynamically) at launch time
and the size of shared memory region which is reserved for the NVIDIA system software use.
PTX ISA Notes
Introduced in PTX ISA version 8.1.
Target ISA Notes
Requires sm_90 or higher.
Examples

mov.u32  %r, %aggr_smem_size;





10.32. Special Registers: %dynamic_smem_sizeï

%dynamic_smem_size
Size of shared memory allocated dynamically at kernel launch.
Syntax (predefined)

.sreg .u32 %dynamic_smem_size;


Description
Size of shared memory allocated dynamically at kernel launch.
A predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time.
PTX ISA Notes
Introduced in PTX ISA version 4.1.
Target ISA Notes
Requires sm_20 or higher.
Examples

mov.u32  %r, %dynamic_smem_size;





10.33. Special Registers: %current_graph_execï

%current_graph_exec
An Identifier for currently executing CUDA device graph.
Syntax (predefined)

.sreg .u64 %current_graph_exec;


Description
A predefined, read-only special register initialized with the identifier referring to the CUDA
device graph being currently executed. This register is 0 if the executing kernel is not part of a
CUDA device graph.
Refer to the CUDA Programming Guide for more details on CUDA device graphs.
PTX ISA Notes
Introduced in PTX ISA version 8.0.
Target ISA Notes
Requires sm_50 or higher.
Examples

mov.u64  r1, %current_graph_exec;






11. Directivesï



11.1. PTX Module Directivesï

The following directives declare the PTX ISA version of the code in the module, the target
architecture for which the code was generated, and the size of addresses within the PTX module.

.version
.target
.address_size



11.1.1. PTX Module Directives: .versionï

.version
PTX ISA version number.
Syntax

.version  major.minor    // major, minor are integers


Description
Specifies the PTX language version number.
The major number is incremented when there are incompatible changes to the PTX language, such as
changes to the syntax or semantics. The version major number is used by the PTX compiler to ensure
correct execution of legacy PTX code.
The minor number is incremented when new features are added to PTX.
Semantics
Indicates that this module must be compiled with tools that support an equal or greater version
number.
Each PTX module must begin with a .version directive, and no other .version directive is
allowed anywhere else within the module.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

.version 3.1
.version 3.0
.version 2.3





11.1.2. PTX Module Directives: .targetï

.target
Architecture and Platform target.
Syntax

.target stringlist         // comma separated list of target specifiers
string = { sm_90a, sm_90,               // sm_9x target architectures
           sm_80, sm_86, sm_87, sm_89,  // sm_8x target architectures
           sm_70, sm_72, sm_75,         // sm_7x target architectures
           sm_60, sm_61, sm_62,         // sm_6x target architectures
           sm_50, sm_52, sm_53,         // sm_5x target architectures
           sm_30, sm_32, sm_35, sm_37,  // sm_3x target architectures
           sm_20,                       // sm_2x target architectures
           sm_10, sm_11, sm_12, sm_13,  // sm_1x target architectures
           texmode_unified, texmode_independent,   // texturing mode
           debug,                                  // platform option
           map_f64_to_f32 };                       // platform option


Description
Specifies the set of features in the target architecture for which the current PTX code was
generated. In general, generations of SM architectures follow an onion layer model, where each
generation adds new features and retains all features of previous generations. The onion layer model
allows the PTX code generated for a given target to be run on later generation devices.
Target architectures with suffix âaâ, such as sm_90a, include architecture-accelerated
features that are supported on the specified architecture only, hence such targets do not follow the
onion layer model. Therefore, PTX code generated for such targets cannot be run on later generation
devices. Architecture-accelerated features can only be used with targets that support these
features.
Semantics
Each PTX module must begin with a .version directive, immediately followed by a .target
directive containing a target architecture and optional platform options. A .target directive
specifies a single target architecture, but subsequent .target directives can be used to change
the set of target features allowed during parsing. A program with multiple .target directives
will compile and run only on devices that support all features of the highest-numbered architecture
listed in the program.
PTX features are checked against the specified target architecture, and an error is generated if an
unsupported feature is used.Â  The following table summarizes the features in PTX that vary according
to target architecture.







Target
Description




sm_90
Baseline feature set for sm_90 architecture.


sm_90a
Adds support for sm_90a accelerated wgmma and setmaxnreg instructions.










Target
Description




sm_80
Baseline feature set for sm_80 architecture.


sm_86
Adds support for .xorsign modifier on min and max instructions.


sm_87
Baseline feature set for sm_86 architecture.


sm_89
Baseline feature set for sm_86 architecture.










Target
Description




sm_70
Baseline feature set for sm_70 architecture.


sm_72

Adds support for integer multiplicand and accumulator matrices in wmma instructions.
Adds support for cvt.pack instruction.



sm_75

Adds support for sub-byte integer and single-bit multiplicant matrices in wmma instructions.
Adds support for ldmatrix instruction.
Adds support for movmatrix instruction.
Adds support for tanh instruction.











Target
Description




sm_60
Baseline feature set for sm_60 architecture.


sm_61
Adds support for dp2a and dp4a instructions.


sm_62
Baseline feature set for sm_61 architecture.










Target
Description




sm_50
Baseline feature set for sm_50 architecture.


sm_52
Baseline feature set for sm_50 architecture.


sm_53
Adds support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types.










Target
Description




sm_30
Baseline feature set for sm_30 architecture.


sm_32

Adds 64-bit {atom,red}.{and,or,xor,min,max}
instructions.
Adds shf instruction.
Adds ld.global.nc instruction.



sm_35
Adds support for CUDA Dynamic Parallelism.


sm_37
Baseline feature set for sm_35 architecture.










Target
Description




sm_20
Baseline feature set for sm_20 architecture.










Target
Description




sm_10

Baseline feature set for sm_10 architecture.
Requires map_f64_to_f32 if any .f64 instructions used.



sm_11

Adds 64-bit {atom,red}.{and,or,xor,min,max} instructions.
Requires map_f64_to_f32 if any .f64 instructions used.



sm_12

Adds {atom,red}.shared, 64-bit {atom,red}.global, vote
instructions.
Requires map_f64_to_f32 if any .f64 instructions used.



sm_13

Adds double-precision support, including expanded rounding modifiers.
Disallows use of map_f64_to_f32.




The texturing mode is specified for an entire module and cannot be changed within the module.
The .target debug option declares that the PTX file contains DWARF debug information, and
subsequent compilation of PTX will retain information needed for source-level debugging. If the
debug option is declared, an error message is generated if no DWARF information is found in the
file. The debug option requires PTX ISA version 3.0 or later.
map_f64_to_f32 indicates that all double-precision instructions map to single-precision
regardless of the target architecture. This enables high-level language compilers to compile
programs containing type double to target device that do not support double-precision
operations. Note that .f64 storage remains as 64-bits, with only half being used by instructions
converted from .f64 to .f32.
Notes
Targets of the form compute_xx are also accepted as synonyms for sm_xx targets.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target strings sm_10 and sm_11 introduced in PTX ISA version 1.0.
Target strings sm_12 and sm_13 introduced in PTX ISA version 1.2.
Texturing mode introduced in PTX ISA version 1.5.
Target string sm_20 introduced in PTX ISA version 2.0.
Target string sm_30 introduced in PTX ISA version 3.0.
Platform option debug introduced in PTX ISA version 3.0.
Target string sm_35 introduced in PTX ISA version 3.1.
Target strings sm_32 and sm_50 introduced in PTX ISA version 4.0.
Target strings sm_37 and sm_52 introduced in PTX ISA version 4.1.
Target string sm_53 introduced in PTX ISA version 4.2.
Target string sm_60, sm_61, sm_62 introduced in PTX ISA version 5.0.
Target string sm_70 introduced in PTX ISA version 6.0.
Target string sm_72 introduced in PTX ISA version 6.1.
Target string sm_75 introduced in PTX ISA version 6.3.
Target string sm_80 introduced in PTX ISA version 7.0.
Target string sm_86 introduced in PTX ISA version 7.1.
Target string sm_87 introduced in PTX ISA version 7.4.
Target string sm_89 introduced in PTX ISA version 7.8.
Target string sm_90 introduced in PTX ISA version 7.8.
Target string sm_90a introduced in PTX ISA version 8.0.
Target ISA Notes
The .target directive is supported on all target architectures.
Examples

.target sm_10       // baseline target architecture
.target sm_13       // supports double-precision
.target sm_20, texmode_independent
.target sm_90       // baseline target architecture
.target sm_90a      // PTX using arch accelerated features





11.1.3. PTX Module Directives: .address_sizeï

.address_size
Address size used throughout PTX module.
Syntax

.address_size  address-size
address-size = { 32, 64 };


Description
Specifies the address size assumed throughout the module by the PTX code and the binary DWARF
information in PTX.
Redefinition of this directive within a module is not allowed. In the presence of separate
compilation all modules must specify (or default to) the same address size.
The .address_size directive is optional, but it must immediately follow the .targetdirective if present within a module.
Semantics
If the .address_size directive is omitted, the address size defaults to 32.
PTX ISA Notes
Introduced in PTX ISA version 2.3.
Target ISA Notes
Supported on all target architectures.
Examples

// example directives
   .address_size 32       // addresses are 32 bit
   .address_size 64       // addresses are 64 bit

// example of directive placement within a module
   .version 2.3
   .target sm_20
   .address_size 64
...
.entry foo () {
...
}






11.2. Specifying Kernel Entry Points and Functionsï

The following directives specify kernel entry points and functions.

.entry
.func



11.2.1. Kernel and Function Directives: .entryï

.entry
Kernel entry point and body, with optional parameters.
Syntax

.entry kernel-name ( param-list )  kernel-body
.entry kernel-name  kernel-body


Description
Defines a kernel entry point name, parameters, and body for the kernel function.
Parameters are passed via .param space memory and are listed within an optional parenthesized
parameter list. Parameters may be referenced by name within the kernel body and loaded into
registers using ld.param{::entry} instructions.
In addition to normal parameters, opaque .texref, .samplerref, and .surfref variables
may be passed as parameters. These parameters can only be referenced by name within texture and
surface load, store, and query instructions and cannot be accessed via ld.param instructions.
The shape and size of the CTA executing the kernel are available in special registers.
Semantics
Specify the entry point for a kernel program.
At kernel launch, the kernel dimensions and properties are established and made available via
special registers, e.g., %ntid, %nctaid, etc.
PTX ISA Notes
For PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter
list. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body.
The maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764
bytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table
shows the allowed parameter size for a PTX ISA version:







PTX ISA Version
Maximum parameter size (In bytes)




PTX ISA version 8.1 and above
32764


PTX ISA version 1.5 and above
4352


PTX ISA version 1.4 and above
256



The CUDA and OpenCL drivers support the following limits for parameter memory:







Driver
Parameter memory size




CUDA
256 bytes for sm_1x, 4096 bytes for sm_2x and higher,
32764 bytes fo sm_70 and higher


OpenCL
32764 bytes for sm_70 and higher, 4352 bytes on sm_6x
and lower



Target ISA Notes
Supported on all target architectures.
Examples

.entry cta_fft
.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )
{
    .reg .b32 %r<99>;
    ld.param.b32  %r1, [x];
    ld.param.b32  %r2, [y];
    ld.param.b32  %r3, [z];
    ...
}

.entry prefix_sum ( .param .align 4 .s32 pitch[8000] )
{
    .reg .s32 %t;
    ld.param::entry.s32  %t, [pitch];
    ...
}





11.2.2. Kernel and Function Directives: .funcï

.func
Function definition.
Syntax

.func {.attribute(attr-list)} fname {.noreturn} function-body
.func {.attribute(attr-list)} fname (param-list) {.noreturn} function-body
.func {.attribute(attr-list)} (ret-param) fname (param-list) function-body


Description
Defines a function, including input and return parameters and optional function body.
An optional .noreturn directive indicates that the function does not return to the caller
function. .noreturn directive cannot be specified on functions which have return parameters. See
the description of .noreturn directive in Performance-Tuning Directives: .noreturn.
An optional .attribute directive specifies additional information associated with the
function. See the description of Variable and Function Attribute Directive: .attribute for allowed attributes.
A .func definition with no body provides a function prototype.
The parameter lists define locally-scoped variables in the function body. Parameters must be base
types in either the register or parameter state space. Parameters in register state space may be
referenced directly within instructions in the function body. Parameters in .param space are
accessed using ld.param{::func} and st.param{::func} instructions in the body. Parameter
passing is call-by-value.
The last parameter in the parameter list may be a .param array of type .b8 with no size
specified. It is used to pass an arbitrary number of parameters to the function packed into a single
array object.
When calling a function with such an unsized last argument, the last argument may be omitted from
the call instruction if no parameter is passed through it. Accesses to this array parameter must
be within the bounds of the array. The result of an access is undefined if no array was passed, or
if the access was outside the bounds of the actual array being passed.
Semantics
The PTX syntax hides all details of the underlying calling convention and ABI.
The implementation of parameter passing is left to the optimizing translator, which may use a
combination of registers and stack locations to pass parameters.
Release Notes
For PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and
recursion is illegal.
PTX ISA versions 2.0 and later with target sm_20 or higher allow parameters in the .param
state space, implements an ABI with stack, and supports recursion.
PTX ISA versions 2.0 and later with target sm_20 or higher support at most one return value.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Support for unsized array parameter introduced in PTX ISA version 6.0.
Support for .noreturn directive introduced in PTX ISA version 6.4.
Support for .attribute directive introduced in PTX ISA version 8.0.
Target ISA Notes
Functions without unsized array parameter supported on all target architectures.
Unsized array parameter requires sm_30 or higher.
.noreturn directive requires sm_30 or higher.
.attribute directive requires sm_90 or higher.
Examples

.func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl)
{
.reg .b32 localVar;

... use N, dbl;
other code;

mov.b32 rval,result;
ret;
}

...
call (fooval), foo, (val0, val1);  // return value in fooval
...

.func foo (.reg .b32 N, .reg .f64 dbl) .noreturn
{
.reg .b32 localVar;
... use N, dbl;
other code;
mov.b32 rval, result;
ret;
}
...
call foo, (val0, val1);
...

.func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[])
{
    .reg .b32 input0, input1;
    ld.param.b32   input0, [numbers + 0];
    ld.param.b32   input1, [numbers + 4];
    ...
    other code;
    ret;
}
...

.param .u32 N;
.param .align 4 .b8 numbers[8];
st.param.u32    [N], 2;
st.param.b32    [numbers + 0], 5;
st.param.b32    [numbers + 4], 10;
call (rval), bar, (N, numbers);
...





11.2.3. Kernel and Function Directives: .aliasï

.alias
Define an alias to existing function symbol.
Syntax

.alias fAlias, fAliasee;


Description
.alias is a module scope directive that defines identifier fAlias to be an alias to function
specified by fAliasee.
Both fAlias and fAliasee are non-entry function symbols.
Identifier fAlias is a function declaration without body.
Identifier fAliasee is a function symbol which must be defined in the same module as .alias
declaration. Function fAliasee cannot have .weak linkage.
Prototype of fAlias and fAliasee must match.
Program can use either fAlias or fAlisee identifiers to reference function defined with
fAliasee.
PTX ISA Notes
.alias directive introduced in PTX ISA 6.3.
Target ISA Notes
.alias directive requires sm_30 or higher.
Examples

.visible .func foo(.param .u32 p) {
   ...
}
.visible .func bar(.param .u32 p);
.alias bar, foo;
.entry test()
{
      .param .u32 p;
      ...
      call foo, (p);       // call foo directly
       ...
       .param .u32 p;
       call bar, (p);        // call foo through alias
}
.entry filter ( .param .b32 x, .param .b32 y, .param .b32 z )
{
    .reg .b32 %r1, %r2, %r3;
    ld.param.b32  %r1, [x];
    ld.param.b32  %r2, [y];
    ld.param.b32  %r3, [z];
    ...
}






11.3. Control Flow Directivesï

PTX provides directives for specifying potential targets for brx.idx and call
instructions. See the descriptions of brx.idx and call for more information.

.branchtargets
.calltargets
.callprototype



11.3.1. Control Flow Directives: .branchtargetsï

.branchtargets
Declare a list of potential branch targets.
Syntax

Label:   .branchtargets  list-of-labels ;


Description
Declares a list of potential branch targets for a subsequent brx.idx, and associates the list
with the label at the start of the line.
All control flow labels in the list must occur within the same function as the declaration.
The list of labels may use the compact, shorthand syntax for enumerating a range of labels having a
common prefix, similar to the syntax described in Parameterized Variable Names.
PTX ISA Notes
Introduced in PTX ISA version 2.1.
Target ISA Notes
Requires sm_20 or higher.
Examples

  .function foo () {
      .reg .u32 %r0;
      ...
      L1:
      ...
      L2:
      ...
      L3:
      ...
      ts: .branchtargets L1, L2, L3;
      @p brx.idx %r0, ts;
      ...

.function bar() {
      .reg .u32 %r0;
      ...
      N0:
      ...
      N1:
      ...
      N2:
      ...
      N3:
      ...
      N4:
      ...
      ts: .branchtargets N<5>;
      @p brx.idx %r0, ts;
      ...





11.3.2. Control Flow Directives: .calltargetsï

.calltargets
Declare a list of potential call targets.
Syntax

Label:   .calltargets  list-of-functions ;


Description
Declares a list of potential call targets for a subsequent indirect call, and associates the list
with the label at the start of the line.
All functions named in the list must be declared prior to the .calltargets directive, and all
functions must have the same type signature.
PTX ISA Notes
Introduced in PTX ISA version 2.1.
Target ISA Notes
Requires sm_20 or higher.
Examples

calltgt:  .calltargets  fastsin, fastcos;
...
@p   call  (%f1), %r0, (%x), calltgt;
...





11.3.3. Control Flow Directives: .callprototypeï

.callprototype
Declare a prototype for use in an indirect call.
Syntax

 // no input or return parameters
label: .callprototype _ .noreturn;
// input params, no return params
label: .callprototype _ (param-list) .noreturn;
// no input params, // return params
label: .callprototype (ret-param) _ ;
// input, return parameters
label: .callprototype (ret-param) _ (param-list);


Description
Defines a prototype with no specific function name, and associates the prototype with a label. The
prototype may then be used in indirect call instructions where there is incomplete knowledge of the
possible call targets.
Parameters may have either base types in the register or parameter state spaces, or array types in
parameter state space. The sink symbol '_' may be used to avoid dummy parameter names.
An optional .noreturn directive indicates that the function does not return to the caller
function. .noreturn directive cannot be specified on functions which have return parameters. See
the description of .noreturn directive in Performance-Tuning Directives: .noreturn.
PTX ISA Notes
Introduced in PTX ISA version 2.1.
Support for .noreturn directive introduced in PTX ISA version 6.4.
Target ISA Notes
Requires sm_20 or higher.
.noreturn directive requires sm_30 or higher.
Examples

Fproto1: .callprototype  _ ;
Fproto2: .callprototype  _ (.param .f32 _);
Fproto3: .callprototype  (.param .u32 _) _ ;
Fproto4: .callprototype  (.param .u32 _) _ (.param .f32 _);
...
@p   call  (%val), %r0, (%f1), Fproto4;
...

// example of array parameter
Fproto5: .callprototype _ (.param .b8 _[12]);

Fproto6: .callprototype  _ (.param .f32 _) .noreturn;
...
@p   call  %r0, (%f1), Fproto6;
...






11.4. Performance-Tuning Directivesï

To provide a mechanism for low-level performance tuning, PTX supports the following directives,
which pass information to the backend optimizing compiler.

.maxnreg
.maxntid
.reqntid
.minnctapersm
.maxnctapersm (deprecated)
.pragma

The .maxnreg directive specifies the maximum number of registers to be allocated to a single
thread; the .maxntid directive specifies the maximum number of threads in a thread block (CTA);
the .reqntid directive specifies the required number of threads in a thread block (CTA); and the
.minnctapersm directive specifies a minimum number of thread blocks to be scheduled on a single
multiprocessor (SM). These can be used, for example, to throttle the resource requirements (e.g.,
registers) to increase total thread count and provide a greater opportunity to hide memory
latency. The .minnctapersm directive can be used together with either the .maxntid or
.reqntid directive to trade-off registers-per-thread against multiprocessor utilization without
needed to directly specify a maximum number of registers. This may achieve better performance when
compiling PTX for multiple devices having different numbers of registers per SM.
Currently, the .maxnreg, .maxntid, .reqntid, and .minnctapersm directives may be
applied per-entry and must appear between an .entry directive and its body. The directives take
precedence over any module-level constraints passed to the optimizing backend. A warning message is
generated if the directivesâ constraints are inconsistent or cannot be met for the specified target
device.
A general .pragma directive is supported for passing information to the PTX backend. The
directive passes a list of strings to the backend, and the strings have no semantics within the PTX
virtual machine model. The interpretation of .pragma values is determined by the backend
implementation and is beyond the scope of the PTX ISA. Note that .pragma directives may appear
at module (file) scope, at entry-scope, or as statements within a kernel or device function body.


11.4.1. Performance-Tuning Directives: .maxnregï

.maxnreg
Maximum number of registers that can be allocated per thread.
Syntax

.maxnreg n


Description
Declare the maximum number of registers per thread in a CTA.
Semantics
The compiler guarantees that this limit will not be exceeded. The actual number of registers used
may be less; for example, the backend may be able to compile to fewer registers, or the maximum
number of registers may be further constrained by .maxntid and .maxctapersm.
PTX ISA Notes
Introduced in PTX ISA version 1.3.
Target ISA Notes
Supported on all target architectures.
Examples

.entry foo .maxnreg 16 { ... }  // max regs per thread = 16





11.4.2. Performance-Tuning Directives: .maxntidï

.maxntid
Maximum number of threads in the thread block (CTA).
Syntax

.maxntid nx
.maxntid nx, ny
.maxntid nx, ny, nz


Description
Declare the maximum number of threads in the thread block (CTA). This maximum is specified by giving
the maximum extent of each dimension of the 1D, 2D, or 3D CTA.Â  The maximum number of threads is the
product of the maximum extent in each dimension.
Semantics
The maximum number of threads in the thread block, computed as the product of the maximum extent
specified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in
which this directive appears. Exceeding the maximum number of threads results in a runtime error or
kernel launch failure.
Note that this directive guarantees that the total number of threads does not exceed the maximum,
but does not guarantee that the limit in any particular dimension is not exceeded.
PTX ISA Notes
Introduced in PTX ISA version 1.3.
Target ISA Notes
Supported on all target architectures.
Examples

.entry foo .maxntid 256       { ... }  // max threads = 256
.entry bar .maxntid 16,16,4   { ... }  // max threads = 1024





11.4.3. Performance-Tuning Directives: .reqntidï

.reqntid
Number of threads in the thread block (CTA).
Syntax

.reqntid nx
.reqntid nx, ny
.reqntid nx, ny, nz


Description
Declare the number of threads in the thread block (CTA) by specifying the extent of each dimension
of the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in
each dimension.
Semantics
The size of each CTA dimension specified in any invocation of the kernel is required to be equal to
that specified in this directive. Specifying a different CTA dimension at launch will result in a
runtime error or kernel launch failure.
Notes
The .reqntid directive cannot be used in conjunction with the .maxntid directive.
PTX ISA Notes
Introduced in PTX ISA version 2.1.
Target ISA Notes
Supported on all target architectures.
Examples

.entry foo .reqntid 256       { ... }  // num threads = 256
.entry bar .reqntid 16,16,4   { ... }  // num threads = 1024





11.4.4. Performance-Tuning Directives: .minnctapersmï

.minnctapersm
Minimum number of CTAs per SM.
Syntax

.minnctapersm ncta


Description
Declare the minimum number of CTAs from the kernelâs grid to be mapped to a single multiprocessor
(SM).
Notes
Optimizations based on .minnctapersm need either .maxntid or .reqntid to be specified as
well.
If the total number of threads on a single SM resulting from .minnctapersm and .maxntid /
.reqntid exceed maximum number of threads supported by an SM then directive .minnctapersm
will be ignored.
In PTX ISA version 2.1 or higher, a warning is generated if .minnctapersm is specified without
specifying either .maxntid or .reqntid.
PTX ISA Notes
Introduced in PTX ISA version 2.0 as a replacement for .maxnctapersm.
Target ISA Notes
Supported on all target architectures.
Examples

.entry foo .maxntid 256 .minnctapersm 4 { ... }





11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated)ï

.maxnctapersm
Maximum number of CTAs per SM.
Syntax

.maxnctapersm ncta


Description
Declare the maximum number of CTAs from the kernelâs grid that may be mapped to a single
multiprocessor (SM).
Notes
Optimizations based on .maxnctapersm generally need .maxntid to be specified as well. The
optimizing backend compiler uses .maxntid and .maxnctapersm to compute an upper-bound on
per-thread register usage so that the specified number of CTAs can be mapped to a single
multiprocessor. However, if the number of registers used by the backend is sufficiently lower than
this bound, additional CTAs may be mapped to a single multiprocessor. For this reason,
.maxnctapersm has been renamed to .minnctapersm in PTX ISA version 2.0.
PTX ISA Notes
Introduced in PTX ISA version 1.3. Deprecated in PTX ISA version 2.0.
Target ISA Notes
Supported on all target architectures.
Examples

.entry foo .maxntid 256 .maxnctapersm 4 { ... }





11.4.6. Performance-Tuning Directives: .noreturnï

.noreturn
Indicate that the function does not return to its caller function.
Syntax

.noreturn


Description
Indicate that the function does not return to its caller function.
Semantics
An optional .noreturn directive indicates that the function does not return to caller
function. .noreturn directive can only be specified on device functions and must appear between
a .func directive and its body.
The directive cannot be specified on functions which have return parameters.
If a function with .noreturn directive returns to the caller function at runtime, then the
behavior is undefined.
PTX ISA Notes
Introduced in PTX ISA version 6.4.
Target ISA Notes
Requires sm_30 or higher.
Examples

.func foo .noreturn { ... }





11.4.7. Performance-Tuning Directives: .pragmaï

.pragma
Pass directives to PTX backend compiler.
Syntax

.pragma list-of-strings ;


Description
Pass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler.
The .pragma directive may occur at module-scope, at entry-scope, or at statement-level.
Semantics
The interpretation of .pragma directive strings is implementation-specific and has no impact on
PTX semantics. See Descriptions of .pragma Strings for
descriptions of the pragma strings defined in ptxas.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
Supported on all target architectures.
Examples

.pragma "nounroll";    // disable unrolling in backend

// disable unrolling for current kernel
.entry foo .pragma "nounroll"; { ... }






11.5. Debugging Directivesï

DWARF-format debug information is passed through PTX modules using the following directives:

@@DWARF
.section
.file
.loc

The .section directive was introduced in PTX ISA version 2.0 and replaces the @@DWARF
syntax. The @@DWARF syntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX
ISA version 1.x code.
Beginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the
.target debug platform option. This forward declaration directs PTX compilation to retain
mappings for source-level debugging.


11.5.1. Debugging Directives: @@dwarfï

@@dwarf
DWARF-format information.
Syntax

@@DWARF dwarf-string

dwarf-string may have one of the
.byte   byte-list   // comma-separated hexadecimal byte values
.4byte  int32-list  // comma-separated hexadecimal integers in range [0..2^32-1]
.quad   int64-list  // comma-separated hexadecimal integers in range [0..2^64-1]
.4byte  label
.quad   label


PTX ISA Notes
Introduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by .section
directive.
Target ISA Notes
Supported on all target architectures.
Examples

@@DWARF .section .debug_pubnames, "", @progbits
@@DWARF .byte   0x2b, 0x00, 0x00, 0x00, 0x02, 0x00
@@DWARF .4byte  .debug_info
@@DWARF .4byte  0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63
@@DWARF .4byte  0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172
@@DWARF .byte   0x00, 0x00, 0x00, 0x00, 0x00





11.5.2. Debugging Directives: .sectionï

.section
PTX section definition.
Syntax

.section section_name { dwarf-lines }

dwarf-lines have the following formats:
  .b8    byte-list       // comma-separated list of integers
                         // in range [-128..255]
  .b16   int16-list      // comma-separated list of integers
                         // in range [-2^15..2^16-1]
  .b32   int32-list      // comma-separated list of integers
                         // in range [-2^31..2^32-1]
  label:                 // Define label inside the debug section
  .b64   int64-list      // comma-separated list of integers
                         // in range [-2^63..2^64-1]
  .b32   label
  .b64   label
  .b32   label+imm       // a sum of label address plus a constant integer byte
                         // offset(signed, 32bit)
  .b64   label+imm       // a sum of label address plus a constant integer byte
                         // offset(signed, 64bit)
  .b32   label1-label2   // a difference in label addresses between labels in
                         // the same dwarf section (32bit)
  .b64   label3-label4   // a difference in label addresses between labels in
                         // the same dwarf section (64bit)


PTX ISA Notes
Introduced in PTX ISA version 2.0, replaces @@DWARF syntax.
label+imm expression introduced in PTX ISA version 3.2.
Support for .b16 integers in dwarf-lines introduced in PTX ISA version 6.0.
Support for defining label inside the DWARF section is introduced in PTX ISA version 7.2.
label1-label2 expression introduced in PTX ISA version 7.5.
Negative numbers in dwarf lines introduced in PTX ISA version 7.5.
Target ISA Notes
Supported on all target architectures.
Examples

.section .debug_pubnames
{
    .b32    LpubNames_end0-LpubNames_begin0
  LpubNames_begin0:
    .b8     0x2b, 0x00, 0x00, 0x00, 0x02, 0x00
    .b32    .debug_info
  info_label1:
    .b32    0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63
    .b32    0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172
    .b8     0x00, 0x00, 0x00, 0x00, 0x00
  LpubNames_end0:
}

.section .debug_info
{
    .b32 11430
    .b8 2, 0
    .b32 .debug_abbrev
    .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49
    .b8 0
    .b32 3, 37, 176, -99
    .b32 info_label1
    .b32 .debug_loc+0x4
    .b8 -11, 11, 112, 97
    .b32 info_label1+12
    .b64 -1
    .b16 -5, -65535
}





11.5.3. Debugging Directives: .fileï

.file
Source file name.
Syntax

.file file_index "filename" {, timestamp, file_size}


Description
Associates a source filename with an integer index. .loc directives reference source files by
index.
.file directive allows optionally specifying an unsigned number representing time of last
modification and an unsigned integer representing size in bytes of source file. timestamp and
file_size value can be 0 to indicate this information is not available.
timestamp value is in format of C and C++ data type time_t.
file_size is an unsigned 64-bit integer.
The .file directive is allowed only in the outermost scope, i.e., at the same level as kernel
and device function declarations.
Semantics
If timestamp and file size are not specified, they default to 0.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Timestamp and file size introduced in PTX ISA version 3.2.
Target ISA Notes
Supported on all target architectures.
Examples

.file 1 "example.cu"
.file 2 "kernel.cu"
.file 1 âkernel.cuâ, 1339013327, 64118





11.5.4. Debugging Directives: .locï

.loc
Source file location.
Syntax

.loc file_index line_number column_position
.loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2


Description
Declares the source file location (source file, line number, and column position) to be associated
with lexically subsequent PTX instructions. .loc refers to file_index which is defined by a
.file directive.
To indicate PTX instructions that are generated from a function that got inlined, additional
attribute .inlined_at can be specified as part of the .loc directive. .inlined_at
attribute specifies source location at which the specified function is inlined. file_index2,
line_number2, and column_position2 specify the location at which function is inlined. Source
location specified as part of .inlined_at directive must lexically precede as source location in
.loc directive.
The function_name attribute specifies an offset in the DWARF section named
.debug_str. Offset is specified as label expression or label + immediate expression
where label is defined in .debug_str section. DWARF section .debug_str contains ASCII
null-terminated strings that specify the name of the function that is inlined.
Note that a PTX instruction may have a single associated source location, determined by the nearest
lexically preceding .loc directive, or no associated source location if there is no preceding .loc
directive. Labels in PTX inherit the location of the closest lexically following instruction. A
label with no following PTX instruction has no associated source location.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
function_name and inlined_at attributes are introduced in PTX ISA version 7.2.
Target ISA Notes
Supported on all target architectures.
Examples

    .loc 2 4237 0
L1:                        // line 4237, col 0 of file #2,
                           // inherited from mov
    mov.u32  %r1,%r2;      // line 4237, col 0 of file #2
    add.u32  %r2,%r1,%r3;  // line 4237, col 0 of file #2
...
L2:                        // line 4239, col 5 of file #2,
                           // inherited from sub
    .loc 2 4239 5
    sub.u32  %r2,%r1,%r3;  // line 4239, col 5 of file #2
    .loc 1 21 3
    .loc 1 9 3, function_name info_string0, inlined_at 1 21 3
    ld.global.u32   %r1, [gg]; // Function at line 9
    setp.lt.s32 %p1, %r1, 8;   // inlined at line 21
    .loc 1 27 3
    .loc 1 10 5, function_name info_string1, inlined_at 1 27 3
    .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5
    setp.ne.s32 %p2, %r1, 18;
    @%p2 bra    BB2_3;

    .section .debug_str {
    info_string0:
     .b8 95  // _
     .b8 90  // z
     .b8 51  // 3
     .b8 102 // f
     .b8 111 // o
     .b8 111 // o
     .b8 118 // v
     .b8 0

    info_string1:
     .b8 95  // _
     .b8 90  // z
     .b8 51  // 3
     .b8 98  // b
     .b8 97  // a
     .b8 114 // r
     .b8 118 // v
     .b8 0
     .b8 95  // _
     .b8 90  // z
     .b8 51  // 3
     .b8 99  // c
     .b8 97  // a
     .b8 114 // r
     .b8 118 // v
     .b8 0
    }






11.6. Linking Directivesï


.extern
.visible
.weak



11.6.1. Linking Directives: .externï

.extern
External symbol declaration.
Syntax

.extern identifier


Description
Declares identifier to be defined external to the current module. The module defining such
identifier must define it as .weak or .visible only once in a single object file. Extern
declaration of symbol may appear multiple times and references to that get resolved against the
single definition of that symbol.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

.extern .global .b32 foo;  // foo is defined in another module





11.6.2. Linking Directives: .visibleï

.visible
Visible (externally) symbol declaration.
Syntax

.visible identifier


Description
Declares identifier to be globally visible. Unlike C, where identifiers are globally visible unless
declared static, PTX identifiers are visible only within the current module unless declared
.visible outside the current.
PTX ISA Notes
Introduced in PTX ISA version 1.0.
Target ISA Notes
Supported on all target architectures.
Examples

.visible .global .b32 foo;  // foo will be externally visible





11.6.3. Linking Directives: .weakï

.weak
Visible (externally) symbol declaration.
Syntax

.weak identifier


Description
Declares identifier to be globally visible but weak. Weak symbols are similar to globally visible
symbols, except during linking, weak symbols are only chosen after globally visible symbols during
symbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak
symbol, and references to a symbol get resolved against a weak symbol only if no global symbols have
the same name.
PTX ISA Notes
Introduced in PTX ISA version 3.1.
Target ISA Notes
Supported on all target architectures.
Examples

.weak .func (.reg .b32 val) foo;  // foo will be externally visible





11.6.4. Linking Directives: .commonï

.common
Visible (externally) symbol declaration.
Syntax

.common identifier


Description
Declares identifier to be globally visible but âcommonâ.
Common symbols are similar to globally visible symbols. However multiple object files may declare
the same common symbol and they may have different types and sizes and references to a symbol get
resolved against a common symbol with the largest size.
Only one object file can initialize a common symbol and that must have the largest size among all
other definitions of that common symbol from different object files.
.common linking directive can be used only on variables with .global storage. It cannot be
used on function symbols or on symbols with opaque type.
PTX ISA Notes
Introduced in PTX ISA version 5.0.
Target ISA Notes
.common directive requires sm_20 or higher.
Examples

.common .global .u32 gbl;






11.7. Cluster Dimension Directivesï

The following directives specify information about clusters:

.reqnctapercluster
.explicitcluster
.maxclusterrank

The .reqnctapercluster directive specifies the number of CTAs in the cluster. The
.explicitcluster directive specifies that the kernel should be launched with explicit cluster
details. The .maxclusterrank directive specifies the maximum number of CTAs in the cluster.
The cluster dimension directives can be applied only on kernel functions.


11.7.1. Cluster Dimension Directives: .reqnctaperclusterï

.reqnctapercluster
Declare the number of CTAs in the cluster.
Syntax

.reqnctapercluster nx
.reqnctapercluster nx, ny
.reqnctapercluster nx, ny, nz


Description
Set the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of
the 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each
dimension. For kernels with .reqnctapercluster directive specified, runtime will use the
specified values for configuring the launch if the same are not specified at launch time.
Semantics
If cluster dimension is explicitly specified at launch time, it should be equal to the values
specified in this directive. Specifying a different cluster dimension at launch will result in a
runtime error or kernel launch failure.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.entry foo .reqnctapercluster 2         { . . . }
.entry bar .reqnctapercluster 2, 2, 1   { . . . }
.entry ker .reqnctapercluster 3, 2      { . . . }





11.7.2. Cluster Dimension Directives: .explicitclusterï

.explicitcluster
Declare that Kernel must be launched with cluster dimensions explicitly specified.
Syntax

.explicitcluster


Description
Declares that this Kernel should be launched with cluster dimension explicitly specified.
Semantics
Kernels with .explicitcluster directive must be launched with cluster dimension explicitly
specified (either at launch time or via .reqnctapercluster), otherwise program will fail with
runtime error or kernel launch failure.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.entry foo .explicitcluster         { . . . }





11.7.3. Cluster Dimension Directives: .maxclusterrankï

.maxclusterrank
Declare the maximum number of CTAs that can be part of the cluster.
Syntax

.maxclusterrank n


Description
Declare the maximum number of thread blocks (CTAs) allowed to be part of the cluster.
Semantics
Product of the number of CTAs in each cluster dimension specified in any invocation of the kernel is
required to be less or equal to that specified in this directive. Otherwise invocation will result
in a runtime error or kernel launch failure.
The .maxclusterrank directive cannot be used in conjunction with the .reqnctapercluster directive.
PTX ISA Notes
Introduced in PTX ISA version 7.8.
Target ISA Notes
Requires sm_90 or higher.
Examples

.entry foo ..maxclusterrank 8         { . . . }







12. Release Notesï

This section describes the history of change in the PTX ISA and implementation. The first section
describes ISA and implementation changes in the current release of PTX ISA version 8.5, and the
remaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX
ISA version 2.0.
Table 32 shows the PTX release history.


Table 32 PTX Release Historyï








PTX ISA Version
CUDA Release
Supported Targets




PTX ISA 1.0
CUDA 1.0
sm_{10,11}


PTX ISA 1.1
CUDA 1.1
sm_{10,11}


PTX ISA 1.2
CUDA 2.0
sm_{10,11,12,13}


PTX ISA 1.3
CUDA 2.1
sm_{10,11,12,13}


PTX ISA 1.4
CUDA 2.2
sm_{10,11,12,13}


PTX ISA 1.5
driver r190
sm_{10,11,12,13}


PTX ISA 2.0
CUDA 3.0, driver r195
sm_{10,11,12,13}, sm_20


PTX ISA 2.1
CUDA 3.1, driver r256
sm_{10,11,12,13}, sm_20


PTX ISA 2.2
CUDA 3.2, driver r260
sm_{10,11,12,13}, sm_20


PTX ISA 2.3
CUDA 4.0, driver r270
sm_{10,11,12,13}, sm_20


PTX ISA 3.0
CUDA 4.1, driver r285
sm_{10,11,12,13}, sm_20


CUDA 4.2, driver r295
sm_{10,11,12,13}, sm_20, sm_30


PTX ISA 3.1
CUDA 5.0, driver r302
sm_{10,11,12,13}, sm_20, sm_{30,35}


PTX ISA 3.2
CUDA 5.5, driver r319
sm_{10,11,12,13}, sm_20, sm_{30,35}


PTX ISA 4.0
CUDA 6.0, driver r331
sm_{10,11,12,13}, sm_20, sm_{30,32,35}, sm_50


PTX ISA 4.1
CUDA 6.5, driver r340
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52}


PTX ISA 4.2
CUDA 7.0, driver r346
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53}


PTX ISA 4.3
CUDA 7.5, driver r352
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53}


PTX ISA 5.0
CUDA 8.0, driver r361
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}


PTX ISA 6.0
CUDA 9.0, driver r384
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_70


PTX ISA 6.1
CUDA 9.1, driver r387
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_70, sm_72


PTX ISA 6.2
CUDA 9.2, driver r396
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_70, sm_72


PTX ISA 6.3
CUDA 10.0, driver r400
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_70, sm_72, sm_75


PTX ISA 6.4
CUDA 10.1, driver r418
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_70, sm_72, sm_75


PTX ISA 6.5
CUDA 10.2, driver r440
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_70, sm_72, sm_75


PTX ISA 7.0
CUDA 11.0, driver r445
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_80


PTX ISA 7.1
CUDA 11.1, driver r455
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86}


PTX ISA 7.2
CUDA 11.2, driver r460
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86}


PTX ISA 7.3
CUDA 11.3, driver r465
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86}


PTX ISA 7.4
CUDA 11.4, driver r470
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87}


PTX ISA 7.5
CUDA 11.5, driver r495
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87}


PTX ISA 7.6
CUDA 11.6, driver r510
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87}


PTX ISA 7.7
CUDA 11.7, driver r515
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87}


PTX ISA 7.8
CUDA 11.8, driver r520
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_90


PTX ISA 8.0
CUDA 12.0, driver r525
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_{90,90a}


PTX ISA 8.1
CUDA 12.1, driver r530
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_{90,90a}


PTX ISA 8.2
CUDA 12.2, driver r535
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_{90,90a}


PTX ISA 8.3
CUDA 12.3, driver r545
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_{90,90a}


PTX ISA 8.4
CUDA 12.4, driver r550
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_{90,90a}


PTX ISA 8.5
CUDA 12.5, driver r555
sm_{10,11,12,13}, sm_20, sm_{30,32,35,37}, sm_{50,52,53},
sm_{60,61,62}, sm_{70,72,75}, sm_{80,86,87,89}, sm_{90,90a}





12.1. Changes in PTX ISA Version 8.5ï

New Features
PTX ISA version 8.5 introduces the following new features:

Adds support for mma.sp::ordered_metadata instruction.

Semantic Changes and Clarifications

Values 0b0000, 0b0101, 0b1010, 0b1111 for sparsity metadata (operand e)
of instruction mma.sp are invalid and their usage results in undefined behavior.




12.2. Changes in PTX ISA Version 8.4ï

New Features
PTX ISA version 8.4 introduces the following new features:

Extends ld, st and atom instructions with .b128 type to support .sys scope.
Extends integer wgmma.mma_async instruction to support .u8.s8 and .s8.u8 as .atype
and .btype respectively.
Extends mma, mma.sp instructions to support FP8 types .e4m3 and .e5m2.

Semantic Changes and Clarifications
None.



12.3. Changes in PTX ISA Version 8.3ï

New Features
PTX ISA version 8.3 introduces the following new features:

Adds support for pragma used_bytes_mask that is used to specify mask for used bytes for a load operation.
Extends isspacep, cvta.to, ld and st instructions to accept ::entry and ::func
sub-qualifiers with .param state space qualifier.
Adds support for .b128 type on instructions ld, ld.global.nc, ldu, st, mov and atom.
Add support for instructions tensormap.replace, tensormap.cp_fenceproxy and support for qualifier
.to_proxykind::from_proxykind on instruction fence.proxy to support modifying tensor-map.

Semantic Changes and Clarifications
None.



12.4. Changes in PTX ISA Version 8.2ï

New Features
PTX ISA version 8.2 introduces the following new features:

Adds support for .mmio qualifier on ld and st instructions.
Extends lop3 instruction to allow predicate destination.
Extends multimem.ld_reduce instruction to support .acc::f32 qualifer to allow .f32
precision of the intermediate accumulation.
Extends the asynchronous warpgroup-level matrix multiply-and-accumulate operation
wgmma.mma_async to support .sp modifier that allows matrix multiply-accumulate operation
when input matrix A is sparse.

Semantic Changes and Clarifications
The .multicast::cluster qualifier on cp.async.bulk and cp.async.bulk.tensor instructions
is optimized for target architecture sm_90a and may have substantially reduced performance on
other targets and hence .multicast::cluster is advised to be used with sm_90a.



12.5. Changes in PTX ISA Version 8.1ï

New Features
PTX ISA version 8.1 introduces the following new features:

Adds support for st.async and red.async instructions for asynchronous store and
asynchronous reduction operations respectively on shared memory.
Adds support for .oob modifier on half-precision fma instruction.
Adds support for .satfinite saturation modifer on cvt instruction for .f16, .bf16
and .tf32 formats.
Extends support for cvt with .e4m3/.e5m2 to sm_89.
Extends atom and red instructions to support vector types.
Adds support for special register %aggr_smem_size.
Extends sured instruction with 64-bit min/max operations.
Adds support for increased kernel parameter size of 32764 bytes.
Adds support for multimem addresses in memory consistency model.
Adds support for multimem.ld_reduce, multimem.st and multimem.red instructions to
perform memory operations on multimem addresses.

Semantic Changes and Clarifications
None.



12.6. Changes in PTX ISA Version 8.0ï

New Features
PTX ISA version 8.0 introduces the following new features:

Adds support for target sm_90a that supports specialized accelerated features.
Adds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma.
Extends the asynchronous copy operations with bulk operations that operate on large data,
including tensor data.
Introduces packed integer types .u16x2 and .s16x2.
Extends integer arithmetic instruction add to allow packed integer types .u16x2 and .s16x2.
Extends integer arithmetic instructions min and max to allow packed integer types
.u16x2 and .s16x2, as well as saturation modifier .relu on .s16x2 and .s32
types.
Adds support for special register %current_graph_exec that identifies the currently executing
CUDA device graph.
Adds support for elect.sync instruction.
Adds support for .unified attribute on functions and variables.
Adds support for setmaxnreg instruction.
Adds support for .sem qualifier on barrier.cluster instruction.
Extends the fence instruction to allow opcode-specific synchronizaion using op_restrict
qualifier.
Adds support for .cluster scope on mbarrier.arrive, mbarrier.arrive_drop,
mbarrier.test_wait and mbarrier.try_wait operations.
Adds support for transaction count operations on mbarrier objects, specified with
.expect_tx and .complete_tx qualifiers.

Semantic Changes and Clarifications
None.



12.7. Changes in PTX ISA Version 7.8ï

New Features
PTX ISA version 7.8 introduces the following new features:

Adds support for sm_89 target architecture.
Adds support for sm_90 target architecture.
Extends bar and barrier instructions to accept optional scope qualifier .cta.
Extends .shared state space qualifier with optional sub-qualifier ::cta.
Adds support for movmatrix instruction which transposes a matrix in registers across a warp.
Adds support for stmatrix instruction which stores one or more matrices to shared memory.
Extends the .f64 floating point type mma operation with shapes .m16n8k4, .m16n8k8,
and .m16n8k16.
Extends add, sub, mul, set, setp, cvt, tanh, ex2, atom and
red instructions with bf16 alternate floating point data format.
Adds support for new alternate floating-point data formats .e4m3 and .e5m2.
Extends cvt instruction to convert .e4m3 and .e5m2 alternate floating point data formats.
Adds support for griddepcontrol instruction as a communication mechanism to control the
execution of dependent grids.
Extends mbarrier instruction to allow a new phase completion check operation try_wait.
Adds support for new thread scope .cluster which is a set of Cooperative Thread Arrays (CTAs).
Extends fence/membar, ld, st, atom, and red instructions to accept
.cluster scope.
Adds support for extended visibility of shared state space to all threads within a cluster.
Extends .shared state space qualifier with ::cluster sub-qualifier for cluster-level
visibility of shared memory.
Extends isspacep, cvta, ld, st, atom, and red instructions to accept
::cluster sub-qualifier with .shared state space qualifier.
Adds support for mapa instruction to map a shared memory address to the corresponding address
in a different CTA within the cluster.
Adds support for getctarank instruction to query the rank of the CTA that contains a given
address.
Adds support for new barrier synchronization instruction barrier.cluster.
Extends the memory consistency model to include the new cluster scope.
Adds support for special registers related to cluster information: %is_explicit_cluster,
%clusterid, %nclusterid, %cluster_ctaid, %cluster_nctaid, %cluster_ctarank,
%cluster_nctarank.
Adds support for cluster dimension directives .reqnctapercluster, .explicitcluster, and
.maxclusterrank.

Semantic Changes and Clarifications
None.



12.8. Changes in PTX ISA Version 7.7ï

New Features
PTX ISA version 7.7 introduces the following new features:

Extends isspacep and cvta instructions to include the .param state space for kernel
function parameters.

Semantic Changes and Clarifications
None.



12.9. Changes in PTX ISA Version 7.6ï

New Features
PTX ISA version 7.6 introduces the following new features:

Support for szext instruction which performs sign-extension or zero-extension on a specified
value.
Support for bmsk instruction which creates a bitmask of the specified width starting at the
specified bit position.
Support for special registers %reserved_smem_offset_begin, %reserved_smem_offset_end,
%reserved_smem_offset_cap, %reserved_smem_offset<2>.

Semantic Changes and Clarifications
None.



12.10. Changes in PTX ISA Version 7.5ï

New Features
PTX ISA version 7.5 introduces the following new features:

Debug information enhancements to support label difference and negative values in the .section
debugging directive.
Support for ignore-src operand on cp.async instruction.

Extensions to the memory consistency model to introduce the following new concepts:



A memory proxy as an abstract label for different methods of memory access.
Virtual aliases as distinct memory addresses accessing the same physical memory location.




Support for new fence.proxy and membar.proxy instructions to allow synchronization of
memory accesses performed via virtual aliases.

Semantic Changes and Clarifications
None.



12.11. Changes in PTX ISA Version 7.4ï

New Features
PTX ISA version 7.4 introduces the following new features:

Support for sm_87 target architecture.
Support for .level::eviction_priority qualifier which allows specifying cache eviction
priority hints on ld, ld.global.nc, st, and prefetch instructions.
Support for .level::prefetch_size qualifier which allows specifying data prefetch hints on
ld and cp.async instructions.
Support for createpolicy instruction which allows construction of different types of cache
eviction policies.
Support for .level::cache_hint qualifier which allows the use of cache eviction policies with
ld, ld.global.nc, st, atom, red and cp.async instructions.
Support for applypriority and discard operations on cached data.

Semantic Changes and Clarifications
None.



12.12. Changes in PTX ISA Version 7.3ï

New Features
PTX ISA version 7.3 introduces the following new features:

Extends mask() operator used in initializers to also support integer constant expression.
Adds support for stack manpulation instructions that allow manipulating stack using stacksave
and stackrestore instructions and allocation of per-thread stack using alloca
instruction.

Semantic Changes and Clarifications
The unimplemented version of alloca from the older PTX ISA specification has been replaced with
new stack manipulation instructions in PTX ISA version 7.3.



12.13. Changes in PTX ISA Version 7.2ï

New Features
PTX ISA version 7.2 introduces the following new features:

Enhances .loc directive to represent inline function information.
Adds support to define labels inside the debug sections.
Extends min and max instructions to support .xorsign and .abs modifiers.

Semantic Changes and Clarifications
None.



12.14. Changes in PTX ISA Version 7.1ï

New Features
PTX ISA version 7.1 introduces the following new features:

Support for sm_86 target architecture.
Adds a new operator, mask(), to extract a specific byte from variableâs address used in
initializers.
Extends tex and tld4 instructions to return an optional predicate that indicates if data
at specified coordinates is resident in memory.
Extends single-bit wmma and mma instructions to support .and operation.
Extends mma instruction to support .sp modifier that allows matrix multiply-accumulate
operation when input matrix A is sparse.
Extends mbarrier.test_wait instruction to test the completion of specific phase parity.

Semantic Changes and Clarifications
None.



12.15. Changes in PTX ISA Version 7.0ï

New Features
PTX ISA version 7.0 introduces the following new features:

Support for sm_80 target architecture.
Adds support for asynchronous copy instructions that allow copying of data asynchronously from one
state space to another.
Adds support for mbarrier instructions that allow creation of mbarrier objects in memory and
use of these objects to synchronize threads and asynchronous copy operations initiated by threads.
Adds support for redux.sync instruction which allows reduction operation across threads in a
warp.
Adds support for new alternate floating-point data formats .bf16 and .tf32.
Extends wmma instruction to support .f64 type with shape .m8n8k4.
Extends wmma instruction to support .bf16 data format.
Extends wmma instruction to support .tf32 data format with shape .m16n16k8.
Extends mma instruction to support .f64 type with shape .m8n8k4.
Extends mma instruction to support .bf16 and .tf32 data formats with shape
.m16n8k8.
Extends mma instruction to support new shapes .m8n8k128, .m16n8k4, .m16n8k16,
.m16n8k32, .m16n8k64, .m16n8k128 and .m16n8k256.
Extends abs and neg instructions to support .bf16 and .bf16x2 data formats.
Extends min and max instructions to support .NaN modifier and .f16, .f16x2,
.bf16 and .bf16x2 data formats.
Extends fma instruction to support .relu saturation mode and .bf16 and .bf16x2
data formats.
Extends cvt instruction to support .relu saturation mode and .f16, .f16x2,
.bf16, .bf16x2 and .tf32 destination formats.
Adds support for tanh instruction that computes hyperbolic-tangent.
Extends ex2 instruction to support .f16 and .f16x2 types.

Semantic Changes and Clarifications
None.



12.16. Changes in PTX ISA Version 6.5ï

New Features
PTX ISA version 6.5 introduces the following new features:

Adds support for integer destination types for half precision comparison instruction set.
Extends abs instruction to support .f16 and .f16x2 types.
Adds support for cvt.pack instruction which allows converting two integer values and packing
the results together.
Adds new shapes .m16n8k8, .m8n8k16 and .m8n8k32 on the mma instruction.
Adds support for ldmatrix instruction which loads one or more matrices from shared memory for
mma instruction.

Removed Features
PTX ISA version 6.5 removes the following features:

Support for .satfinite qualifier on floating point wmma.mma instruction has been
removed. This support was deprecated since PTX ISA version 6.4.

Semantic Changes and Clarifications
None.



12.17. Changes in PTX ISA Version 6.4ï

New Features
PTX ISA version 6.4 introduces the following new features:

Adds support for .noreturn directive which can be used to indicate a function does not return
to itâs caller function.
Adds support for mma instruction which allows performing matrix multiply-and-accumulate
operation.

Deprecated Features
PTX ISA version 6.4 deprecates the following features:

Support for .satfinite qualifier on floating point wmma.mma instruction.

Removed Features
PTX ISA version 6.4 removes the following features:

Support for shfl and vote instructions without the .sync qualifier has been removed
for .targetsm_70 and higher. This support was deprecated since PTX ISA version 6.0 as
documented in PTX ISA version 6.2.

Semantic Changes and Clarifications

Clarified that resolving references of a .weak symbol considers only .weak or .visible
symbols with the same name and does not consider local symbols with the same name.
Clarified that in cvt instruction, modifier .ftz can only be specified when either
.atype or .dtype is .f32.




12.18. Changes in PTX ISA Version 6.3ï

New Features
PTX ISA version 6.3 introduces the following new features:

Support for sm_75 target architecture.
Adds support for a new instruction nanosleep that suspends a thread for a specified duration.
Adds support for .alias directive which allows definining alias to function symbol.
Extends atom instruction to perform .f16 addition operation and .cas.b16 operation.
Extends red instruction to perform .f16 addition operation.
The wmma instructions are extended to support multiplicand matrices of type .s8, .u8,
.s4, .u4, .b1 and accumulator matrices of type .s32.

Semantic Changes and Clarifications

Introduced the mandatory .aligned qualifier for all wmma instructions.
Specified the alignment required for the base address and stride parameters passed to
wmma.load and wmma.store.
Clarified that layout of fragment returned by wmma operation is architecture dependent and
passing wmma fragments around functions compiled for different link compatible SM
architectures may not work as expected.
Clarified that atomicity for {atom/red}.f16x2} operations is guranteed separately for each of
the two .f16 elements but not guranteed to be atomic as single 32-bit access.




12.19. Changes in PTX ISA Version 6.2ï

New Features
PTX ISA version 6.2 introduces the following new features:

A new instruction activemask for querying active threads in a warp.
Extends atomic and reduction instructions to perform .f16x2 addition operation with mandatory
.noftz qualifier.

Deprecated Features
PTX ISA version 6.2 deprecates the following features:

The use of shfl and vote instructions without the .sync is deprecated retrospectively
from PTX ISA version 6.0, which introduced the sm_70 architecture that implements
Independent Thread Scheduling.

Semantic Changes and Clarifications

Clarified that wmma instructions can be used in conditionally executed code only if it is
known that all threads in the warp evaluate the condition identically, otherwise behavior is
undefined.
In the memory consistency model, the definition of morally strong operations was updated to
exclude fences from the requirement of complete overlap since fences do not access memory.




12.20. Changes in PTX ISA Version 6.1ï

New Features
PTX ISA version 6.1 introduces the following new features:

Support for sm_72 target architecture.
Support for new matrix shapes 32x8x16 and 8x32x16 in wmma instruction.

Semantic Changes and Clarifications
None.



12.21. Changes in PTX ISA Version 6.0ï

New Features
PTX ISA version 6.0 introduces the following new features:

Support for sm_70 target architecture.
Specifies the memory consistency model for programs running on sm_70 and later architectures.
Various extensions to memory instructions to specify memory synchronization semantics and scopes
at which such synchronization can be observed.
New instruction wmma for matrix operations which allows loading matrices from memory,
performing multiply-and-accumulate on them and storing result in memory.
Support for new barrier instruction.
Extends neg instruction to support .f16 and .f16x2 types.
A new instruction fns which allows finding n-th set bit in integer.
A new instruction bar.warp.sync which allows synchronizing threads in warp.
Extends vote and shfl instructions with .sync modifier which waits for specified
threads before executing the vote and shfl operation respectively.
A new instruction match.sync which allows broadcasting and comparing a value across threads in
warp.
A new instruction brx.idx which allows branching to a label indexed from list of potential
targets.
Support for unsized array parameter for .func which can be used to implement variadic
functions.
Support for .b16 integer type in dwarf-lines.
Support for taking address of device function return parameters using mov instruction.

Semantic Changes and Clarifications

Semantics of bar instruction were updated to indicate that executing thread waits for other
non-exited threads from itâs warp.
Support for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from
the spec.
Support for taking address of labels, using labels in initializers which was unimplemented has
been removed from the spec.
Support for variadic functions which was unimplemented has been removed from the spec.




12.22. Changes in PTX ISA Version 5.0ï

New Features
PTX ISA version 5.0 introduces the following new features:

Support for sm_60, sm_61, sm_62 target architecture.
Extends atomic and reduction instructions to perform double-precision add operation.
Extends atomic and reduction instructions to specify scope modifier.
A new .common directive to permit linking multiple object files containing declarations of the
same symbol with different size.
A new dp4a instruction which allows 4-way dot product with accumulate operation.
A new dp2a instruction which allows 2-way dot product with accumulate operation.
Support for special register %clock_hi.

Semantic Changes and Clarifications
Semantics of cache modifiers on ld and st instructions were clarified to reflect cache
operations are treated as performance hint only and do not change memory consistency behavior of the
program.
Semantics of volatile operations on ld and st instructions were clarified to reflect how
volatile operations are handled by optimizing compiler.



12.23. Changes in PTX ISA Version 4.3ï

New Features
PTX ISA version 4.3 introduces the following new features:

A new lop3 instruction which allows arbitrary logical operation on 3 inputs.
Adds support for 64-bit computations in extended precision arithmetic instructions.
Extends tex.grad instruction to support cube and acube geometries.
Extends tld4 instruction to support a2d, cube and acube geometries.
Extends tex and tld4 instructions to support optional operands for offset vector and depth
compare.
Extends txq instruction to support querying texture fields from specific LOD.

Semantic Changes and Clarifications
None.



12.24. Changes in PTX ISA Version 4.2ï

New Features
PTX ISA version 4.2 introduces the following new features:

Support for sm_53 target architecture.
Support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types.
Support for memory_layout field for surfaces and suq instruction support for querying this
field.

Semantic Changes and Clarifications
Semantics for parameter passing under ABI were updated to indicate ld.param and st.param
instructions used for argument passing cannot be predicated.
Semantics of {atom/red}.add.f32 were updated to indicate subnormal inputs and results are
flushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on
shared memory preserve subnormal inputs and results and donât flush them to zero.



12.25. Changes in PTX ISA Version 4.1ï

New Features
PTX ISA version 4.1 introduces the following new features:

Support for sm_37 and sm_52 target architectures.
Support for new fields array_size, num_mipmap_levels and num_samples for Textures, and
the txq instruction support for querying these fields.
Support for new field array_size for Surfaces, and the suq instruction support for
querying this field.
Support for special registers %total_smem_size and %dynamic_smem_size.

Semantic Changes and Clarifications
None.



12.26. Changes in PTX ISA Version 4.0ï

New Features
PTX ISA version 4.0 introduces the following new features:

Support for sm_32 and sm_50 target architectures.
Support for 64bit performance counter special registers %pm0_64,..,%pm7_64.
A new istypep instruction.
A new instruction, rsqrt.approx.ftz.f64 has been added to compute a fast approximation of the
square root reciprocal of a value.
Support for a new directive .attribute for specifying special attributes of a variable.
Support for .managed variable attribute.

Semantic Changes and Clarifications
The vote instruction semantics were updated to clearly indicate that an inactive thread in a
warp contributes a 0 for its entry when participating in vote.ballot.b32.



12.27. Changes in PTX ISA Version 3.2ï

New Features
PTX ISA version 3.2 introduces the following new features:

The texture instruction supports reads from multi-sample and multisample array textures.
Extends .section debugging directive to include label + immediate expressions.
Extends .file directive to include timestamp and file size information.

Semantic Changes and Clarifications
The vavrg2 and vavrg4 instruction semantics were updated to indicate that instruction adds 1
only if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than
being divided by 2).



12.28. Changes in PTX ISA Version 3.1ï

New Features
PTX ISA version 3.1 introduces the following new features:

Support for sm_35 target architecture.
Support for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work.
ld.global.nc for loading read-only global data though the non-coherent texture cache.
A new funnel shift instruction, shf.
Extends atomic and reduction instructions to perform 64-bit {and, or, xor} operations, and
64-bit integer {min, max} operations.
Adds support for mipmaps.
Adds support for indirect access to textures and surfaces.
Extends support for generic addressing to include the .const state space, and adds a new
operator, generic(), to form a generic address for .global or .const variables used in
initializers.
A new .weak directive to permit linking multiple object files containing declarations of the
same symbol.

Semantic Changes and Clarifications
PTX 3.1 redefines the default addressing for global variables in initializers, from generic
addresses to offsets in the global state space. Legacy PTX code is treated as having an implicit
generic() operator for each global variable used in an initializer. PTX 3.1 code should either
include explicit generic() operators in initializers, use cvta.global to form generic
addresses at runtime, or load from the non-generic address using ld.global.
Instruction mad.f32 requires a rounding modifier for sm_20 and higher targets. However for
PTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently
defaults to mad.rn.f32. For PTX ISA version 3.1, ptxas generates a warning and defaults to
mad.rn.f32, and in subsequent releases ptxas will enforce the requirement for PTX ISA version
3.2 and later.



12.29. Changes in PTX ISA Version 3.0ï

New Features
PTX ISA version 3.0 introduces the following new features:

Support for sm_30 target architectures.
SIMD video instructions.
A new warp shuffle instruction.
Instructions mad.cc and madc for efficient, extended-precision integer multiplication.
Surface instructions with 3D and array geometries.
The texture instruction supports reads from cubemap and cubemap array textures.
Platform option .target debug to declare that a PTX module contains DWARF debug information.
pmevent.mask, for triggering multiple performance monitor events.
Performance monitor counter special registers %pm4..%pm7.

Semantic Changes and Clarifications
Special register %gridid has been extended from 32-bits to 64-bits.
PTX ISA version 3.0 deprecates module-scoped .reg and .local variables when compiling to the
Application Binary Interface (ABI). When compiling without use of the ABI, module-scoped .reg
and .local variables are supported as before. When compiling legacy PTX code (ISA versions prior
to 3.0) containing module-scoped .reg or .local variables, the compiler silently disables
use of the ABI.
The shfl instruction semantics were updated to clearly indicate that value of source operand
a is unpredictable for inactive and predicated-off threads within the warp.
PTX modules no longer allow duplicate .version directives. This feature was unimplemented, so
there is no semantic change.
Unimplemented instructions suld.p and sust.p.{u32,s32,f32} have been removed.



12.30. Changes in PTX ISA Version 2.3ï

New Features
PTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D
or 2D textures, where an integer indexes into the array of textures, and then one or two
single-precision floating point coordinates are used to address within the selected 1D or 2D
texture.
PTX 2.3 adds a new directive, .address_size, for specifying the size of addresses.
Variables in .const and .global state spaces are initialized to zero by default.
Semantic Changes and Clarifications
The semantics of the .maxntid directive have been updated to match the current
implementation. Specifically, .maxntid only guarantees that the total number of threads in a
thread block does not exceed the maximum. Previously, the semantics indicated that the maximum was
enforced separately in each dimension, which is not the case.
Bit field extract and insert instructions BFE and BFI now indicate that the len and pos
operands are restricted to the value range 0..255.
Unimplemented instructions {atom,red}.{min,max}.f32 have been removed.



12.31. Changes in PTX ISA Version 2.2ï

New Features
PTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a
new directives for specifying that a kernel parameter is a pointer, for specifying to which state
space the parameter points, and for optionally specifying the alignment of the memory to which the
parameter points.
PTX 2.2 adds a new field named force_unnormalized_coords to the .samplerref opaque
type. This field is used in the independent texturing mode to override the normalized_coords
field in the texture header. This field is needed to support languages such as OpenCL, which
represent the property of normalized/unnormalized coordinates in the sampler header rather than in
the texture header.
PTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the
.const state space. Legacy PTX that uses explicit constant banks is still supported.
PTX 2.2 adds a new tld4 instruction for loading a component (r, g, b, or a) from
the four texels compising the bilinear interpolation footprint of a given texture location. This
instruction may be used to compute higher-precision bilerp results in software, or for performing
higher-bandwidth texture loads.
Semantic Changes and Clarifications
None.



12.32. Changes in PTX ISA Version 2.1ï

New Features
The underlying, stack-based ABI is supported in PTX ISA version 2.1 for sm_2x targets.
Support for indirect calls has been implemented for sm_2x targets.
New directives, .branchtargets and .calltargets, have been added for specifying potential
targets for indirect branches and indirect function calls. A .callprototype directive has been
added for declaring the type signatures for indirect function calls.
The names of .global and .const variables can now be specified in variable initializers to
represent their addresses.
A set of thirty-two driver-specific execution environment special registers has been added. These
are named %envreg0..%envreg31.
Textures and surfaces have new fields for channel data type and channel order, and the txq and
suq instructions support queries for these fields.
Directive .minnctapersm has replaced the .maxnctapersm directive.
Directive .reqntid has been added to allow specification of exact CTA dimensions.
A new instruction, rcp.approx.ftz.f64, has been added to compute a fast, gross approximate
reciprocal.
Semantic Changes and Clarifications
A warning is emitted if .minnctapersm is specified without also specifying .maxntid.



12.33. Changes in PTX ISA Version 2.0ï

New Features
Floating Point Extensions
This section describes the floating-point changes in PTX ISA version 2.0 for sm_20 targets. The
goal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility
with legacy PTX ISA version 1.x code and sm_1x targets.
The changes from PTX ISA version 1.x are as follows:

Single-precision instructions support subnormal numbers by default for sm_20 targets. The
.ftz modifier may be used to enforce backward compatibility with sm_1x.
Single-precision add, sub, and mul now support .rm and .rp rounding modifiers
for sm_20 targets.
A single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754
compliant rounding modifiers and support for subnormal numbers. The fma.f32 instruction also
supports .ftz and .sat modifiers. fma.f32 requires sm_20. The mad.f32
instruction has been extended with rounding modifiers so that itâs synonymous with fma.f32
for sm_20 targets. Both fma.f32 and mad.f32 require a rounding modifier for sm_20
targets.
The mad.f32 instruction without rounding is retained so that compilers can generate code for
sm_1x targets. When code compiled for sm_1x is executed on sm_20 devices, mad.f32
maps to fma.rn.f32.
Single- and double-precision div, rcp, and sqrt with IEEE 754 compliant rounding have
been added. These are indicated by the use of a rounding modifier and require sm_20.
Instructions testp and copysign have been added.

New Instructions
A load uniform instruction, ldu, has been added.
Surface instructions support additional .clamp modifiers, .clamp and .zero.
Instruction sust now supports formatted surface stores.
A count leading zeros instruction, clz, has been added.
A find leading non-sign bit instruction, bfind, has been added.
A bit reversal instruction, brev, has been added.
Bit field extract and insert instructions, bfe and bfi, have been added.
A population count instruction, popc, has been added.
A vote ballot instruction, vote.ballot.b32, has been added.
Instructions {atom,red}.add.f32 have been implemented.
Instructions {atom,red}.shared have been extended to handle 64-bit data types for sm_20
targets.
A system-level membar instruction, membar.sys, has been added.
The bar instruction has been extended as follows:

A bar.arrive instruction has been added.
Instructions bar.red.popc.u32 and bar.red.{and,or}.pred have been added.
bar now supports optional thread count and register operands.

Scalar video instructions (includes prmt) have been added.
Instruction isspacep for querying whether a generic address falls within a specified state space
window has been added.
Instruction cvta for converting global, local, and shared addresses to generic address and
vice-versa has been added.
Other New Features
Instructions ld, ldu, st, prefetch, prefetchu, isspacep, cvta, atom,
and red now support generic addressing.
New special registers %nwarpid, %nsmid, %clock64, %lanemask_{eq,le,lt,ge,gt} have
been added.
Cache operations have been added to instructions ld, st, suld, and sust, e.g., for
prefetching to specified level of memory hierarchy. Instructions prefetch and prefetchu
have also been added.
The .maxnctapersm directive was deprecated and replaced with .minnctapersm to better match
its behavior and usage.
A new directive, .section, has been added to replace the @@DWARF syntax for passing
DWARF-format debugging information through PTX.
A new directive, .pragma nounroll, has been added to allow users to disable loop unrolling.
Semantic Changes and Clarifications
The errata in cvt.ftz for PTX ISA versions 1.4 and earlier, where single-precision subnormal
inputs and results were not flushed to zero if either source or destination type size was 64-bits,
has been fixed. In PTX ISA version 1.5 and later, cvt.ftz (and cvt for .target sm_1x,
where .ftz is implied) instructions flush single-precision subnormal inputs and results to
sign-preserving zero for all combinations of floating-point instruction types. To maintain
compatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs
and results are flushed to sign-preserving zero only when neither source nor destination type size
is 64-bits.
Components of special registers %tid, %ntid, %ctaid, and %nctaid have been extended
from 16-bits to 32-bits. These registers now have type .v4.u32.
The number of samplers available in independent texturing mode was incorrectly listed as thirty-two
in PTX ISA version 1.5; the correct number is sixteen.




14. Descriptions of .pragma Stringsï

This section describes the .pragma strings defined by ptxas.


14.1. Pragma Strings: ânounrollâï

ânounrollâ
Disable loop unrolling in optimizing the backend compiler.
Syntax

.pragma "nounroll";


Description
The "nounroll" pragma is a directive to disable loop unrolling in the optimizing backend
compiler.
The "nounroll" pragma is allowed at module, entry-function, and statement levels, with the
following meanings:

module scope

disables unrolling for all loops in module, including loops preceding the .pragma.

entry-function scope

disables unrolling for all loops in the entry function body.

statement-level pragma

disables unrolling of the loop for which the current block is the loop header.


Note that in order to have the desired effect at statement level, the "nounroll" directive must
appear before any instruction statements in the loop header basic block for the desired loop. The
loop header block is defined as the block that dominates all blocks in the loop body and is the
target of the loop backedge. Statement-level "nounroll" directives appearing outside of loop
header blocks are silently ignored.
PTX ISA Notes
Introduced in PTX ISA version 2.0.
Target ISA Notes
Requires sm_20 or higher. Ignored for sm_1x targets.
Examples

.entry foo (...)
.pragma "nounroll";  // do not unroll any loop in this function
{
...
}

.func bar (...)
{
...
L1_head:
     .pragma "nounroll";  // do not unroll this loop
     ...
@p   bra L1_end;
L1_body:
     ...
L1_continue:
     bra L1_head;
L1_end:
     ...
}





14.2. Pragma Strings: âused_bytes_maskâï

âused_bytes_maskâ
Mask for indicating used bytes in data of ld operation.
Syntax

.pragma "used_bytes_mask mask";


Description
The "used_bytes_mask" pragma is a directive that specifies used bytes in a load
operation based on the mask provided.
"used_bytes_mask" pragma needs to be specified prior to a load instruction for which
information about bytes used from the load operation is needed.
Pragma is ignored if instruction following it is not a load instruction.
For a load instruction without this pragma, all bytes from the load operation are assumed
to be used.
Operand mask is a 32-bit integer with set bits indicating the used bytes in data of
load operation.
Semantics

Each bit in mask operand corresponds to a byte data where each set bit represents the used byte.
Most-significant bit corresponds to most-significant byte of data.

// For 4 bytes load with only lower 3 bytes used
.pragma "used_bytes_mask 0x7";
ld.global.u32 %r0, [gbl];     // Higher 1 byte from %r0 is unused

// For vector load of 16 bytes with lower 12 bytes used
.pragma "used_bytes_mask 0xfff";
ld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl];  // %r3 unused


PTX ISA Notes
Introduced in PTX ISA version 8.3.
Target ISA Notes
Requires sm_50 or higher.
Examples

.pragma "used_bytes_mask 0xfff";
ld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used






15. Noticesï



15.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



15.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



15.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















EULA










































1. License Agreement for NVIDIA Software Development Kits

1.1. License
1.1.1. License Grant
1.1.2. Distribution Requirements
1.1.3. Authorized Users
1.1.4. Pre-Release SDK
1.1.5. Updates
1.1.6. Components Under Other Licenses
1.1.7. Reservation of Rights


1.2. Limitations
1.3. Ownership
1.4. No Warranties
1.5. Limitation of Liability
1.6. Termination
1.7. General



2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits
2.1. License Scope
2.2. Distribution
2.3. Operating Systems
2.4. Audio and Video Encoders and Decoders
2.5. Licensing
2.6. Attachment A
2.7. Attachment B








EULA






 Â»

1. License Agreement for NVIDIA Software Development Kits



v12.5 |
PDF
|
Archive
Â 






End User License Agreement
NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021
The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Last updated: October 8, 2021.
Preface
The Software License Agreement in ChapterÂ 1 and the Supplement in ChapterÂ 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.
NVIDIA Driver
Description
This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.
NVIDIA CUDA Toolkit
Description
The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.
Default Install Location of CUDA Toolkit
Windows platform:

%ProgramFiles%\NVIDIA GPU Computing Toolkit\CUDA\v#.#


Linux platform:

/usr/local/cuda-#.#


Mac platform:

/Developer/NVIDIA/CUDA-#.#


NVIDIA CUDA Samples
Description
CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit.
NVIDIA Nsight Visual Studio Edition (Windows only)
Description
NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.
Default Install Location of Nsight Visual Studio Edition
Windows platform:

%ProgramFiles(x86)%\NVIDIA Corporation\Nsight Visual Studio Edition #.#




1. License Agreement for NVIDIA Software Development Kitsï

Important NoticeâRead before downloading, installing, copying or using the licensed software:
This license agreement, including exhibits attached (âAgreementâ) is a legal agreement between you and NVIDIA Corporation (âNVIDIAâ) and governs your use of a NVIDIA software development kit (âSDKâ).
Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.
This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.
If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case âyouâ will mean the entity you represent.
If you donât have the required age or authority to accept this Agreement, or if you donât accept all the terms and conditions of this Agreement, do not download, install or use the SDK.
You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.


1.1. Licenseï



1.1.1. License Grantï

Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to:

Install and use the SDK,
Modify and create derivative works of sample source code delivered in the SDK, and
Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.




1.1.2. Distribution Requirementsï

These are the distribution requirements for you to exercise the distribution grant:

Your application must have material additional functionality, beyond the included portions of the SDK.
The distributable portions of the SDK shall only be accessed by your application.
The following notice shall be included in modifications and derivative works of sample source code distributed: âThis software contains source code provided by NVIDIA Corporation.â
Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.
The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIAâs intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users.
You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.




1.1.3. Authorized Usersï

You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.
If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.
You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didnât follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.



1.1.4. Pre-Release SDKï

The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.
You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.
NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.



1.1.5. Updatesï

NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.



1.1.6. Components Under Other Licensesï

The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.
Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.



1.1.7. Reservation of Rightsï

NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.




1.2. Limitationsï

The following license limitations apply to your use of the SDK:

You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.
Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product.
Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.
You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.

You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be:

Disclosed or distributed in source code form;
Licensed for the purpose of making derivative works; or
Redistributable at no charge.


You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a âCritical Applicationâ). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.
You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorneyâs fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.
You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.




1.3. Ownershipï


NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2. This SDK may include software and materials from NVIDIAâs licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.


You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIAâs rights under Section 1.3.1.
You may, but donât have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com.




1.4. No Warrantiesï

THE SDK IS PROVIDED BY NVIDIA âAS ISâ AND âWITH ALL FAULTS.â TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.



1.5. Limitation of Liabilityï

TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIAâS AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.
These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.



1.6. Terminationï


This Agreement will continue to apply until terminated by either you or NVIDIA as described below.
If you want to terminate this Agreement, you may do so by stopping to use the SDK.

NVIDIA may, at any time, terminate this Agreement if:

(i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIAâs intellectual property rights);
(ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or
(iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIAâs sole discretion, the continued use of it is no longer commercially viable.


Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions.




1.7. Generalï

If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.
You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.
This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language.
The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.
If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. Unless otherwise specified, remedies are cumulative.
Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.
The SDK has been developed entirely at private expense and is âcommercial itemsâ consisting of âcommercial computer softwareâ and âcommercial computer software documentationâ provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.
The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasuryâs Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.
Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.
This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.




2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kitsï

The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (âAgreementâ) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.
This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.


2.1. License Scopeï

The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.



2.2. Distributionï

The portions of the SDK that are distributable under the Agreement are listed in Attachment A.



2.3. Operating Systemsï

Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).



2.4. Audio and Video Encoders and Decodersï

You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.



2.5. Licensingï

If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions@nvidia.com.



2.6. Attachment Aï

The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.







Component
CUDA Runtime


Windows
cudart.dll, cudart_static.lib, cudadevrt.lib


Mac OSX
libcudart.dylib, libcudart_static.a, libcudadevrt.a


Linux
libcudart.so, libcudart_static.a, libcudadevrt.a


Android
libcudart.so, libcudart_static.a, libcudadevrt.a


Component
CUDA FFT Library


Windows
cufft.dll, cufftw.dll, cufft.lib, cufftw.lib


Mac OSX
libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a


Linux
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Android
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Component
CUDA BLAS Library


Windows
cublas.dll, cublasLt.dll


Mac OSX
libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a


Linux
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Android
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Component
NVIDIA âDrop-inâ BLAS Library


Windows
nvblas.dll


Mac OSX
libnvblas.dylib


Linux
libnvblas.so


Component
CUDA Sparse Matrix Library


Windows
cusparse.dll, cusparse.lib


Mac OSX
libcusparse.dylib, libcusparse_static.a


Linux
libcusparse.so, libcusparse_static.a


Android
libcusparse.so, libcusparse_static.a


Component
CUDA Linear Solver Library


Windows
cusolver.dll, cusolver.lib


Mac OSX
libcusolver.dylib, libcusolver_static.a


Linux
libcusolver.so, libcusolver_static.a


Android
libcusolver.so, libcusolver_static.a


Component
CUDA Random Number Generation Library


Windows
curand.dll, curand.lib


Mac OSX
libcurand.dylib, libcurand_static.a


Linux
libcurand.so, libcurand_static.a


Android
libcurand.so, libcurand_static.a


Component
NVIDIA Performance Primitives Library


Windows
nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib


Mac OSX
libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a


Linux
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Android
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Component
NVIDIA JPEG Library


Windows
nvjpeg.lib, nvjpeg.dll


Linux
libnvjpeg.so, libnvjpeg_static.a


Component
Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP


Mac OSX
libculibos.a


Linux
libculibos.a


Component
NVIDIA Runtime Compilation Library and Header


All
nvrtc.h


Windows
nvrtc.dll, nvrtc-builtins.dll


Mac OSX
libnvrtc.dylib, libnvrtc-builtins.dylib


Linux
libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a


Component
NVIDIA Optimizing Compiler Library


Windows
nvvm.dll


Mac OSX
libnvvm.dylib


Linux
libnvvm.so


Component
NVIDIA JIT Linking Library


Windows
libnvJitLink.dll, libnvJitLink.lib


Linux
libnvJitLink.so, libnvJitLink_static.a


Component
NVIDIA Common Device Math Functions Library


Windows
libdevice.10.bc


Mac OSX
libdevice.10.bc


Linux
libdevice.10.bc


Component
CUDA Occupancy Calculation Header Library


All
cuda_occupancy.h


Component
CUDA Half Precision Headers


All
cuda_fp16.h, cuda_fp16.hpp


Component
CUDA Profiling Tools Interface (CUPTI) Library


Windows
cupti.dll


Mac OSX
libcupti.dylib


Linux
libcupti.so


Component
NVIDIA Tools Extension Library


Windows
nvToolsExt.dll, nvToolsExt.lib


Mac OSX
libnvToolsExt.dylib


Linux
libnvToolsExt.so


Component
NVIDIA CUDA Driver Libraries


Linux
libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a


Component
NVIDIA CUDA File IO Libraries and Header


All
cufile.h


Linux
libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a



In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply:

The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.




2.7. Attachment Bï

Additional Licensing Obligations
The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions:


Licenseeâs use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:

This product includes copyrighted third-party software licensed
under the terms of the GNU General Public License v3 ("GPL v3").
All third-party software packages are copyright by their respective
authors. GPL v3 terms and conditions are hereby incorporated into
the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt


Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests@nvidia.com. This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.

Component          License
CUDA-GDB           GPL v3



Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licenseeâs use of the H.264 video codecs are solely the responsibility of Licensee.

Licenseeâs use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. All third-party software packages are copyright by their respective authors. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html
In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Licenseeâs use of the LLVM third party component is subject to the following terms and conditions:

======================================================
LLVM Release License
======================================================
University of Illinois/NCSA
Open Source License

Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.
All rights reserved.

Developed by:

    LLVM Team

    University of Illinois at Urbana-Champaign

    http://llvm.org

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to
deal with the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

*  Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimers.

*  Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimers in the
   documentation and/or other materials provided with the distribution.

*  Neither the names of the LLVM Team, University of Illinois at Urbana-
   Champaign, nor the names of its contributors may be used to endorse or
   promote products derived from this Software without specific prior
   written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS WITH THE SOFTWARE.




Licenseeâs use of the PCRE third party component is subject to the following terms and conditions:

------------
PCRE LICENCE
------------
PCRE is a library of functions to support regular expressions whose syntax
and semantics are as close as possible to those of the Perl 5 language.
Release 8 of PCRE is distributed under the terms of the "BSD" licence, as
specified below. The documentation for PCRE, supplied in the "doc"
directory, is distributed under the same terms as the software itself. The
basic library functions are written in C and are freestanding. Also
included in the distribution is a set of C++ wrapper functions, and a just-
in-time compiler that can be used to optimize pattern matching. These are
both optional features that can be omitted when the library is built.

THE BASIC LIBRARY FUNCTIONS
---------------------------
Written by:       Philip Hazel
Email local part: ph10
Email domain:     cam.ac.uk
University of Cambridge Computing Service,
Cambridge, England.
Copyright (c) 1997-2012 University of Cambridge
All rights reserved.

PCRE JUST-IN-TIME COMPILATION SUPPORT
-------------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2010-2012 Zoltan Herczeg
All rights reserved.

STACK-LESS JUST-IN-TIME COMPILER
--------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2009-2012 Zoltan Herczeg
All rights reserved.

THE C++ WRAPPER FUNCTIONS
-------------------------
Contributed by:   Google Inc.
Copyright (c) 2007-2012, Google Inc.
All rights reserved.



THE "BSD" LICENCE
-----------------
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  * Redistributions of source code must retain the above copyright notice,
    this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

  * Neither the name of the University of Cambridge nor the name of Google
    Inc. nor the names of their contributors may be used to endorse or
    promote products derived from this software without specific prior
    written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2007-2009, Regents of the University of California

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the University of California, Berkeley nor
      the names of its contributors may be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * The name of the author may not be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2010 The University of Tennessee.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer listed in this license in the documentation and/or
      other materials provided with the distribution.
    * Neither the name of the copyright holders nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2012, The Science and Technology Facilities Council (STFC).

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the STFC nor the names of its contributors
      may be used to endorse or promote products derived from this
      software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE STFC BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows:

-- (C) Copyright 2013 King Abdullah University of Science and Technology
 Authors:
 Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa)
 David Keyes (david.keyes@kaust.edu.sa)
 Hatem Ltaief (hatem.ltaief@kaust.edu.sa)

 Redistribution  and  use  in  source and binary forms, with or without
 modification,  are  permitted  provided  that the following conditions
 are met:

 * Redistributions  of  source  code  must  retain  the above copyright
   notice,  this  list  of  conditions  and  the  following  disclaimer.
 * Redistributions  in  binary  form must reproduce the above copyright
   notice,  this list of conditions and the following disclaimer in the
   documentation  and/or other materials provided with the distribution.
 * Neither  the  name of the King Abdullah University of Science and
   Technology nor the names of its contributors may be used to endorse
   or promote products derived from this software without specific prior
   written permission.

 THIS  SOFTWARE  IS  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 ``AS IS''  AND  ANY  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 A  PARTICULAR  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT NOT
 LIMITED  TO,  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 DATA,  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 THEORY  OF  LIABILITY,  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF  THIS  SOFTWARE,  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE




Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows:

Copyright (c) 2012, University of Illinois.

All rights reserved.

Developed by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal with the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimers in the documentation and/or other materials provided
      with the distribution.
    * Neither the names of IMPACT Group, University of Illinois, nor
      the names of its contributors may be used to endorse or promote
      products derived from this Software without specific prior
      written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE
SOFTWARE.




Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license:

Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima
University. All rights reserved.

Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima
University and University of Tokyo.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the Hiroshima University nor the names of
      its contributors may be used to endorse or promote products
      derived from this software without specific prior written
      permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license:

Copyright 2010-2011, D. E. Shaw Research.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions, and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions, and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of D. E. Shaw Research nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license:

Copyright (c) 2015-2017, Norbert Juffa
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Licenseeâs use of the lz4 third party component is subject to the following terms and conditions:

Copyright (C) 2011-2013, Yann Collet.
BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




The NPP library uses code from the Boost Math Toolkit, and is subject to the following license:

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Portions of the Nsight Eclipse Edition is subject to the following license:

The Eclipse Foundation makes available all content in this plug-in
("Content"). Unless otherwise indicated below, the Content is provided
to you under the terms and conditions of the Eclipse Public License
Version 1.0 ("EPL"). A copy of the EPL is available at http://
www.eclipse.org/legal/epl-v10.html. For purposes of the EPL, "Program"
will mean the Content.

If you did not receive this Content directly from the Eclipse
Foundation, the Content is being redistributed by another party
("Redistributor") and different terms and conditions may apply to your
use of any object code in the Content. Check the Redistributor's
license that was provided with the Content. If no such license exists,
contact the Redistributor. Unless otherwise indicated below, the terms
and conditions of the EPL still apply to any source code in the
Content and such source code may be obtained at http://www.eclipse.org.




Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license:

License URL
https://github.com/openai/openai-gemm/blob/master/LICENSE

License Text
The MIT License

Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.




Licenseeâs use of the Visual Studio Setup Configuration Samples is subject to the following license:

The MIT License (MIT)
Copyright (C) Microsoft Corporation. All rights reserved.

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without restriction,
including without limitation the rights to use, copy, modify, merge,
publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



Licenseeâs use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.
The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .

Components of the driver and compiler used for binary management, including nvFatBin, nvcc,
and cuobjdump, use the Zstandard library which is subject to the following license:

BSD License

For Zstandard software

Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice, this
      list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright notice,
      this list of conditions and the following disclaimer in the documentation
      and/or other materials provided with the distribution.

    * Neither the name Facebook, nor Meta, nor the names of its contributors may
      be used to endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.














Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2024, NVIDIA Corporation.


Last updated on Jul 1, 2024.
      

















EULA










































1. License Agreement for NVIDIA Software Development Kits

1.1. License
1.1.1. License Grant
1.1.2. Distribution Requirements
1.1.3. Authorized Users
1.1.4. Pre-Release SDK
1.1.5. Updates
1.1.6. Components Under Other Licenses
1.1.7. Reservation of Rights


1.2. Limitations
1.3. Ownership
1.4. No Warranties
1.5. Limitation of Liability
1.6. Termination
1.7. General



2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits
2.1. License Scope
2.2. Distribution
2.3. Operating Systems
2.4. Audio and Video Encoders and Decoders
2.5. Licensing
2.6. Attachment A
2.7. Attachment B








EULA






 Â»

1. License Agreement for NVIDIA Software Development Kits



v12.5 |
PDF
|
Archive
Â 






End User License Agreement
NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021
The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Last updated: October 8, 2021.
Preface
The Software License Agreement in ChapterÂ 1 and the Supplement in ChapterÂ 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.
NVIDIA Driver
Description
This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.
NVIDIA CUDA Toolkit
Description
The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.
Default Install Location of CUDA Toolkit
Windows platform:

%ProgramFiles%\NVIDIA GPU Computing Toolkit\CUDA\v#.#


Linux platform:

/usr/local/cuda-#.#


Mac platform:

/Developer/NVIDIA/CUDA-#.#


NVIDIA CUDA Samples
Description
CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit.
NVIDIA Nsight Visual Studio Edition (Windows only)
Description
NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.
Default Install Location of Nsight Visual Studio Edition
Windows platform:

%ProgramFiles(x86)%\NVIDIA Corporation\Nsight Visual Studio Edition #.#




1. License Agreement for NVIDIA Software Development Kitsï

Important NoticeâRead before downloading, installing, copying or using the licensed software:
This license agreement, including exhibits attached (âAgreementâ) is a legal agreement between you and NVIDIA Corporation (âNVIDIAâ) and governs your use of a NVIDIA software development kit (âSDKâ).
Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.
This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.
If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case âyouâ will mean the entity you represent.
If you donât have the required age or authority to accept this Agreement, or if you donât accept all the terms and conditions of this Agreement, do not download, install or use the SDK.
You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.


1.1. Licenseï



1.1.1. License Grantï

Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to:

Install and use the SDK,
Modify and create derivative works of sample source code delivered in the SDK, and
Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.




1.1.2. Distribution Requirementsï

These are the distribution requirements for you to exercise the distribution grant:

Your application must have material additional functionality, beyond the included portions of the SDK.
The distributable portions of the SDK shall only be accessed by your application.
The following notice shall be included in modifications and derivative works of sample source code distributed: âThis software contains source code provided by NVIDIA Corporation.â
Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.
The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIAâs intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users.
You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.




1.1.3. Authorized Usersï

You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.
If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.
You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didnât follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.



1.1.4. Pre-Release SDKï

The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.
You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.
NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.



1.1.5. Updatesï

NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.



1.1.6. Components Under Other Licensesï

The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.
Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.



1.1.7. Reservation of Rightsï

NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.




1.2. Limitationsï

The following license limitations apply to your use of the SDK:

You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.
Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product.
Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.
You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.

You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be:

Disclosed or distributed in source code form;
Licensed for the purpose of making derivative works; or
Redistributable at no charge.


You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a âCritical Applicationâ). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.
You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorneyâs fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.
You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.




1.3. Ownershipï


NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2. This SDK may include software and materials from NVIDIAâs licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.


You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIAâs rights under Section 1.3.1.
You may, but donât have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com.




1.4. No Warrantiesï

THE SDK IS PROVIDED BY NVIDIA âAS ISâ AND âWITH ALL FAULTS.â TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.



1.5. Limitation of Liabilityï

TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIAâS AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.
These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.



1.6. Terminationï


This Agreement will continue to apply until terminated by either you or NVIDIA as described below.
If you want to terminate this Agreement, you may do so by stopping to use the SDK.

NVIDIA may, at any time, terminate this Agreement if:

(i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIAâs intellectual property rights);
(ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or
(iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIAâs sole discretion, the continued use of it is no longer commercially viable.


Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions.




1.7. Generalï

If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.
You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.
This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language.
The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.
If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. Unless otherwise specified, remedies are cumulative.
Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.
The SDK has been developed entirely at private expense and is âcommercial itemsâ consisting of âcommercial computer softwareâ and âcommercial computer software documentationâ provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.
The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasuryâs Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.
Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.
This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.




2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kitsï

The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (âAgreementâ) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.
This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.


2.1. License Scopeï

The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.



2.2. Distributionï

The portions of the SDK that are distributable under the Agreement are listed in Attachment A.



2.3. Operating Systemsï

Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).



2.4. Audio and Video Encoders and Decodersï

You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.



2.5. Licensingï

If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions@nvidia.com.



2.6. Attachment Aï

The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.







Component
CUDA Runtime


Windows
cudart.dll, cudart_static.lib, cudadevrt.lib


Mac OSX
libcudart.dylib, libcudart_static.a, libcudadevrt.a


Linux
libcudart.so, libcudart_static.a, libcudadevrt.a


Android
libcudart.so, libcudart_static.a, libcudadevrt.a


Component
CUDA FFT Library


Windows
cufft.dll, cufftw.dll, cufft.lib, cufftw.lib


Mac OSX
libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a


Linux
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Android
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Component
CUDA BLAS Library


Windows
cublas.dll, cublasLt.dll


Mac OSX
libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a


Linux
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Android
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Component
NVIDIA âDrop-inâ BLAS Library


Windows
nvblas.dll


Mac OSX
libnvblas.dylib


Linux
libnvblas.so


Component
CUDA Sparse Matrix Library


Windows
cusparse.dll, cusparse.lib


Mac OSX
libcusparse.dylib, libcusparse_static.a


Linux
libcusparse.so, libcusparse_static.a


Android
libcusparse.so, libcusparse_static.a


Component
CUDA Linear Solver Library


Windows
cusolver.dll, cusolver.lib


Mac OSX
libcusolver.dylib, libcusolver_static.a


Linux
libcusolver.so, libcusolver_static.a


Android
libcusolver.so, libcusolver_static.a


Component
CUDA Random Number Generation Library


Windows
curand.dll, curand.lib


Mac OSX
libcurand.dylib, libcurand_static.a


Linux
libcurand.so, libcurand_static.a


Android
libcurand.so, libcurand_static.a


Component
NVIDIA Performance Primitives Library


Windows
nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib


Mac OSX
libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a


Linux
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Android
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Component
NVIDIA JPEG Library


Windows
nvjpeg.lib, nvjpeg.dll


Linux
libnvjpeg.so, libnvjpeg_static.a


Component
Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP


Mac OSX
libculibos.a


Linux
libculibos.a


Component
NVIDIA Runtime Compilation Library and Header


All
nvrtc.h


Windows
nvrtc.dll, nvrtc-builtins.dll


Mac OSX
libnvrtc.dylib, libnvrtc-builtins.dylib


Linux
libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a


Component
NVIDIA Optimizing Compiler Library


Windows
nvvm.dll


Mac OSX
libnvvm.dylib


Linux
libnvvm.so


Component
NVIDIA JIT Linking Library


Windows
libnvJitLink.dll, libnvJitLink.lib


Linux
libnvJitLink.so, libnvJitLink_static.a


Component
NVIDIA Common Device Math Functions Library


Windows
libdevice.10.bc


Mac OSX
libdevice.10.bc


Linux
libdevice.10.bc


Component
CUDA Occupancy Calculation Header Library


All
cuda_occupancy.h


Component
CUDA Half Precision Headers


All
cuda_fp16.h, cuda_fp16.hpp


Component
CUDA Profiling Tools Interface (CUPTI) Library


Windows
cupti.dll


Mac OSX
libcupti.dylib


Linux
libcupti.so


Component
NVIDIA Tools Extension Library


Windows
nvToolsExt.dll, nvToolsExt.lib


Mac OSX
libnvToolsExt.dylib


Linux
libnvToolsExt.so


Component
NVIDIA CUDA Driver Libraries


Linux
libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a


Component
NVIDIA CUDA File IO Libraries and Header


All
cufile.h


Linux
libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a



In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply:

The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.




2.7. Attachment Bï

Additional Licensing Obligations
The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions:


Licenseeâs use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:

This product includes copyrighted third-party software licensed
under the terms of the GNU General Public License v3 ("GPL v3").
All third-party software packages are copyright by their respective
authors. GPL v3 terms and conditions are hereby incorporated into
the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt


Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests@nvidia.com. This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.

Component          License
CUDA-GDB           GPL v3



Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licenseeâs use of the H.264 video codecs are solely the responsibility of Licensee.

Licenseeâs use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. All third-party software packages are copyright by their respective authors. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html
In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Licenseeâs use of the LLVM third party component is subject to the following terms and conditions:

======================================================
LLVM Release License
======================================================
University of Illinois/NCSA
Open Source License

Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.
All rights reserved.

Developed by:

    LLVM Team

    University of Illinois at Urbana-Champaign

    http://llvm.org

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to
deal with the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

*  Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimers.

*  Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimers in the
   documentation and/or other materials provided with the distribution.

*  Neither the names of the LLVM Team, University of Illinois at Urbana-
   Champaign, nor the names of its contributors may be used to endorse or
   promote products derived from this Software without specific prior
   written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS WITH THE SOFTWARE.




Licenseeâs use of the PCRE third party component is subject to the following terms and conditions:

------------
PCRE LICENCE
------------
PCRE is a library of functions to support regular expressions whose syntax
and semantics are as close as possible to those of the Perl 5 language.
Release 8 of PCRE is distributed under the terms of the "BSD" licence, as
specified below. The documentation for PCRE, supplied in the "doc"
directory, is distributed under the same terms as the software itself. The
basic library functions are written in C and are freestanding. Also
included in the distribution is a set of C++ wrapper functions, and a just-
in-time compiler that can be used to optimize pattern matching. These are
both optional features that can be omitted when the library is built.

THE BASIC LIBRARY FUNCTIONS
---------------------------
Written by:       Philip Hazel
Email local part: ph10
Email domain:     cam.ac.uk
University of Cambridge Computing Service,
Cambridge, England.
Copyright (c) 1997-2012 University of Cambridge
All rights reserved.

PCRE JUST-IN-TIME COMPILATION SUPPORT
-------------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2010-2012 Zoltan Herczeg
All rights reserved.

STACK-LESS JUST-IN-TIME COMPILER
--------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2009-2012 Zoltan Herczeg
All rights reserved.

THE C++ WRAPPER FUNCTIONS
-------------------------
Contributed by:   Google Inc.
Copyright (c) 2007-2012, Google Inc.
All rights reserved.



THE "BSD" LICENCE
-----------------
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  * Redistributions of source code must retain the above copyright notice,
    this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

  * Neither the name of the University of Cambridge nor the name of Google
    Inc. nor the names of their contributors may be used to endorse or
    promote products derived from this software without specific prior
    written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2007-2009, Regents of the University of California

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the University of California, Berkeley nor
      the names of its contributors may be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * The name of the author may not be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2010 The University of Tennessee.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer listed in this license in the documentation and/or
      other materials provided with the distribution.
    * Neither the name of the copyright holders nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2012, The Science and Technology Facilities Council (STFC).

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the STFC nor the names of its contributors
      may be used to endorse or promote products derived from this
      software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE STFC BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows:

-- (C) Copyright 2013 King Abdullah University of Science and Technology
 Authors:
 Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa)
 David Keyes (david.keyes@kaust.edu.sa)
 Hatem Ltaief (hatem.ltaief@kaust.edu.sa)

 Redistribution  and  use  in  source and binary forms, with or without
 modification,  are  permitted  provided  that the following conditions
 are met:

 * Redistributions  of  source  code  must  retain  the above copyright
   notice,  this  list  of  conditions  and  the  following  disclaimer.
 * Redistributions  in  binary  form must reproduce the above copyright
   notice,  this list of conditions and the following disclaimer in the
   documentation  and/or other materials provided with the distribution.
 * Neither  the  name of the King Abdullah University of Science and
   Technology nor the names of its contributors may be used to endorse
   or promote products derived from this software without specific prior
   written permission.

 THIS  SOFTWARE  IS  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 ``AS IS''  AND  ANY  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 A  PARTICULAR  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT NOT
 LIMITED  TO,  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 DATA,  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 THEORY  OF  LIABILITY,  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF  THIS  SOFTWARE,  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE




Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows:

Copyright (c) 2012, University of Illinois.

All rights reserved.

Developed by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal with the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimers in the documentation and/or other materials provided
      with the distribution.
    * Neither the names of IMPACT Group, University of Illinois, nor
      the names of its contributors may be used to endorse or promote
      products derived from this Software without specific prior
      written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE
SOFTWARE.




Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license:

Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima
University. All rights reserved.

Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima
University and University of Tokyo.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the Hiroshima University nor the names of
      its contributors may be used to endorse or promote products
      derived from this software without specific prior written
      permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license:

Copyright 2010-2011, D. E. Shaw Research.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions, and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions, and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of D. E. Shaw Research nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license:

Copyright (c) 2015-2017, Norbert Juffa
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Licenseeâs use of the lz4 third party component is subject to the following terms and conditions:

Copyright (C) 2011-2013, Yann Collet.
BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




The NPP library uses code from the Boost Math Toolkit, and is subject to the following license:

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Portions of the Nsight Eclipse Edition is subject to the following license:

The Eclipse Foundation makes available all content in this plug-in
("Content"). Unless otherwise indicated below, the Content is provided
to you under the terms and conditions of the Eclipse Public License
Version 1.0 ("EPL"). A copy of the EPL is available at http://
www.eclipse.org/legal/epl-v10.html. For purposes of the EPL, "Program"
will mean the Content.

If you did not receive this Content directly from the Eclipse
Foundation, the Content is being redistributed by another party
("Redistributor") and different terms and conditions may apply to your
use of any object code in the Content. Check the Redistributor's
license that was provided with the Content. If no such license exists,
contact the Redistributor. Unless otherwise indicated below, the terms
and conditions of the EPL still apply to any source code in the
Content and such source code may be obtained at http://www.eclipse.org.




Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license:

License URL
https://github.com/openai/openai-gemm/blob/master/LICENSE

License Text
The MIT License

Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.




Licenseeâs use of the Visual Studio Setup Configuration Samples is subject to the following license:

The MIT License (MIT)
Copyright (C) Microsoft Corporation. All rights reserved.

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without restriction,
including without limitation the rights to use, copy, modify, merge,
publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



Licenseeâs use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.
The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .

Components of the driver and compiler used for binary management, including nvFatBin, nvcc,
and cuobjdump, use the Zstandard library which is subject to the following license:

BSD License

For Zstandard software

Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice, this
      list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright notice,
      this list of conditions and the following disclaimer in the documentation
      and/or other materials provided with the distribution.

    * Neither the name Facebook, nor Meta, nor the names of its contributors may
      be used to endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.














Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2024, NVIDIA Corporation.


Last updated on Jul 1, 2024.
      

















EULA










































1. License Agreement for NVIDIA Software Development Kits

1.1. License
1.1.1. License Grant
1.1.2. Distribution Requirements
1.1.3. Authorized Users
1.1.4. Pre-Release SDK
1.1.5. Updates
1.1.6. Components Under Other Licenses
1.1.7. Reservation of Rights


1.2. Limitations
1.3. Ownership
1.4. No Warranties
1.5. Limitation of Liability
1.6. Termination
1.7. General



2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits
2.1. License Scope
2.2. Distribution
2.3. Operating Systems
2.4. Audio and Video Encoders and Decoders
2.5. Licensing
2.6. Attachment A
2.7. Attachment B








EULA






 Â»

1. License Agreement for NVIDIA Software Development Kits



v12.5 |
PDF
|
Archive
Â 






End User License Agreement
NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021
The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Last updated: October 8, 2021.
Preface
The Software License Agreement in ChapterÂ 1 and the Supplement in ChapterÂ 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.
NVIDIA Driver
Description
This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.
NVIDIA CUDA Toolkit
Description
The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.
Default Install Location of CUDA Toolkit
Windows platform:

%ProgramFiles%\NVIDIA GPU Computing Toolkit\CUDA\v#.#


Linux platform:

/usr/local/cuda-#.#


Mac platform:

/Developer/NVIDIA/CUDA-#.#


NVIDIA CUDA Samples
Description
CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit.
NVIDIA Nsight Visual Studio Edition (Windows only)
Description
NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.
Default Install Location of Nsight Visual Studio Edition
Windows platform:

%ProgramFiles(x86)%\NVIDIA Corporation\Nsight Visual Studio Edition #.#




1. License Agreement for NVIDIA Software Development Kitsï

Important NoticeâRead before downloading, installing, copying or using the licensed software:
This license agreement, including exhibits attached (âAgreementâ) is a legal agreement between you and NVIDIA Corporation (âNVIDIAâ) and governs your use of a NVIDIA software development kit (âSDKâ).
Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.
This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.
If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case âyouâ will mean the entity you represent.
If you donât have the required age or authority to accept this Agreement, or if you donât accept all the terms and conditions of this Agreement, do not download, install or use the SDK.
You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.


1.1. Licenseï



1.1.1. License Grantï

Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to:

Install and use the SDK,
Modify and create derivative works of sample source code delivered in the SDK, and
Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.




1.1.2. Distribution Requirementsï

These are the distribution requirements for you to exercise the distribution grant:

Your application must have material additional functionality, beyond the included portions of the SDK.
The distributable portions of the SDK shall only be accessed by your application.
The following notice shall be included in modifications and derivative works of sample source code distributed: âThis software contains source code provided by NVIDIA Corporation.â
Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.
The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIAâs intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users.
You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.




1.1.3. Authorized Usersï

You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.
If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.
You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didnât follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.



1.1.4. Pre-Release SDKï

The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.
You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.
NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.



1.1.5. Updatesï

NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.



1.1.6. Components Under Other Licensesï

The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.
Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.



1.1.7. Reservation of Rightsï

NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.




1.2. Limitationsï

The following license limitations apply to your use of the SDK:

You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.
Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product.
Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.
You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.

You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be:

Disclosed or distributed in source code form;
Licensed for the purpose of making derivative works; or
Redistributable at no charge.


You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a âCritical Applicationâ). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.
You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorneyâs fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.
You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.




1.3. Ownershipï


NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2. This SDK may include software and materials from NVIDIAâs licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.


You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIAâs rights under Section 1.3.1.
You may, but donât have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com.




1.4. No Warrantiesï

THE SDK IS PROVIDED BY NVIDIA âAS ISâ AND âWITH ALL FAULTS.â TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.



1.5. Limitation of Liabilityï

TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIAâS AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.
These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.



1.6. Terminationï


This Agreement will continue to apply until terminated by either you or NVIDIA as described below.
If you want to terminate this Agreement, you may do so by stopping to use the SDK.

NVIDIA may, at any time, terminate this Agreement if:

(i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIAâs intellectual property rights);
(ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or
(iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIAâs sole discretion, the continued use of it is no longer commercially viable.


Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions.




1.7. Generalï

If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.
You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.
This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language.
The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.
If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. Unless otherwise specified, remedies are cumulative.
Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.
The SDK has been developed entirely at private expense and is âcommercial itemsâ consisting of âcommercial computer softwareâ and âcommercial computer software documentationâ provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.
The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasuryâs Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.
Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.
This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.




2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kitsï

The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (âAgreementâ) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.
This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.


2.1. License Scopeï

The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.



2.2. Distributionï

The portions of the SDK that are distributable under the Agreement are listed in Attachment A.



2.3. Operating Systemsï

Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).



2.4. Audio and Video Encoders and Decodersï

You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.



2.5. Licensingï

If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions@nvidia.com.



2.6. Attachment Aï

The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.







Component
CUDA Runtime


Windows
cudart.dll, cudart_static.lib, cudadevrt.lib


Mac OSX
libcudart.dylib, libcudart_static.a, libcudadevrt.a


Linux
libcudart.so, libcudart_static.a, libcudadevrt.a


Android
libcudart.so, libcudart_static.a, libcudadevrt.a


Component
CUDA FFT Library


Windows
cufft.dll, cufftw.dll, cufft.lib, cufftw.lib


Mac OSX
libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a


Linux
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Android
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Component
CUDA BLAS Library


Windows
cublas.dll, cublasLt.dll


Mac OSX
libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a


Linux
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Android
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Component
NVIDIA âDrop-inâ BLAS Library


Windows
nvblas.dll


Mac OSX
libnvblas.dylib


Linux
libnvblas.so


Component
CUDA Sparse Matrix Library


Windows
cusparse.dll, cusparse.lib


Mac OSX
libcusparse.dylib, libcusparse_static.a


Linux
libcusparse.so, libcusparse_static.a


Android
libcusparse.so, libcusparse_static.a


Component
CUDA Linear Solver Library


Windows
cusolver.dll, cusolver.lib


Mac OSX
libcusolver.dylib, libcusolver_static.a


Linux
libcusolver.so, libcusolver_static.a


Android
libcusolver.so, libcusolver_static.a


Component
CUDA Random Number Generation Library


Windows
curand.dll, curand.lib


Mac OSX
libcurand.dylib, libcurand_static.a


Linux
libcurand.so, libcurand_static.a


Android
libcurand.so, libcurand_static.a


Component
NVIDIA Performance Primitives Library


Windows
nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib


Mac OSX
libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a


Linux
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Android
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Component
NVIDIA JPEG Library


Windows
nvjpeg.lib, nvjpeg.dll


Linux
libnvjpeg.so, libnvjpeg_static.a


Component
Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP


Mac OSX
libculibos.a


Linux
libculibos.a


Component
NVIDIA Runtime Compilation Library and Header


All
nvrtc.h


Windows
nvrtc.dll, nvrtc-builtins.dll


Mac OSX
libnvrtc.dylib, libnvrtc-builtins.dylib


Linux
libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a


Component
NVIDIA Optimizing Compiler Library


Windows
nvvm.dll


Mac OSX
libnvvm.dylib


Linux
libnvvm.so


Component
NVIDIA JIT Linking Library


Windows
libnvJitLink.dll, libnvJitLink.lib


Linux
libnvJitLink.so, libnvJitLink_static.a


Component
NVIDIA Common Device Math Functions Library


Windows
libdevice.10.bc


Mac OSX
libdevice.10.bc


Linux
libdevice.10.bc


Component
CUDA Occupancy Calculation Header Library


All
cuda_occupancy.h


Component
CUDA Half Precision Headers


All
cuda_fp16.h, cuda_fp16.hpp


Component
CUDA Profiling Tools Interface (CUPTI) Library


Windows
cupti.dll


Mac OSX
libcupti.dylib


Linux
libcupti.so


Component
NVIDIA Tools Extension Library


Windows
nvToolsExt.dll, nvToolsExt.lib


Mac OSX
libnvToolsExt.dylib


Linux
libnvToolsExt.so


Component
NVIDIA CUDA Driver Libraries


Linux
libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a


Component
NVIDIA CUDA File IO Libraries and Header


All
cufile.h


Linux
libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a



In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply:

The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.




2.7. Attachment Bï

Additional Licensing Obligations
The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions:


Licenseeâs use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:

This product includes copyrighted third-party software licensed
under the terms of the GNU General Public License v3 ("GPL v3").
All third-party software packages are copyright by their respective
authors. GPL v3 terms and conditions are hereby incorporated into
the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt


Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests@nvidia.com. This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.

Component          License
CUDA-GDB           GPL v3



Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licenseeâs use of the H.264 video codecs are solely the responsibility of Licensee.

Licenseeâs use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. All third-party software packages are copyright by their respective authors. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html
In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Licenseeâs use of the LLVM third party component is subject to the following terms and conditions:

======================================================
LLVM Release License
======================================================
University of Illinois/NCSA
Open Source License

Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.
All rights reserved.

Developed by:

    LLVM Team

    University of Illinois at Urbana-Champaign

    http://llvm.org

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to
deal with the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

*  Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimers.

*  Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimers in the
   documentation and/or other materials provided with the distribution.

*  Neither the names of the LLVM Team, University of Illinois at Urbana-
   Champaign, nor the names of its contributors may be used to endorse or
   promote products derived from this Software without specific prior
   written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS WITH THE SOFTWARE.




Licenseeâs use of the PCRE third party component is subject to the following terms and conditions:

------------
PCRE LICENCE
------------
PCRE is a library of functions to support regular expressions whose syntax
and semantics are as close as possible to those of the Perl 5 language.
Release 8 of PCRE is distributed under the terms of the "BSD" licence, as
specified below. The documentation for PCRE, supplied in the "doc"
directory, is distributed under the same terms as the software itself. The
basic library functions are written in C and are freestanding. Also
included in the distribution is a set of C++ wrapper functions, and a just-
in-time compiler that can be used to optimize pattern matching. These are
both optional features that can be omitted when the library is built.

THE BASIC LIBRARY FUNCTIONS
---------------------------
Written by:       Philip Hazel
Email local part: ph10
Email domain:     cam.ac.uk
University of Cambridge Computing Service,
Cambridge, England.
Copyright (c) 1997-2012 University of Cambridge
All rights reserved.

PCRE JUST-IN-TIME COMPILATION SUPPORT
-------------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2010-2012 Zoltan Herczeg
All rights reserved.

STACK-LESS JUST-IN-TIME COMPILER
--------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2009-2012 Zoltan Herczeg
All rights reserved.

THE C++ WRAPPER FUNCTIONS
-------------------------
Contributed by:   Google Inc.
Copyright (c) 2007-2012, Google Inc.
All rights reserved.



THE "BSD" LICENCE
-----------------
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  * Redistributions of source code must retain the above copyright notice,
    this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

  * Neither the name of the University of Cambridge nor the name of Google
    Inc. nor the names of their contributors may be used to endorse or
    promote products derived from this software without specific prior
    written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2007-2009, Regents of the University of California

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the University of California, Berkeley nor
      the names of its contributors may be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * The name of the author may not be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2010 The University of Tennessee.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer listed in this license in the documentation and/or
      other materials provided with the distribution.
    * Neither the name of the copyright holders nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2012, The Science and Technology Facilities Council (STFC).

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the STFC nor the names of its contributors
      may be used to endorse or promote products derived from this
      software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE STFC BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows:

-- (C) Copyright 2013 King Abdullah University of Science and Technology
 Authors:
 Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa)
 David Keyes (david.keyes@kaust.edu.sa)
 Hatem Ltaief (hatem.ltaief@kaust.edu.sa)

 Redistribution  and  use  in  source and binary forms, with or without
 modification,  are  permitted  provided  that the following conditions
 are met:

 * Redistributions  of  source  code  must  retain  the above copyright
   notice,  this  list  of  conditions  and  the  following  disclaimer.
 * Redistributions  in  binary  form must reproduce the above copyright
   notice,  this list of conditions and the following disclaimer in the
   documentation  and/or other materials provided with the distribution.
 * Neither  the  name of the King Abdullah University of Science and
   Technology nor the names of its contributors may be used to endorse
   or promote products derived from this software without specific prior
   written permission.

 THIS  SOFTWARE  IS  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 ``AS IS''  AND  ANY  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 A  PARTICULAR  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT NOT
 LIMITED  TO,  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 DATA,  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 THEORY  OF  LIABILITY,  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF  THIS  SOFTWARE,  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE




Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows:

Copyright (c) 2012, University of Illinois.

All rights reserved.

Developed by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal with the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimers in the documentation and/or other materials provided
      with the distribution.
    * Neither the names of IMPACT Group, University of Illinois, nor
      the names of its contributors may be used to endorse or promote
      products derived from this Software without specific prior
      written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE
SOFTWARE.




Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license:

Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima
University. All rights reserved.

Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima
University and University of Tokyo.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the Hiroshima University nor the names of
      its contributors may be used to endorse or promote products
      derived from this software without specific prior written
      permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license:

Copyright 2010-2011, D. E. Shaw Research.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions, and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions, and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of D. E. Shaw Research nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license:

Copyright (c) 2015-2017, Norbert Juffa
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Licenseeâs use of the lz4 third party component is subject to the following terms and conditions:

Copyright (C) 2011-2013, Yann Collet.
BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




The NPP library uses code from the Boost Math Toolkit, and is subject to the following license:

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Portions of the Nsight Eclipse Edition is subject to the following license:

The Eclipse Foundation makes available all content in this plug-in
("Content"). Unless otherwise indicated below, the Content is provided
to you under the terms and conditions of the Eclipse Public License
Version 1.0 ("EPL"). A copy of the EPL is available at http://
www.eclipse.org/legal/epl-v10.html. For purposes of the EPL, "Program"
will mean the Content.

If you did not receive this Content directly from the Eclipse
Foundation, the Content is being redistributed by another party
("Redistributor") and different terms and conditions may apply to your
use of any object code in the Content. Check the Redistributor's
license that was provided with the Content. If no such license exists,
contact the Redistributor. Unless otherwise indicated below, the terms
and conditions of the EPL still apply to any source code in the
Content and such source code may be obtained at http://www.eclipse.org.




Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license:

License URL
https://github.com/openai/openai-gemm/blob/master/LICENSE

License Text
The MIT License

Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.




Licenseeâs use of the Visual Studio Setup Configuration Samples is subject to the following license:

The MIT License (MIT)
Copyright (C) Microsoft Corporation. All rights reserved.

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without restriction,
including without limitation the rights to use, copy, modify, merge,
publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



Licenseeâs use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.
The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .

Components of the driver and compiler used for binary management, including nvFatBin, nvcc,
and cuobjdump, use the Zstandard library which is subject to the following license:

BSD License

For Zstandard software

Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice, this
      list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright notice,
      this list of conditions and the following disclaimer in the documentation
      and/or other materials provided with the distribution.

    * Neither the name Facebook, nor Meta, nor the names of its contributors may
      be used to endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.














Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2024, NVIDIA Corporation.


Last updated on Jul 1, 2024.
      

















EULA










































1. License Agreement for NVIDIA Software Development Kits

1.1. License
1.1.1. License Grant
1.1.2. Distribution Requirements
1.1.3. Authorized Users
1.1.4. Pre-Release SDK
1.1.5. Updates
1.1.6. Components Under Other Licenses
1.1.7. Reservation of Rights


1.2. Limitations
1.3. Ownership
1.4. No Warranties
1.5. Limitation of Liability
1.6. Termination
1.7. General



2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits
2.1. License Scope
2.2. Distribution
2.3. Operating Systems
2.4. Audio and Video Encoders and Decoders
2.5. Licensing
2.6. Attachment A
2.7. Attachment B








EULA






 Â»

1. License Agreement for NVIDIA Software Development Kits



v12.5 |
PDF
|
Archive
Â 






End User License Agreement
NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021
The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Last updated: October 8, 2021.
Preface
The Software License Agreement in ChapterÂ 1 and the Supplement in ChapterÂ 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.
NVIDIA Driver
Description
This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.
NVIDIA CUDA Toolkit
Description
The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.
Default Install Location of CUDA Toolkit
Windows platform:

%ProgramFiles%\NVIDIA GPU Computing Toolkit\CUDA\v#.#


Linux platform:

/usr/local/cuda-#.#


Mac platform:

/Developer/NVIDIA/CUDA-#.#


NVIDIA CUDA Samples
Description
CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit.
NVIDIA Nsight Visual Studio Edition (Windows only)
Description
NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.
Default Install Location of Nsight Visual Studio Edition
Windows platform:

%ProgramFiles(x86)%\NVIDIA Corporation\Nsight Visual Studio Edition #.#




1. License Agreement for NVIDIA Software Development Kitsï

Important NoticeâRead before downloading, installing, copying or using the licensed software:
This license agreement, including exhibits attached (âAgreementâ) is a legal agreement between you and NVIDIA Corporation (âNVIDIAâ) and governs your use of a NVIDIA software development kit (âSDKâ).
Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.
This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.
If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case âyouâ will mean the entity you represent.
If you donât have the required age or authority to accept this Agreement, or if you donât accept all the terms and conditions of this Agreement, do not download, install or use the SDK.
You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.


1.1. Licenseï



1.1.1. License Grantï

Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to:

Install and use the SDK,
Modify and create derivative works of sample source code delivered in the SDK, and
Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.




1.1.2. Distribution Requirementsï

These are the distribution requirements for you to exercise the distribution grant:

Your application must have material additional functionality, beyond the included portions of the SDK.
The distributable portions of the SDK shall only be accessed by your application.
The following notice shall be included in modifications and derivative works of sample source code distributed: âThis software contains source code provided by NVIDIA Corporation.â
Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.
The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIAâs intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users.
You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.




1.1.3. Authorized Usersï

You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.
If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.
You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didnât follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.



1.1.4. Pre-Release SDKï

The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.
You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.
NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.



1.1.5. Updatesï

NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.



1.1.6. Components Under Other Licensesï

The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.
Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.



1.1.7. Reservation of Rightsï

NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.




1.2. Limitationsï

The following license limitations apply to your use of the SDK:

You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.
Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product.
Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.
You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.

You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be:

Disclosed or distributed in source code form;
Licensed for the purpose of making derivative works; or
Redistributable at no charge.


You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a âCritical Applicationâ). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.
You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorneyâs fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.
You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.




1.3. Ownershipï


NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2. This SDK may include software and materials from NVIDIAâs licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.


You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIAâs rights under Section 1.3.1.
You may, but donât have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com.




1.4. No Warrantiesï

THE SDK IS PROVIDED BY NVIDIA âAS ISâ AND âWITH ALL FAULTS.â TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.



1.5. Limitation of Liabilityï

TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIAâS AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.
These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.



1.6. Terminationï


This Agreement will continue to apply until terminated by either you or NVIDIA as described below.
If you want to terminate this Agreement, you may do so by stopping to use the SDK.

NVIDIA may, at any time, terminate this Agreement if:

(i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIAâs intellectual property rights);
(ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or
(iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIAâs sole discretion, the continued use of it is no longer commercially viable.


Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions.




1.7. Generalï

If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.
You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.
This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language.
The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.
If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. Unless otherwise specified, remedies are cumulative.
Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.
The SDK has been developed entirely at private expense and is âcommercial itemsâ consisting of âcommercial computer softwareâ and âcommercial computer software documentationâ provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.
The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasuryâs Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.
Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.
This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.




2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kitsï

The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (âAgreementâ) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.
This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.


2.1. License Scopeï

The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.



2.2. Distributionï

The portions of the SDK that are distributable under the Agreement are listed in Attachment A.



2.3. Operating Systemsï

Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).



2.4. Audio and Video Encoders and Decodersï

You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.



2.5. Licensingï

If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions@nvidia.com.



2.6. Attachment Aï

The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.







Component
CUDA Runtime


Windows
cudart.dll, cudart_static.lib, cudadevrt.lib


Mac OSX
libcudart.dylib, libcudart_static.a, libcudadevrt.a


Linux
libcudart.so, libcudart_static.a, libcudadevrt.a


Android
libcudart.so, libcudart_static.a, libcudadevrt.a


Component
CUDA FFT Library


Windows
cufft.dll, cufftw.dll, cufft.lib, cufftw.lib


Mac OSX
libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a


Linux
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Android
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Component
CUDA BLAS Library


Windows
cublas.dll, cublasLt.dll


Mac OSX
libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a


Linux
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Android
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Component
NVIDIA âDrop-inâ BLAS Library


Windows
nvblas.dll


Mac OSX
libnvblas.dylib


Linux
libnvblas.so


Component
CUDA Sparse Matrix Library


Windows
cusparse.dll, cusparse.lib


Mac OSX
libcusparse.dylib, libcusparse_static.a


Linux
libcusparse.so, libcusparse_static.a


Android
libcusparse.so, libcusparse_static.a


Component
CUDA Linear Solver Library


Windows
cusolver.dll, cusolver.lib


Mac OSX
libcusolver.dylib, libcusolver_static.a


Linux
libcusolver.so, libcusolver_static.a


Android
libcusolver.so, libcusolver_static.a


Component
CUDA Random Number Generation Library


Windows
curand.dll, curand.lib


Mac OSX
libcurand.dylib, libcurand_static.a


Linux
libcurand.so, libcurand_static.a


Android
libcurand.so, libcurand_static.a


Component
NVIDIA Performance Primitives Library


Windows
nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib


Mac OSX
libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a


Linux
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Android
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Component
NVIDIA JPEG Library


Windows
nvjpeg.lib, nvjpeg.dll


Linux
libnvjpeg.so, libnvjpeg_static.a


Component
Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP


Mac OSX
libculibos.a


Linux
libculibos.a


Component
NVIDIA Runtime Compilation Library and Header


All
nvrtc.h


Windows
nvrtc.dll, nvrtc-builtins.dll


Mac OSX
libnvrtc.dylib, libnvrtc-builtins.dylib


Linux
libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a


Component
NVIDIA Optimizing Compiler Library


Windows
nvvm.dll


Mac OSX
libnvvm.dylib


Linux
libnvvm.so


Component
NVIDIA JIT Linking Library


Windows
libnvJitLink.dll, libnvJitLink.lib


Linux
libnvJitLink.so, libnvJitLink_static.a


Component
NVIDIA Common Device Math Functions Library


Windows
libdevice.10.bc


Mac OSX
libdevice.10.bc


Linux
libdevice.10.bc


Component
CUDA Occupancy Calculation Header Library


All
cuda_occupancy.h


Component
CUDA Half Precision Headers


All
cuda_fp16.h, cuda_fp16.hpp


Component
CUDA Profiling Tools Interface (CUPTI) Library


Windows
cupti.dll


Mac OSX
libcupti.dylib


Linux
libcupti.so


Component
NVIDIA Tools Extension Library


Windows
nvToolsExt.dll, nvToolsExt.lib


Mac OSX
libnvToolsExt.dylib


Linux
libnvToolsExt.so


Component
NVIDIA CUDA Driver Libraries


Linux
libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a


Component
NVIDIA CUDA File IO Libraries and Header


All
cufile.h


Linux
libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a



In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply:

The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.




2.7. Attachment Bï

Additional Licensing Obligations
The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions:


Licenseeâs use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:

This product includes copyrighted third-party software licensed
under the terms of the GNU General Public License v3 ("GPL v3").
All third-party software packages are copyright by their respective
authors. GPL v3 terms and conditions are hereby incorporated into
the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt


Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests@nvidia.com. This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.

Component          License
CUDA-GDB           GPL v3



Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licenseeâs use of the H.264 video codecs are solely the responsibility of Licensee.

Licenseeâs use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. All third-party software packages are copyright by their respective authors. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html
In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Licenseeâs use of the LLVM third party component is subject to the following terms and conditions:

======================================================
LLVM Release License
======================================================
University of Illinois/NCSA
Open Source License

Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.
All rights reserved.

Developed by:

    LLVM Team

    University of Illinois at Urbana-Champaign

    http://llvm.org

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to
deal with the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

*  Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimers.

*  Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimers in the
   documentation and/or other materials provided with the distribution.

*  Neither the names of the LLVM Team, University of Illinois at Urbana-
   Champaign, nor the names of its contributors may be used to endorse or
   promote products derived from this Software without specific prior
   written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS WITH THE SOFTWARE.




Licenseeâs use of the PCRE third party component is subject to the following terms and conditions:

------------
PCRE LICENCE
------------
PCRE is a library of functions to support regular expressions whose syntax
and semantics are as close as possible to those of the Perl 5 language.
Release 8 of PCRE is distributed under the terms of the "BSD" licence, as
specified below. The documentation for PCRE, supplied in the "doc"
directory, is distributed under the same terms as the software itself. The
basic library functions are written in C and are freestanding. Also
included in the distribution is a set of C++ wrapper functions, and a just-
in-time compiler that can be used to optimize pattern matching. These are
both optional features that can be omitted when the library is built.

THE BASIC LIBRARY FUNCTIONS
---------------------------
Written by:       Philip Hazel
Email local part: ph10
Email domain:     cam.ac.uk
University of Cambridge Computing Service,
Cambridge, England.
Copyright (c) 1997-2012 University of Cambridge
All rights reserved.

PCRE JUST-IN-TIME COMPILATION SUPPORT
-------------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2010-2012 Zoltan Herczeg
All rights reserved.

STACK-LESS JUST-IN-TIME COMPILER
--------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2009-2012 Zoltan Herczeg
All rights reserved.

THE C++ WRAPPER FUNCTIONS
-------------------------
Contributed by:   Google Inc.
Copyright (c) 2007-2012, Google Inc.
All rights reserved.



THE "BSD" LICENCE
-----------------
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  * Redistributions of source code must retain the above copyright notice,
    this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

  * Neither the name of the University of Cambridge nor the name of Google
    Inc. nor the names of their contributors may be used to endorse or
    promote products derived from this software without specific prior
    written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2007-2009, Regents of the University of California

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the University of California, Berkeley nor
      the names of its contributors may be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * The name of the author may not be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2010 The University of Tennessee.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer listed in this license in the documentation and/or
      other materials provided with the distribution.
    * Neither the name of the copyright holders nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2012, The Science and Technology Facilities Council (STFC).

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the STFC nor the names of its contributors
      may be used to endorse or promote products derived from this
      software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE STFC BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows:

-- (C) Copyright 2013 King Abdullah University of Science and Technology
 Authors:
 Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa)
 David Keyes (david.keyes@kaust.edu.sa)
 Hatem Ltaief (hatem.ltaief@kaust.edu.sa)

 Redistribution  and  use  in  source and binary forms, with or without
 modification,  are  permitted  provided  that the following conditions
 are met:

 * Redistributions  of  source  code  must  retain  the above copyright
   notice,  this  list  of  conditions  and  the  following  disclaimer.
 * Redistributions  in  binary  form must reproduce the above copyright
   notice,  this list of conditions and the following disclaimer in the
   documentation  and/or other materials provided with the distribution.
 * Neither  the  name of the King Abdullah University of Science and
   Technology nor the names of its contributors may be used to endorse
   or promote products derived from this software without specific prior
   written permission.

 THIS  SOFTWARE  IS  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 ``AS IS''  AND  ANY  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 A  PARTICULAR  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT NOT
 LIMITED  TO,  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 DATA,  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 THEORY  OF  LIABILITY,  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF  THIS  SOFTWARE,  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE




Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows:

Copyright (c) 2012, University of Illinois.

All rights reserved.

Developed by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal with the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimers in the documentation and/or other materials provided
      with the distribution.
    * Neither the names of IMPACT Group, University of Illinois, nor
      the names of its contributors may be used to endorse or promote
      products derived from this Software without specific prior
      written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE
SOFTWARE.




Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license:

Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima
University. All rights reserved.

Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima
University and University of Tokyo.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the Hiroshima University nor the names of
      its contributors may be used to endorse or promote products
      derived from this software without specific prior written
      permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license:

Copyright 2010-2011, D. E. Shaw Research.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions, and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions, and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of D. E. Shaw Research nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license:

Copyright (c) 2015-2017, Norbert Juffa
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Licenseeâs use of the lz4 third party component is subject to the following terms and conditions:

Copyright (C) 2011-2013, Yann Collet.
BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




The NPP library uses code from the Boost Math Toolkit, and is subject to the following license:

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Portions of the Nsight Eclipse Edition is subject to the following license:

The Eclipse Foundation makes available all content in this plug-in
("Content"). Unless otherwise indicated below, the Content is provided
to you under the terms and conditions of the Eclipse Public License
Version 1.0 ("EPL"). A copy of the EPL is available at http://
www.eclipse.org/legal/epl-v10.html. For purposes of the EPL, "Program"
will mean the Content.

If you did not receive this Content directly from the Eclipse
Foundation, the Content is being redistributed by another party
("Redistributor") and different terms and conditions may apply to your
use of any object code in the Content. Check the Redistributor's
license that was provided with the Content. If no such license exists,
contact the Redistributor. Unless otherwise indicated below, the terms
and conditions of the EPL still apply to any source code in the
Content and such source code may be obtained at http://www.eclipse.org.




Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license:

License URL
https://github.com/openai/openai-gemm/blob/master/LICENSE

License Text
The MIT License

Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.




Licenseeâs use of the Visual Studio Setup Configuration Samples is subject to the following license:

The MIT License (MIT)
Copyright (C) Microsoft Corporation. All rights reserved.

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without restriction,
including without limitation the rights to use, copy, modify, merge,
publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



Licenseeâs use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.
The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .

Components of the driver and compiler used for binary management, including nvFatBin, nvcc,
and cuobjdump, use the Zstandard library which is subject to the following license:

BSD License

For Zstandard software

Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice, this
      list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright notice,
      this list of conditions and the following disclaimer in the documentation
      and/or other materials provided with the distribution.

    * Neither the name Facebook, nor Meta, nor the names of its contributors may
      be used to endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.














Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2024, NVIDIA Corporation.


Last updated on Jul 1, 2024.
      

















EULA










































1. License Agreement for NVIDIA Software Development Kits

1.1. License
1.1.1. License Grant
1.1.2. Distribution Requirements
1.1.3. Authorized Users
1.1.4. Pre-Release SDK
1.1.5. Updates
1.1.6. Components Under Other Licenses
1.1.7. Reservation of Rights


1.2. Limitations
1.3. Ownership
1.4. No Warranties
1.5. Limitation of Liability
1.6. Termination
1.7. General



2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits
2.1. License Scope
2.2. Distribution
2.3. Operating Systems
2.4. Audio and Video Encoders and Decoders
2.5. Licensing
2.6. Attachment A
2.7. Attachment B








EULA






 Â»

1. License Agreement for NVIDIA Software Development Kits



v12.5 |
PDF
|
Archive
Â 






End User License Agreement
NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021
The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.
Last updated: October 8, 2021.
Preface
The Software License Agreement in ChapterÂ 1 and the Supplement in ChapterÂ 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.
NVIDIA Driver
Description
This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.
NVIDIA CUDA Toolkit
Description
The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.
Default Install Location of CUDA Toolkit
Windows platform:

%ProgramFiles%\NVIDIA GPU Computing Toolkit\CUDA\v#.#


Linux platform:

/usr/local/cuda-#.#


Mac platform:

/Developer/NVIDIA/CUDA-#.#


NVIDIA CUDA Samples
Description
CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit.
NVIDIA Nsight Visual Studio Edition (Windows only)
Description
NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.
Default Install Location of Nsight Visual Studio Edition
Windows platform:

%ProgramFiles(x86)%\NVIDIA Corporation\Nsight Visual Studio Edition #.#




1. License Agreement for NVIDIA Software Development Kitsï

Important NoticeâRead before downloading, installing, copying or using the licensed software:
This license agreement, including exhibits attached (âAgreementâ) is a legal agreement between you and NVIDIA Corporation (âNVIDIAâ) and governs your use of a NVIDIA software development kit (âSDKâ).
Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.
This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.
If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case âyouâ will mean the entity you represent.
If you donât have the required age or authority to accept this Agreement, or if you donât accept all the terms and conditions of this Agreement, do not download, install or use the SDK.
You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.


1.1. Licenseï



1.1.1. License Grantï

Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to:

Install and use the SDK,
Modify and create derivative works of sample source code delivered in the SDK, and
Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.




1.1.2. Distribution Requirementsï

These are the distribution requirements for you to exercise the distribution grant:

Your application must have material additional functionality, beyond the included portions of the SDK.
The distributable portions of the SDK shall only be accessed by your application.
The following notice shall be included in modifications and derivative works of sample source code distributed: âThis software contains source code provided by NVIDIA Corporation.â
Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.
The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIAâs intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users.
You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.




1.1.3. Authorized Usersï

You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.
If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.
You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didnât follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.



1.1.4. Pre-Release SDKï

The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.
You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.
NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.



1.1.5. Updatesï

NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.



1.1.6. Components Under Other Licensesï

The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.
Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.



1.1.7. Reservation of Rightsï

NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.




1.2. Limitationsï

The following license limitations apply to your use of the SDK:

You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.
Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product.
Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.
You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.

You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be:

Disclosed or distributed in source code form;
Licensed for the purpose of making derivative works; or
Redistributable at no charge.


You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a âCritical Applicationâ). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.
You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorneyâs fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.
You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.




1.3. Ownershipï


NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2. This SDK may include software and materials from NVIDIAâs licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.


You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIAâs rights under Section 1.3.1.
You may, but donât have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com.




1.4. No Warrantiesï

THE SDK IS PROVIDED BY NVIDIA âAS ISâ AND âWITH ALL FAULTS.â TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.



1.5. Limitation of Liabilityï

TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIAâS AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.
These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.



1.6. Terminationï


This Agreement will continue to apply until terminated by either you or NVIDIA as described below.
If you want to terminate this Agreement, you may do so by stopping to use the SDK.

NVIDIA may, at any time, terminate this Agreement if:

(i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIAâs intellectual property rights);
(ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or
(iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIAâs sole discretion, the continued use of it is no longer commercially viable.


Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions.




1.7. Generalï

If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.
You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.
This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language.
The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.
If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. Unless otherwise specified, remedies are cumulative.
Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.
The SDK has been developed entirely at private expense and is âcommercial itemsâ consisting of âcommercial computer softwareâ and âcommercial computer software documentationâ provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.
The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasuryâs Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.
Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.
This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.




2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kitsï

The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (âAgreementâ) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.
This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.


2.1. License Scopeï

The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.



2.2. Distributionï

The portions of the SDK that are distributable under the Agreement are listed in Attachment A.



2.3. Operating Systemsï

Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).



2.4. Audio and Video Encoders and Decodersï

You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.



2.5. Licensingï

If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions@nvidia.com.



2.6. Attachment Aï

The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.







Component
CUDA Runtime


Windows
cudart.dll, cudart_static.lib, cudadevrt.lib


Mac OSX
libcudart.dylib, libcudart_static.a, libcudadevrt.a


Linux
libcudart.so, libcudart_static.a, libcudadevrt.a


Android
libcudart.so, libcudart_static.a, libcudadevrt.a


Component
CUDA FFT Library


Windows
cufft.dll, cufftw.dll, cufft.lib, cufftw.lib


Mac OSX
libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a


Linux
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Android
libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a


Component
CUDA BLAS Library


Windows
cublas.dll, cublasLt.dll


Mac OSX
libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a


Linux
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Android
libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a


Component
NVIDIA âDrop-inâ BLAS Library


Windows
nvblas.dll


Mac OSX
libnvblas.dylib


Linux
libnvblas.so


Component
CUDA Sparse Matrix Library


Windows
cusparse.dll, cusparse.lib


Mac OSX
libcusparse.dylib, libcusparse_static.a


Linux
libcusparse.so, libcusparse_static.a


Android
libcusparse.so, libcusparse_static.a


Component
CUDA Linear Solver Library


Windows
cusolver.dll, cusolver.lib


Mac OSX
libcusolver.dylib, libcusolver_static.a


Linux
libcusolver.so, libcusolver_static.a


Android
libcusolver.so, libcusolver_static.a


Component
CUDA Random Number Generation Library


Windows
curand.dll, curand.lib


Mac OSX
libcurand.dylib, libcurand_static.a


Linux
libcurand.so, libcurand_static.a


Android
libcurand.so, libcurand_static.a


Component
NVIDIA Performance Primitives Library


Windows
nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib


Mac OSX
libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a


Linux
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Android
libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a


Component
NVIDIA JPEG Library


Windows
nvjpeg.lib, nvjpeg.dll


Linux
libnvjpeg.so, libnvjpeg_static.a


Component
Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP


Mac OSX
libculibos.a


Linux
libculibos.a


Component
NVIDIA Runtime Compilation Library and Header


All
nvrtc.h


Windows
nvrtc.dll, nvrtc-builtins.dll


Mac OSX
libnvrtc.dylib, libnvrtc-builtins.dylib


Linux
libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a


Component
NVIDIA Optimizing Compiler Library


Windows
nvvm.dll


Mac OSX
libnvvm.dylib


Linux
libnvvm.so


Component
NVIDIA JIT Linking Library


Windows
libnvJitLink.dll, libnvJitLink.lib


Linux
libnvJitLink.so, libnvJitLink_static.a


Component
NVIDIA Common Device Math Functions Library


Windows
libdevice.10.bc


Mac OSX
libdevice.10.bc


Linux
libdevice.10.bc


Component
CUDA Occupancy Calculation Header Library


All
cuda_occupancy.h


Component
CUDA Half Precision Headers


All
cuda_fp16.h, cuda_fp16.hpp


Component
CUDA Profiling Tools Interface (CUPTI) Library


Windows
cupti.dll


Mac OSX
libcupti.dylib


Linux
libcupti.so


Component
NVIDIA Tools Extension Library


Windows
nvToolsExt.dll, nvToolsExt.lib


Mac OSX
libnvToolsExt.dylib


Linux
libnvToolsExt.so


Component
NVIDIA CUDA Driver Libraries


Linux
libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a


Component
NVIDIA CUDA File IO Libraries and Header


All
cufile.h


Linux
libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a



In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply:

The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.




2.7. Attachment Bï

Additional Licensing Obligations
The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions:


Licenseeâs use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:

This product includes copyrighted third-party software licensed
under the terms of the GNU General Public License v3 ("GPL v3").
All third-party software packages are copyright by their respective
authors. GPL v3 terms and conditions are hereby incorporated into
the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt


Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests@nvidia.com. This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.

Component          License
CUDA-GDB           GPL v3



Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licenseeâs use of the H.264 video codecs are solely the responsibility of Licensee.

Licenseeâs use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. All third-party software packages are copyright by their respective authors. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html
In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Licenseeâs use of the LLVM third party component is subject to the following terms and conditions:

======================================================
LLVM Release License
======================================================
University of Illinois/NCSA
Open Source License

Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.
All rights reserved.

Developed by:

    LLVM Team

    University of Illinois at Urbana-Champaign

    http://llvm.org

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to
deal with the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

*  Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimers.

*  Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimers in the
   documentation and/or other materials provided with the distribution.

*  Neither the names of the LLVM Team, University of Illinois at Urbana-
   Champaign, nor the names of its contributors may be used to endorse or
   promote products derived from this Software without specific prior
   written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS WITH THE SOFTWARE.




Licenseeâs use of the PCRE third party component is subject to the following terms and conditions:

------------
PCRE LICENCE
------------
PCRE is a library of functions to support regular expressions whose syntax
and semantics are as close as possible to those of the Perl 5 language.
Release 8 of PCRE is distributed under the terms of the "BSD" licence, as
specified below. The documentation for PCRE, supplied in the "doc"
directory, is distributed under the same terms as the software itself. The
basic library functions are written in C and are freestanding. Also
included in the distribution is a set of C++ wrapper functions, and a just-
in-time compiler that can be used to optimize pattern matching. These are
both optional features that can be omitted when the library is built.

THE BASIC LIBRARY FUNCTIONS
---------------------------
Written by:       Philip Hazel
Email local part: ph10
Email domain:     cam.ac.uk
University of Cambridge Computing Service,
Cambridge, England.
Copyright (c) 1997-2012 University of Cambridge
All rights reserved.

PCRE JUST-IN-TIME COMPILATION SUPPORT
-------------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2010-2012 Zoltan Herczeg
All rights reserved.

STACK-LESS JUST-IN-TIME COMPILER
--------------------------------
Written by:       Zoltan Herczeg
Email local part: hzmester
Emain domain:     freemail.hu
Copyright(c) 2009-2012 Zoltan Herczeg
All rights reserved.

THE C++ WRAPPER FUNCTIONS
-------------------------
Contributed by:   Google Inc.
Copyright (c) 2007-2012, Google Inc.
All rights reserved.



THE "BSD" LICENCE
-----------------
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  * Redistributions of source code must retain the above copyright notice,
    this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

  * Neither the name of the University of Cambridge nor the name of Google
    Inc. nor the names of their contributors may be used to endorse or
    promote products derived from this software without specific prior
    written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2007-2009, Regents of the University of California

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the University of California, Berkeley nor
      the names of its contributors may be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * The name of the author may not be used to endorse or promote
      products derived from this software without specific prior
      written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2010 The University of Tennessee.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer listed in this license in the documentation and/or
      other materials provided with the distribution.
    * Neither the name of the copyright holders nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows:

Copyright (c) 2012, The Science and Technology Facilities Council (STFC).

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the STFC nor the names of its contributors
      may be used to endorse or promote products derived from this
      software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE STFC BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows:

-- (C) Copyright 2013 King Abdullah University of Science and Technology
 Authors:
 Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa)
 David Keyes (david.keyes@kaust.edu.sa)
 Hatem Ltaief (hatem.ltaief@kaust.edu.sa)

 Redistribution  and  use  in  source and binary forms, with or without
 modification,  are  permitted  provided  that the following conditions
 are met:

 * Redistributions  of  source  code  must  retain  the above copyright
   notice,  this  list  of  conditions  and  the  following  disclaimer.
 * Redistributions  in  binary  form must reproduce the above copyright
   notice,  this list of conditions and the following disclaimer in the
   documentation  and/or other materials provided with the distribution.
 * Neither  the  name of the King Abdullah University of Science and
   Technology nor the names of its contributors may be used to endorse
   or promote products derived from this software without specific prior
   written permission.

 THIS  SOFTWARE  IS  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 ``AS IS''  AND  ANY  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 LIMITED  TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 A  PARTICULAR  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT NOT
 LIMITED  TO,  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 DATA,  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 THEORY  OF  LIABILITY,  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF  THIS  SOFTWARE,  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE




Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows:

Copyright (c) 2012, University of Illinois.

All rights reserved.

Developed by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal with the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimers in the documentation and/or other materials provided
      with the distribution.
    * Neither the names of IMPACT Group, University of Illinois, nor
      the names of its contributors may be used to endorse or promote
      products derived from this Software without specific prior
      written permission.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR
IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE
SOFTWARE.




Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license:

Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima
University. All rights reserved.

Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima
University and University of Tokyo.  All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of the Hiroshima University nor the names of
      its contributors may be used to endorse or promote products
      derived from this software without specific prior written
      permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license:

Copyright 2010-2011, D. E. Shaw Research.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions, and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions, and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of D. E. Shaw Research nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license:

Copyright (c) 2015-2017, Norbert Juffa
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




Licenseeâs use of the lz4 third party component is subject to the following terms and conditions:

Copyright (C) 2011-2013, Yann Collet.
BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php)

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.




The NPP library uses code from the Boost Math Toolkit, and is subject to the following license:

Boost Software License - Version 1.0 - August 17th, 2003
. . . .

Permission is hereby granted, free of charge, to any person or
organization obtaining a copy of the software and accompanying
documentation covered by this license (the "Software") to use,
reproduce, display, distribute, execute, and transmit the Software,
and to prepare derivative works of the Software, and to permit
third-parties to whom the Software is furnished to do so, all
subject to the following:

The copyright notices in the Software and this entire statement,
including the above license grant, this restriction and the following
disclaimer, must be included in all copies of the Software, in whole
or in part, and all derivative works of the Software, unless such
copies or derivative works are solely in the form of machine-executable
object code generated by a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND
NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR
ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR
OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.




Portions of the Nsight Eclipse Edition is subject to the following license:

The Eclipse Foundation makes available all content in this plug-in
("Content"). Unless otherwise indicated below, the Content is provided
to you under the terms and conditions of the Eclipse Public License
Version 1.0 ("EPL"). A copy of the EPL is available at http://
www.eclipse.org/legal/epl-v10.html. For purposes of the EPL, "Program"
will mean the Content.

If you did not receive this Content directly from the Eclipse
Foundation, the Content is being redistributed by another party
("Redistributor") and different terms and conditions may apply to your
use of any object code in the Content. Check the Redistributor's
license that was provided with the Content. If no such license exists,
contact the Redistributor. Unless otherwise indicated below, the terms
and conditions of the EPL still apply to any source code in the
Content and such source code may be obtained at http://www.eclipse.org.




Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license:

License URL
https://github.com/openai/openai-gemm/blob/master/LICENSE

License Text
The MIT License

Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.




Licenseeâs use of the Visual Studio Setup Configuration Samples is subject to the following license:

The MIT License (MIT)
Copyright (C) Microsoft Corporation. All rights reserved.

Permission is hereby granted, free of charge, to any person
obtaining a copy of this software and associated documentation
files (the "Software"), to deal in the Software without restriction,
including without limitation the rights to use, copy, modify, merge,
publish, distribute, sublicense, and/or sell copies of the Software,
and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



Licenseeâs use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.
The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .

Components of the driver and compiler used for binary management, including nvFatBin, nvcc,
and cuobjdump, use the Zstandard library which is subject to the following license:

BSD License

For Zstandard software

Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

    * Redistributions of source code must retain the above copyright notice, this
      list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright notice,
      this list of conditions and the following disclaimer in the documentation
      and/or other materials provided with the distribution.

    * Neither the name Facebook, nor Meta, nor the names of its contributors may
      be used to endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
SUCH DAMAGE.














Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2024, NVIDIA Corporation.


Last updated on Jul 1, 2024.
      


















1. Introduction — Quick Start Guide 12.5 documentation








































1. Introduction
2. Windows
2.1. Network Installer
2.2. Local Installer
2.3. Pip Wheels - Windows
2.4. Conda


3. Linux
3.1. Linux x86_64
3.1.1. Redhat / CentOS
3.1.1.1. RPM Installer
3.1.1.2. Runfile Installer


3.1.2. Fedora
3.1.2.1. RPM Installer
3.1.2.2. Runfile Installer


3.1.3. SUSE Linux Enterprise Server
3.1.3.1. RPM Installer
3.1.3.2. Runfile Installer


3.1.4. OpenSUSE
3.1.4.1. RPM Installer
3.1.4.2. Runfile Installer


3.1.5. Amazon Linux 2023
3.1.5.1. Prepare Amazon Linux 2023
3.1.5.2. Local Repo Installation for Amazon Linux
3.1.5.3. Network Repo Installation for Amazon Linux
3.1.5.4. Common Installation Instructions for Amazon Linux


3.1.6. Pip Wheels - Linux
3.1.7. Conda
3.1.8. WSL
3.1.9. Ubuntu
3.1.9.1. Debian Installer
3.1.9.2. Runfile Installer


3.1.10. Debian
3.1.10.1. Debian Installer
3.1.10.2. Runfile Installer






4. Notices
4.1. Notice
4.2. OpenCL
4.3. Trademarks








Quick Start Guide





 »
1. Introduction



v12.5 |
PDF
|
Archive
 






CUDA Quick Start Guide
Minimal first-steps instructions to get CUDA running on a standard system.

1. Introductionï
This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.
These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide.
The CUDA installation packages can be found on the CUDA Downloads Page.


2. Windowsï
When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide.

2.1. Network Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to download and install all components.
Once the download completes, the installation will begin automatically.
Once the installation completes, click ânextâ to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.2. Local Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to install all components.
Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.3. Pip Wheels - Windowsï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125



2.4. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda





3. Linuxï
CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.

3.1. Linux x86_64ï
For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.

3.1.1. Redhat / CentOSï
When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.1.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install EPEL to satisfy the DKMS dependency by following the instructions at EPELâs website.
Enable optional repos:
On RHEL 8 Linux only, execute the following steps to enable optional repositories.

On x86_64 workstation:
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms





Install the repository meta-data, clean the yum cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo yum clean expire-cache
sudo yum install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.1.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.2. Fedoraï
When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.2.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the RPMFusion free repository to satisfy the Akmods dependency:
su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm'



Install the repository meta-data, clean the dnf cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf clean expire-cache
sudo dnf install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.2.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force



Run the below command:
sudo grub2-mkconfig -o /boot/grub2/grub.cfg



Reboot the system:
sudo reboot





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface.
Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.3. SUSE Linux Enterprise Serverï
When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.3.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo SUSEConnect --product PackageHub/15/x86_64
sudo zypper refresh
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.3.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.4. OpenSUSEï
When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.4.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo zypper refresh
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.4.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initrd:
sudo /sbin/mkinitrd





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                    ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.5. Amazon Linux 2023ï

3.1.5.1. Prepare Amazon Linux 2023ï

Perform the pre-installation actions.
The kernel headers and development packages for the currently running kernel can be installed with:
sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r)



Choose an installation method: local repo or network repo.



3.1.5.2. Local Repo Installation for Amazon Linuxï

Install local repository on file system:
sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm






3.1.5.3. Network Repo Installation for Amazon Linuxï

Enable the network repository and clean the DN cache:
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
sudo dnf clean expire-cache






3.1.5.4. Common Installation Instructions for Amazon Linuxï
These instructions apply to both local and network installation for Amazon Linux.

Install CUDA SDK:
sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit



Install GPUDirect Filesystem:
sudo dnf install nvidia-gds



Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.

Reboot the system:
sudo reboot



Perform the post-installation actions.




3.1.6. Pip Wheels - Linuxï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
python3 -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
python3 -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
python3 -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12
nvidia-opencl-cu12
nvidia-nvjitlink-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125
nvidia-opencl-cu125
nvidia-nvjitlink-cu125



3.1.7. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda




3.1.8. WSLï
These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case.

Install repository meta-data

sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb



Update the CUDA public GPG key
sudo apt-key del 7fa2af80


When installing using the local repo:
sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/


When installing using the network repo:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


Pin file to prioritize CUDA repository:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<architecture>/cuda-<distro>.pin
sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600



Update the Apt repository cache and install CUDA
sudo apt-get update
sudo apt-get install cuda






3.1.9. Ubuntuï
When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the Debian installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.9.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA:
sudo dpkg --install cuda-repo-<distro>-<version>.<architecture>.deb
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.9.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.10. Debianï
When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Linux Installation Guide.

3.1.10.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA:
sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.10.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.








4. Noticesï

4.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


4.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


4.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jun 25, 2024.
      


















1. Introduction — Quick Start Guide 12.5 documentation








































1. Introduction
2. Windows
2.1. Network Installer
2.2. Local Installer
2.3. Pip Wheels - Windows
2.4. Conda


3. Linux
3.1. Linux x86_64
3.1.1. Redhat / CentOS
3.1.1.1. RPM Installer
3.1.1.2. Runfile Installer


3.1.2. Fedora
3.1.2.1. RPM Installer
3.1.2.2. Runfile Installer


3.1.3. SUSE Linux Enterprise Server
3.1.3.1. RPM Installer
3.1.3.2. Runfile Installer


3.1.4. OpenSUSE
3.1.4.1. RPM Installer
3.1.4.2. Runfile Installer


3.1.5. Amazon Linux 2023
3.1.5.1. Prepare Amazon Linux 2023
3.1.5.2. Local Repo Installation for Amazon Linux
3.1.5.3. Network Repo Installation for Amazon Linux
3.1.5.4. Common Installation Instructions for Amazon Linux


3.1.6. Pip Wheels - Linux
3.1.7. Conda
3.1.8. WSL
3.1.9. Ubuntu
3.1.9.1. Debian Installer
3.1.9.2. Runfile Installer


3.1.10. Debian
3.1.10.1. Debian Installer
3.1.10.2. Runfile Installer






4. Notices
4.1. Notice
4.2. OpenCL
4.3. Trademarks








Quick Start Guide





 »
1. Introduction



v12.5 |
PDF
|
Archive
 






CUDA Quick Start Guide
Minimal first-steps instructions to get CUDA running on a standard system.

1. Introductionï
This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.
These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide.
The CUDA installation packages can be found on the CUDA Downloads Page.


2. Windowsï
When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide.

2.1. Network Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to download and install all components.
Once the download completes, the installation will begin automatically.
Once the installation completes, click ânextâ to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.2. Local Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to install all components.
Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.3. Pip Wheels - Windowsï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125



2.4. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda





3. Linuxï
CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.

3.1. Linux x86_64ï
For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.

3.1.1. Redhat / CentOSï
When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.1.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install EPEL to satisfy the DKMS dependency by following the instructions at EPELâs website.
Enable optional repos:
On RHEL 8 Linux only, execute the following steps to enable optional repositories.

On x86_64 workstation:
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms





Install the repository meta-data, clean the yum cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo yum clean expire-cache
sudo yum install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.1.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.2. Fedoraï
When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.2.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the RPMFusion free repository to satisfy the Akmods dependency:
su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm'



Install the repository meta-data, clean the dnf cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf clean expire-cache
sudo dnf install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.2.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force



Run the below command:
sudo grub2-mkconfig -o /boot/grub2/grub.cfg



Reboot the system:
sudo reboot





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface.
Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.3. SUSE Linux Enterprise Serverï
When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.3.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo SUSEConnect --product PackageHub/15/x86_64
sudo zypper refresh
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.3.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.4. OpenSUSEï
When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.4.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo zypper refresh
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.4.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initrd:
sudo /sbin/mkinitrd





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                    ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.5. Amazon Linux 2023ï

3.1.5.1. Prepare Amazon Linux 2023ï

Perform the pre-installation actions.
The kernel headers and development packages for the currently running kernel can be installed with:
sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r)



Choose an installation method: local repo or network repo.



3.1.5.2. Local Repo Installation for Amazon Linuxï

Install local repository on file system:
sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm






3.1.5.3. Network Repo Installation for Amazon Linuxï

Enable the network repository and clean the DN cache:
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
sudo dnf clean expire-cache






3.1.5.4. Common Installation Instructions for Amazon Linuxï
These instructions apply to both local and network installation for Amazon Linux.

Install CUDA SDK:
sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit



Install GPUDirect Filesystem:
sudo dnf install nvidia-gds



Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.

Reboot the system:
sudo reboot



Perform the post-installation actions.




3.1.6. Pip Wheels - Linuxï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
python3 -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
python3 -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
python3 -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12
nvidia-opencl-cu12
nvidia-nvjitlink-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125
nvidia-opencl-cu125
nvidia-nvjitlink-cu125



3.1.7. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda




3.1.8. WSLï
These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case.

Install repository meta-data

sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb



Update the CUDA public GPG key
sudo apt-key del 7fa2af80


When installing using the local repo:
sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/


When installing using the network repo:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


Pin file to prioritize CUDA repository:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<architecture>/cuda-<distro>.pin
sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600



Update the Apt repository cache and install CUDA
sudo apt-get update
sudo apt-get install cuda






3.1.9. Ubuntuï
When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the Debian installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.9.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA:
sudo dpkg --install cuda-repo-<distro>-<version>.<architecture>.deb
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.9.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.10. Debianï
When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Linux Installation Guide.

3.1.10.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA:
sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.10.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.








4. Noticesï

4.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


4.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


4.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jun 25, 2024.
      


















1. Introduction — Quick Start Guide 12.5 documentation








































1. Introduction
2. Windows
2.1. Network Installer
2.2. Local Installer
2.3. Pip Wheels - Windows
2.4. Conda


3. Linux
3.1. Linux x86_64
3.1.1. Redhat / CentOS
3.1.1.1. RPM Installer
3.1.1.2. Runfile Installer


3.1.2. Fedora
3.1.2.1. RPM Installer
3.1.2.2. Runfile Installer


3.1.3. SUSE Linux Enterprise Server
3.1.3.1. RPM Installer
3.1.3.2. Runfile Installer


3.1.4. OpenSUSE
3.1.4.1. RPM Installer
3.1.4.2. Runfile Installer


3.1.5. Amazon Linux 2023
3.1.5.1. Prepare Amazon Linux 2023
3.1.5.2. Local Repo Installation for Amazon Linux
3.1.5.3. Network Repo Installation for Amazon Linux
3.1.5.4. Common Installation Instructions for Amazon Linux


3.1.6. Pip Wheels - Linux
3.1.7. Conda
3.1.8. WSL
3.1.9. Ubuntu
3.1.9.1. Debian Installer
3.1.9.2. Runfile Installer


3.1.10. Debian
3.1.10.1. Debian Installer
3.1.10.2. Runfile Installer






4. Notices
4.1. Notice
4.2. OpenCL
4.3. Trademarks








Quick Start Guide





 »
1. Introduction



v12.5 |
PDF
|
Archive
 






CUDA Quick Start Guide
Minimal first-steps instructions to get CUDA running on a standard system.

1. Introductionï
This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.
These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide.
The CUDA installation packages can be found on the CUDA Downloads Page.


2. Windowsï
When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide.

2.1. Network Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to download and install all components.
Once the download completes, the installation will begin automatically.
Once the installation completes, click ânextâ to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.2. Local Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to install all components.
Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.3. Pip Wheels - Windowsï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125



2.4. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda





3. Linuxï
CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.

3.1. Linux x86_64ï
For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.

3.1.1. Redhat / CentOSï
When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.1.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install EPEL to satisfy the DKMS dependency by following the instructions at EPELâs website.
Enable optional repos:
On RHEL 8 Linux only, execute the following steps to enable optional repositories.

On x86_64 workstation:
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms





Install the repository meta-data, clean the yum cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo yum clean expire-cache
sudo yum install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.1.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.2. Fedoraï
When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.2.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the RPMFusion free repository to satisfy the Akmods dependency:
su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm'



Install the repository meta-data, clean the dnf cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf clean expire-cache
sudo dnf install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.2.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force



Run the below command:
sudo grub2-mkconfig -o /boot/grub2/grub.cfg



Reboot the system:
sudo reboot





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface.
Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.3. SUSE Linux Enterprise Serverï
When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.3.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo SUSEConnect --product PackageHub/15/x86_64
sudo zypper refresh
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.3.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.4. OpenSUSEï
When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.4.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo zypper refresh
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.4.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initrd:
sudo /sbin/mkinitrd





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                    ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.5. Amazon Linux 2023ï

3.1.5.1. Prepare Amazon Linux 2023ï

Perform the pre-installation actions.
The kernel headers and development packages for the currently running kernel can be installed with:
sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r)



Choose an installation method: local repo or network repo.



3.1.5.2. Local Repo Installation for Amazon Linuxï

Install local repository on file system:
sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm






3.1.5.3. Network Repo Installation for Amazon Linuxï

Enable the network repository and clean the DN cache:
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
sudo dnf clean expire-cache






3.1.5.4. Common Installation Instructions for Amazon Linuxï
These instructions apply to both local and network installation for Amazon Linux.

Install CUDA SDK:
sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit



Install GPUDirect Filesystem:
sudo dnf install nvidia-gds



Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.

Reboot the system:
sudo reboot



Perform the post-installation actions.




3.1.6. Pip Wheels - Linuxï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
python3 -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
python3 -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
python3 -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12
nvidia-opencl-cu12
nvidia-nvjitlink-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125
nvidia-opencl-cu125
nvidia-nvjitlink-cu125



3.1.7. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda




3.1.8. WSLï
These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case.

Install repository meta-data

sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb



Update the CUDA public GPG key
sudo apt-key del 7fa2af80


When installing using the local repo:
sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/


When installing using the network repo:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


Pin file to prioritize CUDA repository:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<architecture>/cuda-<distro>.pin
sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600



Update the Apt repository cache and install CUDA
sudo apt-get update
sudo apt-get install cuda






3.1.9. Ubuntuï
When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the Debian installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.9.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA:
sudo dpkg --install cuda-repo-<distro>-<version>.<architecture>.deb
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.9.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.10. Debianï
When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Linux Installation Guide.

3.1.10.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA:
sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.10.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.








4. Noticesï

4.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


4.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


4.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jun 25, 2024.
      


















1. Introduction — Quick Start Guide 12.5 documentation








































1. Introduction
2. Windows
2.1. Network Installer
2.2. Local Installer
2.3. Pip Wheels - Windows
2.4. Conda


3. Linux
3.1. Linux x86_64
3.1.1. Redhat / CentOS
3.1.1.1. RPM Installer
3.1.1.2. Runfile Installer


3.1.2. Fedora
3.1.2.1. RPM Installer
3.1.2.2. Runfile Installer


3.1.3. SUSE Linux Enterprise Server
3.1.3.1. RPM Installer
3.1.3.2. Runfile Installer


3.1.4. OpenSUSE
3.1.4.1. RPM Installer
3.1.4.2. Runfile Installer


3.1.5. Amazon Linux 2023
3.1.5.1. Prepare Amazon Linux 2023
3.1.5.2. Local Repo Installation for Amazon Linux
3.1.5.3. Network Repo Installation for Amazon Linux
3.1.5.4. Common Installation Instructions for Amazon Linux


3.1.6. Pip Wheels - Linux
3.1.7. Conda
3.1.8. WSL
3.1.9. Ubuntu
3.1.9.1. Debian Installer
3.1.9.2. Runfile Installer


3.1.10. Debian
3.1.10.1. Debian Installer
3.1.10.2. Runfile Installer






4. Notices
4.1. Notice
4.2. OpenCL
4.3. Trademarks








Quick Start Guide





 »
1. Introduction



v12.5 |
PDF
|
Archive
 






CUDA Quick Start Guide
Minimal first-steps instructions to get CUDA running on a standard system.

1. Introductionï
This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.
These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide.
The CUDA installation packages can be found on the CUDA Downloads Page.


2. Windowsï
When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide.

2.1. Network Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to download and install all components.
Once the download completes, the installation will begin automatically.
Once the installation completes, click ânextâ to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.2. Local Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to install all components.
Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.3. Pip Wheels - Windowsï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125



2.4. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda





3. Linuxï
CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.

3.1. Linux x86_64ï
For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.

3.1.1. Redhat / CentOSï
When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.1.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install EPEL to satisfy the DKMS dependency by following the instructions at EPELâs website.
Enable optional repos:
On RHEL 8 Linux only, execute the following steps to enable optional repositories.

On x86_64 workstation:
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms





Install the repository meta-data, clean the yum cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo yum clean expire-cache
sudo yum install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.1.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.2. Fedoraï
When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.2.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the RPMFusion free repository to satisfy the Akmods dependency:
su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm'



Install the repository meta-data, clean the dnf cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf clean expire-cache
sudo dnf install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.2.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force



Run the below command:
sudo grub2-mkconfig -o /boot/grub2/grub.cfg



Reboot the system:
sudo reboot





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface.
Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.3. SUSE Linux Enterprise Serverï
When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.3.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo SUSEConnect --product PackageHub/15/x86_64
sudo zypper refresh
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.3.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.4. OpenSUSEï
When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.4.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo zypper refresh
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.4.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initrd:
sudo /sbin/mkinitrd





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                    ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.5. Amazon Linux 2023ï

3.1.5.1. Prepare Amazon Linux 2023ï

Perform the pre-installation actions.
The kernel headers and development packages for the currently running kernel can be installed with:
sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r)



Choose an installation method: local repo or network repo.



3.1.5.2. Local Repo Installation for Amazon Linuxï

Install local repository on file system:
sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm






3.1.5.3. Network Repo Installation for Amazon Linuxï

Enable the network repository and clean the DN cache:
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
sudo dnf clean expire-cache






3.1.5.4. Common Installation Instructions for Amazon Linuxï
These instructions apply to both local and network installation for Amazon Linux.

Install CUDA SDK:
sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit



Install GPUDirect Filesystem:
sudo dnf install nvidia-gds



Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.

Reboot the system:
sudo reboot



Perform the post-installation actions.




3.1.6. Pip Wheels - Linuxï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
python3 -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
python3 -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
python3 -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12
nvidia-opencl-cu12
nvidia-nvjitlink-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125
nvidia-opencl-cu125
nvidia-nvjitlink-cu125



3.1.7. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda




3.1.8. WSLï
These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case.

Install repository meta-data

sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb



Update the CUDA public GPG key
sudo apt-key del 7fa2af80


When installing using the local repo:
sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/


When installing using the network repo:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


Pin file to prioritize CUDA repository:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<architecture>/cuda-<distro>.pin
sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600



Update the Apt repository cache and install CUDA
sudo apt-get update
sudo apt-get install cuda






3.1.9. Ubuntuï
When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the Debian installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.9.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA:
sudo dpkg --install cuda-repo-<distro>-<version>.<architecture>.deb
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.9.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.10. Debianï
When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Linux Installation Guide.

3.1.10.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA:
sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.10.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.








4. Noticesï

4.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


4.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


4.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jun 25, 2024.
      


















1. Introduction — Quick Start Guide 12.5 documentation








































1. Introduction
2. Windows
2.1. Network Installer
2.2. Local Installer
2.3. Pip Wheels - Windows
2.4. Conda


3. Linux
3.1. Linux x86_64
3.1.1. Redhat / CentOS
3.1.1.1. RPM Installer
3.1.1.2. Runfile Installer


3.1.2. Fedora
3.1.2.1. RPM Installer
3.1.2.2. Runfile Installer


3.1.3. SUSE Linux Enterprise Server
3.1.3.1. RPM Installer
3.1.3.2. Runfile Installer


3.1.4. OpenSUSE
3.1.4.1. RPM Installer
3.1.4.2. Runfile Installer


3.1.5. Amazon Linux 2023
3.1.5.1. Prepare Amazon Linux 2023
3.1.5.2. Local Repo Installation for Amazon Linux
3.1.5.3. Network Repo Installation for Amazon Linux
3.1.5.4. Common Installation Instructions for Amazon Linux


3.1.6. Pip Wheels - Linux
3.1.7. Conda
3.1.8. WSL
3.1.9. Ubuntu
3.1.9.1. Debian Installer
3.1.9.2. Runfile Installer


3.1.10. Debian
3.1.10.1. Debian Installer
3.1.10.2. Runfile Installer






4. Notices
4.1. Notice
4.2. OpenCL
4.3. Trademarks








Quick Start Guide





 »
1. Introduction



v12.5 |
PDF
|
Archive
 






CUDA Quick Start Guide
Minimal first-steps instructions to get CUDA running on a standard system.

1. Introductionï
This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.
These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide.
The CUDA installation packages can be found on the CUDA Downloads Page.


2. Windowsï
When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide.

2.1. Network Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to download and install all components.
Once the download completes, the installation will begin automatically.
Once the installation completes, click ânextâ to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.2. Local Installerï
Perform the following steps to install CUDA and verify the installation.

Launch the downloaded installer package.
Read and accept the EULA.
Select next to install all components.
Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.
Click close to close the installer.
Navigate to the Samplesâ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody.
Open the nbody Visual Studio solution file for the version of Visual Studio you have installed.


Open the Build menu within Visual Studio and click Build Solution.


Navigate to the CUDA Samples build directory and run the nbody sample.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





2.3. Pip Wheels - Windowsï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125



2.4. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda





3. Linuxï
CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.

3.1. Linux x86_64ï
For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.

3.1.1. Redhat / CentOSï
When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.1.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install EPEL to satisfy the DKMS dependency by following the instructions at EPELâs website.
Enable optional repos:
On RHEL 8 Linux only, execute the following steps to enable optional repositories.

On x86_64 workstation:
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms





Install the repository meta-data, clean the yum cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo yum clean expire-cache
sudo yum install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.1.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.2. Fedoraï
When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.2.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the RPMFusion free repository to satisfy the Akmods dependency:
su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm'



Install the repository meta-data, clean the dnf cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf clean expire-cache
sudo dnf install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.2.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo dracut --force



Run the below command:
sudo grub2-mkconfig -o /boot/grub2/grub.cfg



Reboot the system:
sudo reboot





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface.
Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.3. SUSE Linux Enterprise Serverï
When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.3.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo SUSEConnect --product PackageHub/15/x86_64
sudo zypper refresh
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.3.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.4. OpenSUSEï
When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.4.1. RPM Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, refresh the Zypper cache, and install CUDA:
sudo rpm --install cuda-repo-<distro>-<version>.<architecture>.rpm
sudo rpm --erase gpg-pubkey-7fa2af80*
sudo zypper refresh
sudo zypper install cuda



Add the user to the video group:
sudo usermod -a -G video <username>



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.4.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initrd:
sudo /sbin/mkinitrd





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                    ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.5. Amazon Linux 2023ï

3.1.5.1. Prepare Amazon Linux 2023ï

Perform the pre-installation actions.
The kernel headers and development packages for the currently running kernel can be installed with:
sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r)



Choose an installation method: local repo or network repo.



3.1.5.2. Local Repo Installation for Amazon Linuxï

Install local repository on file system:
sudo rpm --install cuda-repo-amzn2023-X-Y-local-<version>*.x86_64.rpm






3.1.5.3. Network Repo Installation for Amazon Linuxï

Enable the network repository and clean the DN cache:
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
sudo dnf clean expire-cache






3.1.5.4. Common Installation Instructions for Amazon Linuxï
These instructions apply to both local and network installation for Amazon Linux.

Install CUDA SDK:
sudo dnf module install nvidia-driver:latest-dkms
sudo dnf install cuda-toolkit



Install GPUDirect Filesystem:
sudo dnf install nvidia-gds



Add libcuda.so symbolic link, if necessary:
The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.

Reboot the system:
sudo reboot



Perform the post-installation actions.




3.1.6. Pip Wheels - Linuxï
NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.
python3 -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.
python3 -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:
--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:
python3 -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:
python3 -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-nvml-dev-cu12
nvidia-cuda-nvrtc-cu12
nvidia-nvtx-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cublas-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvjpeg-cu12
nvidia-opencl-cu12
nvidia-nvjitlink-cu12

These metapackages install the following packages:

nvidia-nvml-dev-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cublas-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-nvtx-cu125
nvidia-cuda-nvrtc-cu125
nvidia-npp-cu125
nvidia-cusparse-cu125
nvidia-cusolver-cu125
nvidia-curand-cu125
nvidia-cufft-cu125
nvidia-nvjpeg-cu125
nvidia-opencl-cu125
nvidia-nvjitlink-cu125



3.1.7. Condaï
The Conda packages are available at https://anaconda.org/nvidia.
Installation
To perform a basic install of all CUDA Toolkit components using Conda, run the following command:
conda install cuda -c nvidia


Uninstallation
To uninstall the CUDA Toolkit using Conda, run the following command:
conda remove cuda




3.1.8. WSLï
These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case.

Install repository meta-data

sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb



Update the CUDA public GPG key
sudo apt-key del 7fa2af80


When installing using the local repo:
sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/


When installing using the network repo:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb


Pin file to prioritize CUDA repository:
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<architecture>/cuda-<distro>.pin
sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600



Update the Apt repository cache and install CUDA
sudo apt-get update
sudo apt-get install cuda






3.1.9. Ubuntuï
When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. In the case of the Debian installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide.

3.1.9.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA:
sudo dpkg --install cuda-repo-<distro>-<version>.<architecture>.deb
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.9.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.






3.1.10. Debianï
When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. The Runfile Installer is only available as a Local Installer. The Debian Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Linux Installation Guide.

3.1.10.1. Debian Installerï
Perform the following steps to install CUDA and verify the installation.

Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA:
sudo dpkg -i cuda-repo-<distro>_<version>_<architecture>.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub
sudo apt-key del 7fa2af80
wget https://developer.download.nvidia.com/compute/cuda/repos/<distro>/<arch>/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo add-apt-repository contrib
sudo apt-get update
sudo apt-get -y install cuda



Reboot the system to load the NVIDIA drivers:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.





3.1.10.2. Runfile Installerï
Perform the following steps to install CUDA and verify the installation.

Disable the Nouveau drivers:

Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents:
blacklist nouveau
options nouveau modeset=0



Regenerate the kernel initramfs:
sudo update-initramfs -u





Reboot into runlevel 3 by temporarily adding the number â3â and the word ânomodesetâ to the end of the systemâs kernel boot parameters.
Run the installer silently to install with the default selections (implies acceptance of the EULA):
sudo sh cuda_<version>_linux.run --silent



Create an xorg.conf file to use the NVIDIA GPU for display:
sudo nvidia-xconfig



Reboot the system to load the graphical interface:
sudo reboot



Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:
export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}



Install a writable copy of the samples from https://github.com/nvidia/cuda-samples, then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody.

Note
Run samples by navigating to the executableâs location, otherwise it will fail to locate dependent resources.








4. Noticesï

4.1. Noticeï
This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.


4.2. OpenCLï
OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.


4.3. Trademarksï
NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jun 25, 2024.
      

















CUDA Installation Guide for Microsoft Windows










































1. Introduction
1.1. System Requirements
1.2. About This Document



2. Installing CUDA Development Tools
2.1. Verify You Have a CUDA-Capable GPU
2.2. Download the NVIDIA CUDA Toolkit

2.3. Install the CUDA Software
2.3.1. Uninstalling the CUDA Software



2.4. Using Conda to Install the CUDA Software
2.4.1. Conda Overview
2.4.2. Installation
2.4.3. Uninstallation
2.4.4. Installing Previous CUDA Releases


2.5. Use a Suitable Driver Model

2.6. Verify the Installation
2.6.1. Running the Compiled Examples




3. Pip Wheels

4. Compiling CUDA Programs
4.1. Compiling Sample Projects
4.2. Sample Projects
4.3. Build Customizations for New Projects
4.4. Build Customizations for Existing Projects


5. Additional Considerations

6. Notices
6.1. Notice
6.2. OpenCL
6.3. Trademarks








Installation Guide Windows






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






CUDA Installation Guide for Microsoft Windows
The installation instructions for the CUDA Toolkit on Microsoft Windows systems.


1. Introductionï

CUDAÂ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind:

Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.

CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools.


1.1. System Requirementsï

To use CUDA on your system, you will need the following installed:

A CUDA-capable GPU
A supported version of Linux with a gcc compiler and toolchain
NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads)

Supported Microsoft WindowsÂ® operating systems:

Microsoft Windows 11 21H2
Microsoft Windows 11 22H2-SV2
Microsoft Windows 11 23H2
Microsoft Windows 10 21H2
Microsoft Windows 10 22H2
Microsoft Windows Server 2022



Table 1 Windows Compiler Support in CUDA 12.5ï










Compiler*
IDE
Native x86_64
Cross-compilation (32-bit on 64-bit)
C++ Dialect




MSVC Version 193x
Visual Studio 2022 17.x
YES
Not supported
C++14 (default), C++17, C++20


MSVC Version 192x
Visual Studio 2019 16.x
YES
C++14 (default), C++17


MSVC Version 191x
Visual Studio 2017 15.x (RTW and all updates)
YES
C++14 (default), C++17



* Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries  on  GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications.
Support for running x86 32-bit applications on x86_64 Windows is limited to use with:

CUDA Driver
CUDA Runtime (cudart)
CUDA Math Library (math.h)




1.2. About This Documentï

This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation.




2. Installing CUDA Development Toolsï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.
The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps:

Verify the system has a CUDA-capable GPU.
Download the NVIDIA CUDA Toolkit.
Install the NVIDIA CUDA Toolkit.
Test that the installed software runs correctly and communicates with the hardware.



2.1. Verify You Have a CUDA-Capable GPUï

You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager. Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus, that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products.
The Windows Device Manager can be opened via the following steps:

Open a run window from the Start Menu

Run:

control /name Microsoft.DeviceManager







2.2. Download the NVIDIA CUDA Toolkitï

The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads. Choose the platform you are using and one of the following installer formats:

Network Installer: A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time.
Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment.

The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification
The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.



2.3. Install the CUDA Softwareï

Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.

Note
The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.


Note
The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again.

Graphical Installation
Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.
Silent Installation
The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names.


Table 2 Possible Subpackage Namesï







Subpackage Name
Subpackage Description




Toolkit Subpackages (defaults to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5)


cuda_profiler_api_12.5
CUDA Profiler API.


cudart_12.5
CUDA Runtime libraries.


cuobjdump_12.5
Extracts information from cubin files.


cupti_12.5
The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.


cuxxfilt_12.5
The CUDA cu++ filt demangler tool.


demo_suite_12.5
Prebuilt demo applications using CUDA.


documentation_12.5
CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.


nvcc_12.5
CUDA compiler.


nvdisasm_12.5
Extracts information from standalone cubin files.


nvfatbin_12.5
Library for creating fatbinaries at runtime.


nvjitlink_12.5
nvJitLink library.


nvml_dev_12.5
NVML development libraries and headers.


nvprof_12.5
Tool for collecting and viewing CUDA application profiling data from the command-line.


nvprune_12.5
Prunes host object files and libraries to only contain device code for the specified targets.



nvrtc_12.5
nvrtc_dev_12.5

NVRTC runtime libraries.



nvtx_12.5
NVTX on Windows.


opencl_12.5
OpenCL library.


visual_profiler_12.5
Visual Profiler.


sanitizer_12.5
Compute Sanitizer API.


thrust_12.5
CUDA Thrust.



cublas_12.5
cublas_dev_12.5

cuBLAS runtime libraries.




cufft_12.5
cufft_dev_12.5

cuFFT runtime libraries.




curand_12.5
curand_dev_12.5

cuRAND runtime libraries.




cusolver_12.5
cusolver_dev_12.5

cuSOLVER runtime libraries.




cusparse_12.5
cusparse_dev_12.5

cuSPARSE runtime libraries.




npp_12.5
npp_dev_12.5

NPP runtime libraries.




nvjpeg_12.5
nvjpeg_dev_12.5

nvJPEG libraries.



nsight_compute_12.5
Nsight Compute.


nsight_systems_12.5
Nsight Systems.


nsight_vse_12.5
Installs the Nsight Visual Studio Edition plugin in all VS.


occupancy_calculator_12.5
Installs the CUDA_Occupancy_Calculator.xls tool.


visual_studio_integration_12.5
Installs CUDA project wizard and builds customization files in VS.


Driver Subpackages


Display.Driver
The NVIDIA Display Driver. Required to run CUDA applications.



For example, to install only the compiler and driver components:

<PackageName>.exe -s nvcc_12.1 Display.Driver


Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.
Extracting and Inspecting the Files Manually
Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip.
Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.

Note
Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment.



2.3.1. Uninstalling the CUDA Softwareï

All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.




2.4. Using Conda to Install the CUDA Softwareï

This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia.


2.4.1. Conda Overviewï

The Conda installation installs the CUDA Toolkit. The installation steps are listed below.



2.4.2. Installationï

To perform a basic install of all CUDA Toolkit components using Conda, run the following command:

conda install cuda -c nvidia





2.4.3. Uninstallationï

To uninstall the CUDA Toolkit using Conda, run the following command:

conda remove cuda





2.4.4. Installing Previous CUDA Releasesï

All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as:

conda install cuda -c nvidia/label/cuda-11.3.0



Note
Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as:

conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1


This example will install all packages released as part of CUDA 11.3.1.





2.5. Use a Suitable Driver Modelï

On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate:

The WDDM driver model is used for display devices.
The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.

TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).

Note
Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.


Note
NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.




2.6. Verify the Installationï

Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs.


2.6.1. Running the Compiled Examplesï

The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to:
Start > All Programs > Accessories > Command Prompt
CUDA Samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.
To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder.
This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1.



Figure 1 Valid Results from deviceQuery CUDA Sampleï


The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.
Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2.



Figure 2 Valid Results from bandwidthTest CUDA Sampleï


The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.
If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
To see a graphical representation of what CUDA can do, run the particles sample at

https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles







3. Pip Wheelsï

NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.

py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.

py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:

--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:

py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:

py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cublas-cu12
nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvfatbin-cu12
nvidia-nvjitlink-cu12
nvidia-nvjpeg-cu12
nvidia-nvml-dev-cu12
nvidia-nvtx-cu12
nvidia-opencl-cu12

These metapackages install the following packages:

nvidia-cublas-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-nvrtc-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-cufft-cu125
nvidia-curand-cu125
nvidia-cusolver-cu125
nvidia-cusparse-cu125
nvidia-npp-cu125
nvidia-nvfatbin-cu125
nvidia-nvjitlink-cu125
nvidia-nvjpeg-cu125
nvidia-nvml-dev-cu125
nvidia-nvtx-cu125
nvidia-opencl-cu125




4. Compiling CUDA Programsï

The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code. To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples


4.1. Compiling Sample Projectsï

The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest.
If you elected to use the default installation location, the output is placed in CUDA Samples\v12.5\bin\win64\Release. Build the program using the appropriate solution file and run the executable. If all works correctly, the output should be similar to Figure 2.



4.2. Sample Projectsï

The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.
A few of the example projects require some additional setup.
These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.
The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.


Table 3 CUDA Visual Studio .props locationsï







Visual Studio
CUDA 12.5 .props file Install Directory




Visual Studio 2015 (deprecated)
C:Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations


Visual Studio 2017
<Visual Studio Install Dir>\Common7\IDE\VC\VCTargets\BuildCustomizations


Visual Studio 2019
C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\BuildCustomizations


Visual Studio 2022
C:\Program Files\Microsoft Visual Studio\2022\Professional\MSBuild\Microsoft\VC\v170\BuildCustomizations



You can reference this CUDA 12.5.props file when building your own CUDA applications.



4.3. Build Customizations for New Projectsï

When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Projectâ¦ NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the âCUDA 12.5 Runtimeâ template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIAâs Build Customizations. All standard capabilities of Visual Studio C++ projects will be available.
To specify a custom CUDA Toolkit location, under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations.

Note
A supported version of MSVC must be installed to use this feature.




4.4. Build Customizations for Existing Projectsï

When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods:

Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizationsâ¦, then select the CUDA Toolkit version you would like to target.
Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties. Under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer.

While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.
If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps:

Open a run window from the Start Menu.

Run:

control sysdm.cpl



Select the Advanced tab at the top of the window.
Click Environment Variables at the bottom of the window.

Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item, selecting NVIDIA CUDA 12.5\CodeCUDA C/C++ File, and then selecting the file you wish to add.
For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following:

msbuild <projectname.extension> /t:Rebuild /p:CudaToolkitDir="drive:/path/to/new/toolkit/"






5. Additional Considerationsï

Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDAÂ C Programming Guide, located in the CUDA Toolkit documentation directory.
A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIAÂ® Nsightâ¢ Visual Studio Edition, and NVIDIA Visual Profiler.
For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/.



6. Noticesï



6.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



6.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



6.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Installation Guide for Microsoft Windows










































1. Introduction
1.1. System Requirements
1.2. About This Document



2. Installing CUDA Development Tools
2.1. Verify You Have a CUDA-Capable GPU
2.2. Download the NVIDIA CUDA Toolkit

2.3. Install the CUDA Software
2.3.1. Uninstalling the CUDA Software



2.4. Using Conda to Install the CUDA Software
2.4.1. Conda Overview
2.4.2. Installation
2.4.3. Uninstallation
2.4.4. Installing Previous CUDA Releases


2.5. Use a Suitable Driver Model

2.6. Verify the Installation
2.6.1. Running the Compiled Examples




3. Pip Wheels

4. Compiling CUDA Programs
4.1. Compiling Sample Projects
4.2. Sample Projects
4.3. Build Customizations for New Projects
4.4. Build Customizations for Existing Projects


5. Additional Considerations

6. Notices
6.1. Notice
6.2. OpenCL
6.3. Trademarks








Installation Guide Windows






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






CUDA Installation Guide for Microsoft Windows
The installation instructions for the CUDA Toolkit on Microsoft Windows systems.


1. Introductionï

CUDAÂ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind:

Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.

CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools.


1.1. System Requirementsï

To use CUDA on your system, you will need the following installed:

A CUDA-capable GPU
A supported version of Linux with a gcc compiler and toolchain
NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads)

Supported Microsoft WindowsÂ® operating systems:

Microsoft Windows 11 21H2
Microsoft Windows 11 22H2-SV2
Microsoft Windows 11 23H2
Microsoft Windows 10 21H2
Microsoft Windows 10 22H2
Microsoft Windows Server 2022



Table 1 Windows Compiler Support in CUDA 12.5ï










Compiler*
IDE
Native x86_64
Cross-compilation (32-bit on 64-bit)
C++ Dialect




MSVC Version 193x
Visual Studio 2022 17.x
YES
Not supported
C++14 (default), C++17, C++20


MSVC Version 192x
Visual Studio 2019 16.x
YES
C++14 (default), C++17


MSVC Version 191x
Visual Studio 2017 15.x (RTW and all updates)
YES
C++14 (default), C++17



* Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries  on  GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications.
Support for running x86 32-bit applications on x86_64 Windows is limited to use with:

CUDA Driver
CUDA Runtime (cudart)
CUDA Math Library (math.h)




1.2. About This Documentï

This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation.




2. Installing CUDA Development Toolsï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.
The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps:

Verify the system has a CUDA-capable GPU.
Download the NVIDIA CUDA Toolkit.
Install the NVIDIA CUDA Toolkit.
Test that the installed software runs correctly and communicates with the hardware.



2.1. Verify You Have a CUDA-Capable GPUï

You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager. Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus, that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products.
The Windows Device Manager can be opened via the following steps:

Open a run window from the Start Menu

Run:

control /name Microsoft.DeviceManager







2.2. Download the NVIDIA CUDA Toolkitï

The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads. Choose the platform you are using and one of the following installer formats:

Network Installer: A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time.
Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment.

The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification
The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.



2.3. Install the CUDA Softwareï

Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.

Note
The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.


Note
The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again.

Graphical Installation
Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.
Silent Installation
The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names.


Table 2 Possible Subpackage Namesï







Subpackage Name
Subpackage Description




Toolkit Subpackages (defaults to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5)


cuda_profiler_api_12.5
CUDA Profiler API.


cudart_12.5
CUDA Runtime libraries.


cuobjdump_12.5
Extracts information from cubin files.


cupti_12.5
The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.


cuxxfilt_12.5
The CUDA cu++ filt demangler tool.


demo_suite_12.5
Prebuilt demo applications using CUDA.


documentation_12.5
CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.


nvcc_12.5
CUDA compiler.


nvdisasm_12.5
Extracts information from standalone cubin files.


nvfatbin_12.5
Library for creating fatbinaries at runtime.


nvjitlink_12.5
nvJitLink library.


nvml_dev_12.5
NVML development libraries and headers.


nvprof_12.5
Tool for collecting and viewing CUDA application profiling data from the command-line.


nvprune_12.5
Prunes host object files and libraries to only contain device code for the specified targets.



nvrtc_12.5
nvrtc_dev_12.5

NVRTC runtime libraries.



nvtx_12.5
NVTX on Windows.


opencl_12.5
OpenCL library.


visual_profiler_12.5
Visual Profiler.


sanitizer_12.5
Compute Sanitizer API.


thrust_12.5
CUDA Thrust.



cublas_12.5
cublas_dev_12.5

cuBLAS runtime libraries.




cufft_12.5
cufft_dev_12.5

cuFFT runtime libraries.




curand_12.5
curand_dev_12.5

cuRAND runtime libraries.




cusolver_12.5
cusolver_dev_12.5

cuSOLVER runtime libraries.




cusparse_12.5
cusparse_dev_12.5

cuSPARSE runtime libraries.




npp_12.5
npp_dev_12.5

NPP runtime libraries.




nvjpeg_12.5
nvjpeg_dev_12.5

nvJPEG libraries.



nsight_compute_12.5
Nsight Compute.


nsight_systems_12.5
Nsight Systems.


nsight_vse_12.5
Installs the Nsight Visual Studio Edition plugin in all VS.


occupancy_calculator_12.5
Installs the CUDA_Occupancy_Calculator.xls tool.


visual_studio_integration_12.5
Installs CUDA project wizard and builds customization files in VS.


Driver Subpackages


Display.Driver
The NVIDIA Display Driver. Required to run CUDA applications.



For example, to install only the compiler and driver components:

<PackageName>.exe -s nvcc_12.1 Display.Driver


Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.
Extracting and Inspecting the Files Manually
Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip.
Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.

Note
Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment.



2.3.1. Uninstalling the CUDA Softwareï

All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.




2.4. Using Conda to Install the CUDA Softwareï

This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia.


2.4.1. Conda Overviewï

The Conda installation installs the CUDA Toolkit. The installation steps are listed below.



2.4.2. Installationï

To perform a basic install of all CUDA Toolkit components using Conda, run the following command:

conda install cuda -c nvidia





2.4.3. Uninstallationï

To uninstall the CUDA Toolkit using Conda, run the following command:

conda remove cuda





2.4.4. Installing Previous CUDA Releasesï

All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as:

conda install cuda -c nvidia/label/cuda-11.3.0



Note
Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as:

conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1


This example will install all packages released as part of CUDA 11.3.1.





2.5. Use a Suitable Driver Modelï

On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate:

The WDDM driver model is used for display devices.
The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.

TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).

Note
Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.


Note
NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.




2.6. Verify the Installationï

Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs.


2.6.1. Running the Compiled Examplesï

The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to:
Start > All Programs > Accessories > Command Prompt
CUDA Samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.
To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder.
This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1.



Figure 1 Valid Results from deviceQuery CUDA Sampleï


The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.
Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2.



Figure 2 Valid Results from bandwidthTest CUDA Sampleï


The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.
If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
To see a graphical representation of what CUDA can do, run the particles sample at

https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles







3. Pip Wheelsï

NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.

py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.

py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:

--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:

py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:

py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cublas-cu12
nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvfatbin-cu12
nvidia-nvjitlink-cu12
nvidia-nvjpeg-cu12
nvidia-nvml-dev-cu12
nvidia-nvtx-cu12
nvidia-opencl-cu12

These metapackages install the following packages:

nvidia-cublas-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-nvrtc-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-cufft-cu125
nvidia-curand-cu125
nvidia-cusolver-cu125
nvidia-cusparse-cu125
nvidia-npp-cu125
nvidia-nvfatbin-cu125
nvidia-nvjitlink-cu125
nvidia-nvjpeg-cu125
nvidia-nvml-dev-cu125
nvidia-nvtx-cu125
nvidia-opencl-cu125




4. Compiling CUDA Programsï

The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code. To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples


4.1. Compiling Sample Projectsï

The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest.
If you elected to use the default installation location, the output is placed in CUDA Samples\v12.5\bin\win64\Release. Build the program using the appropriate solution file and run the executable. If all works correctly, the output should be similar to Figure 2.



4.2. Sample Projectsï

The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.
A few of the example projects require some additional setup.
These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.
The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.


Table 3 CUDA Visual Studio .props locationsï







Visual Studio
CUDA 12.5 .props file Install Directory




Visual Studio 2015 (deprecated)
C:Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations


Visual Studio 2017
<Visual Studio Install Dir>\Common7\IDE\VC\VCTargets\BuildCustomizations


Visual Studio 2019
C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\BuildCustomizations


Visual Studio 2022
C:\Program Files\Microsoft Visual Studio\2022\Professional\MSBuild\Microsoft\VC\v170\BuildCustomizations



You can reference this CUDA 12.5.props file when building your own CUDA applications.



4.3. Build Customizations for New Projectsï

When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Projectâ¦ NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the âCUDA 12.5 Runtimeâ template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIAâs Build Customizations. All standard capabilities of Visual Studio C++ projects will be available.
To specify a custom CUDA Toolkit location, under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations.

Note
A supported version of MSVC must be installed to use this feature.




4.4. Build Customizations for Existing Projectsï

When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods:

Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizationsâ¦, then select the CUDA Toolkit version you would like to target.
Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties. Under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer.

While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.
If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps:

Open a run window from the Start Menu.

Run:

control sysdm.cpl



Select the Advanced tab at the top of the window.
Click Environment Variables at the bottom of the window.

Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item, selecting NVIDIA CUDA 12.5\CodeCUDA C/C++ File, and then selecting the file you wish to add.
For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following:

msbuild <projectname.extension> /t:Rebuild /p:CudaToolkitDir="drive:/path/to/new/toolkit/"






5. Additional Considerationsï

Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDAÂ C Programming Guide, located in the CUDA Toolkit documentation directory.
A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIAÂ® Nsightâ¢ Visual Studio Edition, and NVIDIA Visual Profiler.
For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/.



6. Noticesï



6.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



6.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



6.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Installation Guide for Microsoft Windows










































1. Introduction
1.1. System Requirements
1.2. About This Document



2. Installing CUDA Development Tools
2.1. Verify You Have a CUDA-Capable GPU
2.2. Download the NVIDIA CUDA Toolkit

2.3. Install the CUDA Software
2.3.1. Uninstalling the CUDA Software



2.4. Using Conda to Install the CUDA Software
2.4.1. Conda Overview
2.4.2. Installation
2.4.3. Uninstallation
2.4.4. Installing Previous CUDA Releases


2.5. Use a Suitable Driver Model

2.6. Verify the Installation
2.6.1. Running the Compiled Examples




3. Pip Wheels

4. Compiling CUDA Programs
4.1. Compiling Sample Projects
4.2. Sample Projects
4.3. Build Customizations for New Projects
4.4. Build Customizations for Existing Projects


5. Additional Considerations

6. Notices
6.1. Notice
6.2. OpenCL
6.3. Trademarks








Installation Guide Windows






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






CUDA Installation Guide for Microsoft Windows
The installation instructions for the CUDA Toolkit on Microsoft Windows systems.


1. Introductionï

CUDAÂ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind:

Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.

CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools.


1.1. System Requirementsï

To use CUDA on your system, you will need the following installed:

A CUDA-capable GPU
A supported version of Linux with a gcc compiler and toolchain
NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads)

Supported Microsoft WindowsÂ® operating systems:

Microsoft Windows 11 21H2
Microsoft Windows 11 22H2-SV2
Microsoft Windows 11 23H2
Microsoft Windows 10 21H2
Microsoft Windows 10 22H2
Microsoft Windows Server 2022



Table 1 Windows Compiler Support in CUDA 12.5ï










Compiler*
IDE
Native x86_64
Cross-compilation (32-bit on 64-bit)
C++ Dialect




MSVC Version 193x
Visual Studio 2022 17.x
YES
Not supported
C++14 (default), C++17, C++20


MSVC Version 192x
Visual Studio 2019 16.x
YES
C++14 (default), C++17


MSVC Version 191x
Visual Studio 2017 15.x (RTW and all updates)
YES
C++14 (default), C++17



* Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries  on  GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications.
Support for running x86 32-bit applications on x86_64 Windows is limited to use with:

CUDA Driver
CUDA Runtime (cudart)
CUDA Math Library (math.h)




1.2. About This Documentï

This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation.




2. Installing CUDA Development Toolsï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.
The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps:

Verify the system has a CUDA-capable GPU.
Download the NVIDIA CUDA Toolkit.
Install the NVIDIA CUDA Toolkit.
Test that the installed software runs correctly and communicates with the hardware.



2.1. Verify You Have a CUDA-Capable GPUï

You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager. Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus, that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products.
The Windows Device Manager can be opened via the following steps:

Open a run window from the Start Menu

Run:

control /name Microsoft.DeviceManager







2.2. Download the NVIDIA CUDA Toolkitï

The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads. Choose the platform you are using and one of the following installer formats:

Network Installer: A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time.
Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment.

The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification
The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.



2.3. Install the CUDA Softwareï

Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.

Note
The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.


Note
The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again.

Graphical Installation
Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.
Silent Installation
The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names.


Table 2 Possible Subpackage Namesï







Subpackage Name
Subpackage Description




Toolkit Subpackages (defaults to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5)


cuda_profiler_api_12.5
CUDA Profiler API.


cudart_12.5
CUDA Runtime libraries.


cuobjdump_12.5
Extracts information from cubin files.


cupti_12.5
The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.


cuxxfilt_12.5
The CUDA cu++ filt demangler tool.


demo_suite_12.5
Prebuilt demo applications using CUDA.


documentation_12.5
CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.


nvcc_12.5
CUDA compiler.


nvdisasm_12.5
Extracts information from standalone cubin files.


nvfatbin_12.5
Library for creating fatbinaries at runtime.


nvjitlink_12.5
nvJitLink library.


nvml_dev_12.5
NVML development libraries and headers.


nvprof_12.5
Tool for collecting and viewing CUDA application profiling data from the command-line.


nvprune_12.5
Prunes host object files and libraries to only contain device code for the specified targets.



nvrtc_12.5
nvrtc_dev_12.5

NVRTC runtime libraries.



nvtx_12.5
NVTX on Windows.


opencl_12.5
OpenCL library.


visual_profiler_12.5
Visual Profiler.


sanitizer_12.5
Compute Sanitizer API.


thrust_12.5
CUDA Thrust.



cublas_12.5
cublas_dev_12.5

cuBLAS runtime libraries.




cufft_12.5
cufft_dev_12.5

cuFFT runtime libraries.




curand_12.5
curand_dev_12.5

cuRAND runtime libraries.




cusolver_12.5
cusolver_dev_12.5

cuSOLVER runtime libraries.




cusparse_12.5
cusparse_dev_12.5

cuSPARSE runtime libraries.




npp_12.5
npp_dev_12.5

NPP runtime libraries.




nvjpeg_12.5
nvjpeg_dev_12.5

nvJPEG libraries.



nsight_compute_12.5
Nsight Compute.


nsight_systems_12.5
Nsight Systems.


nsight_vse_12.5
Installs the Nsight Visual Studio Edition plugin in all VS.


occupancy_calculator_12.5
Installs the CUDA_Occupancy_Calculator.xls tool.


visual_studio_integration_12.5
Installs CUDA project wizard and builds customization files in VS.


Driver Subpackages


Display.Driver
The NVIDIA Display Driver. Required to run CUDA applications.



For example, to install only the compiler and driver components:

<PackageName>.exe -s nvcc_12.1 Display.Driver


Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.
Extracting and Inspecting the Files Manually
Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip.
Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.

Note
Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment.



2.3.1. Uninstalling the CUDA Softwareï

All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.




2.4. Using Conda to Install the CUDA Softwareï

This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia.


2.4.1. Conda Overviewï

The Conda installation installs the CUDA Toolkit. The installation steps are listed below.



2.4.2. Installationï

To perform a basic install of all CUDA Toolkit components using Conda, run the following command:

conda install cuda -c nvidia





2.4.3. Uninstallationï

To uninstall the CUDA Toolkit using Conda, run the following command:

conda remove cuda





2.4.4. Installing Previous CUDA Releasesï

All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as:

conda install cuda -c nvidia/label/cuda-11.3.0



Note
Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as:

conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1


This example will install all packages released as part of CUDA 11.3.1.





2.5. Use a Suitable Driver Modelï

On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate:

The WDDM driver model is used for display devices.
The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.

TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).

Note
Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.


Note
NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.




2.6. Verify the Installationï

Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs.


2.6.1. Running the Compiled Examplesï

The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to:
Start > All Programs > Accessories > Command Prompt
CUDA Samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.
To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder.
This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1.



Figure 1 Valid Results from deviceQuery CUDA Sampleï


The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.
Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2.



Figure 2 Valid Results from bandwidthTest CUDA Sampleï


The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.
If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
To see a graphical representation of what CUDA can do, run the particles sample at

https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles







3. Pip Wheelsï

NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.

py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.

py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:

--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:

py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:

py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cublas-cu12
nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvfatbin-cu12
nvidia-nvjitlink-cu12
nvidia-nvjpeg-cu12
nvidia-nvml-dev-cu12
nvidia-nvtx-cu12
nvidia-opencl-cu12

These metapackages install the following packages:

nvidia-cublas-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-nvrtc-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-cufft-cu125
nvidia-curand-cu125
nvidia-cusolver-cu125
nvidia-cusparse-cu125
nvidia-npp-cu125
nvidia-nvfatbin-cu125
nvidia-nvjitlink-cu125
nvidia-nvjpeg-cu125
nvidia-nvml-dev-cu125
nvidia-nvtx-cu125
nvidia-opencl-cu125




4. Compiling CUDA Programsï

The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code. To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples


4.1. Compiling Sample Projectsï

The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest.
If you elected to use the default installation location, the output is placed in CUDA Samples\v12.5\bin\win64\Release. Build the program using the appropriate solution file and run the executable. If all works correctly, the output should be similar to Figure 2.



4.2. Sample Projectsï

The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.
A few of the example projects require some additional setup.
These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.
The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.


Table 3 CUDA Visual Studio .props locationsï







Visual Studio
CUDA 12.5 .props file Install Directory




Visual Studio 2015 (deprecated)
C:Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations


Visual Studio 2017
<Visual Studio Install Dir>\Common7\IDE\VC\VCTargets\BuildCustomizations


Visual Studio 2019
C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\BuildCustomizations


Visual Studio 2022
C:\Program Files\Microsoft Visual Studio\2022\Professional\MSBuild\Microsoft\VC\v170\BuildCustomizations



You can reference this CUDA 12.5.props file when building your own CUDA applications.



4.3. Build Customizations for New Projectsï

When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Projectâ¦ NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the âCUDA 12.5 Runtimeâ template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIAâs Build Customizations. All standard capabilities of Visual Studio C++ projects will be available.
To specify a custom CUDA Toolkit location, under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations.

Note
A supported version of MSVC must be installed to use this feature.




4.4. Build Customizations for Existing Projectsï

When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods:

Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizationsâ¦, then select the CUDA Toolkit version you would like to target.
Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties. Under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer.

While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.
If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps:

Open a run window from the Start Menu.

Run:

control sysdm.cpl



Select the Advanced tab at the top of the window.
Click Environment Variables at the bottom of the window.

Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item, selecting NVIDIA CUDA 12.5\CodeCUDA C/C++ File, and then selecting the file you wish to add.
For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following:

msbuild <projectname.extension> /t:Rebuild /p:CudaToolkitDir="drive:/path/to/new/toolkit/"






5. Additional Considerationsï

Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDAÂ C Programming Guide, located in the CUDA Toolkit documentation directory.
A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIAÂ® Nsightâ¢ Visual Studio Edition, and NVIDIA Visual Profiler.
For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/.



6. Noticesï



6.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



6.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



6.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Installation Guide for Microsoft Windows










































1. Introduction
1.1. System Requirements
1.2. About This Document



2. Installing CUDA Development Tools
2.1. Verify You Have a CUDA-Capable GPU
2.2. Download the NVIDIA CUDA Toolkit

2.3. Install the CUDA Software
2.3.1. Uninstalling the CUDA Software



2.4. Using Conda to Install the CUDA Software
2.4.1. Conda Overview
2.4.2. Installation
2.4.3. Uninstallation
2.4.4. Installing Previous CUDA Releases


2.5. Use a Suitable Driver Model

2.6. Verify the Installation
2.6.1. Running the Compiled Examples




3. Pip Wheels

4. Compiling CUDA Programs
4.1. Compiling Sample Projects
4.2. Sample Projects
4.3. Build Customizations for New Projects
4.4. Build Customizations for Existing Projects


5. Additional Considerations

6. Notices
6.1. Notice
6.2. OpenCL
6.3. Trademarks








Installation Guide Windows






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






CUDA Installation Guide for Microsoft Windows
The installation instructions for the CUDA Toolkit on Microsoft Windows systems.


1. Introductionï

CUDAÂ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind:

Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.

CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools.


1.1. System Requirementsï

To use CUDA on your system, you will need the following installed:

A CUDA-capable GPU
A supported version of Linux with a gcc compiler and toolchain
NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads)

Supported Microsoft WindowsÂ® operating systems:

Microsoft Windows 11 21H2
Microsoft Windows 11 22H2-SV2
Microsoft Windows 11 23H2
Microsoft Windows 10 21H2
Microsoft Windows 10 22H2
Microsoft Windows Server 2022



Table 1 Windows Compiler Support in CUDA 12.5ï










Compiler*
IDE
Native x86_64
Cross-compilation (32-bit on 64-bit)
C++ Dialect




MSVC Version 193x
Visual Studio 2022 17.x
YES
Not supported
C++14 (default), C++17, C++20


MSVC Version 192x
Visual Studio 2019 16.x
YES
C++14 (default), C++17


MSVC Version 191x
Visual Studio 2017 15.x (RTW and all updates)
YES
C++14 (default), C++17



* Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries  on  GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications.
Support for running x86 32-bit applications on x86_64 Windows is limited to use with:

CUDA Driver
CUDA Runtime (cudart)
CUDA Math Library (math.h)




1.2. About This Documentï

This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation.




2. Installing CUDA Development Toolsï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.
The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps:

Verify the system has a CUDA-capable GPU.
Download the NVIDIA CUDA Toolkit.
Install the NVIDIA CUDA Toolkit.
Test that the installed software runs correctly and communicates with the hardware.



2.1. Verify You Have a CUDA-Capable GPUï

You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager. Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus, that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products.
The Windows Device Manager can be opened via the following steps:

Open a run window from the Start Menu

Run:

control /name Microsoft.DeviceManager







2.2. Download the NVIDIA CUDA Toolkitï

The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads. Choose the platform you are using and one of the following installer formats:

Network Installer: A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time.
Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment.

The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification
The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.



2.3. Install the CUDA Softwareï

Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.

Note
The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.


Note
The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again.

Graphical Installation
Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.
Silent Installation
The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names.


Table 2 Possible Subpackage Namesï







Subpackage Name
Subpackage Description




Toolkit Subpackages (defaults to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5)


cuda_profiler_api_12.5
CUDA Profiler API.


cudart_12.5
CUDA Runtime libraries.


cuobjdump_12.5
Extracts information from cubin files.


cupti_12.5
The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.


cuxxfilt_12.5
The CUDA cu++ filt demangler tool.


demo_suite_12.5
Prebuilt demo applications using CUDA.


documentation_12.5
CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.


nvcc_12.5
CUDA compiler.


nvdisasm_12.5
Extracts information from standalone cubin files.


nvfatbin_12.5
Library for creating fatbinaries at runtime.


nvjitlink_12.5
nvJitLink library.


nvml_dev_12.5
NVML development libraries and headers.


nvprof_12.5
Tool for collecting and viewing CUDA application profiling data from the command-line.


nvprune_12.5
Prunes host object files and libraries to only contain device code for the specified targets.



nvrtc_12.5
nvrtc_dev_12.5

NVRTC runtime libraries.



nvtx_12.5
NVTX on Windows.


opencl_12.5
OpenCL library.


visual_profiler_12.5
Visual Profiler.


sanitizer_12.5
Compute Sanitizer API.


thrust_12.5
CUDA Thrust.



cublas_12.5
cublas_dev_12.5

cuBLAS runtime libraries.




cufft_12.5
cufft_dev_12.5

cuFFT runtime libraries.




curand_12.5
curand_dev_12.5

cuRAND runtime libraries.




cusolver_12.5
cusolver_dev_12.5

cuSOLVER runtime libraries.




cusparse_12.5
cusparse_dev_12.5

cuSPARSE runtime libraries.




npp_12.5
npp_dev_12.5

NPP runtime libraries.




nvjpeg_12.5
nvjpeg_dev_12.5

nvJPEG libraries.



nsight_compute_12.5
Nsight Compute.


nsight_systems_12.5
Nsight Systems.


nsight_vse_12.5
Installs the Nsight Visual Studio Edition plugin in all VS.


occupancy_calculator_12.5
Installs the CUDA_Occupancy_Calculator.xls tool.


visual_studio_integration_12.5
Installs CUDA project wizard and builds customization files in VS.


Driver Subpackages


Display.Driver
The NVIDIA Display Driver. Required to run CUDA applications.



For example, to install only the compiler and driver components:

<PackageName>.exe -s nvcc_12.1 Display.Driver


Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.
Extracting and Inspecting the Files Manually
Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip.
Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.

Note
Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment.



2.3.1. Uninstalling the CUDA Softwareï

All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.




2.4. Using Conda to Install the CUDA Softwareï

This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia.


2.4.1. Conda Overviewï

The Conda installation installs the CUDA Toolkit. The installation steps are listed below.



2.4.2. Installationï

To perform a basic install of all CUDA Toolkit components using Conda, run the following command:

conda install cuda -c nvidia





2.4.3. Uninstallationï

To uninstall the CUDA Toolkit using Conda, run the following command:

conda remove cuda





2.4.4. Installing Previous CUDA Releasesï

All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as:

conda install cuda -c nvidia/label/cuda-11.3.0



Note
Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as:

conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1


This example will install all packages released as part of CUDA 11.3.1.





2.5. Use a Suitable Driver Modelï

On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate:

The WDDM driver model is used for display devices.
The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.

TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).

Note
Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.


Note
NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.




2.6. Verify the Installationï

Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs.


2.6.1. Running the Compiled Examplesï

The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to:
Start > All Programs > Accessories > Command Prompt
CUDA Samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.
To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder.
This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1.



Figure 1 Valid Results from deviceQuery CUDA Sampleï


The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.
Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2.



Figure 2 Valid Results from bandwidthTest CUDA Sampleï


The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.
If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
To see a graphical representation of what CUDA can do, run the particles sample at

https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles







3. Pip Wheelsï

NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.

py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.

py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:

--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:

py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:

py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cublas-cu12
nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvfatbin-cu12
nvidia-nvjitlink-cu12
nvidia-nvjpeg-cu12
nvidia-nvml-dev-cu12
nvidia-nvtx-cu12
nvidia-opencl-cu12

These metapackages install the following packages:

nvidia-cublas-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-nvrtc-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-cufft-cu125
nvidia-curand-cu125
nvidia-cusolver-cu125
nvidia-cusparse-cu125
nvidia-npp-cu125
nvidia-nvfatbin-cu125
nvidia-nvjitlink-cu125
nvidia-nvjpeg-cu125
nvidia-nvml-dev-cu125
nvidia-nvtx-cu125
nvidia-opencl-cu125




4. Compiling CUDA Programsï

The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code. To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples


4.1. Compiling Sample Projectsï

The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest.
If you elected to use the default installation location, the output is placed in CUDA Samples\v12.5\bin\win64\Release. Build the program using the appropriate solution file and run the executable. If all works correctly, the output should be similar to Figure 2.



4.2. Sample Projectsï

The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.
A few of the example projects require some additional setup.
These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.
The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.


Table 3 CUDA Visual Studio .props locationsï







Visual Studio
CUDA 12.5 .props file Install Directory




Visual Studio 2015 (deprecated)
C:Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations


Visual Studio 2017
<Visual Studio Install Dir>\Common7\IDE\VC\VCTargets\BuildCustomizations


Visual Studio 2019
C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\BuildCustomizations


Visual Studio 2022
C:\Program Files\Microsoft Visual Studio\2022\Professional\MSBuild\Microsoft\VC\v170\BuildCustomizations



You can reference this CUDA 12.5.props file when building your own CUDA applications.



4.3. Build Customizations for New Projectsï

When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Projectâ¦ NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the âCUDA 12.5 Runtimeâ template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIAâs Build Customizations. All standard capabilities of Visual Studio C++ projects will be available.
To specify a custom CUDA Toolkit location, under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations.

Note
A supported version of MSVC must be installed to use this feature.




4.4. Build Customizations for Existing Projectsï

When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods:

Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizationsâ¦, then select the CUDA Toolkit version you would like to target.
Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties. Under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer.

While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.
If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps:

Open a run window from the Start Menu.

Run:

control sysdm.cpl



Select the Advanced tab at the top of the window.
Click Environment Variables at the bottom of the window.

Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item, selecting NVIDIA CUDA 12.5\CodeCUDA C/C++ File, and then selecting the file you wish to add.
For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following:

msbuild <projectname.extension> /t:Rebuild /p:CudaToolkitDir="drive:/path/to/new/toolkit/"






5. Additional Considerationsï

Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDAÂ C Programming Guide, located in the CUDA Toolkit documentation directory.
A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIAÂ® Nsightâ¢ Visual Studio Edition, and NVIDIA Visual Profiler.
For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/.



6. Noticesï



6.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



6.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



6.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      


















CUDA Installation Guide for Microsoft Windows










































1. Introduction
1.1. System Requirements
1.2. About This Document



2. Installing CUDA Development Tools
2.1. Verify You Have a CUDA-Capable GPU
2.2. Download the NVIDIA CUDA Toolkit

2.3. Install the CUDA Software
2.3.1. Uninstalling the CUDA Software



2.4. Using Conda to Install the CUDA Software
2.4.1. Conda Overview
2.4.2. Installation
2.4.3. Uninstallation
2.4.4. Installing Previous CUDA Releases


2.5. Use a Suitable Driver Model

2.6. Verify the Installation
2.6.1. Running the Compiled Examples




3. Pip Wheels

4. Compiling CUDA Programs
4.1. Compiling Sample Projects
4.2. Sample Projects
4.3. Build Customizations for New Projects
4.4. Build Customizations for Existing Projects


5. Additional Considerations

6. Notices
6.1. Notice
6.2. OpenCL
6.3. Trademarks








Installation Guide Windows






 Â»

1. Introduction



v12.5 |
PDF
|
Archive
Â 






CUDA Installation Guide for Microsoft Windows
The installation instructions for the CUDA Toolkit on Microsoft Windows systems.


1. Introductionï

CUDAÂ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).
CUDA was developed with several design goals in mind:

Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.
Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.

CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.
This guide will show you how to install and check the correct operation of the CUDA development tools.


1.1. System Requirementsï

To use CUDA on your system, you will need the following installed:

A CUDA-capable GPU
A supported version of Linux with a gcc compiler and toolchain
NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads)

Supported Microsoft WindowsÂ® operating systems:

Microsoft Windows 11 21H2
Microsoft Windows 11 22H2-SV2
Microsoft Windows 11 23H2
Microsoft Windows 10 21H2
Microsoft Windows 10 22H2
Microsoft Windows Server 2022



Table 1 Windows Compiler Support in CUDA 12.5ï










Compiler*
IDE
Native x86_64
Cross-compilation (32-bit on 64-bit)
C++ Dialect




MSVC Version 193x
Visual Studio 2022 17.x
YES
Not supported
C++14 (default), C++17, C++20


MSVC Version 192x
Visual Studio 2019 16.x
YES
C++14 (default), C++17


MSVC Version 191x
Visual Studio 2017 15.x (RTW and all updates)
YES
C++14 (default), C++17



* Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.
32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries  on  GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications.
Support for running x86 32-bit applications on x86_64 Windows is limited to use with:

CUDA Driver
CUDA Runtime (cudart)
CUDA Math Library (math.h)




1.2. About This Documentï

This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation.




2. Installing CUDA Development Toolsï

Basic instructions can be found in the Quick Start Guide. Read on for more detailed instructions.
The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps:

Verify the system has a CUDA-capable GPU.
Download the NVIDIA CUDA Toolkit.
Install the NVIDIA CUDA Toolkit.
Test that the installed software runs correctly and communicates with the hardware.



2.1. Verify You Have a CUDA-Capable GPUï

You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager. Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus, that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products.
The Windows Device Manager can be opened via the following steps:

Open a run window from the Start Menu

Run:

control /name Microsoft.DeviceManager







2.2. Download the NVIDIA CUDA Toolkitï

The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads. Choose the platform you are using and one of the following installer formats:

Network Installer: A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time.
Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment.

The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.
Download Verification
The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.



2.3. Install the CUDA Softwareï

Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.

Note
The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.


Note
The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again.

Graphical Installation
Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.
Silent Installation
The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names.


Table 2 Possible Subpackage Namesï







Subpackage Name
Subpackage Description




Toolkit Subpackages (defaults to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5)


cuda_profiler_api_12.5
CUDA Profiler API.


cudart_12.5
CUDA Runtime libraries.


cuobjdump_12.5
Extracts information from cubin files.


cupti_12.5
The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.


cuxxfilt_12.5
The CUDA cu++ filt demangler tool.


demo_suite_12.5
Prebuilt demo applications using CUDA.


documentation_12.5
CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.


nvcc_12.5
CUDA compiler.


nvdisasm_12.5
Extracts information from standalone cubin files.


nvfatbin_12.5
Library for creating fatbinaries at runtime.


nvjitlink_12.5
nvJitLink library.


nvml_dev_12.5
NVML development libraries and headers.


nvprof_12.5
Tool for collecting and viewing CUDA application profiling data from the command-line.


nvprune_12.5
Prunes host object files and libraries to only contain device code for the specified targets.



nvrtc_12.5
nvrtc_dev_12.5

NVRTC runtime libraries.



nvtx_12.5
NVTX on Windows.


opencl_12.5
OpenCL library.


visual_profiler_12.5
Visual Profiler.


sanitizer_12.5
Compute Sanitizer API.


thrust_12.5
CUDA Thrust.



cublas_12.5
cublas_dev_12.5

cuBLAS runtime libraries.




cufft_12.5
cufft_dev_12.5

cuFFT runtime libraries.




curand_12.5
curand_dev_12.5

cuRAND runtime libraries.




cusolver_12.5
cusolver_dev_12.5

cuSOLVER runtime libraries.




cusparse_12.5
cusparse_dev_12.5

cuSPARSE runtime libraries.




npp_12.5
npp_dev_12.5

NPP runtime libraries.




nvjpeg_12.5
nvjpeg_dev_12.5

nvJPEG libraries.



nsight_compute_12.5
Nsight Compute.


nsight_systems_12.5
Nsight Systems.


nsight_vse_12.5
Installs the Nsight Visual Studio Edition plugin in all VS.


occupancy_calculator_12.5
Installs the CUDA_Occupancy_Calculator.xls tool.


visual_studio_integration_12.5
Installs CUDA project wizard and builds customization files in VS.


Driver Subpackages


Display.Driver
The NVIDIA Display Driver. Required to run CUDA applications.



For example, to install only the compiler and driver components:

<PackageName>.exe -s nvcc_12.1 Display.Driver


Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.
Extracting and Inspecting the Files Manually
Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip.
Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.

Note
Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment.



2.3.1. Uninstalling the CUDA Softwareï

All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.




2.4. Using Conda to Install the CUDA Softwareï

This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia.


2.4.1. Conda Overviewï

The Conda installation installs the CUDA Toolkit. The installation steps are listed below.



2.4.2. Installationï

To perform a basic install of all CUDA Toolkit components using Conda, run the following command:

conda install cuda -c nvidia





2.4.3. Uninstallationï

To uninstall the CUDA Toolkit using Conda, run the following command:

conda remove cuda





2.4.4. Installing Previous CUDA Releasesï

All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as:

conda install cuda -c nvidia/label/cuda-11.3.0



Note
Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as:

conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1


This example will install all packages released as part of CUDA 11.3.1.





2.5. Use a Suitable Driver Modelï

On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate:

The WDDM driver model is used for display devices.
The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.

TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).

Note
Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.


Note
NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.




2.6. Verify the Installationï

Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs.


2.6.1. Running the Compiled Examplesï

The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to:
Start > All Programs > Accessories > Command Prompt
CUDA Samples are located in https://github.com/nvidia/cuda-samples. To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.
To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder.
This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1.



Figure 1 Valid Results from deviceQuery CUDA Sampleï


The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.
If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.
Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2.



Figure 2 Valid Results from bandwidthTest CUDA Sampleï


The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.
If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.
To see a graphical representation of what CUDA can do, run the particles sample at

https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles







3. Pip Wheelsï

NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately).
Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.
Prerequisites
To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail.

py -m pip install --upgrade setuptools pip wheel


You should now be able to install the nvidia-pyindex module.

py -m pip install nvidia-pyindex


If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package:

--extra-index-url https://pypi.ngc.nvidia.com


Procedure
Install the CUDA runtime package:

py -m pip install nvidia-cuda-runtime-cu12


Optionally, install additional packages as listed below using the following command:

py -m pip install nvidia-<library>


Metapackages
The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. âcu12â should be read as âcuda12â.

nvidia-cublas-cu12
nvidia-cuda-runtime-cu12
nvidia-cuda-cupti-cu12
nvidia-cuda-nvcc-cu12
nvidia-cuda-nvrtc-cu12
nvidia-cuda-sanitizer-api-cu12
nvidia-cufft-cu12
nvidia-curand-cu12
nvidia-cusolver-cu12
nvidia-cusparse-cu12
nvidia-npp-cu12
nvidia-nvfatbin-cu12
nvidia-nvjitlink-cu12
nvidia-nvjpeg-cu12
nvidia-nvml-dev-cu12
nvidia-nvtx-cu12
nvidia-opencl-cu12

These metapackages install the following packages:

nvidia-cublas-cu125
nvidia-cuda-runtime-cu125
nvidia-cuda-cupti-cu125
nvidia-cuda-nvcc-cu125
nvidia-cuda-nvrtc-cu125
nvidia-cuda-sanitizer-api-cu125
nvidia-cufft-cu125
nvidia-curand-cu125
nvidia-cusolver-cu125
nvidia-cusparse-cu125
nvidia-npp-cu125
nvidia-nvfatbin-cu125
nvidia-nvjitlink-cu125
nvidia-nvjpeg-cu125
nvidia-nvml-dev-cu125
nvidia-nvtx-cu125
nvidia-opencl-cu125




4. Compiling CUDA Programsï

The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code. To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples


4.1. Compiling Sample Projectsï

The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest.
If you elected to use the default installation location, the output is placed in CUDA Samples\v12.5\bin\win64\Release. Build the program using the appropriate solution file and run the executable. If all works correctly, the output should be similar to Figure 2.



4.2. Sample Projectsï

The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.
A few of the example projects require some additional setup.
These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.
The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.


Table 3 CUDA Visual Studio .props locationsï







Visual Studio
CUDA 12.5 .props file Install Directory




Visual Studio 2015 (deprecated)
C:Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations


Visual Studio 2017
<Visual Studio Install Dir>\Common7\IDE\VC\VCTargets\BuildCustomizations


Visual Studio 2019
C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\BuildCustomizations


Visual Studio 2022
C:\Program Files\Microsoft Visual Studio\2022\Professional\MSBuild\Microsoft\VC\v170\BuildCustomizations



You can reference this CUDA 12.5.props file when building your own CUDA applications.



4.3. Build Customizations for New Projectsï

When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Projectâ¦ NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the âCUDA 12.5 Runtimeâ template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIAâs Build Customizations. All standard capabilities of Visual Studio C++ projects will be available.
To specify a custom CUDA Toolkit location, under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations.

Note
A supported version of MSVC must be installed to use this feature.




4.4. Build Customizations for Existing Projectsï

When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods:

Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizationsâ¦, then select the CUDA Toolkit version you would like to target.
Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties. Under CUDA C/C++, select Common, and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer.

While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.
If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps:

Open a run window from the Start Menu.

Run:

control sysdm.cpl



Select the Advanced tab at the top of the window.
Click Environment Variables at the bottom of the window.

Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item, selecting NVIDIA CUDA 12.5\CodeCUDA C/C++ File, and then selecting the file you wish to add.
For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following:

msbuild <projectname.extension> /t:Rebuild /p:CudaToolkitDir="drive:/path/to/new/toolkit/"






5. Additional Considerationsï

Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDAÂ C Programming Guide, located in the CUDA Toolkit documentation directory.
A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIAÂ® Nsightâ¢ Visual Studio Edition, and NVIDIA Visual Profiler.
For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/.



6. Noticesï



6.1. Noticeï

This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.



6.2. OpenCLï

OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.



6.3. Trademarksï

NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.










Privacy Policy
|
Manage My Privacy
|
Do Not Sell or Share My Data
|
Terms of Service
|
Accessibility
|
Corporate Policies
|
Product Security
|
Contact


  Copyright Â© 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.


Last updated on Jul 1, 2024.
      












