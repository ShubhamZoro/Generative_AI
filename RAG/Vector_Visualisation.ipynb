{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by employees of Insurellm, an Insurance Tech company\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy.\n",
    "\n",
    "## TODAY:\n",
    "\n",
    "- Part A: We will divide our documents into CHUNKS\n",
    "- Part B: We will encode our CHUNKS into VECTORS and put in Chroma\n",
    "- Part C: We will visualize our vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769edb3",
   "metadata": {},
   "source": [
    "### PART A: Divide our documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4.1-nano\"\n",
    "db_name = \"vector_db\"\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 files in the knowledge base\n",
      "Total characters in knowledge base: 304,434\n"
     ]
    }
   ],
   "source": [
    "# How many characters in all the documents?\n",
    "\n",
    "knowledge_base_path = \"knowledge-base/**/*.md\"\n",
    "files = glob.glob(knowledge_base_path, recursive=True)\n",
    "print(f\"Found {len(files)} files in the knowledge base\")\n",
    "\n",
    "entire_knowledge_base = \"\"\n",
    "\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        entire_knowledge_base += f.read()\n",
    "        entire_knowledge_base += \"\\n\\n\"\n",
    "\n",
    "print(f\"Total characters in knowledge base: {len(entire_knowledge_base):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b53a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for gpt-4.1-nano: 63,555\n"
     ]
    }
   ],
   "source": [
    "# How many tokens in all the documents?\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "tokens = encoding.encode(entire_knowledge_base)\n",
    "token_count = len(tokens)\n",
    "print(f\"Total tokens for {MODEL}: {token_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 76 documents\n"
     ]
    }
   ],
   "source": [
    "# Load in everything in the knowledgebase using LangChain's loaders\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68dab1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'knowledge-base\\\\company\\\\careers.md', 'doc_type': 'company'}, page_content=\"# Careers at Insurellm\\n\\n## Why Join Insurellm?\\n\\nAt Insurellm, we're not just building software—we're revolutionizing an entire industry. Since our founding in 2015, we've evolved from a high-growth startup to a lean, profitable company with 32 highly talented employees managing 32 active contracts across all eight of our product lines.\\n\\nAfter reaching 200 employees in 2020, we strategically restructured in 2022-2023 to focus on sustainable growth, operational excellence, and building a world-class remote-first culture. Today, we're a tight-knit team of exceptional professionals who deliver outsized impact through automation, AI, and strategic focus on high-value enterprise clients—from regional insurers to global reinsurance partners.\\n\\n### Our Culture\\n\\nWe live by our core values every day:\\n- **Innovation First**: We encourage experimentation and creative problem-solving\\n- **Customer Obsession**: Your work directly impacts 32 active client operations spanning the entire insurance value chain\\n- **Integrity & Transparency**: We build trust through ethical behavior and open communication\\n- **Collaborative Excellence**: Diverse perspectives and teamwork drive our success\\n\\n### What We Offer\\n\\n- Competitive compensation with equity participation\\n- Comprehensive health, dental, and vision insurance\\n- Flexible working arrangements and generous PTO\\n- Professional development programs and mentorship\\n- Clear career progression paths\\n- Latest technologies and tools\\n- Inclusive, diverse work environment\\n\\n## Current Opportunities\\n\\n### Engineering\\n\\n**Senior Full Stack Engineer** - San Francisco, CA\\n- Lead development of next-generation insurance platform features\\n- Work with React, Node.js, Python, and cloud technologies\\n- Mentor junior engineers and drive technical decisions\\n- 5+ years experience required\\n\\n**Backend Software Engineer** - San Francisco, CA / Austin, TX\\n- Build scalable microservices and APIs\\n- Optimize system performance and reliability\\n- Collaborate with product and design teams\\n- 3+ years experience with Java, Python, or Go\\n\\n**Frontend Developer** - Remote\\n- Create intuitive user interfaces for our insurance platforms\\n- Work with modern frameworks (React, Vue, or Angular)\\n- Ensure accessibility and responsive design\\n- 2+ years experience required\\n\\n**DevOps Engineer** - New York, NY\\n- Manage cloud infrastructure (AWS/GCP)\\n- Implement CI/CD pipelines and automation\\n- Monitor system performance and security\\n- 3+ years experience in DevOps/SRE\\n\\n**Mobile Developer (iOS/Android)** - San Francisco, CA\\n- Build native mobile applications for our marketplace\\n- Create seamless user experiences\\n- Integrate with backend APIs\\n- 3+ years mobile development experience\\n\\n### Data & Analytics\\n\\n**Senior Data Scientist** - San Francisco, CA / New York, NY\\n- Develop predictive models for insurance risk assessment\\n- Build recommendation systems for our marketplace\\n- Lead data-driven product initiatives\\n- PhD or MS in related field preferred, 4+ years experience\\n\\n**Data Engineer** - Austin, TX\\n- Design and maintain data pipelines\\n- Build data warehousing solutions\\n- Optimize data infrastructure for scale\\n- 3+ years experience with SQL, Python, and ETL tools\\n\\n**Business Intelligence Analyst** - Chicago, IL\\n- Create dashboards and reports for stakeholders\\n- Analyze business metrics and trends\\n- Support data-driven decision making\\n- 2+ years experience with BI tools (Tableau, Looker, etc.)\\n\\n### Product & Design\\n\\n**Product Manager** - San Francisco, CA\\n- Define product roadmap and strategy\\n- Work closely with engineering and design teams\\n- Gather customer insights and market research\\n- 3+ years product management experience in B2B SaaS\\n\\n**UX/UI Designer** - Remote\\n- Design user experiences for our insurance platforms\\n- Conduct user research and usability testing\\n- Create design systems and prototypes\\n- 3+ years design experience, insurance/fintech preferred\\n\\n### Sales & Customer Success\\n\\n**Account Executive** - New York, NY / Chicago, IL / Austin, TX\\n- Manage B2B sales cycle for enterprise clients\\n- Build relationships with insurance companies\\n- Exceed revenue targets and grow territory\\n- 3+ years B2B sales experience, SaaS preferred\\n\\n**Sales Development Representative** - Austin, TX / Remote\\n- Generate qualified leads for sales team\\n- Conduct outreach to prospective clients\\n- Research and identify target accounts\\n- 1+ years SDR/BDR experience preferred\\n\\n**Customer Success Manager** - San Francisco, CA / New York, NY\\n- Ensure client satisfaction and retention\\n- Drive product adoption and expansion\\n- Serve as trusted advisor to clients\\n- 2+ years customer success experience in SaaS\\n\\n**Solutions Engineer** - Remote\\n- Provide technical expertise during sales process\\n- Conduct product demonstrations and POCs\\n- Support implementation and integration\\n- 3+ years technical pre-sales experience\\n\\n### Operations & Support\\n\\n**Technical Support Specialist** - Remote\\n- Provide tier 2/3 technical support to clients\\n- Troubleshoot platform issues\\n- Create documentation and knowledge base articles\\n- 2+ years technical support experience\\n\\n**HR Business Partner** - San Francisco, CA\\n- Partner with leadership on people strategy\\n- Support talent development and retention\\n- Drive culture and engagement initiatives\\n- 4+ years HR experience, tech industry preferred\\n\\n## How to Apply\\n\\nVisit our careers portal at careers.insurellm.com or send your resume to jobs@insurellm.com. Please include the position title in your subject line.\\n\\nInsurellm is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25987306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 413 chunks\n",
      "First chunk:\n",
      "\n",
      "page_content='# About Insurellm\n",
      "\n",
      "Insurellm was founded by Avery Lancaster in 2015 as an insurance tech startup designed to disrupt an industry in need of innovative products. Its first product was Markellm, the marketplace connecting consumers with insurance providers.\n",
      "\n",
      "The company experienced rapid growth in its first five years, expanding its product portfolio to include Carllm (auto insurance portal), Homellm (home insurance portal), and Rellm (enterprise reinsurance platform). By 2020, Insurellm had reached a peak of 200 employees with 12 offices across the US.' metadata={'source': 'knowledge-base\\\\company\\\\about.md', 'doc_type': 'company'}\n"
     ]
    }
   ],
   "source": [
    "# Divide into chunks using the RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Divided into {len(chunks)} chunks\")\n",
    "print(f\"First chunk:\\n\\n{chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb209db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'knowledge-base\\\\contracts\\\\Contract with GlobalRe Partners for Rellm.md', 'doc_type': 'contracts'}, page_content='13. **Climate Risk Analytics:** Forward-looking climate modeling:\\n    - IPCC climate scenario analysis (RCP 2.6, 4.5, 8.5)\\n    - Transition risk assessment\\n    - Physical risk modeling for perils (hurricane, wildfire, flood, drought)\\n    - Sea level rise impact analysis\\n    - Temperature trend incorporation\\n    - Climate-adjusted pricing recommendations\\n    - Stranded asset identification\\n    - Green reinsurance opportunities\\n\\n---\\n\\n## Support\\n\\nInsurellm commits to comprehensive Enterprise-level support for GlobalRe Partners:\\n\\n1. **Dedicated Success Team:**\\n   - Executive sponsor (CEO-level) with quarterly strategic reviews\\n   - Dedicated Senior Vice President of Customer Success with bi-weekly engagement\\n   - Technical Account Manager for platform optimization\\n   - Solutions Architect team (2 FTE) for strategic initiatives\\n   - Catastrophe modeling specialist for analytics support\\n   - Quarterly executive business reviews with C-suite participation from both organizations')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee2169",
   "metadata": {},
   "source": [
    "### PART B: Make vectors and store in Chroma\n",
    "\n",
    "In Week 3, you set up a Hugging Face account and got an HF_TOKEN\n",
    "\n",
    "At this point, you might want to add it to your `.env` file and run `load_dotenv(override=True)`\n",
    "\n",
    "(This actually shouldn't be required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9203dc357487381978ce7f3e4f83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shubh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shubh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5b1df4a55545a9806502b05b94394f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067d95c02cfd4ee694f312e0d94c5611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b5ea200d9148e89b07b6346bde4594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebdb31ebf2547febf6f137d2740dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1ce71a5c0f4556bb01155a149734ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef5dc41fb994f888edb55642ad50fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75418811c6a64564a540e7dceb1dcf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5f3dadaac244f99e0269220ae06230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47c92c10564d6882d81c5f0588b2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b305936e9a3a4177a2d6e9f06933657c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 413 documents\n"
     ]
    }
   ],
   "source": [
    "# Pick an embedding model\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252f17e9-3529-4e81-996c-cfa9f08e75a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 413 vectors with 384 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30096a7",
   "metadata": {},
   "source": [
    "### Part C: Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d48dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e8decb0-d9b0-4d51-8402-7a6174d22159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"   # ⭐ Force rendering in browser\n",
    "\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65489941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
